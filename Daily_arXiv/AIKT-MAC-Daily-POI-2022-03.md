# MA C.'s Daily Paper Of Interest - March, 2022

# Index

- [2022-03-31](#2022-03-31)
  - [1. WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models](#2022-03-31-1)
  - [2. Image Retrieval from Contextual Descriptions](#2022-03-31-2)
  - [3. LinkBERT: Pretraining Language Models with Document Links](#2022-03-31-3)
  - [4. Autoregressive Co-Training for Learning Discrete Speech Representations](#2022-03-31-4)
  - [5. Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics](#2022-03-31-5)
  - [6. Visualizing the Relationship Between Encoded Linguistic Information and Task Performance](#2022-03-31-6)
  - [7. Auto-MLM: Improved Contrastive Learning for Self-supervised Multi-lingual Knowledge Retrieval](#2022-03-31-7)
  - [8. Non-autoregressive Translation with Dependency-Aware Decoder](#2022-03-31-8)
  - [9. Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models](#2022-03-31-9)
  - [10. Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding](#2022-03-31-10)
  
- [2022-03-30](#2022-03-30)
  - [1. Word Discovery in Visually Grounded, Self-Supervised Speech Models](#2022-03-30-1)

  - [2. Shifted Chunk Encoder for Transformer Based Streaming End-to-End ASR](#2022-03-30-2)

  - [3. Interactive Audio-text Representation for Automated Audio Captioning with Contrastive Learning](#2022-03-30-3)

  - [4. LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT](#2022-03-30-4)

  - [5. Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation](#2022-03-30-5)

  - [6. Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation](#2022-03-30-6)

  - [7. Integrate Lattice-Free MMI into End-to-End Speech Recognition](#2022-03-30-7)

  - [8. On Decoding Strategies for Neural Text Generators](#2022-03-30-8)

  - [9. Streaming parallel transducer beam search with fast-slow cascaded encoders](#2022-03-30-9)

- [2022-03-29](#2022-03-29)
  - [1. A Roadmap for Big Model](#2022-03-29-1)

  - [2. Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition](#2022-03-29-2)

  - [3. Large-scale Bilingual Language-Image Contrastive Learning](#2022-03-29-3)

  - [4. Specialized Document Embeddings for Aspect-based Similarity of Research Papers](#2022-03-29-4)

  - [5. Data Selection Curriculum for Neural Machine Translation](#2022-03-29-5)

  - [6. Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages](#2022-03-29-6)

  - [7. Multilingual Simultaneous Speech Translation](#2022-03-29-7)

- [2022-03-28](#2022-03-28)
  - [1. Searching for fingerspelled content in American Sign Language](#2022-03-28-1)

  - [2. Impact of Dataset on Acoustic Models for Automatic Speech Recognition](#2022-03-28-2)

  - [3. DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning](#2022-03-28-3)

  - [4. Chain-based Discriminative Autoencoders for Speech Recognition](#2022-03-28-4)

  - [5. Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation](#2022-03-28-5)

  - [6. Automatic Song Translation for Tonal Languages](#2022-03-28-6)

  - [7. Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation](#2022-03-28-7)

  - [8. Modeling Target-Side Morphology in Neural Machine Translation: A Comparison of Strategies](#2022-03-28-8)

- [2022-03-25](#2022-03-25)
  - [1. Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions](#2022-03-25-1)

  - [2. Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors](#2022-03-25-2)

  - [3. Multilingual CheckList: Generation and Evaluation](#2022-03-25-3)

  - [4. Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction](#2022-03-25-4)

  - [5. Multi-armed bandits for online optimization of language model pre-training: the use case of dynamic masking](#2022-03-25-5)

  - [6. Token Dropping for Efficient BERT Pretraining](#2022-03-25-6)
- [2022-03-24](#2022-03-24)
  - [1. Text Transformations in Contrastive Self-Supervised Learning: A Review](#2022-03-24-1)
  - [2. An Empirical Study of Memorization in NLP](#2022-03-24-2)
  - [3. Integrating Vectorized Lexical Constraints for Neural Machine Translation](#2022-03-24-3)
  - [4. A Context-Aware Feature Fusion Framework for Punctuation Restoration](#2022-03-24-4)
- [2022-03-23](#2022-03-23)
  - [1. WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models](#2022-03-23-1)
  - [2. DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization](#2022-03-23-2)
  - [3. Enhancing Speech Recognition Decoding via Layer Aggregation](#2022-03-23-3)
  - [4. Learning Confidence for Transformer-based Neural Machine Translation](#2022-03-23-4)
  - [5. Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data](#2022-03-23-5)
  - [6. Factual Consistency of Multilingual Pretrained Language Models](#2022-03-23-6)
- [2022-03-22](#2022-03-22)
  - [1. Transformer-based HTR for Historical Documents](#2022-03-22-1)
  - [2. Neural Machine Translation with Phrase-Level Universal Visual Representations](#2022-03-22-2)
  - [3. STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation](#2022-03-22-3)
  - [4. Small Batch Sizes Improve Training of Low-Resource Neural MT](#2022-03-22-4)
  - [5. Mitigating Gender Bias in Machine Translation through Adversarial Learning](#2022-03-22-5)
  - [6. Compression of Generative Pre-trained Language Models via Quantization](#2022-03-22-6)
  - [7. Match the Script, Adapt if Multilingual: Analyzing the Effect of Multilingual Pretraining on Cross-lingual Transferability](#2022-03-22-7)
- [2022-03-21](#2022-03-21)
  - [1. A3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing](#2022-03-21-1)
  - [2. Prototypical Verbalizer for Prompt-based Few-shot Tuning](#2022-03-21-2)
  - [3. Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation](#2022-03-21-3)
  - [4. Do Multilingual Language Models Capture Differing Moral Norms?](#2022-03-21-4)
  - [5. Towards Lithuanian grammatical error correction](#2022-03-21-5)
- [2022-03-18](#2022-03-18)
  - [1. DU-VLG: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training](#2022-03-18-1)
  - [2. UNIMO-2: End-to-End Unified Vision-Language Grounded Learning](#2022-03-18-2)
  - [3. Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?](#2022-03-18-3)
  - [4. Triangular Transfer: Freezing the Pivot for Triangular Machine Translation](#2022-03-18-4)
  - [5. Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework](#2022-03-18-5)
  - [6. Gaussian Multi-head Attention for Simultaneous Machine Translation](#2022-03-18-6)
  - [7. Type-Driven Multi-Turn Corrections for Grammatical Error Correction](#2022-03-18-7)
  - [8. Modeling Dual Read/Write Paths for Simultaneous Machine Translation](#2022-03-18-8)
  - [9. On Vision Features in Multimodal Machine Translation](#2022-03-18-9)
  - [10. Universal Conditional Masked Language Pre-training for Neural Machine Translation](#2022-03-18-10)
  - [11. Finding Structural Knowledge in Multimodal-BERT](#2022-03-18-11)
  - [12. Combining Static and Contextualised Multilingual Embeddings](#2022-03-18-12)
- [2022-03-17](#2022-03-17)
  - [1. Hyperdecoders: Instance-specific decoders for multi-task NLP](#2022-03-17-1)
  - [2. Improving Word Translation via Two-Stage Contrastive Learning](#2022-03-17-2)
  - [3. Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation](#2022-03-17-3)
  - [4. Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation](#2022-03-17-4)
  - [5. ConTinTin: Continual Learning from Task Instructions](#2022-03-17-5)
  - [6. Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer](#2022-03-17-6)
  - [7. Geographic Adaptation of Pretrained Language Models](#2022-03-17-7)
  - [8. Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation](#2022-03-17-8)
  - [9. ](#2022-03-17-9)
- [2022-03-16](#2022-03-16)
  - [1. Leveraging Uni-Modal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition](#2022-03-16-1)
  - [2. Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer](#2022-03-16-2)
  - [3. Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation](#2022-03-16-3)
  - [4. Modular and Parameter-Efficient Multimodal Fusion with Prompting](#2022-03-16-4)
  - [5. Does Corpus Quality Really Matter for Low-Resource Languages?](#2022-03-16-5)
- [2022-03-15](#2022-03-15)
  - [1. XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding](#2022-03-15-1)
  - [2. ELLE: Efficient Lifelong Pre-training for Emerging Data](#2022-03-15-2)
  - [3. Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation](#2022-03-15-3)
  - [4. Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models](#2022-03-15-4)
  - [5. PERT: Pre-training BERT with Permuted Language Model](#2022-03-15-5)
  - [6. Modelling word learning and recognition using visually grounded speech](#2022-03-15-6)
  - [7. Interpretability for Language Learners Using Example-Based Grammatical Error Correction](#2022-03-15-7)
  - [8. Interpretable Dysarthric Speaker Adaptation based on Optimal-Transport](#2022-03-15-8)
  - [9. RED-ACE: Robust Error Detection for ASR using Confidence Embeddings](#2022-03-15-9)
- [2022-03-14](#2022-03-14)
  - [1. A new approach to calculating BERTScore for automatic assessment of translation quality](#2022-03-14-1)
- [2022-03-11](#2022-03-11)
  - [1. NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks](#2022-03-11-1)
  - [2. Conditional Prompt Learning for Vision-Language Models](#2022-03-11-2)
  - [3. Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods](#2022-03-11-3)
  - [4. Look Backward and Forward: Self-Knowledge Distillation with Bidirectional Decoder for Neural Machine Translation](#2022-03-11-4)
- [2022-03-10](#2022-03-10)
  - [1. Efficient Sub-structured Knowledge Distillation](#2022-03-10-1)
  - [2. Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning](#2022-03-10-2)
  - [3. Pose Guided Multi-person Image Generation From Text](#2022-03-10-3)
  - [4. Onception: Active Learning with Expert Advice for Real World Machine Translation](#2022-03-10-4)
- [2022-03-09](#2022-03-09)
  - [1. Multi-Modal Mixup for Robust Fine-tuning](#2022-03-09-1)
  - [2. IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](#2022-03-09-2)
  - [3. UniXcoder: Unified Cross-Modal Pre-training for Code Representation](#2022-03-09-3)
  - [4. HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks](#2022-03-09-4)
  - [5. Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation](#2022-03-09-5)
  - [6. Adaptr: Objective-Centric Adaptation Framework for Language Models](#2022-03-09-6)
- [2022-03-08](#2022-03-08)
  - [1. OCR quality affects perceived usefulness of historical newspaper clippings -- a user study](#2022-03-08-1)
  - [2. Focus on the Target's Vocabulary: Masked Label Smoothing for Machine Translation](#2022-03-08-2)
  - [3. Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation](#2022-03-08-3)
  - [4. Recent Advances in Neural Text Generation: A Task-Agnostic Survey](#2022-03-08-4)
  - [5. Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models](#2022-03-08-5)
  - [6. One Model, Multiple Tasks: Pathways for Natural Language Understanding](#2022-03-08-6)
- [2022-03-07](#2022-03-07)
  - [1. Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages](#2022-03-07-1)
  - [2. Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning](#2022-03-07-2)
  - [3. EAG: Extract and Generate Multi-way Aligned Corpus for Complete Multi-lingual Neural Machine Translation](#2022-03-07-3)
  - [4. Comprehension of Subtitles from Re-Translating Simultaneous Speech Translation](#2022-03-07-4)
  - [5. From Simultaneous to Streaming Machine Translation by Leveraging Streaming History](#2022-03-07-5)
- [2022-03-04](#2022-03-04)
  - [1. Recent, rapid advancement in visual question answering architecture](#2022-03-04-1)
  - [2. Vision-Language Intelligence: Tasks, Representation Learning, and Large Models](#2022-03-04-2)
  - [3. UDAAN - Machine Learning based Post-Editing tool for Document Translation](#2022-03-04-3)
- [2022-03-03](#2022-03-03)
  - [1. HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning](#2022-03-03-1)
  - [2. Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation in Few Shots](#2022-03-03-2)
  - [3. HyperPrompt: Prompt-based Task-Conditioning of Transformers](#2022-03-03-3)
  - [4. Do Prompts Solve NLP Tasks Using Natural Language?](#2022-03-03-4)
  - [5. Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models](#2022-03-03-5)
- [2022-03-02](#2022-03-02)
  - [1. Exploring and Adapting Chinese GPT to Pinyin Input Method](#2022-03-02-1)
  - [2. TableFormer: Robust Transformer Modeling for Table-Text Encoding](#2022-03-02-2)
  - [3. DeepNet: Scaling Transformers to 1,000 Layers](#2022-03-02-3)
- [2022-03-01](#2022-03-01)
  - [1. Interactive Machine Learning for Image Captioning](#2022-03-01-1)
  - [2. Multi-Level Contrastive Learning for Cross-Lingual Alignment](#2022-03-01-2)
  - [3. OCR Improves Machine Translation for Low-Resource Languages](#2022-03-01-3)
  - [4. CINO: A Chinese Minority Pre-trained Language Model](#2022-03-01-4)
  - [5. LCP-dropout: Compression-based Multiple Subword Segmentation for Neural Machine Translation](#2022-03-01-5)
  - [6. MSCTD: A Multimodal Sentiment Chat Translation Dataset](#2022-03-01-6)
  - [7. Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation](#2022-03-01-7)
  - [8. LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](#2022-03-01-8)
- [2022-02-28](#2022-02-28)
  - [1. Screening Gender Transfer in Neural Machine Translation](#2022-02-28-1)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MAC-Daily-POI-index.md)



# 2022-03-31

[Return to Index](#Index)



<h2 id="2022-03-31-1">1. WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models
</h2>

Title: [WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models](https://arxiv.org/abs/2203.15863)

Authors: [Heting Gao](https://arxiv.org/search/eess?searchtype=author&query=Gao%2C+H), [Junrui Ni](https://arxiv.org/search/eess?searchtype=author&query=Ni%2C+J), [Kaizhi Qian](https://arxiv.org/search/eess?searchtype=author&query=Qian%2C+K), [Yang Zhang](https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+Y), [Shiyu Chang](https://arxiv.org/search/eess?searchtype=author&query=Chang%2C+S), [Mark Hasegawa-Johnson](https://arxiv.org/search/eess?searchtype=author&query=Hasegawa-Johnson%2C+M)

> Large-scale auto-regressive language models pretrained on massive text have demonstrated their impressive ability to perform new natural language tasks with only a few text examples, without the need for fine-tuning. Recent studies further show that such a few-shot learning ability can be extended to the text-image setting by training an encoder to encode the images into embeddings functioning like the text embeddings of the language model. Interested in exploring the possibility of transferring the few-shot learning ability to the audio-text setting, we propose a novel speech understanding framework, WavPrompt, where we finetune a wav2vec model to generate a sequence of audio embeddings understood by the language model. We show that WavPrompt is a few-shot learner that can perform speech understanding tasks better than a naive text baseline. We conduct detailed ablation studies on different components and hyperparameters to empirically identify the best model configuration. In addition, we conduct a non-speech understanding experiment to show WavPrompt can extract more information than just the transcriptions.

| Comments: | submitted to INTERSPEECH 2022                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2203.15863](https://arxiv.org/abs/2203.15863) [eess.AS]** |
|           | (or **[arXiv:2203.15863v1](https://arxiv.org/abs/2203.15863v1) [eess.AS]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15863Focus to learn more |





<h2 id="2022-03-31-2">2. Image Retrieval from Contextual Descriptions
</h2>

Title: [Image Retrieval from Contextual Descriptions](https://arxiv.org/abs/2203.15867)

Authors: [Benno Krojer](https://arxiv.org/search/cs?searchtype=author&query=Krojer%2C+B), [Vaibhav Adlakha](https://arxiv.org/search/cs?searchtype=author&query=Adlakha%2C+V), [Vibhav Vineet](https://arxiv.org/search/cs?searchtype=author&query=Vineet%2C+V), [Yash Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+Y), [Edoardo Ponti](https://arxiv.org/search/cs?searchtype=author&query=Ponti%2C+E), [Siva Reddy](https://arxiv.org/search/cs?searchtype=author&query=Reddy%2C+S)

> The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance. In order to measure to what extent current vision-and-language models master this ability, we devise a new multimodal challenge, Image Retrieval from Contextual Descriptions (ImageCoDe). In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description. As such, each description contains only the details that help distinguish between images. Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences. Images are sourced from both static pictures and video frames. We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on ImageCoDe. Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans. Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains. Our hope is that ImageCoDE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences.

| Comments: | accepted to ACL 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2203.15867](https://arxiv.org/abs/2203.15867) [cs.CV]** |
|           | (or **[arXiv:2203.15867v1](https://arxiv.org/abs/2203.15867v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15867Focus to learn more |







<h2 id="2022-03-31-3">3. LinkBERT: Pretraining Language Models with Document Links
</h2>

Title: [LinkBERT: Pretraining Language Models with Document Links](https://arxiv.org/abs/2203.15827)

Authors: [Michihiro Yasunaga](https://arxiv.org/search/cs?searchtype=author&query=Yasunaga%2C+M), [Jure Leskovec](https://arxiv.org/search/cs?searchtype=author&query=Leskovec%2C+J), [Percy Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P)

> Language model (LM) pretraining can learn various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data at [this https URL](https://github.com/michiyasunaga/LinkBERT).

| Comments: | Published at ACL 2022. Code, data, and pretrained models are available at [this https URL](https://github.com/michiyasunaga/LinkBERT) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.15827](https://arxiv.org/abs/2203.15827) [cs.CL]** |
|           | (or **[arXiv:2203.15827v1](https://arxiv.org/abs/2203.15827v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15827Focus to learn more |



<h2 id="2022-03-31-4">4. Autoregressive Co-Training for Learning Discrete Speech Representations
</h2>

Title: [Autoregressive Co-Training for Learning Discrete Speech Representations](https://arxiv.org/abs/2203.15840)

Authors: [Sung-Lin Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+S), [Hao Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+H)

> While several self-supervised approaches for learning discrete speech representation have been proposed, it is unclear how these seemingly similar approaches relate to each other. In this paper, we consider a generative model with discrete latent variables that learns a discrete representation for speech. The objective of learning the generative model is formulated as information-theoretic co-training. Besides the wide generality, the objective can be optimized with several approaches, subsuming HuBERT-like training and vector quantization for learning discrete representation. Empirically, we find that the proposed approach learns discrete representation that is highly correlated with phonetic units, more correlated than HuBERT-like training and vector quantization.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.15840](https://arxiv.org/abs/2203.15840) [cs.CL]** |
|           | (or **[arXiv:2203.15840v1](https://arxiv.org/abs/2203.15840v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15840Focus to learn more |





<h2 id="2022-03-31-5">5. Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics
</h2>

Title: [Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics](https://arxiv.org/abs/2203.15858)

Authors: [Jiannan Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+J), [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Yahui Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Defu Lian](https://arxiv.org/search/cs?searchtype=author&query=Lian%2C+D), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> Current practices in metric evaluation focus on one single dataset, e.g., Newstest dataset in each year's WMT Metrics Shared Task. However, in this paper, we qualitatively and quantitatively show that the performances of metrics are sensitive to data. The ranking of metrics varies when the evaluation is conducted on different datasets. Then this paper further investigates two potential hypotheses, i.e., insignificant data points and the deviation of Independent and Identically Distributed (i.i.d) assumption, which may take responsibility for the issue of data variance. In conclusion, our findings suggest that when evaluating automatic translation metrics, researchers should take data variance into account and be cautious to claim the result on a single dataset, because it may leads to inconsistent results with most of other datasets.

| Comments: | Findings of ACL 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.15858](https://arxiv.org/abs/2203.15858) [cs.CL]** |
|           | (or **[arXiv:2203.15858v1](https://arxiv.org/abs/2203.15858v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15858Focus to learn more |





<h2 id="2022-03-31-6">6. Visualizing the Relationship Between Encoded Linguistic Information and Task Performance
</h2>

Title: [Visualizing the Relationship Between Encoded Linguistic Information and Task Performance](https://arxiv.org/abs/2203.15860)

Authors: [Jiannan Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+J), [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Defu Lian](https://arxiv.org/search/cs?searchtype=author&query=Lian%2C+D), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Taro Watanabe](https://arxiv.org/search/cs?searchtype=author&query=Watanabe%2C+T), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L)

> Probing is popular to analyze whether linguistic information can be captured by a well-trained deep neural model, but it is hard to answer how the change of the encoded linguistic information will affect task performance. To this end, we study the dynamic relationship between the encoded linguistic information and task performance from the viewpoint of Pareto Optimality. Its key idea is to obtain a set of models which are Pareto-optimal in terms of both objectives. From this viewpoint, we propose a method to optimize the Pareto-optimal models by formalizing it as a multi-objective optimization problem. We conduct experiments on two popular NLP tasks, i.e., machine translation and language modeling, and investigate the relationship between several kinds of linguistic information and task performances. Experimental results demonstrate that the proposed method is better than a baseline method. Our empirical findings suggest that some syntactic information is helpful for NLP tasks whereas encoding more syntactic information does not necessarily lead to better performance, because the model architecture is also an important factor.

| Comments: | Findings of ACL 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.15860](https://arxiv.org/abs/2203.15860) [cs.CL]** |
|           | (or **[arXiv:2203.15860v1](https://arxiv.org/abs/2203.15860v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15860Focus to learn more |





<h2 id="2022-03-31-7">7. Auto-MLM: Improved Contrastive Learning for Self-supervised Multi-lingual Knowledge Retrieval
</h2>

Title: [Auto-MLM: Improved Contrastive Learning for Self-supervised Multi-lingual Knowledge Retrieval](https://arxiv.org/abs/2203.16187)

Authors: [Wenshen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Mieradilijiang Maimaiti](https://arxiv.org/search/cs?searchtype=author&query=Maimaiti%2C+M), [Yuanhang Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y), [Xin Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+X), [Ji Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J)

> Contrastive learning (CL) has become a ubiquitous approach for several natural language processing (NLP) downstream tasks, especially for question answering (QA). However, the major challenge, how to efficiently train the knowledge retrieval model in an unsupervised manner, is still unresolved. Recently the commonly used methods are composed of CL and masked language model (MLM). Unexpectedly, MLM ignores the sentence-level training, and CL also neglects extraction of the internal info from the query. To optimize the CL hardly obtain internal information from the original query, we introduce a joint training method by combining CL and Auto-MLM for self-supervised multi-lingual knowledge retrieval. First, we acquire the fixed dimensional sentence vector. Then, mask some words among the original sentences with random strategy. Finally, we generate a new token representation for predicting the masked tokens. Experimental results show that our proposed approach consistently outperforms all the previous SOTA methods on both AliExpress & LAZADA service corpus and openly available corpora in 8 languages.

| Comments: | 9 pages, 5 figures, 3 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.16187](https://arxiv.org/abs/2203.16187) [cs.CL]** |
|           | (or **[arXiv:2203.16187v1](https://arxiv.org/abs/2203.16187v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.16187Focus to learn more |





<h2 id="2022-03-31-8">8. Non-autoregressive Translation with Dependency-Aware Decoder
</h2>

Title: [Non-autoregressive Translation with Dependency-Aware Decoder](https://arxiv.org/abs/2203.16266)

Authors: [Jiaao Zhan](https://arxiv.org/search/cs?searchtype=author&query=Zhan%2C+J), [Qian Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Wen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Yu Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+Y), [Yang Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y)

> Non-autoregressive translation (NAT) models suffer from inferior translation quality due to removal of dependency on previous target tokens from inputs to the decoder. In this paper, we propose a novel and general approach to enhance the target dependency within the NAT decoder from two perspectives: decoder input and decoder self-attention. First, we transform the initial decoder input from the source language space to the target language space through a novel attentive transformation process. The transformation reassembles the decoder input based on target token embeddings and conditions the final output on the target-side information. Second, before NAT training, we introduce an effective forward-backward pre-training phase, implemented with different triangle attention masks. This pre-training phase enables the model to gradually learn bidirectional dependencies for the final NAT decoding process. Experimental results demonstrate that the proposed approaches consistently improve highly competitive NAT models on four WMT translation directions by up to 1.88 BLEU score, while overall maintaining inference latency comparable to other fully NAT models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.16266](https://arxiv.org/abs/2203.16266) [cs.CL]** |
|           | (or **[arXiv:2203.16266v1](https://arxiv.org/abs/2203.16266v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.16266Focus to learn more |





<h2 id="2022-03-31-9">9. Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models
</h2>

Title: [Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models](https://arxiv.org/abs/2203.16474)

Authors: [Harshvardhan Srivastava](https://arxiv.org/search/cs?searchtype=author&query=Srivastava%2C+H)

> Eye tracking data during reading is a useful source of information to understand the cognitive processes that take place during language comprehension processes. Different languages account for different brain triggers , however there seems to be some uniform indicators. In this paper, we describe our submission to the CMCL 2022 shared task on predicting human reading patterns for multi-lingual dataset. Our model uses text representations from transformers and some hand engineered features with a regression layer on top to predict statistical measures of mean and standard deviation for 2 main eye-tracking features. We train an end to end model to extract meaningful information from different languages and test our model on two seperate datasets. We compare different transformer models and show ablation studies affecting model performance. Our final submission ranked 4th place for SubTask-1 and 1st place for SubTask-2 for the shared task.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.16474](https://arxiv.org/abs/2203.16474) [cs.CL]** |
|           | (or **[arXiv:2203.16474v1](https://arxiv.org/abs/2203.16474v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.16474Focus to learn more |





<h2 id="2022-03-31-10">10. Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding
</h2>

Title: [Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding](https://arxiv.org/abs/2203.16487)

Authors: [Heming Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+H), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Zhifang Sui](https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Z)

> In this paper, we propose Generalized Aggressive Decoding (GAD) -- a novel approach to accelerating autoregressive translation with no quality loss, through the collaboration of autoregressive and non-autoregressive translation (NAT) of the Transformer. At each decoding iteration, GAD aggressively decodes a number of tokens in parallel as a draft through NAT and then verifies them in the autoregressive manner, where only the tokens that pass the verification are kept as decoded tokens. GAD can achieve the same performance as autoregressive translation but perform much more efficiently because both NAT drafting and autoregressive verification are fast due to parallel computing. We conduct experiments in the WMT14 English-German translation task and confirm that the vanilla GAD yields exactly the same results as greedy decoding with about 3x speedup, and that its variant (GAD++) with an advanced verification strategy not only outperforms the greedy translation and even achieves the comparable translation quality with the beam search result, but also further improves the decoding speed, resulting in an around 5x speedup over autoregressive translation.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.16487](https://arxiv.org/abs/2203.16487) [cs.CL]** |
|           | (or **[arXiv:2203.16487v1](https://arxiv.org/abs/2203.16487v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.16487Focus to learn more |







# 2022-03-30

[Return to Index](#Index)



<h2 id="2022-03-30-1">1. Word Discovery in Visually Grounded, Self-Supervised Speech Models
</h2>

Title: [Word Discovery in Visually Grounded, Self-Supervised Speech Models](https://arxiv.org/abs/2203.15081)

Authors: [Puyuan Peng](https://arxiv.org/search/eess?searchtype=author&query=Peng%2C+P), [David Harwath](https://arxiv.org/search/eess?searchtype=author&query=Harwath%2C+D)

> We present a method for visually-grounded spoken term discovery. After training either a HuBERT or wav2vec2.0 model to associate spoken captions with natural images, we show that powerful word segmentation and clustering capability emerges within the model's self-attention heads. Our experiments reveal that this ability is not present to nearly the same extent in the base HuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a crucial component of the word discovery capability we observe. We also evaluate our method on the Buckeye word segmentation and ZeroSpeech spoken term discovery tasks, where we outperform all currently published methods on several metrics.

| Comments: | submitted to Interspeech 2022                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2203.15081](https://arxiv.org/abs/2203.15081) [eess.AS]** |
|           | (or **[arXiv:2203.15081v1](https://arxiv.org/abs/2203.15081v1) [eess.AS]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15081Focus to learn more |





<h2 id="2022-03-30-2">2. Shifted Chunk Encoder for Transformer Based Streaming End-to-End ASR
</h2>

Title: [Shifted Chunk Encoder for Transformer Based Streaming End-to-End ASR](https://arxiv.org/abs/2203.15206)

Authors: [Fangyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+F), [Bo Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+B)

> Currently, there are mainly three Transformer encoder based streaming End to End (E2E) Automatic Speech Recognition (ASR) approaches, namely time-restricted methods, chunk-wise methods, and memory based methods. However, all of them have some limitations in aspects of global context modeling, linear computational complexity, and model parallelism. In this work, we aim to build a single model to achieve the benefits of all the three aspects for streaming E2E ASR. Particularly, we propose to use a shifted chunk mechanism instead of the conventional chunk mechanism for streaming Transformer and Conformer. This shifted chunk mechanism can significantly enhance modeling power through allowing chunk self-attention to capture global context across local chunks, while keeping linear computational complexity and parallel trainable. We name the Shifted Chunk Transformer and Conformer as SChunk-Transofromer and SChunk-Conformer, respectively. And we verify their performance on the widely used AISHELL-1 benckmark. Experiments show that the SChunk-Transformer and SChunk-Conformer achieve CER 6.43% and 5.77%, respectively. That surpasses the existing chunk-wise and memory based methods by a large margin, and is competitive even compared with the state-of-the-art time-restricted methods which have quadratic computational complexity.

| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.15206](https://arxiv.org/abs/2203.15206) [cs.SD]** |
|           | (or **[arXiv:2203.15206v1](https://arxiv.org/abs/2203.15206v1) [cs.SD]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15206Focus to learn more |





<h2 id="2022-03-30-3">3. Interactive Audio-text Representation for Automated Audio Captioning with Contrastive Learning
</h2>

Title: [Interactive Audio-text Representation for Automated Audio Captioning with Contrastive Learning](https://arxiv.org/abs/2203.15526)

Authors: [Chen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Nana Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+N), [Yuchen Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Heqing Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+H), [Xiaofeng Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+X), [Eng Siong Chng](https://arxiv.org/search/cs?searchtype=author&query=Chng%2C+E+S)

> Automated Audio captioning (AAC) is a cross-modal task that generates natural language to describe the content of input audio. Most prior works usually extract single-modality acoustic features and are therefore sub-optimal for the cross-modal decoding task. In this work, we propose a novel AAC system called CLIP-AAC to learn interactive cross-modality representation with both acoustic and textual information. Specifically, the proposed CLIP-AAC introduces an audio-head and a text-head in the pre-trained encoder to extract audio-text information. Furthermore, we also apply contrastive learning to narrow the domain difference by learning the correspondence between the audio signal and its paired captions. Experimental results show that the proposed CLIP-AAC approach surpasses the best baseline by a significant margin on the Clotho dataset in terms of NLP evaluation metrics. The ablation study indicates that both the pre-trained model and contrastive learning contribute to the performance gain of the AAC model.

| Comments: | Submitted to Interspeech 2022                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.15526](https://arxiv.org/abs/2203.15526) [cs.SD]** |
|           | (or **[arXiv:2203.15526v1](https://arxiv.org/abs/2203.15526v1) [cs.SD]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15526Focus to learn more |





<h2 id="2022-03-30-4">4. LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT
</h2>

Title: [LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT](https://arxiv.org/abs/2203.15610)

Authors: [Rui Wang](https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+R), [Qibing Bai](https://arxiv.org/search/eess?searchtype=author&query=Bai%2C+Q), [Junyi Ao](https://arxiv.org/search/eess?searchtype=author&query=Ao%2C+J), [Long Zhou](https://arxiv.org/search/eess?searchtype=author&query=Zhou%2C+L), [Zhixiang Xiong](https://arxiv.org/search/eess?searchtype=author&query=Xiong%2C+Z), [Zhihua Wei](https://arxiv.org/search/eess?searchtype=author&query=Wei%2C+Z), [Yu Zhang](https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+Y), [Tom Ko](https://arxiv.org/search/eess?searchtype=author&query=Ko%2C+T), [Haizhou Li](https://arxiv.org/search/eess?searchtype=author&query=Li%2C+H)

> Self-supervised speech representation learning has shown promising results in various speech processing tasks. However, the pre-trained models, e.g., HuBERT, are storage-intensive Transformers, limiting their scope of applications under low-resource settings. To this end, we propose LightHuBERT, a once-for-all Transformer compression framework, to find the desired architectures automatically by pruning structured parameters. More precisely, we create a Transformer-based supernet that is nested with thousands of weight-sharing subnets and design a two-stage distillation strategy to leverage the contextualized latent representations from HuBERT. Experiments on automatic speech recognition (ASR) and the SUPERB benchmark show the proposed LightHuBERT enables over 109 architectures concerning the embedding dimension, attention dimension, head number, feed-forward network ratio, and network depth. LightHuBERT outperforms the original HuBERT on ASR and five SUPERB tasks with the HuBERT size, achieves comparable performance to the teacher model in most tasks with a reduction of 29% parameters, and obtains a 3.5× compression ratio in three SUPERB tasks, e.g., automatic speaker verification, keyword spotting, and intent classification, with a slight accuracy loss. The code and pre-trained models are available at [this https URL](https://github.com/mechanicalsea/lighthubert).

| Comments: | 5 pages, 2 figures, submitted to Insterspeech 2022           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD) |
| Cite as:  | **[arXiv:2203.15610](https://arxiv.org/abs/2203.15610) [eess.AS]** |
|           | (or **[arXiv:2203.15610v1](https://arxiv.org/abs/2203.15610v1) [eess.AS]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15610Focus to learn more |





<h2 id="2022-03-30-5">5. Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation
</h2>

Title: [Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation](https://arxiv.org/abs/2203.15319)

Authors: [Pietro Liguori](https://arxiv.org/search/cs?searchtype=author&query=Liguori%2C+P), [Cristina Improta](https://arxiv.org/search/cs?searchtype=author&query=Improta%2C+C), [Simona De Vivo](https://arxiv.org/search/cs?searchtype=author&query=De+Vivo%2C+S), [Roberto Natella](https://arxiv.org/search/cs?searchtype=author&query=Natella%2C+R), [Bojan Cukic](https://arxiv.org/search/cs?searchtype=author&query=Cukic%2C+B), [Domenico Cotroneo](https://arxiv.org/search/cs?searchtype=author&query=Cotroneo%2C+D)

> Neural Machine Translation (NMT) has reached a level of maturity to be recognized as the premier method for the translation between different languages and aroused interest in different research areas, including software engineering. A key step to validate the robustness of the NMT models consists in evaluating the performance of the models on adversarial inputs, i.e., inputs obtained from the original ones by adding small amounts of perturbation. However, when dealing with the specific task of the code generation (i.e., the generation of code starting from a description in natural language), it has not yet been defined an approach to validate the robustness of the NMT models. In this work, we address the problem by identifying a set of perturbations and metrics tailored for the robustness assessment of such models. We present a preliminary experimental evaluation, showing what type of perturbations affect the model the most and deriving useful insights for future directions.

| Comments:    | Paper accepted for publication in the proceedings of The 1st Intl. Workshop on Natural Language-based Software Engineering (NLBSE) to be held with ICSE 2022 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Software Engineering (cs.SE) |
| Cite as:     | **[arXiv:2203.15319](https://arxiv.org/abs/2203.15319) [cs.CL]** |
|              | (or **[arXiv:2203.15319v1](https://arxiv.org/abs/2203.15319v1) [cs.CL]** for this version) |
|              | https://doi.org/10.48550/arXiv.2203.15319Focus to learn more |
| Related DOI: | https://doi.org/10.1145/3528588.3528653Focus to learn more   |





<h2 id="2022-03-30-6">6. Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation
</h2>

Title: [Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation](https://arxiv.org/abs/2203.15479)

Authors: [Ryo Fukuda](https://arxiv.org/search/cs?searchtype=author&query=Fukuda%2C+R), [Katsuhito Sudoh](https://arxiv.org/search/cs?searchtype=author&query=Sudoh%2C+K), [Satoshi Nakamura](https://arxiv.org/search/cs?searchtype=author&query=Nakamura%2C+S)

> Speech segmentation, which splits long speech into short segments, is essential for speech translation (ST). Popular VAD tools like WebRTC VAD have generally relied on pause-based segmentation. Unfortunately, pauses in speech do not necessarily match sentence boundaries, and sentences can be connected by a very short pause that is difficult to detect by VAD. In this study, we propose a speech segmentation method using a binary classification model trained using a segmented bilingual speech corpus. We also propose a hybrid method that combines VAD and the above speech segmentation method. Experimental results revealed that the proposed method is more suitable for cascade and end-to-end ST systems than conventional segmentation methods. The hybrid approach further improved the translation performance.

| Comments: | Submitted to INTERSPEECH 2022                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.15479](https://arxiv.org/abs/2203.15479) [cs.CL]** |
|           | (or **[arXiv:2203.15479v1](https://arxiv.org/abs/2203.15479v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15479Focus to learn more |





<h2 id="2022-03-30-7">7. Integrate Lattice-Free MMI into End-to-End Speech Recognition
</h2>

Title: [Integrate Lattice-Free MMI into End-to-End Speech Recognition](https://arxiv.org/abs/2203.15614)

Authors: [Jinchuan Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+J), [Jianwei Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J), [Chao Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+C), [Yuexian Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+Y), [Dong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D)

> In automatic speech recognition (ASR) research, discriminative criteria have achieved superior performance in DNN-HMM systems. Given this success, the adoption of discriminative criteria is promising to boost the performance of end-to-end (E2E) ASR systems. With this motivation, previous works have introduced the minimum Bayesian risk (MBR, one of the discriminative criteria) into E2E ASR systems. However, the effectiveness and efficiency of the MBR-based methods are compromised: the MBR criterion is only used in system training, which creates a mismatch between training and decoding; the on-the-fly decoding process in MBR-based methods results in the need for pre-trained models and slow training speeds. To this end, novel algorithms are proposed in this work to integrate another widely used discriminative criterion, lattice-free maximum mutual information (LF-MMI), into E2E ASR systems not only in the training stage but also in the decoding process. The proposed LF-MMI training and decoding methods show their effectiveness on two widely used E2E frameworks: Attention-Based Encoder-Decoders (AEDs) and Neural Transducers (NTs). Compared with MBR-based methods, the proposed LF-MMI method: maintains the consistency between training and decoding; eschews the on-the-fly decoding process; trains from randomly initialized models with superior training efficiency. Experiments suggest that the LF-MMI method outperforms its MBR counterparts and consistently leads to statistically significant performance improvements on various frameworks and datasets from 30 hours to 14.3k hours. The proposed method achieves state-of-the-art (SOTA) results on Aishell-1 (CER 4.10%) and Aishell-2 (CER 5.02%) datasets. Code is released.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.15614](https://arxiv.org/abs/2203.15614) [cs.CL]** |
|           | (or **[arXiv:2203.15614v1](https://arxiv.org/abs/2203.15614v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.15614Focus to learn more |





<h2 id="2022-03-30-8">8. On Decoding Strategies for Neural Text Generators
</h2>

Title: [On Decoding Strategies for Neural Text Generators](https://arxiv.org/abs/2203.15721)

Authors: [Gian Wiher](https://arxiv.org/search/cs?searchtype=author&query=Wiher%2C+G), [Clara Meister](https://arxiv.org/search/cs?searchtype=author&query=Meister%2C+C), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R)

> When generating text from probabilistic models, the chosen decoding strategy has a profound effect on the resulting text. Yet the properties elicited by various decoding strategies do not always transfer across natural language generation tasks. For example, while mode-seeking methods like beam search perform remarkably well for machine translation, they have been observed to lead to incoherent and repetitive text in story generation. Despite such observations, the effectiveness of decoding strategies is often assessed with respect to only a single task. This work -- in contrast -- provides a comprehensive analysis of the interaction between language generation tasks and decoding strategies. Specifically, we measure changes in attributes of generated text as a function of both decoding strategy and task using human and automatic evaluation. Our results reveal both previously-observed and surprising findings. For example, the nature of the diversity-quality trade-off in language generation is very task-specific; the length bias often attributed to beam search is not constant across tasks.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.15721](https://arxiv.org/abs/2203.15721) [cs.CL]** |
|           | (or **[arXiv:2203.15721v1](https://arxiv.org/abs/2203.15721v1) [cs.CL]** for this version) |



<h2 id="2022-03-29-9">9. Streaming parallel transducer beam search with fast-slow cascaded encoders
</h2>

Title: [Streaming parallel transducer beam search with fast-slow cascaded encoders](https://arxiv.org/abs/2203.15773)

Authors: [Jay Mahadeokar](https://arxiv.org/search/cs?searchtype=author&query=Mahadeokar%2C+J), [Yangyang Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y), [Ke Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+K), [Duc Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+D), [Jiedan Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Vikas Chandra](https://arxiv.org/search/cs?searchtype=author&query=Chandra%2C+V), [Ozlem Kalinli](https://arxiv.org/search/cs?searchtype=author&query=Kalinli%2C+O), [Michael L Seltzer](https://arxiv.org/search/cs?searchtype=author&query=Seltzer%2C+M+L)

> Streaming ASR with strict latency constraints is required in many speech recognition applications. In order to achieve the required latency, streaming ASR models sacrifice accuracy compared to non-streaming ASR models due to lack of future input context. Previous research has shown that streaming and non-streaming ASR for RNN Transducers can be unified by cascading causal and non-causal encoders. This work improves upon this cascaded encoders framework by leveraging two streaming non-causal encoders with variable input context sizes that can produce outputs at different audio intervals (e.g. fast and slow). We propose a novel parallel time-synchronous beam search algorithm for transducers that decodes from fast-slow encoders, where the slow encoder corrects the mistakes generated from the fast encoder. The proposed algorithm, achieves up to 20% WER reduction with a slight increase in token emission delays on the public Librispeech dataset and in-house datasets. We also explore techniques to reduce the computation by distributing processing between the fast and slow encoders. Lastly, we explore sharing the parameters in the fast encoder to reduce the memory footprint. This enables low latency processing on edge devices with low computation cost and a low memory footprint.

| Comments: | 5 pages, 2 figures, Interspeech 2022 submission              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.15773](https://arxiv.org/abs/2203.15773) [cs.CL]** |
|           | (or **[arXiv:2203.15773v1](https://arxiv.org/abs/2203.15773v1) [cs.CL]** for this version) |







# 2022-03-29

[Return to Index](#Index)



<h2 id="2022-03-29-1">1. A Roadmap for Big Model
</h2>

Title: [A Roadmap for Big Model](https://arxiv.org/abs/2203.14101)

Authors: [Sha Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+S), [Hanyu Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Shuai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+S), [Jiahong Leng](https://arxiv.org/search/cs?searchtype=author&query=Leng%2C+J), [Yangxiao Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Xiaozhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Jifan Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J), [Xin Lv](https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+X), [Zhou Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+Z), [Jiaao He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+J), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Xu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Zhenghao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Ning Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Yongming Rao](https://arxiv.org/search/cs?searchtype=author&query=Rao%2C+Y), [Yizhao Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Liang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Ming Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+M), [Cong Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+C), [Yisen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Mingsheng Long](https://arxiv.org/search/cs?searchtype=author&query=Long%2C+M), [Jing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Yinpeng Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Y), [Tianyu Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+T), [Peng Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+P), [Lingxiao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L), [Zheng Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z), [Huawei Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+H), [Hui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Quanshi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q), [Qingxiu Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Q), [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Shuo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Long Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Haoran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Junwei Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+J), [Yingwei Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+Y), [Weinan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Zhou Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z), [Rui Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+R), [Chence Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+C), [Minghao Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+M), [Zuobai Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Guoqiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G), [Xiang Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+X), [Mengjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M), [Xiaoyu Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+X), [Zijun Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Z), [Fangwei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+F), [Shulin Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+S), [Weicheng Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+W), [Zixuan Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Z), [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Shengding Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S), [Yujia Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y), [Chaojun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C), [Zheni Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+Z), [Ganqu Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+G), [Weize Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Weilin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Yuan Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Wenzhao Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+W), [Wenliang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Ziyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Borui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Nanyi Fei](https://arxiv.org/search/cs?searchtype=author&query=Fei%2C+N), [Anwen Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+A), [Zenan Ling](https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+Z), [Haoyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Boxi Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+B), [Xianpei Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Weidong Zhan](https://arxiv.org/search/cs?searchtype=author&query=Zhan%2C+W), [Baobao Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+B), [Hao Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H), [Jiawen Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+J), [Juanzi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Lei Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+L), [Xigang Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+X), [Jidong Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+J), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Jiwen Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+J), [Zhiwu Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Z), [Qin Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Q), [Ruihua Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+R), [Ji-Rong Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+J), [Zhouchen Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Liwei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Hang Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+H), [Jun Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J), [Zhifang Sui](https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Z), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Xiaodong He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X), [Minlie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M), [Jian Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J), [Jie Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J)

> With the rapid development of deep learning, training Big Models (BMs) for multiple downstream tasks becomes a popular paradigm. Researchers have achieved various outcomes in the construction of BMs and the BM application in many fields. At present, there is a lack of research work that sorts out the overall progress of BMs and guides the follow-up research. In this paper, we cover not only the BM technologies themselves but also the prerequisites for BM training and applications with BMs, dividing the BM review into four parts: Resource, Models, Key Technologies and Application. We introduce 16 specific BM-related topics in those four parts, they are Data, Knowledge, Computing System, Parallel Training System, Language Model, Vision Model, Multi-modal Model, Theory&Interpretability, Commonsense Reasoning, Reliability&Security, Governance, Evaluation, Machine Translation, Text Generation, Dialogue and Protein Research. In each topic, we summarize clearly the current studies and propose some future research directions. At the end of this paper, we conclude the further development of BMs in a more general view.

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.14101](https://arxiv.org/abs/2203.14101) [cs.LG]** |
|           | (or **[arXiv:2203.14101v1](https://arxiv.org/abs/2203.14101v1) [cs.LG]** for this version) |





<h2 id="2022-03-29-2">2. Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition
</h2>

Title: [Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition](https://arxiv.org/abs/2203.14222)

Authors: [Guan-Ting Lin](https://arxiv.org/search/eess?searchtype=author&query=Lin%2C+G), [Shang-Wen Li](https://arxiv.org/search/eess?searchtype=author&query=Li%2C+S), [Hung-yi Lee](https://arxiv.org/search/eess?searchtype=author&query=Lee%2C+H)

> Although deep learning-based end-to-end Automatic Speech Recognition (ASR) has shown remarkable performance in recent years, it suffers severe performance regression on test samples drawn from different data distributions. Test-time Adaptation (TTA), previously explored in the computer vision area, aims to adapt the model trained on source domains to yield better predictions for test samples, often out-of-domain, without accessing the source data. Here, we propose the Single-Utterance Test-time Adaptation (SUTA) framework for ASR, which is the first TTA study in speech area to our best knowledge. The single-utterance TTA is a more realistic setting that does not assume test data are sampled from identical distribution and does not delay on-demand inference due to pre-collection for the batch of adaptation data. SUTA consists of unsupervised objectives with an efficient adaptation strategy. The empirical results demonstrate that SUTA effectively improves the performance of the source ASR model evaluated on multiple out-of-domain target corpora and in-domain test samples.

| Comments: | Submitted to Interspeech 2022                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2203.14222](https://arxiv.org/abs/2203.14222) [eess.AS]** |
|           | (or **[arXiv:2203.14222v1](https://arxiv.org/abs/2203.14222v1) [eess.AS]** for this version) |





<h2 id="2022-03-29-3">3. Large-scale Bilingual Language-Image Contrastive Learning
</h2>

Title: [Large-scale Bilingual Language-Image Contrastive Learning](https://arxiv.org/abs/2203.14463)

Authors: [Byungsoo Ko](https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+B), [Geonmo Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+G)

> This paper is a technical report to share our experience and findings building a Korean and English bilingual multimodal model. While many of the multimodal datasets focus on English and multilingual multimodal research uses machine-translated texts, employing such machine-translated texts is limited to describing unique expressions, cultural information, and proper noun in languages other than English. In this work, we collect 1.1 billion image-text pairs (708 million Korean and 476 million English) and train a bilingual multimodal model named KELIP. We introduce simple yet effective training schemes, including MAE pre-training and multi-crop augmentation. Extensive experiments demonstrate that a model trained with such training schemes shows competitive performance in both languages. Moreover, we discuss multimodal-related research questions: 1) strong augmentation-based methods can distract the model from learning proper multimodal relations; 2) training multimodal model without cross-lingual relation can learn the relation via visual semantics; 3) our bilingual KELIP can capture cultural differences of visual semantics for the same meaning of words; 4) a large-scale multimodal model can be used for multimodal feature analogy. We hope that this work will provide helpful experience and findings for future research. We provide an open-source pre-trained KELIP.

| Comments: | Accepted by ICLRW2022                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2203.14463](https://arxiv.org/abs/2203.14463) [cs.CV]** |
|           | (or **[arXiv:2203.14463v1](https://arxiv.org/abs/2203.14463v1) [cs.CV]** for this version) |





<h2 id="2022-03-29-4">4. Specialized Document Embeddings for Aspect-based Similarity of Research Papers
</h2>

Title: [Specialized Document Embeddings for Aspect-based Similarity of Research Papers](https://arxiv.org/abs/2203.14541)

Authors: [Malte Ostendorff](https://arxiv.org/search/cs?searchtype=author&query=Ostendorff%2C+M), [Till Blume](https://arxiv.org/search/cs?searchtype=author&query=Blume%2C+T), [Terry Ruas](https://arxiv.org/search/cs?searchtype=author&query=Ruas%2C+T), [Bela Gipp](https://arxiv.org/search/cs?searchtype=author&query=Gipp%2C+B), [Georg Rehm](https://arxiv.org/search/cs?searchtype=author&query=Rehm%2C+G)

> Document embeddings and similarity measures underpin content-based recommender systems, whereby a document is commonly represented as a single generic embedding. However, similarity computed on single vector representations provides only one perspective on document similarity that ignores which aspects make two documents alike. To address this limitation, aspect-based similarity measures have been developed using document segmentation or pairwise multi-class document classification. While segmentation harms the document coherence, the pairwise classification approach scales poorly to large scale corpora. In this paper, we treat aspect-based similarity as a classical vector similarity problem in aspect-specific embedding spaces. We represent a document not as a single generic embedding but as multiple specialized embeddings. Our approach avoids document segmentation and scales linearly w.r.t.the corpus size. In an empirical study, we use the Papers with Code corpus containing 157,606 research papers and consider the task, method, and dataset of the respective research papers as their aspects. We compare and analyze three generic document embeddings, six specialized document embeddings and a pairwise classification baseline in the context of research paper recommendations. As generic document embeddings, we consider FastText, SciBERT, and SPECTER. To compute the specialized document embeddings, we compare three alternative methods inspired by retrofitting, fine-tuning, and Siamese networks. In our experiments, Siamese SciBERT achieved the highest scores. Additional analyses indicate an implicit bias of the generic document embeddings towards the dataset aspect and against the method aspect of each research paper. Our approach of aspect-based document embeddings mitigates potential risks arising from implicit biases by making them explicit.

| Comments: | Accepted for publication at JCDL 2022                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2203.14541](https://arxiv.org/abs/2203.14541) [cs.IR]** |
|           | (or **[arXiv:2203.14541v1](https://arxiv.org/abs/2203.14541v1) [cs.IR]** for this version) |





<h2 id="2022-03-29-5">5. Data Selection Curriculum for Neural Machine Translation
</h2>

Title: [Data Selection Curriculum for Neural Machine Translation](https://arxiv.org/abs/2203.13867)

Authors: [Tasnim Mohiuddin](https://arxiv.org/search/cs?searchtype=author&query=Mohiuddin%2C+T), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [James Cross](https://arxiv.org/search/cs?searchtype=author&query=Cross%2C+J), [Shruti Bhosale](https://arxiv.org/search/cs?searchtype=author&query=Bhosale%2C+S), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S)

> Neural Machine Translation (NMT) models are typically trained on heterogeneous data that are concatenated and randomly shuffled. However, not all of the training data are equally useful to the model. Curriculum training aims to present the data to the NMT models in a meaningful order. In this work, we introduce a two-stage curriculum training framework for NMT where we fine-tune a base NMT model on subsets of data, selected by both deterministic scoring using pre-trained methods and online scoring that considers prediction scores of the emerging NMT model. Through comprehensive experiments on six language pairs comprising low- and high-resource languages from WMT'21, we have shown that our curriculum strategies consistently demonstrate better quality (up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer updates).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.13867](https://arxiv.org/abs/2203.13867) [cs.CL]** |
|           | (or **[arXiv:2203.13867v1](https://arxiv.org/abs/2203.13867v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13867Focus to learn more |





<h2 id="2022-03-29-6">6. Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages
</h2>

Title: [Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages](https://arxiv.org/abs/2203.14139)

Authors: [Ehsan Aghazadeh](https://arxiv.org/search/cs?searchtype=author&query=Aghazadeh%2C+E), [Mohsen Fayyaz](https://arxiv.org/search/cs?searchtype=author&query=Fayyaz%2C+M), [Yadollah Yaghoobzadeh](https://arxiv.org/search/cs?searchtype=author&query=Yaghoobzadeh%2C+Y)

> Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.

| Comments: | Accepted to ACL 2022 (main conference)                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.14139](https://arxiv.org/abs/2203.14139) [cs.CL]** |
|           | (or **[arXiv:2203.14139v1](https://arxiv.org/abs/2203.14139v1) [cs.CL]** for this version) |





<h2 id="2022-03-29-7">7. Multilingual Simultaneous Speech Translation
</h2>

Title: [Multilingual Simultaneous Speech Translation](https://arxiv.org/abs/2203.14835)

Authors: [Shashank Subramanya](https://arxiv.org/search/cs?searchtype=author&query=Subramanya%2C+S), [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J)

> Applications designed for simultaneous speech translation during events such as conferences or meetings need to balance quality and lag while displaying translated text to deliver a good user experience. One common approach to building online spoken language translation systems is by leveraging models built for offline speech translation. Based on a technique to adapt end-to-end monolingual models, we investigate multilingual models and different architectures (end-to-end and cascade) on the ability to perform online speech translation. On the multilingual TEDx corpus, we show that the approach generalizes to different architectures. We see similar gains in latency reduction (40% relative) across languages and architectures. However, the end-to-end architecture leads to smaller translation quality losses after adapting to the online model. Furthermore, the approach even scales to zero-shot directions.

| Comments: | Interspeech 2022                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.14835](https://arxiv.org/abs/2203.14835) [cs.CL]** |
|           | (or **[arXiv:2203.14835v1](https://arxiv.org/abs/2203.14835v1) [cs.CL]** for this version) |





# 2022-03-28

[Return to Index](#Index)



<h2 id="2022-03-28-1">1. Searching for fingerspelled content in American Sign Language
</h2>

Title: [Searching for fingerspelled content in American Sign Language](https://arxiv.org/abs/2203.13291)

Authors: [Bowen Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+B), [Diane Brentari](https://arxiv.org/search/cs?searchtype=author&query=Brentari%2C+D), [Greg Shakhnarovich](https://arxiv.org/search/cs?searchtype=author&query=Shakhnarovich%2C+G), [Karen Livescu](https://arxiv.org/search/cs?searchtype=author&query=Livescu%2C+K)

> Natural language processing for sign language video - including tasks like recognition, translation, and search - is crucial for making artificial intelligence technologies accessible to deaf individuals, and is gaining research interest in recent years. In this paper, we address the problem of searching for fingerspelled key-words or key phrases in raw sign language videos. This is an important task since significant content in sign language is often conveyed via fingerspelling, and to our knowledge the task has not been studied before. We propose an end-to-end model for this task, FSS-Net, that jointly detects fingerspelling and matches it to a text sequence. Our experiments, done on a large public dataset of ASL fingerspelling in the wild, show the importance of fingerspelling detection as a component of a search and retrieval model. Our model significantly outperforms baseline methods adapted from prior work on related tasks

| Comments: | ACL 2022                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2203.13291](https://arxiv.org/abs/2203.13291) [cs.CV]** |
|           | (or **[arXiv:2203.13291v1](https://arxiv.org/abs/2203.13291v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13291Focus to learn more |





<h2 id="2022-03-28-2">2. Impact of Dataset on Acoustic Models for Automatic Speech Recognition
</h2>

Title: [Impact of Dataset on Acoustic Models for Automatic Speech Recognition](https://arxiv.org/abs/2203.13590)

Authors: [Siddhesh Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+S)

> In Automatic Speech Recognition, GMM-HMM had been widely used for acoustic modelling. With the current advancement of deep learning, the Gaussian Mixture Model (GMM) from acoustic models has been replaced with Deep Neural Network, namely DNN-HMM Acoustic Models. The GMM models are widely used to create the alignments of the training data for the hybrid deep neural network model, thus making it an important task to create accurate alignments. Many factors such as training dataset size, training data augmentation, model hyperparameters, etc., affect the model learning. Traditionally in machine learning, larger datasets tend to have better performance, while smaller datasets tend to trigger over-fitting. The collection of speech data and their accurate transcriptions is a significant challenge that varies over different languages, and in most cases, it might be limited to big organizations. Moreover, in the case of available large datasets, training a model using such data requires additional time and computing resources, which may not be available. While the data about the accuracy of state-of-the-art ASR models on open-source datasets are published, the study about the impact of the size of a dataset on acoustic models is not readily available. This work aims to investigate the impact of dataset size variations on the performance of various GMM-HMM Acoustic Models and their respective computational costs.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.13590](https://arxiv.org/abs/2203.13590) [cs.LG]** |
|           | (or **[arXiv:2203.13590v1](https://arxiv.org/abs/2203.13590v1) [cs.LG]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13590Focus to learn more |





<h2 id="2022-03-28-3">3. DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning
</h2>

Title: [DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning](https://arxiv.org/abs/2203.13628)

Authors: [Sreyan Ghosh](https://arxiv.org/search/cs?searchtype=author&query=Ghosh%2C+S), [Ashish Seth](https://arxiv.org/search/cs?searchtype=author&query=Seth%2C+A), [S Umesh](https://arxiv.org/search/cs?searchtype=author&query=Umesh%2C+S)

> Inspired by the recent progress in self-supervised learning for computer vision, in this paper, through the DeLoRes learning framework, we introduce two new general-purpose audio representation learning approaches, the DeLoRes-S and DeLoRes-M. Our main objective is to make our network learn representations in a resource-constrained setting (both data and compute), that can generalize well across a diverse set of downstream tasks. Inspired from the Barlow Twins objective function, we propose to learn embeddings that are invariant to distortions of an input audio sample, while making sure that they contain non-redundant information about the sample. To achieve this, we measure the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of an audio segment sampled from an audio file and make it as close to the identity matrix as possible. We call this the DeLoRes learning framework, which we employ in different fashions with the DeLoRes-S and DeLoRes-M. We use a combination of a small subset of the large-scale AudioSet dataset and FSD50K for self-supervised learning and are able to learn with less than half the parameters compared to state-of-the-art algorithms. For evaluation, we transfer these learned representations to 11 downstream classification tasks, including speech, music, and animal sounds, and achieve state-of-the-art results on 7 out of 11 tasks on linear evaluation with DeLoRes-M and show competitive results with DeLoRes-S, even when pre-trained using only a fraction of the total data when compared to prior art. Our transfer learning evaluation setup also shows extremely competitive results for both DeLoRes-S and DeLoRes-M, with DeLoRes-M achieving state-of-the-art in 4 tasks.

| Comments: | Submitted to IEEE JSTSP Special Issue on Self-Supervised Learning for Speech and Audio Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.13628](https://arxiv.org/abs/2203.13628) [cs.SD]** |
|           | (or **[arXiv:2203.13628v1](https://arxiv.org/abs/2203.13628v1) [cs.SD]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13628Focus to learn more |





<h2 id="2022-03-28-4">4. Chain-based Discriminative Autoencoders for Speech Recognition
</h2>

Title: [Chain-based Discriminative Autoencoders for Speech Recognition](https://arxiv.org/abs/2203.13687)

Authors: [Hung-Shin Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H), [Pin-Tuan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+P), [Yao-Fei Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Hsin-Min Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> In our previous work, we proposed a discriminative autoencoder (DcAE) for speech recognition. DcAE combines two training schemes into one. First, since DcAE aims to learn encoder-decoder mappings, the squared error between the reconstructed speech and the input speech is minimized. Second, in the code layer, frame-based phonetic embeddings are obtained by minimizing the categorical cross-entropy between ground truth labels and predicted triphone-state scores. DcAE is developed based on the Kaldi toolkit by treating various TDNN models as encoders. In this paper, we further propose three new versions of DcAE. First, a new objective function that considers both categorical cross-entropy and mutual information between ground truth and predicted triphone-state sequences is used. The resulting DcAE is called a chain-based DcAE (c-DcAE). For application to robust speech recognition, we further extend c-DcAE to hierarchical and parallel structures, resulting in hc-DcAE and pc-DcAE. In these two models, both the error between the reconstructed noisy speech and the input noisy speech and the error between the enhanced speech and the reference clean speech are taken into the objective function. Experimental results on the WSJ and Aurora-4 corpora show that our DcAE models outperform baseline systems.

| Comments: | submitted to Interspeech 2022                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Sound (cs.SD)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.13687](https://arxiv.org/abs/2203.13687) [cs.SD]** |
|           | (or **[arXiv:2203.13687v1](https://arxiv.org/abs/2203.13687v1) [cs.SD]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13687Focus to learn more |





<h2 id="2022-03-28-5">5. Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation
</h2>

Title: [Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation](https://arxiv.org/abs/2203.13339)

Authors: [Ye Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+Y), [Yifan Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Y), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [Yu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A), [Nobuyuki Morioka](https://arxiv.org/search/cs?searchtype=author&query=Morioka%2C+N)

> End-to-end speech-to-speech translation (S2ST) without relying on intermediate text representations is a rapidly emerging frontier of research. Recent works have demonstrated that the performance of such direct S2ST systems is approaching that of conventional cascade S2ST when trained on comparable datasets. However, in practice, the performance of direct S2ST is bounded by the availability of paired S2ST training data. In this work, we explore multiple approaches for leveraging much more widely available unsupervised and weakly-supervised speech and text data to improve the performance of direct S2ST based on Translatotron 2. With our most effective approaches, the average translation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is improved by +13.6 BLEU (or +113% relatively), as compared to the previous state-of-the-art trained without additional data. The improvements on low-resource language are even more significant (+398% relatively on average). Our comparative studies suggest future research directions for S2ST and speech representation learning.

| Comments: | Submitted to Interspeech 2022                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.13339](https://arxiv.org/abs/2203.13339) [cs.CL]** |
|           | (or **[arXiv:2203.13339v1](https://arxiv.org/abs/2203.13339v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13339Focus to learn more |





<h2 id="2022-03-28-6">6. Automatic Song Translation for Tonal Languages
</h2>

Title: [Automatic Song Translation for Tonal Languages](https://arxiv.org/abs/2203.13420)

Authors: [Fenfei Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+F), [Chen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Qixin He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Q), [Kejun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+K), [Jun Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+J), [Jordan Boyd-Graber](https://arxiv.org/search/cs?searchtype=author&query=Boyd-Graber%2C+J)

> This paper develops automatic song translation (AST) for tonal languages and addresses the unique challenge of aligning words' tones with melody of a song in addition to conveying the original meaning. We propose three criteria for effective AST -- preserving meaning, singability and intelligibility -- and design metrics for these criteria. We develop a new benchmark for English--Mandarin song translation and develop an unsupervised AST system, Guided AliGnment for Automatic Song Translation (GagaST), which combines pre-training with three decoding constraints. Both automatic and human evaluations show GagaST successfully balances semantics and singability.

| Comments: | Accepted at Findings of ACL 2022, 15 pages, 4 Tables and 10 Figures |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.13420](https://arxiv.org/abs/2203.13420) [cs.CL]** |
|           | (or **[arXiv:2203.13420v1](https://arxiv.org/abs/2203.13420v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13420Focus to learn more |





<h2 id="2022-03-28-7">7. Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation
</h2>

Title: [Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation](https://arxiv.org/abs/2203.13528)

Authors: [Sho Takase](https://arxiv.org/search/cs?searchtype=author&query=Takase%2C+S), [Tatsuya Hiraoka](https://arxiv.org/search/cs?searchtype=author&query=Hiraoka%2C+T), [Naoaki Okazaki](https://arxiv.org/search/cs?searchtype=author&query=Okazaki%2C+N)

> Subword regularizations use multiple subword segmentations during training to improve the robustness of neural machine translation models. In previous subword regularizations, we use multiple segmentations in the training process but use only one segmentation in the inference. In this study, we propose an inference strategy to address this discrepancy. The proposed strategy approximates the marginalized likelihood by using multiple segmentations including the most plausible segmentation and several sampled segmentations. Because the proposed strategy aggregates predictions from several segmentations, we can regard it as a single model ensemble that does not require any additional cost for training. Experimental results show that the proposed strategy improves the performance of models trained with subword regularization in low-resource machine translation tasks.

| Comments: | Findings of ACL 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.13528](https://arxiv.org/abs/2203.13528) [cs.CL]** |
|           | (or **[arXiv:2203.13528v1](https://arxiv.org/abs/2203.13528v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13528Focus to learn more |





<h2 id="2022-03-28-8">8. Modeling Target-Side Morphology in Neural Machine Translation: A Comparison of Strategies
</h2>

Title: [Modeling Target-Side Morphology in Neural Machine Translation: A Comparison of Strategies](https://arxiv.org/abs/2203.13550)

Authors: [Marion Weller-Di Marco](https://arxiv.org/search/cs?searchtype=author&query=Marco%2C+M+W), [Matthias Huck](https://arxiv.org/search/cs?searchtype=author&query=Huck%2C+M), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> Morphologically rich languages pose difficulties to machine translation. Machine translation engines that rely on statistical learning from parallel training data, such as state-of-the-art neural systems, face challenges especially with rich morphology on the output language side. Key challenges of rich target-side morphology in data-driven machine translation include: (1) A large amount of differently inflected word surface forms entails a larger vocabulary and thus data sparsity. (2) Some inflected forms of infrequent terms typically do not appear in the training corpus, which makes closed-vocabulary systems unable to generate these unobserved variants. (3) Linguistic agreement requires the system to correctly match the grammatical categories between inflected word forms in the output sentence, both in terms of target-side morpho-syntactic wellformedness and semantic adequacy with respect to the input. 
> In this paper, we re-investigate two target-side linguistic processing techniques: a lemma-tag strategy and a linguistically informed word segmentation strategy. Our experiments are conducted on a English-German translation task under three training corpus conditions of different magnitudes. We find that a stronger Transformer baseline leaves less room for improvement than a shallow-RNN encoder-decoder model when translating in-domain. However, we find that linguistic modeling of target-side morphology does benefit the Transformer model when the same system is applied to out-of-domain input text. We also successfully apply our approach to English to Czech translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.13550](https://arxiv.org/abs/2203.13550) [cs.CL]** |
|           | (or **[arXiv:2203.13550v1](https://arxiv.org/abs/2203.13550v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13550Focus to learn more |






# 2022-03-25

[Return to Index](#Index)



<h2 id="2022-03-25-1">1. Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions
</h2>

Title: [Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions](https://arxiv.org/abs/2203.12667)

Authors: [Jing Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Eliana Stefani](https://arxiv.org/search/cs?searchtype=author&query=Stefani%2C+E), [Qi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q), [Jesse Thomason](https://arxiv.org/search/cs?searchtype=author&query=Thomason%2C+J), [Xin Eric Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X+E)

> A long-term goal of AI research is to build intelligent agents that can communicate with humans in natural language, perceive the environment, and perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental and interdisciplinary research topic towards this goal, and receives increasing attention from natural language processing, computer vision, robotics, and machine learning communities. In this paper, we review contemporary studies in the emerging field of VLN, covering tasks, evaluation metrics, methods, etc. Through structured analysis of current progress and challenges, we highlight the limitations of current VLN and opportunities for future work. This paper serves as a thorough reference for the VLN research community.

| Comments: | 18 pages. Accepted to ACL 2022                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.12667](https://arxiv.org/abs/2203.12667) [cs.CV]** |
|           | (or **[arXiv:2203.12667v1](https://arxiv.org/abs/2203.12667v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.12667Focus to learn more |





<h2 id="2022-03-25-2">2. Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors
</h2>

Title: [Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors](https://arxiv.org/abs/2203.13131)

Authors: [Oran Gafni](https://arxiv.org/search/cs?searchtype=author&query=Gafni%2C+O), [Adam Polyak](https://arxiv.org/search/cs?searchtype=author&query=Polyak%2C+A), [Oron Ashual](https://arxiv.org/search/cs?searchtype=author&query=Ashual%2C+O), [Shelly Sheynin](https://arxiv.org/search/cs?searchtype=author&query=Sheynin%2C+S), [Devi Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+D), [Yaniv Taigman](https://arxiv.org/search/cs?searchtype=author&query=Taigman%2C+Y)

> Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Graphics (cs.GR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.13131](https://arxiv.org/abs/2203.13131) [cs.CV]** |
|           | (or **[arXiv:2203.13131v1](https://arxiv.org/abs/2203.13131v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13131Focus to learn more |





<h2 id="2022-03-25-3">3. Multilingual CheckList: Generation and Evaluation
</h2>

Title: [Multilingual CheckList: Generation and Evaluation](https://arxiv.org/abs/2203.12865)

Authors: [Karthikeyan K](https://arxiv.org/search/cs?searchtype=author&query=K%2C+K), [Shaily Bhatt](https://arxiv.org/search/cs?searchtype=author&query=Bhatt%2C+S), [Pankaj Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+P), [Somak Aditya](https://arxiv.org/search/cs?searchtype=author&query=Aditya%2C+S), [Sandipan Dandapat](https://arxiv.org/search/cs?searchtype=author&query=Dandapat%2C+S), [Sunayana Sitaram](https://arxiv.org/search/cs?searchtype=author&query=Sitaram%2C+S), [Monojit Choudhary](https://arxiv.org/search/cs?searchtype=author&query=Choudhary%2C+M)

> The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation of NLP systems has revealed high failure rates for basic capabilities for multiple state-of-the-art and commercial models. However, the CheckList creation process is manual which creates a bottleneck towards creation of multilingual CheckLists catering 100s of languages. In this work, we explore multiple approaches to generate and evaluate the quality of Multilingual CheckList. We device an algorithm -- Automated Multilingual Checklist Generation (AMCG) for automatically transferring a CheckList from a source to a target language that relies on a reasonable machine translation system. We then compare the CheckList generated by AMCG with CheckLists generated with different levels of human intervention. Through in-depth crosslingual experiments between English and Hindi, and broad multilingual experiments spanning 11 languages, we show that the automatic approach can provide accurate estimates of failure rates of a model across capabilities, as would a human-verified CheckList, and better than CheckLists generated by humans from scratch.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.12865](https://arxiv.org/abs/2203.12865) [cs.CL]** |
|           | (or **[arXiv:2203.12865v1](https://arxiv.org/abs/2203.12865v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.12865Focus to learn more |





<h2 id="2022-03-25-4">4. Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction
</h2>

Title: [Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction](https://arxiv.org/abs/2203.13064)

Authors: [Maksym Tarnavskyi](https://arxiv.org/search/cs?searchtype=author&query=Tarnavskyi%2C+M), [Artem Chernodub](https://arxiv.org/search/cs?searchtype=author&query=Chernodub%2C+A), [Kostiantyn Omelianchuk](https://arxiv.org/search/cs?searchtype=author&query=Omelianchuk%2C+K)

> In this paper, we investigate improvements to the GEC sequence tagging architecture with a focus on ensembling of recent cutting-edge Transformer-based encoders in Large configurations. We encourage ensembling models by majority votes on span-level edits because this approach is tolerant to the model architecture and vocabulary size. Our best ensemble achieves a new SOTA result with an F0.5 score of 76.05 on BEA-2019 (test), even without pre-training on synthetic datasets. In addition, we perform knowledge distillation with a trained ensemble to generate new synthetic training datasets, "Troy-Blogs" and "Troy-1BW". Our best single sequence tagging model that is pretrained on the generated Troy-datasets in combination with the publicly available synthetic PIE dataset achieves a near-SOTA (To the best of our knowledge, our best single model gives way only to much heavier T5 model result with an F0.5 score of 73.21 on BEA-2019 (test). The code, datasets, and trained models are publicly available).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.13064](https://arxiv.org/abs/2203.13064) [cs.CL]** |
|           | (or **[arXiv:2203.13064v1](https://arxiv.org/abs/2203.13064v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13064Focus to learn more |





<h2 id="2022-03-25-5">5. Multi-armed bandits for online optimization of language model pre-training: the use case of dynamic masking
</h2>

Title: [Multi-armed bandits for online optimization of language model pre-training: the use case of dynamic masking](https://arxiv.org/abs/2203.13151)

Authors: [Iñigo Urteaga](https://arxiv.org/search/cs?searchtype=author&query=Urteaga%2C+I), [Moulay-Zaïdane Draïdia](https://arxiv.org/search/cs?searchtype=author&query=Draïdia%2C+M), [Tomer Lancewicki](https://arxiv.org/search/cs?searchtype=author&query=Lancewicki%2C+T), [Shahram Khadivi](https://arxiv.org/search/cs?searchtype=author&query=Khadivi%2C+S)

> Transformer-based language models (TLMs) provide state-of-the-art performance in many modern natural language processing applications. TLM training is conducted in two phases. First, the model is pre-trained over large volumes of text to minimize a generic objective function, such as the Masked Language Model (MLM). Second, the model is fine-tuned in specific downstream tasks. Pre-training requires large volumes of data and high computational resources, while introducing many still unresolved design choices. For instance, selecting hyperparameters for language model pre-training is often carried out based on heuristics or grid-based searches. In this work, we propose a multi-armed bandit-based online optimization framework for the sequential selection of pre-training hyperparameters to optimize language model performance. We pose the pre-training procedure as a sequential decision-making task, where at each pre-training step, an agent must determine what hyperparameters to use towards optimizing the pre-training objective. We propose a Thompson sampling bandit algorithm, based on a surrogate Gaussian process reward model of the MLM pre-training objective, for its sequential minimization. We empirically show how the proposed Gaussian process based Thompson sampling pre-trains robust and well-performing language models. Namely, by sequentially selecting masking hyperparameters of the TLM, we achieve satisfactory performance in less epochs, not only in terms of the pre-training MLM objective, but in diverse downstream fine-tuning tasks. The proposed bandit-based technique provides an automated hyperparameter selection method for pre-training TLMs of interest to practitioners. In addition, our results indicate that, instead of MLM pre-training with fixed masking probabilities, sequentially adapting the masking hyperparameters improves both pre-training loss and downstream task metrics.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.13151](https://arxiv.org/abs/2203.13151) [cs.CL]** |
|           | (or **[arXiv:2203.13151v1](https://arxiv.org/abs/2203.13151v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13151Focus to learn more |





<h2 id="2022-03-25-6">6. Token Dropping for Efficient BERT Pretraining
</h2>

Title: [Token Dropping for Efficient BERT Pretraining](https://arxiv.org/abs/2203.13240)

Authors: [Le Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+L), [Richard Yuanzhe Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+R+Y), [Tianyi Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+T), [Yuexin Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Xinying Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Xiaodan Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Denny Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+D)

> Transformer-based models generally allocate the same amount of computation for each token in a given sequence. We develop a simple but effective "token dropping" method to accelerate the pretraining of transformer models, such as BERT, without degrading its performance on downstream tasks. In short, we drop unimportant tokens starting from an intermediate layer in the model to make the model focus on important tokens; the dropped tokens are later picked up by the last layer of the model so that the model still produces full-length sequences. We leverage the already built-in masked language modeling (MLM) loss to identify unimportant tokens with practically no computational overhead. In our experiments, this simple approach reduces the pretraining cost of BERT by 25% while achieving similar overall fine-tuning performance on standard downstream tasks.

| Comments: | ACL 2022                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.13240](https://arxiv.org/abs/2203.13240) [cs.CL]** |
|           | (or **[arXiv:2203.13240v1](https://arxiv.org/abs/2203.13240v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.13240Focus to learn more |






# 2022-03-24

[Return to Index](#Index)



<h2 id="2022-03-24-1">1. Text Transformations in Contrastive Self-Supervised Learning: A Review
</h2>

Title: [Text Transformations in Contrastive Self-Supervised Learning: A Review](https://arxiv.org/abs/2203.12000)

Authors: [Amrita Bhattacharjee](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharjee%2C+A), [Mansooreh Karami](https://arxiv.org/search/cs?searchtype=author&query=Karami%2C+M), [Huan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H)

> Contrastive self-supervised learning has become a prominent technique in representation learning. The main step in these methods is to contrast semantically similar and dissimilar pairs of samples. However, in the domain of Natural Language, the augmentation methods used in creating similar pairs with regard to contrastive learning assumptions are challenging. This is because, even simply modifying a word in the input might change the semantic meaning of the sentence, and hence, would violate the distributional hypothesis. In this review paper, we formalize the contrastive learning framework in the domain of natural language processing. We emphasize the considerations that need to be addressed in the data transformation step and review the state-of-the-art methods and evaluations for contrastive representation learning in NLP. Finally, we describe some challenges and potential directions for learning better text representations using contrastive methods.

| Comments: | under review at IJCAI'22 Survey Track                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.12000](https://arxiv.org/abs/2203.12000) [cs.CL]** |
|           | (or **[arXiv:2203.12000v1](https://arxiv.org/abs/2203.12000v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.12000Focus to learn more |





<h2 id="2022-03-24-2">2. An Empirical Study of Memorization in NLP
</h2>

Title: [An Empirical Study of Memorization in NLP](https://arxiv.org/abs/2203.12171)

Authors: [Xiaosen Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X), [Jing Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+J)

> A recent study by Feldman (2020) proposed a long-tail theory to explain the memorization behavior of deep learning models. However, memorization has not been empirically verified in the context of NLP, a gap addressed by this work. In this paper, we use three different NLP tasks to check if the long-tail theory holds. Our experiments demonstrate that top-ranked memorized training instances are likely atypical, and removing the top-memorized training instances leads to a more serious drop in test accuracy compared with removing training instances randomly. Furthermore, we develop an attribution method to better understand why a training instance is memorized. We empirically show that our memorization attribution method is faithful, and share our interesting finding that the top-memorized parts of a training instance tend to be features negatively correlated with the class label.

| Comments: | ACL 2022. Code & data available at [this https URL](https://github.com/xszheng2020/memorization) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.12171](https://arxiv.org/abs/2203.12171) [cs.CL]** |
|           | (or **[arXiv:2203.12171v1](https://arxiv.org/abs/2203.12171v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.12171Focus to learn more |





<h2 id="2022-03-24-3">3. Integrating Vectorized Lexical Constraints for Neural Machine Translation
</h2>

Title: [Integrating Vectorized Lexical Constraints for Neural Machine Translation](https://arxiv.org/abs/2203.12210)

Authors: [Shuo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Lexically constrained neural machine translation (NMT), which controls the generation of NMT models with pre-specified constraints, is important in many practical scenarios. Due to the representation gap between discrete constraints and continuous vectors in NMT models, most existing works choose to construct synthetic data or modify the decoding algorithm to impose lexical constraints, treating the NMT model as a black box. In this work, we propose to open this black box by directly integrating the constraints into NMT models. Specifically, we vectorize source and target constraints into continuous keys and values, which can be utilized by the attention modules of NMT models. The proposed integration method is based on the assumption that the correspondence between keys and values in attention modules is naturally suitable for modeling constraint pairs. Experimental results show that our method consistently outperforms several representative baselines on four language pairs, demonstrating the superiority of integrating vectorized lexical constraints.

| Comments: | Accepted by ACL 2022 (main conference)                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.12210](https://arxiv.org/abs/2203.12210) [cs.CL]** |
|           | (or **[arXiv:2203.12210v1](https://arxiv.org/abs/2203.12210v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.12210Focus to learn more |





<h2 id="2022-03-24-4">4. A Context-Aware Feature Fusion Framework for Punctuation Restoration
</h2>

Title: [A Context-Aware Feature Fusion Framework for Punctuation Restoration](https://arxiv.org/abs/2203.12487)

Authors: [Yangjun Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Kebin Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+K), [Yao Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y)

> To accomplish the punctuation restoration task, most existing approaches focused on leveraging extra information (e.g., part-of-speech tags) or addressing the class imbalance problem. Recent works have widely applied the transformer-based language models and significantly improved their effectiveness. To the best of our knowledge, an inherent issue has remained neglected: the attention of individual heads in the transformer will be diluted or powerless while feeding the long non-punctuation utterances. Since those previous contexts, not the followings, are comparatively more valuable to the current position, it's hard to achieve a good balance by independent attention. In this paper, we propose a novel Feature Fusion framework based on two-type Attentions (FFA) to alleviate the shortage. It introduces a two-stream architecture. One module involves interaction between attention heads to encourage the communication, and another masked attention module captures the dependent feature representation. Then, it aggregates two feature embeddings to fuse information and enhances context-awareness. The experiments on the popular benchmark dataset IWSLT demonstrate that our approach is effective. Without additional data, it obtains comparable performance to the current state-of-the-art models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.12487](https://arxiv.org/abs/2203.12487) [cs.CL]** |
|           | (or **[arXiv:2203.12487v1](https://arxiv.org/abs/2203.12487v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.12487Focus to learn more |





# 2022-03-23

[Return to Index](#Index)



<h2 id="2022-03-23-1">1. WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models
</h2>

Title: [WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models](https://arxiv.org/abs/2203.11480)

Authors: [Sha Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+S), [Zhao Shuai](https://arxiv.org/search/cs?searchtype=author&query=Shuai%2C+Z), [Leng Jiahong](https://arxiv.org/search/cs?searchtype=author&query=Jiahong%2C+L), [Xue Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+X), [Zhao Hanyu](https://arxiv.org/search/cs?searchtype=author&query=Hanyu%2C+Z), [Tang Jie](https://arxiv.org/search/cs?searchtype=author&query=Jie%2C+T)

> Compared with the domain-specific model, the vision-language pre-training models (VLPMs) have shown superior performance on downstream tasks with fast fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with a uniform transformers stack architecture and large amounts of image-text paired data, achieving remarkable results on downstream tasks such as image-text reference(IR and TR), vision question answering (VQA) and image captioning (IC) etc. During the training phase, VLPMs are always fed with a combination of multiple public datasets to meet the demand of large-scare training data. However, due to the unevenness of data distribution including size, task type and quality, using the mixture of multiple datasets for model training can be problematic. In this work, we introduce a large-scale multi-modal corpora named WuDaoMM, totally containing more than 650M image-text pairs. Specifically, about 600 million pairs of data are collected from multiple webpages in which image and caption present weak correlation, and the other 50 million strong-related image-text pairs are collected from some high-quality graphic websites. We also release a base version of WuDaoMM with 5 million strong-correlated image-text pairs, which is sufficient to support the common cross-modal model pre-training. Besides, we trained both an understanding and a generation vision-language (VL) model to test the dataset effectiveness. The results show that WuDaoMM can be applied as an efficient dataset for VLPMs, especially for the model in text-to-image generation task. The data is released at [this https URL](https://data.wudaoai.cn/)

| Comments: | 7 pages, 2 tables, 4 figures                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2203.11480](https://arxiv.org/abs/2203.11480) [cs.CV]** |
|           | (or **[arXiv:2203.11480v1](https://arxiv.org/abs/2203.11480v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.11480Focus to learn more |





<h2 id="2022-03-23-2">2. DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization
</h2>

Title: [DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization](https://arxiv.org/abs/2203.11239)

Authors: [Zheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Zijian Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Ming Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M), [Ramesh Nallapati](https://arxiv.org/search/cs?searchtype=author&query=Nallapati%2C+R), [Parminder Bhatia](https://arxiv.org/search/cs?searchtype=author&query=Bhatia%2C+P), [Andrew Arnold](https://arxiv.org/search/cs?searchtype=author&query=Arnold%2C+A), [Bing Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+B), [Dan Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+D)

> Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks.

| Comments: | ACL 2022                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.11239](https://arxiv.org/abs/2203.11239) [cs.CL]** |
|           | (or **[arXiv:2203.11239v1](https://arxiv.org/abs/2203.11239v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.11239Focus to learn more |





<h2 id="2022-03-23-3">3. Enhancing Speech Recognition Decoding via Layer Aggregation
</h2>

Title: [Enhancing Speech Recognition Decoding via Layer Aggregation](https://arxiv.org/abs/2203.11325)

Authors: [Tomer Wullach](https://arxiv.org/search/cs?searchtype=author&query=Wullach%2C+T), [Shlomo E. Chazan](https://arxiv.org/search/cs?searchtype=author&query=Chazan%2C+S+E)

> Recently proposed speech recognition systems are designed to predict using representations generated by their top layers, employing greedy decoding which isolates each timestep from the rest of the sequence. Aiming for improved performance, a beam search algorithm is frequently utilized and a language model is incorporated to assist with ranking the top candidates. In this work, we experiment with several speech recognition models and find that logits predicted using the top layers may hamper beam search from achieving optimal results. Specifically, we show that fined-tuned Wav2Vec 2.0 and HuBERT yield highly confident predictions, and hypothesize that the predictions are based on local information and may not take full advantage of the information encoded in intermediate layers. To this end, we perform a layer analysis to reveal and visualize how predictions evolve throughout the inference flow. We then propose a prediction method that aggregates the top M layers, potentially leveraging useful information encoded in intermediate layers and relaxing model confidence. We showcase the effectiveness of our approach via beam search decoding, conducting our experiments on Librispeech test and dev sets and achieving WER, and CER reduction of up to 10% and 22%, respectively.

| Comments: | Submitted to Interspeech 2022                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.11325](https://arxiv.org/abs/2203.11325) [cs.CL]** |
|           | (or **[arXiv:2203.11325v1](https://arxiv.org/abs/2203.11325v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.11325Focus to learn more |





<h2 id="2022-03-23-4">4. Learning Confidence for Transformer-based Neural Machine Translation
</h2>

Title: [Learning Confidence for Transformer-based Neural Machine Translation](https://arxiv.org/abs/2203.11413)

Authors: [Yu Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Jiali Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+J), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Shuangzhi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Mu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M)

> Confidence estimation aims to quantify the confidence of the model prediction, providing an expectation of success. A well-calibrated confidence estimate enables accurate failure prediction and proper risk measurement when given noisy samples and out-of-distribution data in real-world settings. However, this task remains a severe challenge for neural machine translation (NMT), where probabilities from softmax distribution fail to describe when the model is probably mistaken. To address this problem, we propose an unsupervised confidence estimate learning jointly with the training of the NMT model. We explain confidence as how many hints the NMT model needs to make a correct prediction, and more hints indicate low confidence. Specifically, the NMT model is given the option to ask for hints to improve translation accuracy at the cost of some slight penalty. Then, we approximate their level of confidence by counting the number of hints the model uses. We demonstrate that our learned confidence estimate achieves high accuracy on extensive sentence/word-level quality estimation tasks. Analytical results verify that our confidence estimate can correctly assess underlying risk in two real-world scenarios: (1) discovering noisy samples and (2) detecting out-of-domain data. We further propose a novel confidence-based instance-specific label smoothing approach based on our learned confidence estimate, which outperforms standard label smoothing.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.11413](https://arxiv.org/abs/2203.11413) [cs.CL]** |
|           | (or **[arXiv:2203.11413v1](https://arxiv.org/abs/2203.11413v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.11413Focus to learn more |





<h2 id="2022-03-23-5">5. Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data
</h2>

Title: [Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data](https://arxiv.org/abs/2203.11476)

Authors: [Gašper Beguš](https://arxiv.org/search/cs?searchtype=author&query=Beguš%2C+G), [Alan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+A)

> Human speakers encode information into raw speech which is then decoded by the listeners. This complex relationship between encoding (production) and decoding (perception) is often modeled separately. Here, we test how decoding of lexical and sublexical semantic information can emerge automatically from raw speech in unsupervised generative deep convolutional networks that combine both the production and perception principle. We introduce, to our knowledge, the most challenging objective in unsupervised lexical learning: an unsupervised network that must learn to assign unique representations for lexical items with no direct access to training data. We train several models (ciwGAN and fiwGAN by [1]) and test how the networks classify raw acoustic lexical items in the unobserved test data. Strong evidence in favor of lexical learning emerges. The architecture that combines the production and perception principles is thus able to learn to decode unique information from raw acoustic data in an unsupervised manner without ever accessing real training data. We propose a technique to explore lexical and sublexical learned representations in the classifier network. The results bear implications for both unsupervised speech synthesis and recognition as well as for unsupervised semantic modeling as language models increasingly bypass text and operate from raw acoustics.

| Comments: | Submitted to Interspeech 2022                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.11476](https://arxiv.org/abs/2203.11476) [cs.CL]** |
|           | (or **[arXiv:2203.11476v1](https://arxiv.org/abs/2203.11476v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.11476Focus to learn more |





<h2 id="2022-03-23-6">6. Factual Consistency of Multilingual Pretrained Language Models
</h2>

Title: [Factual Consistency of Multilingual Pretrained Language Models](https://arxiv.org/abs/2203.11552)

Authors: [Constanza Fierro](https://arxiv.org/search/cs?searchtype=author&query=Fierro%2C+C), [Anders Søgaard](https://arxiv.org/search/cs?searchtype=author&query=Søgaard%2C+A)

> Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts; and (ii) if such models are equally consistent across languages. We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.11552](https://arxiv.org/abs/2203.11552) [cs.CL]** |
|           | (or **[arXiv:2203.11552v1](https://arxiv.org/abs/2203.11552v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.11552Focus to learn more |



# 2022-03-22

[Return to Index](#Index)



<h2 id="2022-03-22-1">1. Transformer-based HTR for Historical Documents
</h2>

Title: [Transformer-based HTR for Historical Documents](https://arxiv.org/abs/2203.11008)

Authors: [Phillip Benjamin Ströbel](https://arxiv.org/search/cs?searchtype=author&query=Ströbel%2C+P+B), [Simon Clematide](https://arxiv.org/search/cs?searchtype=author&query=Clematide%2C+S), [Martin Volk](https://arxiv.org/search/cs?searchtype=author&query=Volk%2C+M), [Tobias Hodel](https://arxiv.org/search/cs?searchtype=author&query=Hodel%2C+T)

> We apply the TrOCR framework to real-world, historical manuscripts and show that TrOCR per se is a strong model, ideal for transfer learning. TrOCR has been trained on English only, but it can adapt to other languages that use the Latin alphabet fairly easily and with little training material. We compare TrOCR against a SOTA HTR framework (Transkribus) and show that it can beat such systems. This finding is essential since Transkribus performs best when it has access to baseline information, which is not needed at all to fine-tune TrOCR.

| Comments: | This is an abstract submitted and accepted at ComHum 2022 in Lausanne. We will be elaborating on these initial findings in the paper that we will submit after the conference |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2203.11008](https://arxiv.org/abs/2203.11008) [cs.CV]** |
|           | (or **[arXiv:2203.11008v1](https://arxiv.org/abs/2203.11008v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.11008Focus to learn more |





<h2 id="2022-03-22-2">2. Neural Machine Translation with Phrase-Level Universal Visual Representations
</h2>

Title: [Neural Machine Translation with Phrase-Level Universal Visual Representations](https://arxiv.org/abs/2203.10299)

Authors: [Qingkai Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Q), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y)

> Multimodal machine translation (MMT) aims to improve neural machine translation (NMT) with additional visual information, but most existing MMT methods require paired input of source sentence and image, which makes them suffer from shortage of sentence-image pairs. In this paper, we propose a phrase-level retrieval-based method for MMT to get visual information for the source input from existing sentence-image data sets so that MMT can break the limitation of paired sentence-image input. Our method performs retrieval at the phrase level and hence learns visual information from pairs of source phrase and grounded region, which can mitigate data sparsity. Furthermore, our method employs the conditional variational auto-encoder to learn visual representations which can filter redundant visual information and only retain visual information related to the phrase. Experiments show that the proposed method significantly outperforms strong baselines on multiple MMT datasets, especially when the textual context is limited.

| Comments:    | ACL 2022 main conference                                     |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2203.10299](https://arxiv.org/abs/2203.10299) [cs.CL]** |
|              | (or **[arXiv:2203.10299v1](https://arxiv.org/abs/2203.10299v1) [cs.CL]** for this version) |
|              | https://doi.org/10.48550/arXiv.2203.10299Focus to learn more |





<h2 id="2022-03-22-3">3. STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation
</h2>

Title: [STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation](https://arxiv.org/abs/2203.10426)

Authors: [Qingkai Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Q), [Rong Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+R), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M)

> How to learn a better speech representation for end-to-end speech-to-text translation (ST) with limited labeled data? Existing techniques often attempt to transfer powerful machine translation (MT) capabilities to ST, but neglect the representation discrepancy across modalities. In this paper, we propose the Speech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy. Specifically, we mix up the representation sequences of different modalities, and take both unimodal speech sequences and multimodal mixed sequences as input to the translation model in parallel, and regularize their output predictions with a self-learning framework. Experiments on MuST-C speech translation benchmark and further analysis show that our method effectively alleviates the cross-modal representation discrepancy, and achieves significant improvements over a strong baseline on eight translation directions.

| Comments:    | ACL 2022 main conference                                     |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2203.10426](https://arxiv.org/abs/2203.10426) [cs.CL]** |
|              | (or **[arXiv:2203.10426v1](https://arxiv.org/abs/2203.10426v1) [cs.CL]** for this version) |
|              | https://doi.org/10.48550/arXiv.2203.10426Focus to learn more |





<h2 id="2022-03-22-4">4. Small Batch Sizes Improve Training of Low-Resource Neural MT
</h2>

Title: [Small Batch Sizes Improve Training of Low-Resource Neural MT](https://arxiv.org/abs/2203.10579)

Authors: [Àlex R. Atrio](https://arxiv.org/search/cs?searchtype=author&query=Atrio%2C+À+R), [Andrei Popescu-Belis](https://arxiv.org/search/cs?searchtype=author&query=Popescu-Belis%2C+A)

> We study the role of an essential hyper-parameter that governs the training of Transformers for neural machine translation in a low-resource setting: the batch size. Using theoretical insights and experimental evidence, we argue against the widespread belief that batch size should be set as large as allowed by the memory of the GPUs. We show that in a low-resource setting, a smaller batch size leads to higher scores in a shorter training time, and argue that this is due to better regularization of the gradients during training.

| Comments: | To be published in 18th International Conference on Natural Language Processing (ICON 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.10579](https://arxiv.org/abs/2203.10579) [cs.CL]** |
|           | (or **[arXiv:2203.10579v1](https://arxiv.org/abs/2203.10579v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.10579Focus to learn more |





<h2 id="2022-03-22-5">5. Mitigating Gender Bias in Machine Translation through Adversarial Learning
</h2>

Title: [Mitigating Gender Bias in Machine Translation through Adversarial Learning](https://arxiv.org/abs/2203.10675)

Authors: [Eve Fleisig](https://arxiv.org/search/cs?searchtype=author&query=Fleisig%2C+E), [Christiane Fellbaum](https://arxiv.org/search/cs?searchtype=author&query=Fellbaum%2C+C)

> Machine translation and other NLP systems often contain significant biases regarding sensitive attributes, such as gender or race, that worsen system performance and perpetuate harmful stereotypes. Recent preliminary research suggests that adversarial learning can be used as part of a model-agnostic bias mitigation method that requires no data modifications. However, adapting this strategy for machine translation and other modern NLP domains requires (1) restructuring training objectives in the context of fine-tuning pretrained large language models and (2) developing measures for gender or other protected variables for tasks in which these attributes must be deduced from the data itself. 
> We present an adversarial learning framework that addresses these challenges to mitigate gender bias in seq2seq machine translation. Our framework improves the disparity in translation quality for sentences with male vs. female entities by 86% for English-German translation and 91% for English-French translation, with minimal effect on translation quality. The results suggest that adversarial learning is a promising technique for mitigating gender bias in machine translation.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.10675](https://arxiv.org/abs/2203.10675) [cs.CL]** |
|           | (or **[arXiv:2203.10675v1](https://arxiv.org/abs/2203.10675v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.10675Focus to learn more |





<h2 id="2022-03-22-6">6. Compression of Generative Pre-trained Language Models via Quantization
</h2>

Title: [Compression of Generative Pre-trained Language Models via Quantization](https://arxiv.org/abs/2203.10705)

Authors: [Chaofan Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+C), [Lu Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+L), [Wei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Lifeng Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Ping Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+P), [Ngai Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+N)

> The increasing size of generative Pre-trained Language Models (PLMs) has greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the \textit{homogeneous word embeddings} caused by reduced capacity, and \textit{varied distribution of weights}. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rates on GPT-2 and BART, respectively.

| Comments: | ACL 2022                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2203.10705](https://arxiv.org/abs/2203.10705) [cs.CL]** |
|           | (or **[arXiv:2203.10705v1](https://arxiv.org/abs/2203.10705v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.10705Focus to learn more |





<h2 id="2022-03-22-7">7. Match the Script, Adapt if Multilingual: Analyzing the Effect of Multilingual Pretraining on Cross-lingual Transferability
</h2>

Title: [Match the Script, Adapt if Multilingual: Analyzing the Effect of Multilingual Pretraining on Cross-lingual Transferability](https://arxiv.org/abs/2203.10753)

Authors: [Yoshinari Fujinuma](https://arxiv.org/search/cs?searchtype=author&query=Fujinuma%2C+Y), [Jordan Boyd-Graber](https://arxiv.org/search/cs?searchtype=author&query=Boyd-Graber%2C+J), [Katharina Kann](https://arxiv.org/search/cs?searchtype=author&query=Kann%2C+K)

> Pretrained multilingual models enable zero-shot learning even for unseen languages, and that performance can be further improved via adaptation prior to finetuning. However, it is unclear how the number of pretraining languages influences a model's zero-shot learning for languages unseen during pretraining. To fill this gap, we ask the following research questions: (1) How does the number of pretraining languages influence zero-shot performance on unseen target languages? (2) Does the answer to that question change with model adaptation? (3) Do the findings for our first question change if the languages used for pretraining are all related? Our experiments on pretraining with related languages indicate that choosing a diverse set of languages is crucial. Without model adaptation, surprisingly, increasing the number of pretraining languages yields better results up to adding related languages, after which performance plateaus. In contrast, with model adaptation via continued pretraining, pretraining on a larger number of languages often gives further improvement, suggesting that model adaptation is crucial to exploit additional pretraining languages.

| Comments: | ACL 2022 camera ready                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.10753](https://arxiv.org/abs/2203.10753) [cs.CL]** |
|           | (or **[arXiv:2203.10753v1](https://arxiv.org/abs/2203.10753v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.10753Focus to learn more |






# 2022-03-21

[Return to Index](#Index)



<h2 id="2022-03-21-1">1. A3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing
</h2>

Title: [A3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing](https://arxiv.org/abs/2203.09690)

Authors: [He Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+H), [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Junkun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Xintong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

> Recently, speech representation learning has improved many speech-related tasks such as speech recognition, speech classification, and speech-to-text translation. However, all the above tasks are in the direction of speech understanding, but for the inverse direction, speech synthesis, the potential of representation learning is yet to be realized, due to the challenging nature of generating high-quality speech. To address this problem, we propose our framework, Alignment-Aware Acoustic-Text Pretraining (A3T), which reconstructs masked acoustic signals with text input and acoustic-text alignment during training. In this way, the pretrained model can generate high quality of reconstructed spectrogram, which can be applied to the speech editing and unseen speaker TTS directly. Experiments show A3T outperforms SOTA models on speech editing, and improves multi-speaker speech synthesis without the external speaker verification model.

| Comments: | under review, 12 pages, 10 figures                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.09690](https://arxiv.org/abs/2203.09690) [cs.CL]** |
|           | (or **[arXiv:2203.09690v1](https://arxiv.org/abs/2203.09690v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09690Focus to learn more |





<h2 id="2022-03-21-2">2. Prototypical Verbalizer for Prompt-based Few-shot Tuning
</h2>

Title: [Prototypical Verbalizer for Prompt-based Few-shot Tuning](https://arxiv.org/abs/2203.09770)

Authors: [Ganqu Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+G), [Shengding Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S), [Ning Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Longtao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z)

> Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning. Typically, prompt-based tuning wraps the input text into a cloze question. To make predictions, the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built. However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains [this http URL](http://challenging.in/) this work, we propose the prototypical verbalizer (ProtoVerb) which is built directly from training data. Specifically, ProtoVerb learns prototype vectors as verbalizers by contrastive learning. In this way, the prototypes summarize training instances and are able to enclose rich class-level semantics. We conduct experiments on both topic classification and entity typing tasks, and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers, especially when training data is extremely scarce. More surprisingly, ProtoVerb consistently boosts prompt-based tuning even on untuned PLMs, indicating an elegant non-tuning way to utilize PLMs. Our codes are avaliable at [this https URL](https://github.com/thunlp/OpenPrompt).

| Comments: | 11 pages. ACL 2022 main conference                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.09770](https://arxiv.org/abs/2203.09770) [cs.CL]** |
|           | (or **[arXiv:2203.09770v1](https://arxiv.org/abs/2203.09770v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09770Focus to learn more |





<h2 id="2022-03-21-3">3. Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation
</h2>

Title: [Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation](https://arxiv.org/abs/2203.09866)

Authors: [Beatrice Savoldi](https://arxiv.org/search/cs?searchtype=author&query=Savoldi%2C+B), [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Gender bias is largely recognized as a problematic phenomenon affecting language technologies, with recent studies underscoring that it might surface differently across languages. However, most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions. Such protocols overlook key features of grammatical gender languages, which are characterized by morphosyntactic chains of gender agreement, marked on a variety of lexical items and parts-of-speech (POS). To overcome this limitation, we enrich the natural, gender-sensitive MuST-SHE corpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS and agreement chains), and explore to what extent different lexical categories and agreement phenomena are impacted by gender skews. Focusing on speech translation, we conduct a multifaceted evaluation on three language directions (English-French/Italian/Spanish), with models trained on varying amounts of data and different word segmentation techniques. By shedding light on model behaviours, gender bias, and its detection at several levels of granularity, our findings emphasize the value of dedicated analyses beyond aggregated overall results.

| Comments: | Accepted at ACL 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.09866](https://arxiv.org/abs/2203.09866) [cs.CL]** |
|           | (or **[arXiv:2203.09866v1](https://arxiv.org/abs/2203.09866v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09866Focus to learn more |





<h2 id="2022-03-21-4">4. Do Multilingual Language Models Capture Differing Moral Norms?
</h2>

Title: [Do Multilingual Language Models Capture Differing Moral Norms?](https://arxiv.org/abs/2203.09904)

Authors: [Katharina Hämmerl](https://arxiv.org/search/cs?searchtype=author&query=Hämmerl%2C+K), [Björn Deiseroth](https://arxiv.org/search/cs?searchtype=author&query=Deiseroth%2C+B), [Patrick Schramowski](https://arxiv.org/search/cs?searchtype=author&query=Schramowski%2C+P), [Jindřich Libovický](https://arxiv.org/search/cs?searchtype=author&query=Libovický%2C+J), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A), [Kristian Kersting](https://arxiv.org/search/cs?searchtype=author&query=Kersting%2C+K)

> Massively multilingual sentence representations are trained on large corpora of uncurated data, with a very imbalanced proportion of languages included in the training. This may cause the models to grasp cultural values including moral judgments from the high-resource languages and impose them on the low-resource languages. The lack of data in certain languages can also lead to developing random and thus potentially harmful beliefs. Both these issues can negatively influence zero-shot cross-lingual model transfer and potentially lead to harmful outcomes. Therefore, we aim to (1) detect and quantify these issues by comparing different models in different languages, (2) develop methods for improving undesirable properties of the models. Our initial experiments using the multilingual model XLM-R show that indeed multilingual LMs capture moral norms, even with potentially higher human-agreement than monolingual ones. However, it is not yet clear to what extent these moral norms differ between languages.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.09904](https://arxiv.org/abs/2203.09904) [cs.CL]** |
|           | (or **[arXiv:2203.09904v1](https://arxiv.org/abs/2203.09904v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09904Focus to learn more |





<h2 id="2022-03-21-5">5. Towards Lithuanian grammatical error correction
</h2>

Title: [Towards Lithuanian grammatical error correction](https://arxiv.org/abs/2203.09963)

Authors: [Lukas Stankevičius](https://arxiv.org/search/cs?searchtype=author&query=Stankevičius%2C+L), [Mantas Lukoševičius](https://arxiv.org/search/cs?searchtype=author&query=Lukoševičius%2C+M)

> Everyone wants to write beautiful and correct text, yet the lack of language skills, experience, or hasty typing can result in errors. By employing the recent advances in transformer architectures, we construct a grammatical error correction model for Lithuanian, the language rich in archaic features. We compare subword and byte-level approaches and share our best trained model, achieving F0.5=0.92, and accompanying code, in an online open-source repository.

| Subjects:    | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG); Machine Learning (stat.ML) |
| ------------ | ------------------------------------------------------------ |
| MSC classes: | 68T07, 68T50, 68T05                                          |
| ACM classes: | I.2.6; I.2.7                                                 |
| Cite as:     | **[arXiv:2203.09963](https://arxiv.org/abs/2203.09963) [cs.CL]** |
|              | (or **[arXiv:2203.09963v1](https://arxiv.org/abs/2203.09963v1) [cs.CL]** for this version) |
|              | https://doi.org/10.48550/arXiv.2203.09963Focus to learn more |






# 2022-03-18

[Return to Index](#Index)



<h2 id="2022-03-18-1">1. DU-VLG: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training
</h2>

Title: [DU-VLG: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2203.09052)

Authors: [Luyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L), [Guocheng Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+G), [Jiachen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Xinyan Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+X), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H)

> Due to the limitations of the model structure and pre-training objectives, existing vision-and-language generation models cannot utilize pair-wise images and text through bi-directional generation. In this paper, we propose DU-VLG, a framework which unifies vision-and-language generation as sequence generation problems. DU-VLG is trained with novel dual pre-training tasks: multi-modal denoising autoencoder tasks and modality translation tasks. To bridge the gap between image understanding and generation, we further design a novel commitment loss. We compare pre-training objectives on image captioning and text-to-image generation datasets. Results show that DU-VLG yields better performance than variants trained with uni-directional generation objectives or the variant without the commitment loss. We also obtain higher scores compared to previous state-of-the-art systems on three vision-and-language generation tasks. In addition, human judges further confirm that our model generates real and relevant images as well as faithful and informative captions.

| Comments: | To appear at Findings of ACL 2022                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2203.09052](https://arxiv.org/abs/2203.09052) [cs.CV]** |
|           | (or **[arXiv:2203.09052v1](https://arxiv.org/abs/2203.09052v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09052Focus to learn more |





<h2 id="2022-03-18-2">2. UNIMO-2: End-to-End Unified Vision-Language Grounded Learning
</h2>

Title: [UNIMO-2: End-to-End Unified Vision-Language Grounded Learning](https://arxiv.org/abs/2203.09067)

Authors: [Wei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W), [Can Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C), [Guocheng Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+G), [Xinyan Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+X), [Hao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Jiachen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Vision-Language Pre-training (VLP) has achieved impressive performance on various cross-modal downstream tasks. However, most existing methods can only learn from aligned image-caption data and rely heavily on expensive regional features, which greatly limits their scalability and performance. In this paper, we propose an end-to-end unified-modal pre-training framework, namely UNIMO-2, for joint learning on both aligned image-caption data and unaligned image-only and text-only corpus. We build a unified Transformer model to jointly learn visual representations, textual representations and semantic alignment between images and texts. In particular, we propose to conduct grounded learning on both images and texts via a sharing grounded space, which helps bridge unaligned images and texts, and align the visual and textual semantic spaces on different types of corpora. The experiments show that our grounded learning method can improve textual and visual semantic alignment for improving performance on various cross-modal tasks. Moreover, benefiting from effective joint modeling of different types of corpora, our model also achieves impressive performance on single-modal visual and textual tasks. Our code and models are public at the UNIMO project page [this https URL](https://unimo-ptm.github.io/).

| Comments: | Accepted by ACL2022                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2203.09067](https://arxiv.org/abs/2203.09067) [cs.CV]** |
|           | (or **[arXiv:2203.09067v1](https://arxiv.org/abs/2203.09067v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09067Focus to learn more |





<h2 id="2022-03-18-3">3. Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?
</h2>

Title: [Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?](https://arxiv.org/abs/2203.08850)

Authors: [En-Shiun Annie Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+E+A), [Sarubi Thillainathan](https://arxiv.org/search/cs?searchtype=author&query=Thillainathan%2C+S), [Shravan Nayak](https://arxiv.org/search/cs?searchtype=author&query=Nayak%2C+S), [Surangika Ranathunga](https://arxiv.org/search/cs?searchtype=author&query=Ranathunga%2C+S), [David Ifeoluwa Adelani](https://arxiv.org/search/cs?searchtype=author&query=Adelani%2C+D+I), [Ruisi Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+R), [Arya D. McCarthy](https://arxiv.org/search/cs?searchtype=author&query=McCarthy%2C+A+D)

> What can pre-trained multilingual sequence-to-sequence models like mBART contribute to translating low-resource languages? We conduct a thorough empirical experiment in 10 languages to ascertain this, considering five factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning data, (3) the amount of pre-training data in the model, (4) the impact of domain mismatch, and (5) language typology. In addition to yielding several heuristics, the experiments form a framework for evaluating the data sensitivities of machine translation systems. While mBART is robust to domain differences, its translations for unseen and typologically distant languages remain below 3.0 BLEU. In answer to our title's question, mBART is not a low-resource panacea; we therefore encourage shifting the emphasis from new models to new data.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.08850](https://arxiv.org/abs/2203.08850) [cs.CL]** |
|           | (or **[arXiv:2203.08850v1](https://arxiv.org/abs/2203.08850v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.08850Focus to learn more |





<h2 id="2022-03-18-4">4. Triangular Transfer: Freezing the Pivot for Triangular Machine Translation
</h2>

Title: [Triangular Transfer: Freezing the Pivot for Triangular Machine Translation](https://arxiv.org/abs/2203.09027)

Authors: [Meng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Triangular machine translation is a special case of low-resource machine translation where the language pair of interest has limited parallel data, but both languages have abundant parallel data with a pivot language. Naturally, the key to triangular machine translation is the successful exploitation of such auxiliary data. In this work, we propose a transfer-learning-based approach that utilizes all types of auxiliary data. As we train auxiliary source-pivot and pivot-target translation models, we initialize some parameters of the pivot side with a pre-trained language model and freeze them to encourage both translation models to work in the same pivot language space, so that they can be smoothly transferred to the source-target translation model. Experiments show that our approach can outperform previous ones.

| Comments: | ACL 2022                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.09027](https://arxiv.org/abs/2203.09027) [cs.CL]** |
|           | (or **[arXiv:2203.09027v1](https://arxiv.org/abs/2203.09027v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09027Focus to learn more |





<h2 id="2022-03-18-5">5. Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework
</h2>

Title: [Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework](https://arxiv.org/abs/2203.09053)

Authors: [Shaolei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y)

> Simultaneous machine translation (SiMT) starts translating while receiving the streaming source inputs, and hence the source sentence is always incomplete during translating. Different from the full-sentence MT using the conventional seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture, which forces each target word to only align with a partial source prefix to adapt to the incomplete source in streaming inputs. However, the source words in the front positions are always illusoryly considered more important since they appear in more prefixes, resulting in position bias, which makes the model pay more attention on the front source positions in testing. In this paper, we first analyze the phenomenon of position bias in SiMT, and develop a Length-Aware Framework to reduce the position bias by bridging the structural gap between SiMT and full-sentence MT. Specifically, given the streaming inputs, we first predict the full-sentence length and then fill the future source position with positional encoding, thereby turning the streaming inputs into a pseudo full-sentence. The proposed framework can be integrated into most existing SiMT methods to further improve performance. Experiments on two representative SiMT methods, including the state-of-the-art adaptive policy, show that our method successfully reduces the position bias and achieves better SiMT performance.

| Comments: | Accept to ACL 2022 main conference. 14 pages, 11 figures     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2203.09053](https://arxiv.org/abs/2203.09053) [cs.CL]** |
|           | (or **[arXiv:2203.09053v1](https://arxiv.org/abs/2203.09053v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09053Focus to learn more |





<h2 id="2022-03-18-6">6. Gaussian Multi-head Attention for Simultaneous Machine Translation
</h2>

Title: [Gaussian Multi-head Attention for Simultaneous Machine Translation](https://arxiv.org/abs/2203.09072)

Authors: [Shaolei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y)

> Simultaneous machine translation (SiMT) outputs translation while receiving the streaming source inputs, and hence needs a policy to determine where to start translating. The alignment between target and source words often implies the most informative source word for each target word, and hence provides the unified control over translation quality and latency, but unfortunately the existing SiMT methods do not explicitly model the alignment to perform the control. In this paper, we propose Gaussian Multi-head Attention (GMA) to develop a new SiMT policy by modeling alignment and translation in a unified manner. For SiMT policy, GMA models the aligned source position of each target word, and accordingly waits until its aligned position to start translating. To integrate the learning of alignment into the translation model, a Gaussian distribution centered on predicted aligned position is introduced as an alignment-related prior, which cooperates with translation-related soft attention to determine the final attention. Experiments on En-Vi and De-En tasks show that our method outperforms strong baselines on the trade-off between translation and latency.

| Comments: | Accept to ACL 2022 findings. 12 pages, 8 figures             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2203.09072](https://arxiv.org/abs/2203.09072) [cs.CL]** |
|           | (or **[arXiv:2203.09072v1](https://arxiv.org/abs/2203.09072v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09072Focus to learn more |





<h2 id="2022-03-18-7">7. Type-Driven Multi-Turn Corrections for Grammatical Error Correction
</h2>

Title: [Type-Driven Multi-Turn Corrections for Grammatical Error Correction](https://arxiv.org/abs/2203.09136)

Authors: [Shaopeng Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+S), [Qingyu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Q), [Jiali Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+J), [Zhongli Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Chao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Yunbo Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J)

> Grammatical Error Correction (GEC) aims to automatically detect and correct grammatical errors. In this aspect, dominant models are trained by one-iteration learning while performing multiple iterations of corrections during inference. Previous studies mainly focus on the data augmentation approach to combat the exposure bias, which suffers from two drawbacks. First, they simply mix additionally-constructed training instances and original ones to train models, which fails to help models be explicitly aware of the procedure of gradual corrections. Second, they ignore the interdependence between different types of corrections. In this paper, we propose a Type-Driven Multi-Turn Corrections approach for GEC. Using this approach, from each training instance, we additionally construct multiple training instances, each of which involves the correction of a specific type of errors. Then, we use these additionally-constructed training instances and the original one to train the model in turn. Experimental results and in-depth analysis show that our approach significantly benefits the model training. Particularly, our enhanced model achieves state-of-the-art single-model performance on English GEC benchmarks. We release our code at Github.

| Comments: | Findings of ACL2022                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.09136](https://arxiv.org/abs/2203.09136) [cs.CL]** |
|           | (or **[arXiv:2203.09136v1](https://arxiv.org/abs/2203.09136v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09136Focus to learn more |





<h2 id="2022-03-18-8">8. Modeling Dual Read/Write Paths for Simultaneous Machine Translation
</h2>

Title: [Modeling Dual Read/Write Paths for Simultaneous Machine Translation](https://arxiv.org/abs/2203.09163)

Authors: [Shaolei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y)

> Simultaneous machine translation (SiMT) outputs the translation while reading the source sentence and hence requires a policy to determine whether to wait for the next source word (READ) or generate a target word (WRITE), the actions of which form a read/write path. Although the read/write path is essential to SiMT performance, there is no direct supervision given to the path in the existing methods. In this paper, we propose a method of Dual Path SiMT which introduces duality constraints to guide the read/write path. According to duality constraints, the read/write paths in source-to-target and target-to-source SiMT models can be mapped to each other. Therefore, the SiMT models in two directions are jointly optimized by forcing their read/write paths to satisfy the mapping relation. Experiments on En-Vi and De-En SiMT tasks show that our method can outperform strong baselines under all latency.

| Comments: | Accept to ACL 2022 main conference. 18 pages, 12 figures     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2203.09163](https://arxiv.org/abs/2203.09163) [cs.CL]** |
|           | (or **[arXiv:2203.09163v1](https://arxiv.org/abs/2203.09163v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09163Focus to learn more |





<h2 id="2022-03-18-9">9. On Vision Features in Multimodal Machine Translation
</h2>

Title: [On Vision Features in Multimodal Machine Translation](https://arxiv.org/abs/2203.09173)

Authors: [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Chuanhao Lv](https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+C), [Zefan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Z), [Tao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+T), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Anxiang Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+A), [JingBo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Previous work on multimodal machine translation (MMT) has focused on the way of incorporating vision features into translation but little attention is on the quality of vision models. In this work, we investigate the impact of vision models on MMT. Given the fact that Transformer is becoming popular in computer vision, we experiment with various strong models (such as Vision Transformer) and enhanced features (such as object-detection and image captioning). We develop a selective attention model to study the patch-level contribution of an image in MMT. On detailed probing tasks, we find that stronger vision models are helpful for learning translation from the visual modality. Our results also suggest the need of carefully examining MMT models, especially when current benchmarks are small-scale and biased. Our code could be found at \url{[this https URL](https://github.com/libeineu/fairseq_mmt)}.

| Comments: | Long paper accepted by ACL2022 main conference               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.09173](https://arxiv.org/abs/2203.09173) [cs.CL]** |
|           | (or **[arXiv:2203.09173v1](https://arxiv.org/abs/2203.09173v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09173Focus to learn more |





<h2 id="2022-03-18-10">10. Universal Conditional Masked Language Pre-training for Neural Machine Translation
</h2>

Title: [Universal Conditional Masked Language Pre-training for Neural Machine Translation](https://arxiv.org/abs/2203.09210)

Authors: [Pengfei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Meng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Minghao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+M), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. We also introduce two simple but effective methods to enhance the CeMAT, aligned code-switching & masking and dynamic dual-masking. We conduct extensive experiments and show that our CeMAT can achieve significant performance improvement for all scenarios from low to extremely high resource, i.e., up to 14.4 BLEU on low resource and 7.9 BLEU improvements on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to 5.3 BLEU. As far as we know, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks. Code, data, and pre-trained models are available at [this https URL](https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT)

| Comments: | Accepted to ACL 2022 Main conference                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.09210](https://arxiv.org/abs/2203.09210) [cs.CL]** |
|           | (or **[arXiv:2203.09210v1](https://arxiv.org/abs/2203.09210v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09210Focus to learn more |





<h2 id="2022-03-18-11">11. Finding Structural Knowledge in Multimodal-BERT
</h2>

Title: [Finding Structural Knowledge in Multimodal-BERT](https://arxiv.org/abs/2203.09306)

Authors: [Victor Milewski](https://arxiv.org/search/cs?searchtype=author&query=Milewski%2C+V), [Miryam de Lhoneux](https://arxiv.org/search/cs?searchtype=author&query=de+Lhoneux%2C+M), [Marie-Francine Moens](https://arxiv.org/search/cs?searchtype=author&query=Moens%2C+M)

> In this work, we investigate the knowledge learned in the embeddings of multimodal-BERT models. More specifically, we probe their capabilities of storing the grammatical structure of linguistic data and the structure learned over objects in visual data. To reach that goal, we first make the inherent structure of language and visuals explicit by a dependency parse of the sentences that describe the image and by the dependencies between the object regions in the image, respectively. We call this explicit visual structure the \textit{scene tree}, that is based on the dependency tree of the language description. Extensive probing experiments show that the multimodal-BERT models do not encode these scene trees.Code available at \url{[this https URL](https://github.com/VSJMilewski/multimodal-probes)}.

| Comments: | Accepted at ACL 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2203.09306](https://arxiv.org/abs/2203.09306) [cs.CL]** |
|           | (or **[arXiv:2203.09306v1](https://arxiv.org/abs/2203.09306v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09306Focus to learn more |





<h2 id="2022-03-18-12">12. Combining Static and Contextualised Multilingual Embeddings
</h2>

Title: [Combining Static and Contextualised Multilingual Embeddings](https://arxiv.org/abs/2203.09326)

Authors: [Katharina Hämmerl](https://arxiv.org/search/cs?searchtype=author&query=Hämmerl%2C+K), [Jindřich Libovický](https://arxiv.org/search/cs?searchtype=author&query=Libovický%2C+J), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> Static and contextual multilingual embeddings have complementary strengths. Static embeddings, while less expressive than contextual language models, can be more straightforwardly aligned across multiple languages. We combine the strengths of static and contextual models to improve multilingual representations. We extract static embeddings for 40 languages from XLM-R, validate those embeddings with cross-lingual word retrieval, and then align them using VecMap. This results in high-quality, highly multilingual static embeddings. Then we apply a novel continued pre-training approach to XLM-R, leveraging the high quality alignment of our static embeddings to better align the representation space of XLM-R. We show positive results for multiple complex semantic tasks. We release the static embeddings and the continued pre-training code. Unlike most previous work, our continued pre-training approach does not require parallel text.

| Comments: | Accepted to Findings of ACL 2022                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.09326](https://arxiv.org/abs/2203.09326) [cs.CL]** |
|           | (or **[arXiv:2203.09326v1](https://arxiv.org/abs/2203.09326v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.09326Focus to learn more |






# 2022-03-17

[Return to Index](#Index)



<h2 id="2022-03-17-1">1. Hyperdecoders: Instance-specific decoders for multi-task NLP
</h2>

Title: [Hyperdecoders: Instance-specific decoders for multi-task NLP](https://arxiv.org/abs/2203.08304)

Authors: [Hamish Ivison](https://arxiv.org/search/cs?searchtype=author&query=Ivison%2C+H), [Matthew E. Peters](https://arxiv.org/search/cs?searchtype=author&query=Peters%2C+M+E)

> We investigate input-conditioned hypernetworks for multi-tasking in NLP, generating parameter-efficient adaptations for a decoder using a hypernetwork conditioned on the output of an encoder. This approach produces a unique decoder for every input instance, allowing the network a larger degree of flexibility than prior work that specializes the decoder for each task. We apply our method to sequence classification tasks, extractive QA, and summarisation and find that it often outperforms fully finetuning the underlying model and surpasses previous parameter efficient fine-tuning methods. Gains are particularly large when evaluated out-of-domain on the MRQA benchmark. In addition, as the pretrained model is frozen, our method eliminates negative interference among unrelated tasks, a common failure mode in fully fine-tuned approaches. An analysis of the embeddings produced by our model suggests that a large benefit of our approach is allowing the encoder more effective control over the decoder, allowing mapping from hidden representations to a final text-based label without interference from other tasks' output formats or labels.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.08304](https://arxiv.org/abs/2203.08304) [cs.CL]** |
|           | (or **[arXiv:2203.08304v1](https://arxiv.org/abs/2203.08304v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.08304Focus to learn more |





<h2 id="2022-03-17-2">2. Improving Word Translation via Two-Stage Contrastive Learning
</h2>

Title: [Improving Word Translation via Two-Stage Contrastive Learning](https://arxiv.org/abs/2203.08307)

Authors: [Yaoyiran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Fangyu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Nigel Collier](https://arxiv.org/search/cs?searchtype=author&query=Collier%2C+N), [Anna Korhonen](https://arxiv.org/search/cs?searchtype=author&query=Korhonen%2C+A), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I)

> Word translation or bilingual lexicon induction (BLI) is a key cross-lingual task, aiming to bridge the lexical gap between different languages. In this work, we propose a robust and effective two-stage contrastive learning framework for the BLI task. At Stage C1, we propose to refine standard cross-lingual linear maps between static word embeddings (WEs) via a contrastive learning objective; we also show how to integrate it into the self-learning procedure for even more refined cross-lingual maps. In Stage C2, we conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word translation capability. We also show that static WEs induced from the `C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments on standard BLI datasets for diverse languages and different experimental setups demonstrate substantial gains achieved by our framework. While the BLI method from Stage C1 already yields substantial gains over all state-of-the-art BLI methods in our comparison, even stronger improvements are met with the full two-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28 language pairs.

| Comments: | ACL 2022 Main                                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.08307](https://arxiv.org/abs/2203.08307) [cs.CL]** |
|           | (or **[arXiv:2203.08307v1](https://arxiv.org/abs/2203.08307v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.08307Focus to learn more |





<h2 id="2022-03-17-3">3. Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation
</h2>

Title: [Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation](https://arxiv.org/abs/2203.08394)

Authors: [Zhiwei He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Z), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Back-translation is a critical component of Unsupervised Neural Machine Translation (UNMT), which generates pseudo parallel data from target monolingual data. A UNMT model is trained on the pseudo parallel data with translated source, and translates natural source sentences in inference. The source discrepancy between training and inference hinders the translation performance of UNMT models. By carefully designing experiments, we identify two representative characteristics of the data gap in source: (1) style gap (i.e., translated vs. natural text style) that leads to poor generalization capability; (2) content gap that induces the model to produce hallucination content biased towards the target language. To narrow the data gap, we propose an online self-training approach, which simultaneously uses the pseudo parallel data {natural source, translated target} to mimic the inference scenario. Experimental results on several widely-used language pairs show that our approach outperforms two strong baselines (XLM and MASS) by remedying the style and content gaps.

| Comments: | ACL 2022 (long paper, main conference)                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.08394](https://arxiv.org/abs/2203.08394) [cs.CL]** |
|           | (or **[arXiv:2203.08394v1](https://arxiv.org/abs/2203.08394v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.08394Focus to learn more |





<h2 id="2022-03-17-4">4. Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation
</h2>

Title: [Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation](https://arxiv.org/abs/2203.08442)

Authors: [Wenxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Wenxiang Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+W), [Yongchang Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+Y), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Michael Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+M)

> In this paper, we present a substantial step in better understanding the SOTA sequence-to-sequence (Seq2Seq) pretraining for neural machine translation~(NMT). We focus on studying the impact of the jointly pretrained decoder, which is the main difference between Seq2Seq pretraining and previous encoder-based pretraining approaches for NMT. By carefully designing experiments on three language pairs, we find that Seq2Seq pretraining is a double-edged sword: On one hand, it helps NMT models to produce more diverse translations and reduce adequacy-related translation errors. On the other hand, the discrepancies between Seq2Seq pretraining and NMT finetuning limit the translation quality (i.e., domain discrepancy) and induce the over-estimation issue (i.e., objective discrepancy). Based on these observations, we further propose simple and effective strategies, named in-domain pretraining and input adaptation to remedy the domain and objective discrepancies, respectively. Experimental results on several language pairs show that our approach can consistently improve both translation performance and model robustness upon Seq2Seq pretraining.

| Comments: | Accepted by ACL 2022 main conference                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2203.08442](https://arxiv.org/abs/2203.08442) [cs.CL]** |
|           | (or **[arXiv:2203.08442v1](https://arxiv.org/abs/2203.08442v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.08442Focus to learn more |





<h2 id="2022-03-17-5">5. ConTinTin: Continual Learning from Task Instructions
</h2>

Title: [ConTinTin: Continual Learning from Task Instructions](https://arxiv.org/abs/2203.08512)

Authors: [Wenpeng Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+W), [Jia Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Caiming Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+C)

> The mainstream machine learning paradigms for NLP often work with two underlying presumptions. First, the target task is predefined and static, a system just needs to learn to solve it exclusively. Second, the supervision of a task mainly comes from a set of labeled examples. A question arises: how to build a system that can keep learning new tasks from their instructions? This work defines a new learning paradigm ConTinTin (Continual Learning from Task Instructions), in which a system should learn a sequence of new tasks one by one, each task is explained by a piece of textual instruction. The system is required to (i) generate the expected outputs of a new task by learning from its instruction, (ii) transfer the knowledge acquired from upstream tasks to help solve downstream tasks (i.e, forward-transfer), and (iii) retain or even improve the performance on earlier tasks after learning new tasks (i.e., backward-transfer). This new problem is studied on a stream of more than 60 tasks, each equipped with an instruction. Technically, our method InstructionSpeak contains two strategies that make full use of task instructions to improve forward-transfer and backward-transfer: one is to learn from the negative output, the other is to re-visit instructions of prior tasks. To our knowledge, this is the first time to study ConTinTin in NLP. In addition to the problem formulation and our promising approach, this work also contributes to providing rich analyses for the community to better understand this novel learning problem.

| Comments: | ACL'2022 camera-ready                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.08512](https://arxiv.org/abs/2203.08512) [cs.CL]** |
|           | (or **[arXiv:2203.08512v1](https://arxiv.org/abs/2203.08512v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.08512Focus to learn more |





<h2 id="2022-03-17-6">6. Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer
</h2>

Title: [Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer](https://arxiv.org/abs/2203.08552)

Authors: [Huiyuan Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+H), [Antonio Toral](https://arxiv.org/search/cs?searchtype=author&query=Toral%2C+A), [Malvina Nissim](https://arxiv.org/search/cs?searchtype=author&query=Nissim%2C+M)

> We exploit the pre-trained seq2seq model mBART for multilingual text style transfer. Using machine translated data as well as gold aligned English sentences yields state-of-the-art results in the three target languages we consider. Besides, in view of the general scarcity of parallel data, we propose a modular approach for multilingual formality transfer, which consists of two training strategies that target adaptation to both language and task. Our approach achieves competitive performance without monolingual task-specific parallel data and can be applied to other style transfer tasks as well as to other languages.

| Comments: | Accepted to ACL 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.08552](https://arxiv.org/abs/2203.08552) [cs.CL]** |
|           | (or **[arXiv:2203.08552v1](https://arxiv.org/abs/2203.08552v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.08552Focus to learn more |





<h2 id="2022-03-17-7">7. Geographic Adaptation of Pretrained Language Models
</h2>

Title: [Geographic Adaptation of Pretrained Language Models](https://arxiv.org/abs/2203.08565)

Authors: [Valentin Hofmann](https://arxiv.org/search/cs?searchtype=author&query=Hofmann%2C+V), [Goran Glavaš](https://arxiv.org/search/cs?searchtype=author&query=Glavaš%2C+G), [Nikola Ljubešić](https://arxiv.org/search/cs?searchtype=author&query=Ljubešić%2C+N), [Janet B. Pierrehumbert](https://arxiv.org/search/cs?searchtype=author&query=Pierrehumbert%2C+J+B), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> Geographic linguistic features are commonly used to improve the performance of pretrained language models (PLMs) on NLP tasks where geographic knowledge is intuitively beneficial (e.g., geolocation prediction and dialect feature prediction). Existing work, however, leverages such geographic information in task-specific fine-tuning, failing to incorporate it into PLMs' geo-linguistic knowledge, which would make it transferable across different tasks. In this work, we introduce an approach to task-agnostic geoadaptation of PLMs that forces the PLM to learn associations between linguistic phenomena and geographic locations. More specifically, geoadaptation is an intermediate training step that couples masked language modeling and geolocation prediction in a dynamic multitask learning setup. In our experiments, we geoadapt BERTić -- a PLM for Bosnian, Croatian, Montenegrin, and Serbian (BCMS) -- using a corpus of geotagged BCMS tweets. Evaluation on three different tasks, namely unsupervised (zero-shot) and supervised geolocation prediction and (unsupervised) prediction of dialect features, shows that our geoadaptation approach is very effective: e.g., we obtain new state-of-the-art performance in supervised geolocation prediction and report massive gains over geographically uninformed PLMs on zero-shot geolocation prediction.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.08565](https://arxiv.org/abs/2203.08565) [cs.CL]** |
|           | (or **[arXiv:2203.08565v1](https://arxiv.org/abs/2203.08565v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.08565Focus to learn more |





<h2 id="2022-03-17-8">8. Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation
</h2>

Title: [Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation](https://arxiv.org/abs/2203.08757)

Authors: [Tsz Kin Lam](https://arxiv.org/search/cs?searchtype=author&query=Lam%2C+T+K), [Shigehiko Schamoni](https://arxiv.org/search/cs?searchtype=author&query=Schamoni%2C+S), [Stefan Riezler](https://arxiv.org/search/cs?searchtype=author&query=Riezler%2C+S)

> End-to-end speech translation relies on data that pair source-language speech inputs with corresponding translations into a target language. Such data are notoriously scarce, making synthetic data augmentation by back-translation or knowledge distillation a necessary ingredient of end-to-end training. In this paper, we present a novel approach to data augmentation that leverages audio alignments, linguistic properties, and translation. First, we augment a transcription by sampling from a suffix memory that stores text and audio data. Second, we translate the augmented transcript. Finally, we recombine concatenated audio segments and the generated translation. Besides training an MT-system, we only use basic off-the-shelf components without fine-tuning. While having similar resource demands as knowledge distillation, adding our method delivers consistent improvements of up to 0.9 and 1.1 BLEU points on five language pairs on CoVoST 2 and on two language pairs on Europarl-ST, respectively.

| Comments: | Accepted at ACL 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.08757](https://arxiv.org/abs/2203.08757) [cs.CL]** |
|           | (or **[arXiv:2203.08757v1](https://arxiv.org/abs/2203.08757v1) [cs.CL]** for this version) |





<h2 id="2022-03-17-9">9. Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data
</h2>

Title: [Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data](https://arxiv.org/abs/2203.08773)

Authors: [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yichong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Yuwei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Siqi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+S), [Ruochen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+R), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M)

> Retrieval-based methods have been shown to be effective in NLP tasks via introducing external knowledge. However, the indexing and retrieving of large-scale corpora bring considerable computational cost. Surprisingly, we found that REtrieving from the traINing datA (REINA) only can lead to significant gains on multiple NLG and NLU tasks. We retrieve the labeled training instances most similar to the input text and then concatenate them with the input to feed into the model to generate the output. Experimental results show that this simple method can achieve significantly better performance on a variety of NLU and NLG tasks, including summarization, machine translation, language modeling, and question answering tasks. For instance, our proposed method achieved state-of-the-art results on XSum, BigPatent, and CommonsenseQA. Our code is released, [this https URL](https://github.com/microsoft/REINA) .

| Comments: | Accept to ACL 2022 main conference                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR) |
| Cite as:  | **[arXiv:2203.08773](https://arxiv.org/abs/2203.08773) [cs.CL]** |
|           | (or **[arXiv:2203.08773v1](https://arxiv.org/abs/2203.08773v1) [cs.CL]** for this version) |










# 2022-03-16

[Return to Index](#Index)



<h2 id="2022-03-16-1">1. Leveraging Uni-Modal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition
</h2>

Ttile: [Leveraging Uni-Modal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition](https://arxiv.org/abs/2203.07996)

Authors: [Xichen Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+X), [Peiyu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Yichen Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y), [Helong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Xinbing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhouhan Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z)

> Training Transformer-based models demands a large amount of data, while obtaining parallel aligned and labelled data in multimodality is rather cost-demanding, especially for audio-visual speech recognition (AVSR). Thus it makes a lot of sense to make use of unlabelled uni-modal data. On the other side, although the effectiveness of large-scale self-supervised learning is well established in both audio and visual modalities, how to integrate those pre-trained models into a multimodal scenario remains underexplored. In this work, we successfully leverage uni-modal self-supervised learning to promote the multimodal AVSR. In particular, we first train audio and visual encoders on a large-scale uni-modal dataset, then we integrate components of both encoders into a larger multimodal framework which learns to recognize paired audio-visual data into characters through a combination of CTC and seq2seq decoding. We show that both components inherited from uni-modal self-supervised learning cooperate well, resulting in that the multimodal framework yields competitive results through fine-tuning. Our model is experimentally validated on both word-level and sentence-level AVSR tasks. Especially, even without an external language model, our proposed model raises the state-of-the-art performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative improvement of 30%.

| Comments: | 8 pages (main), 2 pages (appendix) and to be appeared in ACL2022 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Sound (cs.SD)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.07996](https://arxiv.org/abs/2203.07996) [cs.SD]** |
|           | (or **[arXiv:2203.07996v1](https://arxiv.org/abs/2203.07996v1) [cs.SD]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.07996Focus to learn more |





<h2 id="2022-03-16-2">2. Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer
</h2>

Ttile: [Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer](https://arxiv.org/abs/2203.07519)

Authors: [Woojeong Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+W), [Dong-Ho Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+D), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Jay Pujara](https://arxiv.org/search/cs?searchtype=author&query=Pujara%2C+J), [Xiang Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X)

> Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias. In this work, we study whether integrating visual knowledge into a language model can fill the gap. We investigate two types of knowledge transfer: (1) text knowledge transfer using image captions that may contain enriched visual knowledge and (2) cross-modal knowledge transfer using both images and captions with vision-language training objectives. On 5 downstream tasks that may need visual knowledge to solve the problem, we perform extensive empirical comparisons over the presented objectives. Our experiments show that visual knowledge transfer can improve performance in both low-resource and fully supervised settings.

| Comments: | Accepted to ACL 2022, 13 pages, 4 figures                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.07519](https://arxiv.org/abs/2203.07519) [cs.CL]** |
|           | (or **[arXiv:2203.07519v1](https://arxiv.org/abs/2203.07519v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.07519Focus to learn more |





<h2 id="2022-03-16-3">3. Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation
</h2>

Ttile: [Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation](https://arxiv.org/abs/2203.07627)

Authors: [Yong Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Yuan Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y), [Pidong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Wolfgang Macherey](https://arxiv.org/search/cs?searchtype=author&query=Macherey%2C+W)

> Multilingual neural machine translation models are trained to maximize the likelihood of a mix of examples drawn from multiple language pairs. The dominant inductive bias applied to these models is a shared vocabulary and a shared set of parameters across languages; the inputs and labels corresponding to examples drawn from different language pairs might still reside in distinct sub-spaces. In this paper, we introduce multilingual crossover encoder-decoder (mXEncDec) to fuse language pairs at an instance level. Our approach interpolates instances from different language pairs into joint `crossover examples' in order to encourage sharing input and output spaces across languages. To ensure better fusion of examples in multilingual settings, we propose several techniques to improve example interpolation across dissimilar languages under heavy data imbalance. Experiments on a large-scale WMT multilingual dataset demonstrate that our approach significantly improves quality on English-to-Many, Many-to-English and zero-shot translation tasks (from +0.5 BLEU up to +5.5 BLEU points). Results on code-switching sets demonstrate the capability of our approach to improve model generalization to out-of-distribution multilingual examples. We also conduct qualitative and quantitative representation comparisons to analyze the advantages of our approach at the representation level.

| Comments: | ACL 2022                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2203.07627](https://arxiv.org/abs/2203.07627) [cs.CL]** |
|           | (or **[arXiv:2203.07627v1](https://arxiv.org/abs/2203.07627v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.07627Focus to learn more |





<h2 id="2022-03-16-4">4. Modular and Parameter-Efficient Multimodal Fusion with Prompting
</h2>

Ttile: [Modular and Parameter-Efficient Multimodal Fusion with Prompting](https://arxiv.org/abs/2203.08055)

Authors: [Sheng Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+S), [Mengjie Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+M), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> Recent research has made impressive progress in large-scale multimodal pre-training. In the context of the rapid growth of model size, it is necessary to seek efficient and flexible methods other than finetuning. In this paper, we propose to use prompt vectors to align the modalities. Our method achieves comparable performance to several other multimodal fusion methods in low-resource settings. We further show that our method is modular and parameter-efficient for processing tasks involving two or more data modalities.

| Comments: | Accepted to Findings of ACL 2022                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.08055](https://arxiv.org/abs/2203.08055) [cs.CL]** |
|           | (or **[arXiv:2203.08055v1](https://arxiv.org/abs/2203.08055v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.08055Focus to learn more |





<h2 id="2022-03-16-5">5. Does Corpus Quality Really Matter for Low-Resource Languages?
</h2>

Ttile: [Does Corpus Quality Really Matter for Low-Resource Languages?](https://arxiv.org/abs/2203.08111)

Authors: [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Itziar Aldabe](https://arxiv.org/search/cs?searchtype=author&query=Aldabe%2C+I), [Rodrigo Agerri](https://arxiv.org/search/cs?searchtype=author&query=Agerri%2C+R), [Olatz Perez-de-Viñaspre](https://arxiv.org/search/cs?searchtype=author&query=Perez-de-Viñaspre%2C+O), [Aitor Soroa](https://arxiv.org/search/cs?searchtype=author&query=Soroa%2C+A)

> The vast majority of non-English corpora are derived from automatically filtered versions of CommonCrawl. While prior work has identified major issues on the quality of these datasets (Kreutzer et al., 2021), it is not clear how this impacts downstream performance. Taking Basque as a case study, we explore tailored crawling (manually identifying and scraping websites with high-quality content) as an alternative to filtering CommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque portion of popular multilingual corpora like CC100 and mC4, yet it has a much higher quality according to native annotators. For instance, 66% of documents are rated as high-quality for EusCrawl, in contrast with <33% for both mC4 and CC100. Nevertheless, we obtain similar results on downstream tasks regardless of the corpus used for pre-training. Our work suggests that NLU performance in low-resource languages is primarily constrained by the quantity rather than the quality of the data, prompting for methods to exploit more diverse data sources.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.08111](https://arxiv.org/abs/2203.08111) [cs.CL]** |
|           | (or **[arXiv:2203.08111v1](https://arxiv.org/abs/2203.08111v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.08111Focus to learn more |







# 2022-03-15

[Return to Index](#Index)



<h2 id="2022-03-15-1">1. XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding
</h2>

Title: [XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding](https://arxiv.org/abs/2203.06947)

Authors: [Zhangxuan Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+Z), [Changhua Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+C), [Ke Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+K), [Jun Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+J), [Weiqiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Ming Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+M), [Liqing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L)

> Recently, various multimodal networks for Visually-Rich Document Understanding(VRDU) have been proposed, showing the promotion of transformers by integrating visual and layout information with the text embeddings. However, most existing approaches utilize the position embeddings to incorporate the sequence information, neglecting the noisy improper reading order obtained by OCR tools. In this paper, we propose a robust layout-aware multimodal network named XYLayoutLM to capture and leverage rich layout information from proper reading orders produced by our Augmented XY Cut. Moreover, a Dilated Conditional Position Encoding module is proposed to deal with the input sequence of variable lengths, and it additionally extracts local layout information from both textual and visual modalities while generating position embeddings. Experiment results show that our XYLayoutLM achieves competitive results on document understanding tasks.

| Comments: | Accepted by CVPR2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2203.06947](https://arxiv.org/abs/2203.06947) [cs.CV]** |
|           | (or **[arXiv:2203.06947v1](https://arxiv.org/abs/2203.06947v1) [cs.CV]** for this version) |





<h2 id="2022-03-15-2">2. ELLE: Efficient Lifelong Pre-training for Emerging Data
</h2>

Title: [ELLE: Efficient Lifelong Pre-training for Emerging Data](https://arxiv.org/abs/2203.06311)

Authors: [Yujia Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y), [Jiajie Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive. To this end, we propose ELLE, aiming at efficient lifelong pre-training for emerging data. Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM's width and depth to improve the efficiency of knowledge acquisition; and (2) pre-trained domain prompts, which disentangle the versatile knowledge learned during pre-training and stimulate the proper knowledge for downstream tasks. We experiment ELLE with streaming data from 5 domains on BERT and GPT. The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances. The codes are publicly available at [this https URL](https://github.com/thunlp/ELLE).

| Comments: | Findings of ACL 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2203.06311](https://arxiv.org/abs/2203.06311) [cs.CL]** |
|           | (or **[arXiv:2203.06311v1](https://arxiv.org/abs/2203.06311v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.06311Focus to learn more |





<h2 id="2022-03-15-3">3. Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation
</h2>

Title: [Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation](https://arxiv.org/abs/2203.06386)

Authors: [Wenliang Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+W), [Lu Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+L), [Lifeng Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

> The recent large-scale vision-language pre-training (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks. Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder. To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. VLKD is pretty data- and computation-efficient compared to the pre-training from scratch. Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning. For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with 7× fewer parameters. Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks.

| Comments: | Accepted to ACL 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2203.06386](https://arxiv.org/abs/2203.06386) [cs.CL]** |
|           | (or **[arXiv:2203.06386v1](https://arxiv.org/abs/2203.06386v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.06386Focus to learn more |





<h2 id="2022-03-15-4">4. Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models
</h2>

Title: [Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models](https://arxiv.org/abs/2203.06904)

Authors: [Ning Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Yujia Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y), [Guang Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+G), [Fuchao Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Zonghan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Yusheng Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y), [Shengding Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S), [Yulin Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Chi-Min Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+C), [Weize Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Jing Yi](https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+J), [Weilin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Xiaozhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Hai-Tao Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H), [Jianfei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Jie Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J), [Juanzi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

> Despite the success, the process of fine-tuning large-scale PLMs brings prohibitive adaptation costs. In fact, fine-tuning all the parameters of a colossal model and retaining separate instances for different tasks are practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes a small portion of the model parameters while keeping the rest untouched, largely reducing both the computation and storage costs. Recent studies have demonstrated that a series of delta tuning methods with distinct tuned parameter selection could achieve performance on a par with full-parameter fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In this paper, we first formally describe the problem of delta tuning and then comprehensively review recent delta tuning approaches. We also propose a unified categorization criterion that divide existing delta tuning methods into three groups: addition-based, specification-based, and reparameterization-based methods. Though initially proposed as an efficient method to steer large models, we believe that some of the fascinating evidence discovered along with delta tuning could help further reveal the mechanisms of PLMs and even deep neural networks. To this end, we discuss the theoretical principles underlying the effectiveness of delta tuning and propose frameworks to interpret delta tuning from the perspective of optimization and optimal control, respectively. Furthermore, we provide a holistic empirical study of representative methods, where results on over 100 NLP tasks demonstrate a comprehensive performance comparison of different approaches. The experimental results also cover the analysis of combinatorial, scaling and transferable properties of delta tuning.

| Comments: | 49 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.06904](https://arxiv.org/abs/2203.06904) [cs.CL]** |
|           | (or **[arXiv:2203.06904v1](https://arxiv.org/abs/2203.06904v1) [cs.CL]** for this version) |





<h2 id="2022-03-15-5">5. PERT: Pre-training BERT with Permuted Language Model
</h2>

Title: [PERT: Pre-training BERT with Permuted Language Model](https://arxiv.org/abs/2203.06906)

Authors: [Yiming Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y), [Ziqing Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Ting Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> Pre-trained Language Models (PLMs) have been widely used in various natural language processing (NLP) tasks, owing to their powerful text representations trained on large-scale corpora. In this paper, we propose a new PLM called PERT for natural language understanding (NLU). PERT is an auto-encoding model (like BERT) trained with Permuted Language Model (PerLM). The formulation of the proposed PerLM is straightforward. We permute a proportion of the input text, and the training objective is to predict the position of the original token. Moreover, we also apply whole word masking and N-gram masking to improve the performance of PERT. We carried out extensive experiments on both Chinese and English NLU benchmarks. The experimental results show that PERT can bring improvements over various comparable baselines on some of the tasks, while others are not. These results indicate that developing more diverse pre-training tasks is possible instead of masked language model variants. Several quantitative studies are carried out to better understand PERT, which might help design PLMs in the future. Resources are available: [this https URL](https://github.com/ymcui/PERT)

| Comments: | 14 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.06906](https://arxiv.org/abs/2203.06906) [cs.CL]** |
|           | (or **[arXiv:2203.06906v1](https://arxiv.org/abs/2203.06906v1) [cs.CL]** for this version) |





<h2 id="2022-03-15-6">6. Modelling word learning and recognition using visually grounded speech
</h2>

Title: [Modelling word learning and recognition using visually grounded speech](https://arxiv.org/abs/2203.06937)

Authors: [Danny Merkx](https://arxiv.org/search/cs?searchtype=author&query=Merkx%2C+D), [Sebastiaan Scholten](https://arxiv.org/search/cs?searchtype=author&query=Scholten%2C+S), [Stefan L. Frank](https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+S+L), [Mirjam Ernestus](https://arxiv.org/search/cs?searchtype=author&query=Ernestus%2C+M), [Odette Scharenborg](https://arxiv.org/search/cs?searchtype=author&query=Scharenborg%2C+O)

> Background: Computational models of speech recognition often assume that the set of target words is already given. This implies that these models do not learn to recognise speech from scratch without prior knowledge and explicit supervision. Visually grounded speech models learn to recognise speech without prior knowledge by exploiting statistical dependencies between spoken and visual input. While it has previously been shown that visually grounded speech models learn to recognise the presence of words in the input, we explicitly investigate such a model as a model of human speech recognition. 
> Methods: We investigate the time-course of word recognition as simulated by the model using a gating paradigm to test whether its recognition is affected by well-known word-competition effects in human speech processing. We furthermore investigate whether vector quantisation, a technique for discrete representation learning, aids the model in the discovery and recognition of words. 
> Results/Conclusion: Our experiments show that the model is able to recognise nouns in isolation and even learns to properly differentiate between plural and singular nouns. We also find that recognition is influenced by word competition from the word-initial cohort and neighbourhood density, mirroring word competition effects in human speech comprehension. Lastly, we find no evidence that vector quantisation is helpful in discovering and recognising words. Our gating experiments even show that the vector quantised model requires more of the input sequence for correct recognition.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.06937](https://arxiv.org/abs/2203.06937) [cs.CL]** |
|           | (or **[arXiv:2203.06937v1](https://arxiv.org/abs/2203.06937v1) [cs.CL]** for this version) |





<h2 id="2022-03-15-7">7. Interpretability for Language Learners Using Example-Based Grammatical Error Correction
</h2>

Title: [Interpretability for Language Learners Using Example-Based Grammatical Error Correction](https://arxiv.org/abs/2203.07085)

Authors: [Masahiro Kaneko](https://arxiv.org/search/cs?searchtype=author&query=Kaneko%2C+M), [Sho Takase](https://arxiv.org/search/cs?searchtype=author&query=Takase%2C+S), [Ayana Niwa](https://arxiv.org/search/cs?searchtype=author&query=Niwa%2C+A), [Naoaki Okazaki](https://arxiv.org/search/cs?searchtype=author&query=Okazaki%2C+N)

> Grammatical Error Correction (GEC) should not focus only on high accuracy of corrections but also on interpretability for language learning. However, existing neural-based GEC models mainly aim at improving accuracy, and their interpretability has not been explored. A promising approach for improving interpretability is an example-based method, which uses similar retrieved examples to generate corrections. In addition, examples are beneficial in language learning, helping learners understand the basis of grammatically incorrect/correct texts and improve their confidence in writing. Therefore, we hypothesize that incorporating an example-based method into GEC can improve interpretability as well as support language learners. In this study, we introduce an Example-Based GEC (EB-GEC) that presents examples to language learners as a basis for a correction result. The examples consist of pairs of correct and incorrect sentences similar to a given input and its predicted correction. Experiments demonstrate that the examples presented by EB-GEC help language learners decide to accept or refuse suggestions from the GEC output. Furthermore, the experiments also show that retrieved examples improve the accuracy of corrections.

| Comments: | ACL 2022                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.07085](https://arxiv.org/abs/2203.07085) [cs.CL]** |
|           | (or **[arXiv:2203.07085v1](https://arxiv.org/abs/2203.07085v1) [cs.CL]** for this version) |





<h2 id="2022-03-15-8">8. Interpretable Dysarthric Speaker Adaptation based on Optimal-Transport
</h2>

Title: [Interpretable Dysarthric Speaker Adaptation based on Optimal-Transport](https://arxiv.org/abs/2203.07143)

Authors: [Rosanna Turrisi](https://arxiv.org/search/cs?searchtype=author&query=Turrisi%2C+R), [Leonardo Badino](https://arxiv.org/search/cs?searchtype=author&query=Badino%2C+L)

> This work addresses the mismatch problem between the distribution of training data (source) and testing data (target), in the challenging context of dysarthric speech recognition. We focus on Speaker Adaptation (SA) in command speech recognition, where data from multiple sources (i.e., multiple speakers) are available. Specifically, we propose an unsupervised Multi-Source Domain Adaptation (MSDA) algorithm based on optimal-transport, called MSDA via Weighted Joint Optimal Transport (MSDA-WJDOT). We achieve a Command Error Rate relative reduction of 16% and 7% over the speaker-independent model and the best competitor method, respectively. The strength of the proposed approach is that, differently from any other existing SA method, it offers an interpretable model that can also be exploited, in this context, to diagnose dysarthria without any specific training. Indeed, it provides a closeness measure between the target and the source speakers, reflecting their similarity in terms of speech characteristics. Based on the similarity between the target speaker and the healthy/dysarthric source speakers, we then define the healthy/dysarthric score of the target speaker that we leverage to perform dysarthria detection. This approach does not require any additional training and achieves a 95% accuracy in the dysarthria diagnosis.

| Comments: | submitted to interspeech 2022                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2203.07143](https://arxiv.org/abs/2203.07143) [cs.CL]** |
|           | (or **[arXiv:2203.07143v1](https://arxiv.org/abs/2203.07143v1) [cs.CL]** for this version) |





<h2 id="2022-03-15-9">9. RED-ACE: Robust Error Detection for ASR using Confidence Embeddings
</h2>

Title: [RED-ACE: Robust Error Detection for ASR using Confidence Embeddings](https://arxiv.org/abs/2203.07172)

Authors: [Zorik Gekhman](https://arxiv.org/search/cs?searchtype=author&query=Gekhman%2C+Z), [Dina Zverinski](https://arxiv.org/search/cs?searchtype=author&query=Zverinski%2C+D), [Jonathan Mallinson](https://arxiv.org/search/cs?searchtype=author&query=Mallinson%2C+J), [Genady Beryozkin](https://arxiv.org/search/cs?searchtype=author&query=Beryozkin%2C+G)

> ASR Error Detection (AED) models aim to post-process the output of Automatic Speech Recognition (ASR) systems, in order to detect transcription errors. Modern approaches usually use text-based input, comprised solely of the ASR transcription hypothesis, disregarding additional signals from the ASR model. Instead, we propose to utilize the ASR system's word-level confidence scores for improving AED performance. Specifically, we add an ASR Confidence Embedding (ACE) layer to the AED model's encoder, allowing us to jointly encode the confidence scores and the transcribed text into a contextualized representation. Our experiments show the benefits of ASR confidence scores for AED, their complementary effect over the textual signal, as well as the effectiveness and robustness of ACE for combining these signals. To foster further research, we publish a novel AED dataset consisting of ASR outputs on the LibriSpeech corpus with annotated transcription errors.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.07172](https://arxiv.org/abs/2203.07172) [cs.CL]** |
|           | (or **[arXiv:2203.07172v1](https://arxiv.org/abs/2203.07172v1) [cs.CL]** for this version) |





# 2022-03-14

[Return to Index](#Index)



<h2 id="2022-03-14-1">1. A new approach to calculating BERTScore for automatic assessment of translation quality
</h2>

Title: [A new approach to calculating BERTScore for automatic assessment of translation quality](https://arxiv.org/abs/2203.05598)

Authors: [A.A. Vetrov](https://arxiv.org/search/cs?searchtype=author&query=Vetrov%2C+A), [E.A. Gorn](https://arxiv.org/search/cs?searchtype=author&query=Gorn%2C+E)

> The study of the applicability of the BERTScore metric was conducted to translation quality assessment at the sentence level for English -> Russian direction. Experiments were performed with a pre-trained multilingual BERT as well as with a pair of monolingual BERT models. To align the monolingual embeddings, an orthogonal transformation based on anchor tokens was used. It was demonstrated that such transformation helps to prevent mismatching issue and shown that this approach gives better results than using embeddings of the multilingual model. To improve the token matching process it is proposed to combine all incomplete WorkPiece tokens into meaningful words and use simple averaging of corresponding vectors and to calculate BERTScore based on anchor tokens only. Such modifications allowed us to achieve a better correlation of the model predictions with human estimates. In addition to evaluating machine translation, several versions of human translation were evaluated as well, the problems of this approach were listed.

| Comments:    | 8 pages, 4 figures                                           |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2203.05598](https://arxiv.org/abs/2203.05598) [cs.CL]** |
|              | (or **[arXiv:2203.05598v1](https://arxiv.org/abs/2203.05598v1) [cs.CL]** for this version) |
|              | https://doi.org/10.48550/arXiv.2203.05598Focus to learn more |










# 2022-03-11

[Return to Index](#Index)



<h2 id="2022-03-11-1">1. NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks
</h2>

Title: [NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks](https://arxiv.org/abs/2203.05081)

Authors: [Fawaz Sammani](https://arxiv.org/search/cs?searchtype=author&query=Sammani%2C+F), [Tanmoy Mukherjee](https://arxiv.org/search/cs?searchtype=author&query=Mukherjee%2C+T), [Nikos Deligiannis](https://arxiv.org/search/cs?searchtype=author&query=Deligiannis%2C+N)

> Natural language explanation (NLE) models aim at explaining the decision-making process of a black box system via generating natural language sentences which are human-friendly, high-level and fine-grained. Current NLE models explain the decision-making process of a vision or vision-language model (a.k.a., task model), e.g., a VQA model, via a language model (a.k.a., explanation model), e.g., GPT. Other than the additional memory resources and inference time required by the task model, the task and explanation models are completely independent, which disassociates the explanation from the reasoning process made to predict the answer. We introduce NLX-GPT, a general, compact and faithful language model that can simultaneously predict an answer and explain it. We first conduct pre-training on large scale data of image-caption pairs for general understanding of images, and then formulate the answer as a text prediction task along with the explanation. Without region proposals nor a task model, our resulting overall framework attains better evaluation scores, contains much less parameters and is 15× faster than the current SoA model. We then address the problem of evaluating the explanations which can be in many times generic, data-biased and can come in several forms. We therefore design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based attack, a self-evaluation framework that requires no labels. Code is at: [this https URL](https://github.com/fawazsammani/nlxgpt).

| Comments: | Accepted to CVPR 2022                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2203.05081](https://arxiv.org/abs/2203.05081) [cs.CV]** |
|           | (or **[arXiv:2203.05081v1](https://arxiv.org/abs/2203.05081v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.05081Focus to learn more |





<h2 id="2022-03-11-2">2. Conditional Prompt Learning for Vision-Language Models
</h2>

Title: [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557)

Authors: [Kaiyang Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+K), [Jingkang Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Chen Change Loy](https://arxiv.org/search/cs?searchtype=author&query=Loy%2C+C+C), [Ziwei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z)

> With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning -- a recent trend in NLP -- to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at [this https URL](https://github.com/KaiyangZhou/CoOp).

| Comments: | CVPR 2022. TL;DR: We propose a conditional prompt learning approach to solve the generalizability issue of static prompts |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.05557](https://arxiv.org/abs/2203.05557) [cs.CV]** |
|           | (or **[arXiv:2203.05557v1](https://arxiv.org/abs/2203.05557v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.05557Focus to learn more |





<h2 id="2022-03-11-3">3. Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods
</h2>

Title: [Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods](https://arxiv.org/abs/2203.05227)

Authors: [Wei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W), [Wenhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+W), [Moye Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Jiachen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Xinyan Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+X), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H)

> Natural Language Generation (NLG) has made great progress in recent years due to the development of deep learning techniques such as pre-trained language models. This advancement has resulted in more fluent, coherent and even properties controllable (e.g. stylistic, sentiment, length etc.) generation, naturally leading to development in downstream tasks such as abstractive summarization, dialogue generation, machine translation, and data-to-text generation. However, the faithfulness problem that the generated text usually contains unfaithful or non-factual information has become the biggest challenge, which makes the performance of text generation unsatisfactory for practical applications in many real-world scenarios. Many studies on analysis, evaluation, and optimization methods for faithfulness problems have been proposed for various tasks, but have not been organized, compared and discussed in a combined manner. In this survey, we provide a systematic overview of the research progress on the faithfulness problem of NLG, including problem analysis, evaluation metrics and optimization methods. We organize the evaluation and optimization methods for different tasks into a unified taxonomy to facilitate comparison and learning across tasks. Several research trends are discussed further.

| Comments: | The first version                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.05227](https://arxiv.org/abs/2203.05227) [cs.CL]** |
|           | (or **[arXiv:2203.05227v1](https://arxiv.org/abs/2203.05227v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.05227Focus to learn more |





<h2 id="2022-03-11-4">4. Look Backward and Forward: Self-Knowledge Distillation with Bidirectional Decoder for Neural Machine Translation
</h2>

Title: [Look Backward and Forward: Self-Knowledge Distillation with Bidirectional Decoder for Neural Machine Translation](https://arxiv.org/abs/2203.05248)

Authors: [Xuanwei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Libin Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+L), [Disheng Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+D), [Liang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Yanjun Miao](https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+Y)

> Neural Machine Translation(NMT) models are usually trained via unidirectional decoder which corresponds to optimizing one-step-ahead prediction. However, this kind of unidirectional decoding framework may incline to focus on local structure rather than global coherence. To alleviate this problem, we propose a novel method, Self-Knowledge Distillation with Bidirectional Decoder for Neural Machine Translation(SBD-NMT). We deploy a backward decoder which can act as an effective regularization method to the forward decoder. By leveraging the backward decoder's information about the longer-term future, distilling knowledge learned in the backward decoder can encourage auto-regressive NMT models to plan ahead. Experiments show that our method is significantly better than the strong Transformer baselines on multiple machine translation data sets. Our codes will be released on github soon.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.05248](https://arxiv.org/abs/2203.05248) [cs.CL]** |
|           | (or **[arXiv:2203.05248v1](https://arxiv.org/abs/2203.05248v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.05248Focus to learn more |







# 2022-03-10

[Return to Index](#Index)



<h2 id="2022-03-10-1">1. Efficient Sub-structured Knowledge Distillation
</h2>

Title: [Efficient Sub-structured Knowledge Distillation](https://arxiv.org/abs/2203.04825)

Authors: [Wenye Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+W), [Yangming Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Hai-tao Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H)

> Structured prediction models aim at solving a type of problem where the output is a complex structure, rather than a single variable. Performing knowledge distillation for such models is not trivial due to their exponentially large output space. In this work, we propose an approach that is much simpler in its formulation and far more efficient for training than existing approaches. Specifically, we transfer the knowledge from a teacher model to its student model by locally matching their predictions on all sub-structures, instead of the whole output space. In this manner, we avoid adopting some time-consuming techniques like dynamic programming (DP) for decoding output structures, which permits parallel computation and makes the training process even faster in practice. Besides, it encourages the student model to better mimic the internal behavior of the teacher model. Experiments on two structured prediction tasks demonstrate that our approach outperforms previous methods and halves the time cost for one training epoch.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.04825](https://arxiv.org/abs/2203.04825) [cs.LG]** |
|           | (or **[arXiv:2203.04825v1](https://arxiv.org/abs/2203.04825v1) [cs.LG]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.04825Focus to learn more |





<h2 id="2022-03-10-2">2. Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning
</h2>

Title: [Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning](https://arxiv.org/abs/2203.04904)

Authors: [Zhenhailong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Hang Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Manling Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M), [Han Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Heng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H)

> Despite achieving state-of-the-art zero-shot performance, existing vision-language models, e.g., CLIP, still fall short of domain-specific classification tasks, e.g., Fungi Classification. In the context of few-shot transfer learning, traditional fine-tuning fails to prevent highly expressive model from exploiting spurious correlations in the training data. On the other hand, although model-agnostic meta-learning (MAML) presents as a natural alternative for transfer learning, the expensive computation due to implicit second-order optimization limits its use in large-scale models and datasets. In this work we aim to further improve the generalization of existing vision-language models on unseen tasks via a simple yet efficient fine-tuning strategy based on uniform task sampling. We term our method as Model-Agnostic Multitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level optimization and uses only first-order gradients, which makes it easily scalable and computationally efficient. Due to the uniform task sampling procedure, MAMF consistently outperforms the classical fine-tuning method for few-shot transfer learning on five benchmark datasets. Empirically, we further discover that the effectiveness of first-order MAML is highly dependent on the zero-shot performance of the pretrained model, and our simple algorithm can outperform first-order MAML on more challenging datasets with low zero-shot performance.

| Comments: | 7 pages, 6 figures, under review                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Multimedia (cs.MM)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2203.04904](https://arxiv.org/abs/2203.04904) [cs.MM]** |
|           | (or **[arXiv:2203.04904v1](https://arxiv.org/abs/2203.04904v1) [cs.MM]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.04904Focus to learn more |





<h2 id="2022-03-10-3">3. Pose Guided Multi-person Image Generation From Text
</h2>

Title: [Pose Guided Multi-person Image Generation From Text](https://arxiv.org/abs/2203.04907)

Authors: [Soon Yau Cheong](https://arxiv.org/search/cs?searchtype=author&query=Cheong%2C+S+Y), [Armin Mustafa](https://arxiv.org/search/cs?searchtype=author&query=Mustafa%2C+A), [Andrew Gilbert](https://arxiv.org/search/cs?searchtype=author&query=Gilbert%2C+A)

> Transformers have recently been shown to generate high quality images from texts. However, existing methods struggle to create high fidelity full-body images, especially multiple people. A person's pose has a high degree of freedom that is difficult to describe using words only; this creates errors in the generated image, such as incorrect body proportions and pose. We propose a pose-guided text-to-image model, using pose as an additional input constraint. Using the proposed Keypoint Pose Encoding (KPE) to encode human pose into low dimensional representation, our model can generate novel multi-person images accurately representing the pose and text descriptions provided, with minimal errors. We demonstrate that KPE is invariant to changes in the target image domain and image resolution; we show results on the Deepfashion dataset and create a new multi-person Deepfashion dataset to demonstrate the multi-capabilities of our approach.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.04907](https://arxiv.org/abs/2203.04907) [cs.CV]** |
|           | (or **[arXiv:2203.04907v1](https://arxiv.org/abs/2203.04907v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.04907Focus to learn more |





<h2 id="2022-03-10-4">4. Onception: Active Learning with Expert Advice for Real World Machine Translation
</h2>

Title: [Onception: Active Learning with Expert Advice for Real World Machine Translation](https://arxiv.org/abs/2203.04507)

Authors: [Vânia Mendonça](https://arxiv.org/search/cs?searchtype=author&query=Mendonça%2C+V) (1 and 2), [Ricardo Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+R) (1 and 2 and 3), [Luisa Coheur](https://arxiv.org/search/cs?searchtype=author&query=Coheur%2C+L) (1 and 2), [Alberto Sardinha](https://arxiv.org/search/cs?searchtype=author&query=Sardinha%2C+A) (1 and 2) ((1) INESC-ID Lisboa, (2) Instituto Superior Técnico, (3) Unbabel AI)

> Active learning can play an important role in low-resource settings (i.e., where annotated data is scarce), by selecting which instances may be more worthy to annotate. Most active learning approaches for Machine Translation assume the existence of a pool of sentences in a source language, and rely on human annotators to provide translations or post-edits, which can still be costly. In this paper, we assume a real world human-in-the-loop scenario in which: (i) the source sentences may not be readily available, but instead arrive in a stream; (ii) the automatic translations receive feedback in the form of a rating, instead of a correct/edited translation, since the human-in-the-loop might be a user looking for a translation, but not be able to provide one. To tackle the challenge of deciding whether each incoming pair source-translations is worthy to query for human feedback, we resort to a number of stream-based active learning query strategies. Moreover, since we not know in advance which query strategy will be the most adequate for a certain language pair and set of Machine Translation models, we propose to dynamically combine multiple strategies using prediction with expert advice. Our experiments show that using active learning allows to converge to the best Machine Translation systems with fewer human interactions. Furthermore, combining multiple strategies using prediction with expert advice often outperforms several individual active learning strategies with even fewer interactions.

| Comments: | Submitted to Machine Translation                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.04507](https://arxiv.org/abs/2203.04507) [cs.CL]** |
|           | (or **[arXiv:2203.04507v1](https://arxiv.org/abs/2203.04507v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.04507Focus to learn more |










# 2022-03-09

[Return to Index](#Index)



<h2 id="2022-03-09-1">1. Multi-Modal Mixup for Robust Fine-tuning
</h2>

Title: [Multi-Modal Mixup for Robust Fine-tuning](https://arxiv.org/abs/2203.03897)

Authors: [Junhyuk So](https://arxiv.org/search/cs?searchtype=author&query=So%2C+J), [Changdae Oh](https://arxiv.org/search/cs?searchtype=author&query=Oh%2C+C), [Minchul Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+M), [Kyungwoo Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+K)

> Pre-trained large-scale models provide a transferable embedding, and they show comparable performance on the diverse downstream task. However, the transferability of multi-modal learning is restricted, and the analysis of learned embedding has not been explored well. This paper provides a perspective to understand the multi-modal embedding in terms of uniformity and alignment. We newly find that the representation learned by multi-modal learning models such as CLIP has a two separated representation space for each heterogeneous dataset with less alignment. Besides, there are unexplored large intermediate areas between two modalities with less uniformity. Less robust embedding might restrict the transferability of the representation for the downstream task. This paper provides a new end-to-end fine-tuning method for robust representation that encourages better uniformity and alignment score. First, we propose a multi-modal Mixup, m2-Mix that mixes the representation of image and text to generate the hard negative samples. Second, we fine-tune the multi-modal model on a hard negative sample as well as normal negative and positive samples with contrastive learning. Our multi-modal Mixup provides a robust representation, and we validate our methods on classification, retrieval, and structure-awareness task.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.03897](https://arxiv.org/abs/2203.03897) [cs.CV]** |
|           | (or **[arXiv:2203.03897v1](https://arxiv.org/abs/2203.03897v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.03897Focus to learn more |







<h2 id="2022-03-09-2">2. IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation
</h2>

Title: [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759)

Authors: [Gabriele Sarti](https://arxiv.org/search/cs?searchtype=author&query=Sarti%2C+G), [Malvina Nissim](https://arxiv.org/search/cs?searchtype=author&query=Nissim%2C+M)

> The T5 model and its unified text-to-text paradigm contributed in advancing the state-of-the-art for many natural language processing tasks. While some multilingual variants of the T5 model have recently been introduced, their performances were found to provide suboptimal performances for languages other than English if compared to monolingual variants. We are motivated by these findings to introduce IT5, the first family of encoder-decoder transformer models pretrained specifically on Italian. We perform a thorough cleaning of a web-crawled Italian corpus including more than 40 billion words and use it to pretrain three IT5 models of different sizes. The performance of IT5 models and their multilingual counterparts is then evaluated on a broad range of natural language understanding and generation benchmarks for Italian. We find the monolingual IT5 models to provide the best scale-to-performance ratio across tested models, consistently outperforming their multilingual counterparts and setting a new state-of-the-art for most Italian conditional language generation tasks.

| Comments: | 13 pages, 7 tables, 1 figure. Code and checkpoints available: [this https URL](https://github.com/gsarti/it5) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.03759](https://arxiv.org/abs/2203.03759) [cs.CL]** |
|           | (or **[arXiv:2203.03759v1](https://arxiv.org/abs/2203.03759v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.03759Focus to learn more |





<h2 id="2022-03-09-3">3. UniXcoder: Unified Cross-Modal Pre-training for Code Representation
</h2>

Title: [UniXcoder: Unified Cross-Modal Pre-training for Code Representation](https://arxiv.org/abs/2203.03850)

Authors: [Daya Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+D), [Shuai Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+S), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Yanlin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M), [Jian Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+J)

> Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.

| Comments: | Published in ACL 2022                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Programming Languages (cs.PL); Software Engineering (cs.SE) |
| Cite as:  | **[arXiv:2203.03850](https://arxiv.org/abs/2203.03850) [cs.CL]** |
|           | (or **[arXiv:2203.03850v1](https://arxiv.org/abs/2203.03850v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.03850Focus to learn more |





<h2 id="2022-03-09-4">4. HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks
</h2>

Title: [HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks](https://arxiv.org/abs/2203.03878)

Authors: [Zhengkun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Wenya Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+W), [Xiaojun Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+X), [Yasheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Yadao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Zhenglu Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z)

> The workflow of pretraining and fine-tuning has emerged as a popular paradigm for solving various NLP and V&L (Vision-and-Language) downstream tasks. With the capacity of pretrained models growing rapidly, how to perform parameter-efficient fine-tuning has become fairly important for quick transfer learning and deployment. In this paper, we design a novel unified parameter-efficient transfer learning framework that works effectively on both pure language and V&L tasks. In particular, we use a shared hypernetwork that takes trainable hyper-embeddings as input, and outputs weights for fine-tuning different small modules in a pretrained language model, such as tuning the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning). We define a set of embeddings (e.g., layer, block, task and visual embeddings) as the key components to calculate hyper-embeddings, which thus can support both pure language and V&L tasks. Our proposed framework adds fewer trainable parameters in multi-task learning while achieving superior performances and transfer ability compared to state-of-the-art methods. Empirical results on the GLUE benchmark and multiple V&L tasks confirm the effectiveness of our framework on both textual and visual modalities.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.03878](https://arxiv.org/abs/2203.03878) [cs.CL]** |
|           | (or **[arXiv:2203.03878v1](https://arxiv.org/abs/2203.03878v1) [cs.CL]** for this version) |





<h2 id="2022-03-09-5">5. Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation
</h2>

Title: [Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation](https://arxiv.org/abs/2203.03910)

Authors: [Chenze Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+C), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y)

> Neural networks tend to gradually forget the previously learned knowledge when learning multiple tasks sequentially from dynamic data distributions. This problem is called \textit{catastrophic forgetting}, which is a fundamental challenge in the continual learning of neural networks. In this work, we observe that catastrophic forgetting not only occurs in continual learning but also affects the traditional static training. Neural networks, especially neural machine translation models, suffer from catastrophic forgetting even if they learn from a static training set. To be specific, the final model pays imbalanced attention to training samples, where recently exposed samples attract more attention than earlier samples. The underlying cause is that training samples do not get balanced training in each model update, so we name this problem \textit{imbalanced training}. To alleviate this problem, we propose Complementary Online Knowledge Distillation (COKD), which uses dynamically updated teacher models trained on specific data orders to iteratively provide complementary knowledge to the student model. Experimental results on multiple machine translation tasks show that our method successfully alleviates the problem of imbalanced training and achieves substantial improvements over strong baseline systems.

| Comments:    | ACL 2022 main conference                                     |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2203.03910](https://arxiv.org/abs/2203.03910) [cs.CL]** |
|              | (or **[arXiv:2203.03910v1](https://arxiv.org/abs/2203.03910v1) [cs.CL]** for this version) |





<h2 id="2022-03-09-6">6. Adaptr: Objective-Centric Adaptation Framework for Language Models
</h2>

Title: [Adaptr: Objective-Centric Adaptation Framework for Language Models](https://arxiv.org/abs/2203.03989)

Authors: [Michal Štefánik](https://arxiv.org/search/cs?searchtype=author&query=Štefánik%2C+M), [Vít Novotný](https://arxiv.org/search/cs?searchtype=author&query=Novotný%2C+V), [Nikola Groverová](https://arxiv.org/search/cs?searchtype=author&query=Groverová%2C+N), [Petr Sojka](https://arxiv.org/search/cs?searchtype=author&query=Sojka%2C+P)

> Progress in natural language processing research is catalyzed by the possibilities given by the widespread software frameworks. This paper introduces Adaptor library that transposes the traditional model-centric approach composed of pre-training + fine-tuning steps to objective-centric approach, composing the training process by applications of selected objectives. We survey research directions that can benefit from enhanced objective-centric experimentation in multitask training, custom objectives development, dynamic training curricula, or domain adaptation. Adaptor aims to ease reproducibility of these research directions in practice. Finally, we demonstrate the practical applicability of Adaptor in selected unsupervised domain adaptation scenarios.

| Comments: | 60th Annual Meeting of the ACL (ACL 2022): System Demonstrations paper |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.03989](https://arxiv.org/abs/2203.03989) [cs.CL]** |
|           | (or **[arXiv:2203.03989v1](https://arxiv.org/abs/2203.03989v1) [cs.CL]** for this version) |



# 2022-03-08

[Return to Index](#Index)



<h2 id="2022-03-08-1">1. OCR quality affects perceived usefulness of historical newspaper clippings -- a user study
</h2>

Title: [OCR quality affects perceived usefulness of historical newspaper clippings -- a user study](https://arxiv.org/abs/2203.03557)
Authors: [Kimmo Kettunen](https://arxiv.org/search/cs?searchtype=author&query=Kettunen%2C+K), [Heikki Keskustalo](https://arxiv.org/search/cs?searchtype=author&query=Keskustalo%2C+H), [Sanna Kumpulainen](https://arxiv.org/search/cs?searchtype=author&query=Kumpulainen%2C+S), [Tuula Pääkkönen](https://arxiv.org/search/cs?searchtype=author&query=Pääkkönen%2C+T), [Juha Rautiainen](https://arxiv.org/search/cs?searchtype=author&query=Rautiainen%2C+J)

> Effects of Optical Character Recognition (OCR) quality on historical information retrieval have so far been studied in data-oriented scenarios regarding the effectiveness of retrieval results. Such studies have either focused on the effects of artificially degraded OCR quality (see, e.g., [1-2]) or utilized test collections containing texts based on authentic low quality OCR data (see, e.g., [3]). In this paper the effects of OCR quality are studied in a user-oriented information retrieval setting. Thirty-two users evaluated subjectively query results of six topics each (out of 30 topics) based on pre-formulated queries using a simulated work task setting. To the best of our knowledge our simulated work task experiment is the first one showing empirically that users' subjective relevance assessments of retrieved documents are affected by a change in the quality of optically read text. Users of historical newspaper collections have so far commented effects of OCR'ed data quality mainly in impressionistic ways, and controlled user environments for studying effects of OCR quality on users' relevance assessments of the retrieval results have so far been missing. To remedy this The National Library of Finland (NLF) set up an experimental query environment for the contents of one Finnish historical newspaper, Uusi Suometar 1869-1918, to be able to compare users' evaluation of search results of two different OCR qualities for digitized newspaper articles. The query interface was able to present the same underlying document for the user based on two alternatives: either based on the lower OCR quality, or based on the higher OCR quality, and the choice was randomized. The users did not know about quality differences in the article texts they evaluated. The main result of the study is that improved optical character recognition quality affects perceived usefulness of historical newspaper articles significantly. The mean average evaluation score for the improved OCR results was 7.94% higher than the mean average evaluation score of the old OCR results.

| Comments: | IRCDL2022                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL); Digital Libraries (cs.DL) |
| Cite as:  | **[arXiv:2203.03557](https://arxiv.org/abs/2203.03557) [cs.IR]** |
|           | (or **[arXiv:2203.03557v1](https://arxiv.org/abs/2203.03557v1) [cs.IR]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.03557Focus to learn more |





<h2 id="2022-03-08-2">2. Focus on the Target's Vocabulary: Masked Label Smoothing for Machine Translation
</h2>

Title: [Focus on the Target's Vocabulary: Masked Label Smoothing for Machine Translation](https://arxiv.org/abs/2203.02889)
Authors: [Liang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L), [Runxin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+R), [Baobao Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+B)

> Label smoothing and vocabulary sharing are two widely used techniques in neural machine translation models. However, we argue that simply applying both techniques can be conflicting and even leads to sub-optimal performance. When allocating smoothed probability, original label smoothing treats the source-side words that would never appear in the target language equally to the real target-side words, which could bias the translation model. To address this issue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the soft label probability of source-side words to zero. Simple yet effective, MLS manages to better integrate label smoothing with vocabulary sharing. Our extensive experiments show that MLS consistently yields improvement over original label smoothing on different datasets, including bilingual and multilingual translation from both translation quality and model's calibration. Our code is released at [this https URL](https://github.com/PKUnlp-icler/MLS)

| Comments: | ACL 2022 Main Conference, released at [this https URL](https://github.com/PKUnlp-icler/MLS) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2203.02889](https://arxiv.org/abs/2203.02889) [cs.CL]** |
|           | (or **[arXiv:2203.02889v1](https://arxiv.org/abs/2203.02889v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.02889Focus to learn more |





<h2 id="2022-03-08-3">3. Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation
</h2>

Title: [Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation](https://arxiv.org/abs/2203.02951)
Authors: [Songming Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Yijin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yufeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Jinan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Jian Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Token-level adaptive training approaches can alleviate the token imbalance problem and thus improve neural machine translation, through re-weighting the losses of different target tokens based on specific statistical metrics (e.g., token frequency or mutual information). Given that standard translation models make predictions on the condition of previous target contexts, we argue that the above statistical metrics ignore target context information and may assign inappropriate weights to target tokens. While one possible solution is to directly take target contexts into these statistical metrics, the target-context-aware statistical computing is extremely expensive, and the corresponding storage overhead is unrealistic. To solve the above issues, we propose a target-context-aware metric, named conditional bilingual mutual information (CBMI), which makes it feasible to supplement target context information for statistical metrics. Particularly, our CBMI can be formalized as the log quotient of the translation model probability and language model probability by decomposing the conditional joint distribution. Thus CBMI can be efficiently calculated during model training without any pre-specific statistical calculations and large storage overhead. Furthermore, we propose an effective adaptive training approach based on both the token- and sentence-level CBMI. Experimental results on WMT14 English-German and WMT19 Chinese-English tasks show our approach can significantly outperform the Transformer baseline and other related methods.

| Comments: | Accepted at ACL 2022 as a long paper of main conference. The code is available at: [this https URL](https://github.com/songmzhang/CBMI) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.02951](https://arxiv.org/abs/2203.02951) [cs.CL]** |
|           | (or **[arXiv:2203.02951v1](https://arxiv.org/abs/2203.02951v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.02951Focus to learn more |





<h2 id="2022-03-08-4">4. Recent Advances in Neural Text Generation: A Task-Agnostic Survey
</h2>

Title: [Recent Advances in Neural Text Generation: A Task-Agnostic Survey](https://arxiv.org/abs/2203.03047)
Authors: [Chen Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+C), [Frank Guerin](https://arxiv.org/search/cs?searchtype=author&query=Guerin%2C+F), [Yucheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Chenghua Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+C)

> In recent years much effort has been devoted to applying neural models to the task of natural language generation. The challenge is to generate natural human-like text, and to control the generation process. This paper presents a task-agnostic survey of recent advances in neural text generation. These advances have been achieved by numerous developments, which we group under the following four headings: data construction, neural frameworks, training and inference strategies, and evaluation metrics. Finally we discuss the future directions for the development of neural text generation including neural pipelines and exploiting back-ground knowledge.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.03047](https://arxiv.org/abs/2203.03047) [cs.CL]** |
|           | (or **[arXiv:2203.03047v1](https://arxiv.org/abs/2203.03047v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.03047Focus to learn more |





<h2 id="2022-03-08-5">5. Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models
</h2>

Title: [Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models](https://arxiv.org/abs/2203.03131)
Authors: [Shengnan An](https://arxiv.org/search/cs?searchtype=author&query=An%2C+S), [Yifei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Zeqi Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Qian Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Bei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Qiang Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Q), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Nanning Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+N), [Jian-Guang Lou](https://arxiv.org/search/cs?searchtype=author&query=Lou%2C+J)

> Recently the prompt-tuning paradigm has attracted significant attention. By only tuning continuous prompts with a frozen pre-trained language model (PLM), prompt-tuning takes a step towards deploying a shared frozen PLM to serve numerous downstream tasks. Although prompt-tuning shows good performance on certain natural language understanding (NLU) tasks, its effectiveness on natural language generation (NLG) tasks is still under-explored. In this paper, we argue that one of the factors hindering the development of prompt-tuning on NLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different from the pretraining corpus). For example, our preliminary exploration reveals a large performance gap between prompt-tuning and fine-tuning when unfamiliar inputs occur frequently in NLG tasks. This motivates us to propose input-tuning, which fine-tunes both the continuous prompts and the input representations, leading to a more effective way to adapt unfamiliar inputs to frozen PLMs. Our proposed input-tuning is conceptually simple and empirically powerful. Experimental results on seven NLG tasks demonstrate that input-tuning is significantly and consistently better than prompt-tuning. Furthermore, on three of these tasks, input-tuning can achieve a comparable or even better performance than fine-tuning.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.03131](https://arxiv.org/abs/2203.03131) [cs.CL]** |
|           | (or **[arXiv:2203.03131v1](https://arxiv.org/abs/2203.03131v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.03131Focus to learn more |





<h2 id="2022-03-08-6">6. One Model, Multiple Tasks: Pathways for Natural Language Understanding
</h2>

Title: [One Model, Multiple Tasks: Pathways for Natural Language Understanding](https://arxiv.org/abs/2203.03312)
Authors: [Duyu Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D), [Fan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+F), [Yong Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+Y), [Cong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Shuangzhi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> This paper presents a Pathways approach to handle many tasks at once. Our approach is general-purpose and sparse. Unlike prevailing single-purpose models that overspecialize at individual tasks and learn from scratch when being extended to new tasks, our approach is general-purpose with the ability of stitching together existing skills to learn new tasks more effectively. Different from traditional dense models that always activate all the model parameters, our approach is sparsely activated: only relevant parts of the model (like pathways through the network) are activated. 
> We take natural language understanding as a case study and define a set of skills like \textit{the skill of understanding the sentiment of text} and \textit{the skill of understanding natural language questions}. These skills can be reused and combined to support many different tasks and situations. We develop our system using Transformer as the backbone. For each skill, we implement skill-specific feed-forward networks, which are activated only if the skill is relevant to the task. An appealing feature of our model is that it not only supports sparsely activated fine-tuning, but also allows us to pretrain skills in the same sparse way with masked language modeling and next sentence prediction. We call this model \textbf{SkillNet}. 
> We have three major findings. First, with only one model checkpoint, SkillNet performs better than task-specific fine-tuning and two multi-task learning baselines (i.e., dense model and Mixture-of-Experts model) on six tasks. Second, sparsely activated pre-training further improves the overall performance. Third, SkillNet significantly outperforms baseline systems when being extended to new tasks.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.03312](https://arxiv.org/abs/2203.03312) [cs.CL]** |
|           | (or **[arXiv:2203.03312v1](https://arxiv.org/abs/2203.03312v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.03312Focus to learn more |







# 2022-03-07

[Return to Index](#Index)



<h2 id="2022-03-07-1">1. Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages
</h2>

Title: [Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages](https://arxiv.org/abs/2203.01976)

Authors: [Vaidehi Patil](https://arxiv.org/search/cs?searchtype=author&query=Patil%2C+V), [Partha Talukdar](https://arxiv.org/search/cs?searchtype=author&query=Talukdar%2C+P), [Sunita Sarawagi](https://arxiv.org/search/cs?searchtype=author&query=Sarawagi%2C+S)

> Pre-trained multilingual language models such as mBERT and XLM-R have demonstrated great potential for zero-shot cross-lingual transfer to low web-resource languages (LRL). However, due to limited model capacity, the large difference in the sizes of available monolingual corpora between high web-resource languages (HRL) and LRLs does not provide enough scope of co-embedding the LRL with the HRL, thereby affecting downstream task performance of LRLs. In this paper, we argue that relatedness among languages in a language family along the dimension of lexical overlap may be leveraged to overcome some of the corpora limitations of LRLs. We propose Overlap BPE (OBPE), a simple yet effective modification to the BPE vocabulary generation algorithm which enhances overlap across related languages. Through extensive experiments on multiple NLP tasks and datasets, we observe that OBPE generates a vocabulary that increases the representation of LRLs via tokens shared with HRLs. This results in improved zero-shot transfer from related HRLs to LRLs without reducing HRL representation and accuracy. Unlike previous studies that dismissed the importance of token-overlap, we show that in the low-resource related language setting, token overlap matters. Synthetically reducing the overlap to zero can cause as much as a four-fold drop in zero-shot transfer accuracy.

| Comments: | Accepted to appear at the ACL 2022 Main conference           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.01976](https://arxiv.org/abs/2203.01976) [cs.CL]** |
|           | (or **[arXiv:2203.01976v1](https://arxiv.org/abs/2203.01976v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.01976Focus to learn more |





<h2 id="2022-03-07-2">2. Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning
</h2>

Title: [Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning](https://arxiv.org/abs/2203.02053)

Authors: [Weixin Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+W), [Yuhui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Yongchan Kwon](https://arxiv.org/search/cs?searchtype=author&query=Kwon%2C+Y), [Serena Yeung](https://arxiv.org/search/cs?searchtype=author&query=Yeung%2C+S), [James Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+J)

> We present modality gap, an intriguing geometric phenomenon of the representation space of multi-modal models. Specifically, we show that different data modalities (e.g. images and text) are embedded at arm's length in their shared representation in multi-modal models such as CLIP. Our systematic analysis demonstrates that this gap is caused by a combination of model initialization and contrastive learning optimization. In model initialization, we show empirically and theoretically that the representation of a common deep neural network is restricted to a narrow cone. As a consequence, in a multi-modal model with two encoders, the representations of the two modalities are clearly apart when the model is initialized. During optimization, contrastive learning keeps the different modalities separate by a certain distance, which is influenced by the temperature parameter in the loss function. Our experiments further demonstrate that varying the modality gap distance has a significant impact in improving the model's downstream zero-shot classification performance and fairness. Our code and data are available at [this https URL](https://modalitygap.readthedocs.io/)

| Comments: | Our code and data are available at [this https URL](https://modalitygap.readthedocs.io/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM) |
| Cite as:  | **[arXiv:2203.02053](https://arxiv.org/abs/2203.02053) [cs.CL]** |
|           | (or **[arXiv:2203.02053v1](https://arxiv.org/abs/2203.02053v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.02053Focus to learn more |





<h2 id="2022-03-07-3">3. EAG: Extract and Generate Multi-way Aligned Corpus for Complete Multi-lingual Neural Machine Translation
</h2>

Title: [EAG: Extract and Generate Multi-way Aligned Corpus for Complete Multi-lingual Neural Machine Translation](https://arxiv.org/abs/2203.02180)

Authors: [Yulin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Zhen Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [JieZhou](https://arxiv.org/search/cs?searchtype=author&query=JieZhou)

> Complete Multi-lingual Neural Machine Translation (C-MNMT) achieves superior performance against the conventional MNMT by constructing multi-way aligned corpus, i.e., aligning bilingual training examples from different language pairs when either their source or target sides are identical. However, since exactly identical sentences from different language pairs are scarce, the power of the multi-way aligned corpus is limited by its scale. To handle this problem, this paper proposes "Extract and Generate" (EAG), a two-step approach to construct large-scale and high-quality multi-way aligned corpus from bilingual data. Specifically, we first extract candidate aligned examples by pairing the bilingual examples from different language pairs with highly similar source or target sentences; and then generate the final aligned examples from the candidates with a well-trained generation model. With this two-step pipeline, EAG can construct a large-scale and multi-way aligned corpus whose diversity is almost identical to the original bilingual corpus. Experiments on two publicly available datasets i.e., WMT-5 and OPUS-100, show that the proposed method achieves significant improvements over strong baselines, with +1.1 and +1.4 BLEU points improvements on the two datasets respectively.

| Comments: | Accepted as a long paper at ACL 2022                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2203.02180](https://arxiv.org/abs/2203.02180) [cs.CL]** |
|           | (or **[arXiv:2203.02180v1](https://arxiv.org/abs/2203.02180v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.02180Focus to learn more |





<h2 id="2022-03-07-4">4. Comprehension of Subtitles from Re-Translating Simultaneous Speech Translation
</h2>

Title: [Comprehension of Subtitles from Re-Translating Simultaneous Speech Translation](https://arxiv.org/abs/2203.02458)

Authors: [Dávid Javorský](https://arxiv.org/search/cs?searchtype=author&query=Javorský%2C+D), [Dominik Macháček](https://arxiv.org/search/cs?searchtype=author&query=Macháček%2C+D), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

> In simultaneous speech translation, one can vary the size of the output window, system latency and sometimes the allowed level of rewriting. The effect of these properties on readability and comprehensibility has not been tested with modern neural translation systems. In this work, we propose an evaluation method and investigate the effects on comprehension and user preferences. It is a pilot study with 14 users on 2 hours of German documentaries or speeches with online translations into Czech. We collect continuous feedback and answers on factual questions. Our results show that the subtitling layout or flicker have a little effect on comprehension, in contrast to machine translation itself and individual competence. Other results show that users with a limited knowledge of the source language have different preferences to stability and latency than the users with zero knowledge. The results are statistically insignificant, however, we show that our method works and can be reproduced in larger volume.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.02458](https://arxiv.org/abs/2203.02458) [cs.CL]** |
|           | (or **[arXiv:2203.02458v1](https://arxiv.org/abs/2203.02458v1) [cs.CL]** for this version) |





<h2 id="2022-03-07-5">5. From Simultaneous to Streaming Machine Translation by Leveraging Streaming History
</h2>

Title: [From Simultaneous to Streaming Machine Translation by Leveraging Streaming History](https://arxiv.org/abs/2203.02459)

Authors: [Javier Iranzo-Sánchez](https://arxiv.org/search/cs?searchtype=author&query=Iranzo-Sánchez%2C+J), [Jorge Civera](https://arxiv.org/search/cs?searchtype=author&query=Civera%2C+J), [Alfons Juan](https://arxiv.org/search/cs?searchtype=author&query=Juan%2C+A)

> Simultaneous Machine Translation is the task of incrementally translating an input sentence before it is fully available. Currently, simultaneous translation is carried out by translating each sentence independently of the previously translated text. More generally, Streaming MT can be understood as an extension of Simultaneous MT to the incremental translation of a continuous input text stream. In this work, a state-of-the-art simultaneous sentence-level MT system is extended to the streaming setup by leveraging the streaming history. Extensive empirical results are reported on IWSLT Translation Tasks, showing that leveraging the streaming history leads to significant quality gains. In particular, the proposed system proves to compare favorably to the best performing systems.

| Comments: | ACL 2022 - Camera ready                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.02459](https://arxiv.org/abs/2203.02459) [cs.CL]** |
|           | (or **[arXiv:2203.02459v1](https://arxiv.org/abs/2203.02459v1) [cs.CL]** for this version) |








# 2022-03-04

[Return to Index](#Index)



<h2 id="2022-03-04-1">1. Recent, rapid advancement in visual question answering architecture
</h2>

Title: [Recent, rapid advancement in visual question answering architecture](https://arxiv.org/abs/2203.01322)

Authors: [Venkat Kodali](https://arxiv.org/search/cs?searchtype=author&query=Kodali%2C+V), [Daniel Berleant](https://arxiv.org/search/cs?searchtype=author&query=Berleant%2C+D)

> Understanding visual question answering is going to be crucial for numerous human activities. However, it presents major challenges at the heart of the artificial intelligence endeavor. This paper presents an update on the rapid advancements in visual question answering using images that have occurred in the last couple of years. Tremendous growth in research on improving visual question answering system architecture has been published recently, showing the importance of multimodal architectures. Several points on the benefits of visual question answering are mentioned in the review paper by Manmadhan et al. (2020), on which the present article builds, including subsequent updates in the field.

| Comments: | 11 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multimedia (cs.MM) |
| Cite as:  | **[arXiv:2203.01322](https://arxiv.org/abs/2203.01322) [cs.CV]** |
|           | (or **[arXiv:2203.01322v1](https://arxiv.org/abs/2203.01322v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.01322Focus to learn more |





<h2 id="2022-03-04-2">2. Vision-Language Intelligence: Tasks, Representation Learning, and Large Models
</h2>

Title: [Vision-Language Intelligence: Tasks, Representation Learning, and Large Models](https://arxiv.org/abs/2203.01922)

Authors: [Feng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+F), [Hao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Yi-Fan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Shilong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Jian Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Lionel M. Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+L+M), [PengChuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Lei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L)

> This paper presents a comprehensive survey of vision-language (VL) intelligence from the perspective of time. This survey is inspired by the remarkable progress in both computer vision and natural language processing, and recent trends shifting from single modality processing to multiple modality comprehension. We summarize the development in this field into three time periods, namely task-specific methods, vision-language pre-training (VLP) methods, and larger models empowered by large-scale weakly-labeled data. We first take some common VL tasks as examples to introduce the development of task-specific methods. Then we focus on VLP methods and comprehensively review key components of the model structures and training methods. After that, we show how recent work utilizes large-scale raw image-text data to learn language-aligned visual representations that generalize better on zero or few shot learning tasks. Finally, we discuss some potential future trends towards modality cooperation, unified representation, and knowledge incorporation. We believe that this review will be of help for researchers and practitioners of AI and ML, especially those interested in computer vision and natural language processing.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.01922](https://arxiv.org/abs/2203.01922) [cs.CV]** |
|           | (or **[arXiv:2203.01922v1](https://arxiv.org/abs/2203.01922v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.01922Focus to learn more |





<h2 id="2022-03-04-3">3. UDAAN - Machine Learning based Post-Editing tool for Document Translation
</h2>

Title: [UDAAN - Machine Learning based Post-Editing tool for Document Translation](https://arxiv.org/abs/2203.01644)

Authors: [Ayush Maheshwari](https://arxiv.org/search/cs?searchtype=author&query=Maheshwari%2C+A), [Ajay Ravindran](https://arxiv.org/search/cs?searchtype=author&query=Ravindran%2C+A), [Venkatapathy Subramanian](https://arxiv.org/search/cs?searchtype=author&query=Subramanian%2C+V), [Akshay Jalan](https://arxiv.org/search/cs?searchtype=author&query=Jalan%2C+A), [Ganesh Ramakrishnan](https://arxiv.org/search/cs?searchtype=author&query=Ramakrishnan%2C+G)

> We introduce UDAAN, an open-source post-editing tool that can reduce manual editing efforts to quickly produce publishable-standard documents in different languages. UDAAN has an end-to-end Machine Translation (MT) plus post-editing pipeline wherein users can upload a document to obtain raw MT output. Further, users can edit the raw translations using our tool. UDAAN offers several advantages: a) Domain-aware, vocabulary-based lexical constrained MT. b) source-target and target-target lexicon suggestions for users. Replacements are based on the source and target texts lexicon alignment. c) Suggestions for translations are based on logs created during user interaction. d) Source-target sentence alignment visualisation that reduces the cognitive load of users during editing. e) Translated outputs from our tool are available in multiple formats: docs, latex, and PDF. Although we limit our experiments to English-to-Hindi translation for the current study, our tool is independent of the source and target languages. Experimental results based on the usage of the tools and users feedback show that our tool speeds up the translation time approximately by a factor of three compared to the baseline method of translating documents from scratch.

| Comments: | system demonstration paper                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.01644](https://arxiv.org/abs/2203.01644) [cs.CL]** |
|           | (or **[arXiv:2203.01644v1](https://arxiv.org/abs/2203.01644v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.01644Focus to learn more |





# 2022-03-03

[Return to Index](#Index)



<h2 id="2022-03-03-1">1. HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning
</h2>

Title: [HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning](https://arxiv.org/abs/2203.01311)

Authors: [Paul Pu Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P+P), [Yiwei Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+Y), [Xiang Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+X), [Shengtong Mo](https://arxiv.org/search/cs?searchtype=author&query=Mo%2C+S), [Dani Yogatama](https://arxiv.org/search/cs?searchtype=author&query=Yogatama%2C+D), [Louis-Philippe Morency](https://arxiv.org/search/cs?searchtype=author&query=Morency%2C+L), [Ruslan Salakhutdinov](https://arxiv.org/search/cs?searchtype=author&query=Salakhutdinov%2C+R)

> Learning multimodal representations involves discovering correspondences and integrating information from multiple heterogeneous sources of data. While recent research has begun to explore the design of more general-purpose multimodal models (contrary to prior focus on domain and modality-specific architectures), these methods are still largely focused on a small set of modalities in the language, vision, and audio space. In order to accelerate generalization towards diverse and understudied modalities, we investigate methods for high-modality (a large set of diverse modalities) and partially-observable (each task only defined on a small subset of modalities) scenarios. To tackle these challenges, we design a general multimodal model that enables multitask and transfer learning: multitask learning with shared parameters enables stable parameter counts (addressing scalability), and cross-modal transfer learning enables information sharing across modalities and tasks (addressing partial observability). Our resulting model generalizes across text, image, video, audio, time-series, sensors, tables, and set modalities from different research areas, improves the tradeoff between performance and efficiency, transfers to new modalities and tasks, and reveals surprising insights on the nature of information sharing in multitask models. We release our code and benchmarks which we hope will present a unified platform for subsequent theoretical and empirical analysis: [this https URL](https://github.com/pliang279/HighMMT).

| Comments: | Code available at [this https URL](https://github.com/pliang279/HighMMT) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM) |
| Cite as:  | **[arXiv:2203.01311](https://arxiv.org/abs/2203.01311) [cs.LG]** |
|           | (or **[arXiv:2203.01311v1](https://arxiv.org/abs/2203.01311v1) [cs.LG]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.01311Focus to learn more |





<h2 id="2022-03-03-2">2. Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation in Few Shots
</h2>

Title: [Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation in Few Shots](https://arxiv.org/abs/2203.00732)

Authors: [Wenting Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Ye Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Yao Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+Y), [Philip S. Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+P+S)

> Few-shot table-to-text generation is a task of composing fluent and faithful sentences to convey table content using limited data. Despite many efforts having been made towards generating impressive fluent sentences by fine-tuning powerful pre-trained language models, the faithfulness of generated content still needs to be improved. To this end, this paper proposes a novel approach Attend, Memorize and Generate (called AMG), inspired by the text generation process of humans. In particular, AMG (1) attends over the multi-granularity of context using a novel strategy based on table slot level and traditional token-by-token level attention to exploit both the table structure and natural linguistic information; (2) dynamically memorizes the table slot allocation states; and (3) generates faithful sentences according to both the context and memory allocation states. Comprehensive experiments with human evaluation on three domains (i.e., humans, songs, and books) of the Wiki dataset show that our model can generate higher qualified texts when compared with several state-of-the-art baselines, in both fluency and faithfulness.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.00732](https://arxiv.org/abs/2203.00732) [cs.CL]** |
|           | (or **[arXiv:2203.00732v1](https://arxiv.org/abs/2203.00732v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.00732Focus to learn more |





<h2 id="2022-03-03-3">3. HyperPrompt: Prompt-based Task-Conditioning of Transformers
</h2>

Title: [HyperPrompt: Prompt-based Task-Conditioning of Transformers](https://arxiv.org/abs/2203.00759)

Authors: [Yun He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Y), [Huaixiu Steven Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H+S), [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Jai Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+J), [Yu Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y), [Vamsi Aribandi](https://arxiv.org/search/cs?searchtype=author&query=Aribandi%2C+V), [Zhe Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [YaGuang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Zhao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z), [Donald Metzler](https://arxiv.org/search/cs?searchtype=author&query=Metzler%2C+D), [Heng-Tze Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+H), [Ed H. Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+E+H)

> Prompt-Tuning is a new paradigm for finetuning pre-trained language models in a parameter-efficient way. Here, we explore the use of HyperNetworks to generate hyper-prompts: we propose HyperPrompt, a novel architecture for prompt-based task-conditioning of self-attention in Transformers. The hyper-prompts are end-to-end learnable via generation by a HyperNetwork. HyperPrompt allows the network to learn task-specific feature maps where the hyper-prompts serve as task global memories for the queries to attend to, at the same time enabling flexible information sharing among tasks. We show that HyperPrompt is competitive against strong multi-task learning baselines with as few as 0.14% of additional task-conditioning parameters, achieving great parameter and computational efficiency. Through extensive empirical experiments, we demonstrate that HyperPrompt can achieve superior performances over strong T5 multi-task learning baselines and parameter-efficient adapter variants including Prompt-Tuning and HyperFormer++ on Natural Language Understanding benchmarks of GLUE and SuperGLUE across many model sizes.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.00759](https://arxiv.org/abs/2203.00759) [cs.CL]** |
|           | (or **[arXiv:2203.00759v1](https://arxiv.org/abs/2203.00759v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.00759Focus to learn more |





<h2 id="2022-03-03-4">4. Do Prompts Solve NLP Tasks Using Natural Language?
</h2>

Title: [Do Prompts Solve NLP Tasks Using Natural Language?](https://arxiv.org/abs/2203.00902)

Authors: [Sen Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Yunchen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Leyang Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Thanks to the advanced improvement of large pre-trained language models, prompt-based fine-tuning is shown to be effective on a variety of downstream tasks. Though many prompting methods have been investigated, it remains unknown which type of prompts are the most effective among three types of prompts (i.e., human-designed prompts, schema prompts and null prompts). In this work, we empirically compare the three types of prompts under both few-shot and fully-supervised settings. Our experimental results show that schema prompts are the most effective in general. Besides, the performance gaps tend to diminish when the scale of training data grows large.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.00902](https://arxiv.org/abs/2203.00902) [cs.CL]** |
|           | (or **[arXiv:2203.00902v1](https://arxiv.org/abs/2203.00902v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.00902Focus to learn more |





<h2 id="2022-03-03-5">5. Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models
</h2>

Title: [Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models](https://arxiv.org/abs/2203.01104)

Authors: [Ze-Feng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Z), [Peiyu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P), [Wayne Xin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W+X), [Zhong-Yi Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Z), [Ji-Rong Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+J)

> The state-of-the-art Mixture-of-Experts (short as MoE) architecture has achieved several remarkable successes in terms of increasing model capacity. However, MoE has been hindered widespread adoption due to complexity, communication costs, and training instability. Here we present a novel MoE architecture based on matrix product operators (MPO) from quantum many-body physics. It can decompose an original matrix into central tensors (containing the core information) and auxiliary tensors (with only a small proportion of parameters). With the decomposed MPO structure, we can reduce the parameters of the original MoE architecture by sharing a global central tensor across experts and keeping expert-specific auxiliary tensors. We also design the gradient mask strategy for the tensor structure of MPO to alleviate the overfitting problem. Experiments on the three well-known downstream natural language datasets based on GPT2 show improved performance and efficiency in increasing model capacity (7.26x fewer parameters with the same amount of experts). We additionally demonstrate an improvement in the positive transfer effects of our approach for multi-task learning.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Quantum Physics (quant-ph) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.01104](https://arxiv.org/abs/2203.01104) [cs.CL]** |
|           | (or **[arXiv:2203.01104v1](https://arxiv.org/abs/2203.01104v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.01104Focus to learn more |





# 2022-03-02

[Return to Index](#Index)



<h2 id="2022-03-02-1">1. Exploring and Adapting Chinese GPT to Pinyin Input Method
</h2>

Title: [Exploring and Adapting Chinese GPT to Pinyin Input Method](https://arxiv.org/abs/2203.00249)

Authors: [Minghuan Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M), [Yong Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+Y), [Duyu Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D), [Zhangyin Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Z), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Jing Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+J), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> While GPT has become the de-facto method for text generation tasks, its application to pinyin input method remains unexplored. In this work, we make the first exploration to leverage Chinese GPT for pinyin input method. We find that a frozen GPT achieves state-of-the-art performance on perfect pinyin. However, the performance drops dramatically when the input includes abbreviated pinyin. A reason is that an abbreviated pinyin can be mapped to many perfect pinyin, which links to even larger number of Chinese characters. We mitigate this issue with two strategies, including enriching the context with pinyin and optimizing the training process to help distinguish homophones. To further facilitate the evaluation of pinyin input method, we create a dataset consisting of 270K instances from 15 domains. Results show that our approach improves performance on abbreviated pinyin across all domains. Model analysis demonstrates that both strategies contribute to the performance boost.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2203.00249](https://arxiv.org/abs/2203.00249) [cs.CL]** |
|           | (or **[arXiv:2203.00249v1](https://arxiv.org/abs/2203.00249v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.00249Focus to learn more |





<h2 id="2022-03-02-2">2. TableFormer: Robust Transformer Modeling for Table-Text Encoding
</h22

Title: [TableFormer: Robust Transformer Modeling for Table-Text Encoding]()

Authors: [Jingfeng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Aditya Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Shyam Upadhyay](https://arxiv.org/search/cs?searchtype=author&query=Upadhyay%2C+S), [Luheng He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+L), [Rahul Goel](https://arxiv.org/search/cs?searchtype=author&query=Goel%2C+R), [Shachi Paul](https://arxiv.org/search/cs?searchtype=author&query=Paul%2C+S)

> Understanding tables is an important aspect of natural language understanding. Existing models for table understanding require linearization of the table structure, where row or column order is encoded as an unwanted bias. Such spurious biases make the model vulnerable to row and column order perturbations. Additionally, prior work has not thoroughly modeled the table structures or table-text alignments, hindering the table-text understanding ability. In this work, we propose a robust and structurally aware table-text encoding architecture TableFormer, where tabular structural biases are incorporated completely through learnable attention biases. TableFormer is (1) strictly invariant to row and column orders, and, (2) could understand tables better due to its tabular inductive biases. Our evaluations showed that TableFormer outperforms strong baselines in all settings on SQA, WTQ and TabFact table reasoning datasets, and achieves state-of-the-art performance on SQA, especially when facing answer-invariant row and column order perturbations (6% improvement over the best baseline), because previous SOTA models' performance drops by 4% - 6% when facing such perturbations while TableFormer is not affected.

| Comments: | ACL 2022, 10 pages                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2203.00274](https://arxiv.org/abs/2203.00274) [cs.CL]** |
|           | (or **[arXiv:2203.00274v1](https://arxiv.org/abs/2203.00274v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.00274Focus to learn more |





<h2 id="2022-03-02-3">3. DeepNet: Scaling Transformers to 1,000 Layers
</h2>

Title: [DeepNet: Scaling Transformers to 1,000 Layers](https://arxiv.org/abs/2203.00555)

Authors: [Hongyu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making DeepNorm a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2203.00555](https://arxiv.org/abs/2203.00555) [cs.CL]** |
|           | (or **[arXiv:2203.00555v1](https://arxiv.org/abs/2203.00555v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2203.00555Focus to learn more |





# 2022-03-01

[Return to Index](#Index)



<h2 id="2022-03-01-1">1. Interactive Machine Learning for Image Captioning
</h2>

Title: [Interactive Machine Learning for Image Captioning](https://arxiv.org/abs/2202.13623)

Authors: [Mareike Hartmann](https://arxiv.org/search/cs?searchtype=author&query=Hartmann%2C+M), [Aliki Anagnostopoulou](https://arxiv.org/search/cs?searchtype=author&query=Anagnostopoulou%2C+A), [Daniel Sonntag](https://arxiv.org/search/cs?searchtype=author&query=Sonntag%2C+D)

> We propose an approach for interactive learning for an image captioning model. As human feedback is expensive and modern neural network based approaches often require large amounts of supervised data to be trained, we envision a system that exploits human feedback as good as possible by multiplying the feedback using data augmentation methods, and integrating the resulting training examples into the model in a smart way. This approach has three key components, for which we need to find suitable practical implementations: feedback collection, data augmentation, and model update. We outline our idea and review different possibilities to address these tasks.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.13623](https://arxiv.org/abs/2202.13623) [cs.CV]** |
|           | (or **[arXiv:2202.13623v1](https://arxiv.org/abs/2202.13623v1) [cs.CV]** for this version) |





<h2 id="2022-03-01-2">2. Multi-Level Contrastive Learning for Cross-Lingual Alignment
</h2>

Title: [Multi-Level Contrastive Learning for Cross-Lingual Alignment](https://arxiv.org/abs/2202.13083)

Authors: [Beiduo Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Wu Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+W), [Bin Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+B), [Quan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Yongchao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y)

> Cross-language pre-trained models such as multilingual BERT (mBERT) have achieved significant performance in various cross-lingual downstream NLP tasks. This paper proposes a multi-level contrastive learning (ML-CTL) framework to further improve the cross-lingual ability of pre-trained models. The proposed method uses translated parallel data to encourage the model to generate similar semantic embeddings for different languages. However, unlike the sentence-level alignment used in most previous studies, in this paper, we explicitly integrate the word-level information of each pair of parallel sentences into contrastive learning. Moreover, cross-zero noise contrastive estimation (CZ-NCE) loss is proposed to alleviate the impact of the floating-point error in the training process with a small batch size. The proposed method significantly improves the cross-lingual transfer ability of our basic model (mBERT) and outperforms on multiple zero-shot cross-lingual downstream tasks compared to the same-size models in the Xtreme benchmark.

| Comments: | Accepted by ICASSP 2022                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.13083](https://arxiv.org/abs/2202.13083) [cs.CL]** |
|           | (or **[arXiv:2202.13083v1](https://arxiv.org/abs/2202.13083v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.13083Focus to learn more |





<h2 id="2022-03-01-3">3. OCR Improves Machine Translation for Low-Resource Languages
</h2>

Title: [OCR Improves Machine Translation for Low-Resource Languages](https://arxiv.org/abs/2202.13274)

Authors: [Oana Ignat](https://arxiv.org/search/cs?searchtype=author&query=Ignat%2C+O), [Jean Maillard](https://arxiv.org/search/cs?searchtype=author&query=Maillard%2C+J), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F)

> We aim to investigate the performance of current OCR systems on low resource languages and low resource scripts. We introduce and make publicly available a novel benchmark, \textsc{OCR4MT}, consisting of real and synthetic data, enriched with noise, for 60 low-resource languages in low resource scripts. We evaluate state-of-the-art OCR systems on our benchmark and analyse most common errors. We show that OCR monolingual data is a valuable resource that can increase performance of Machine Translation models, when used in backtranslation. We then perform an ablation study to investigate how OCR errors impact Machine Translation performance and determine what is the minimum level of OCR quality needed for the monolingual data to be useful for Machine Translation.

| Comments: | Accepted at ACL Findings 2022                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.13274](https://arxiv.org/abs/2202.13274) [cs.CL]** |
|           | (or **[arXiv:2202.13274v1](https://arxiv.org/abs/2202.13274v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.13274Focus to learn more |





<h2 id="2022-03-01-4">4. CINO: A Chinese Minority Pre-trained Language Model
</h2>

Title: [CINO: A Chinese Minority Pre-trained Language Model](https://arxiv.org/abs/2202.13558)

Authors: [Ziqing Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Zihang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Z), [Yiming Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y), [Baoxin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Min Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+M), [Dayong Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+D), [Zhigang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z)

> Multilingual pre-trained language models have shown impressive performance on cross-lingual tasks. It greatly facilitates the applications of natural language processing on low-resource languages. However, there are still some languages that the existing multilingual models do not perform well on. In this paper, we propose CINO (Chinese Minority Pre-trained Language Model), a multilingual pre-trained language model for Chinese minority languages. It covers Standard Chinese, Cantonese, and six other Chinese minority languages. To evaluate the cross-lingual ability of the multilingual models on the minority languages, we collect documents from Wikipedia and build a text classification dataset WCM (Wiki-Chinese-Minority). We test CINO on WCM and two other text classification tasks. Experiments show that CINO outperforms the baselines notably. The CINO model and the WCM dataset are available at [this http URL](http://cino.hfl-rc.com/).

| Comments: | 4 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.13558](https://arxiv.org/abs/2202.13558) [cs.CL]** |
|           | (or **[arXiv:2202.13558v1](https://arxiv.org/abs/2202.13558v1) [cs.CL]** for this version) |





<h2 id="2022-03-01-5">5. LCP-dropout: Compression-based Multiple Subword Segmentation for Neural Machine Translation
</h2>

Title: [LCP-dropout: Compression-based Multiple Subword Segmentation for Neural Machine Translation](https://arxiv.org/abs/2202.13590)

Authors: [Keita Nonaka](https://arxiv.org/search/cs?searchtype=author&query=Nonaka%2C+K), [Kazutaka Yamanouchi](https://arxiv.org/search/cs?searchtype=author&query=Yamanouchi%2C+K), [Tomohiro I](https://arxiv.org/search/cs?searchtype=author&query=I%2C+T), [Tsuyoshi Okita](https://arxiv.org/search/cs?searchtype=author&query=Okita%2C+T), [Kazutaka Shimada](https://arxiv.org/search/cs?searchtype=author&query=Shimada%2C+K), [Hiroshi Sakamoto](https://arxiv.org/search/cs?searchtype=author&query=Sakamoto%2C+H)

> In this study, we propose a simple and effective preprocessing method for subword segmentation based on a data compression algorithm. Compression-based subword segmentation has recently attracted significant attention as a preprocessing method for training data in Neural Machine Translation. Among them, BPE/BPE-dropout is one of the fastest and most effective method compared to conventional approaches. However, compression-based approach has a drawback in that generating multiple segmentations is difficult due to the determinism. To overcome this difficulty, we focus on a probabilistic string algorithm, called locally-consistent parsing (LCP), that has been applied to achieve optimum compression. Employing the probabilistic mechanism of LCP, we propose LCP-dropout for multiple subword segmentation that improves BPE/BPE-dropout, and show that it outperforms various baselines in learning from especially small training data.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2202.13590](https://arxiv.org/abs/2202.13590) [cs.CL]** |
|           | (or **[arXiv:2202.13590v1](https://arxiv.org/abs/2202.13590v1) [cs.CL]** for this version) |





<h2 id="2022-03-01-6">6. MSCTD: A Multimodal Sentiment Chat Translation Dataset
</h2>

Title: [MSCTD: A Multimodal Sentiment Chat Translation Dataset](https://arxiv.org/abs/2202.13645)

Authors: [Yunlong Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jinan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Yufeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Multimodal machine translation and textual chat translation have received considerable attention in recent years. Although the conversation in its natural form is usually multimodal, there still lacks work on multimodal machine translation in conversations. In this work, we introduce a new task named Multimodal Chat Translation (MCT), aiming to generate more accurate translations with the help of the associated dialogue history and visual context. To this end, we firstly construct a Multimodal Sentiment Chat Translation Dataset (MSCTD) containing 142,871 English-Chinese utterance pairs in 14,762 bilingual dialogues and 30,370 English-German utterance pairs in 3,079 bilingual dialogues. Each utterance pair, corresponding to the visual context that reflects the current conversational scene, is annotated with a sentiment label. Then, we benchmark the task by establishing multiple baseline systems that incorporate multimodal and sentiment features for MCT. Preliminary experiments on four language directions (English-Chinese and English-German) verify the potential of contextual and multimodal information fusion and the positive impact of sentiment on the MCT task. Additionally, as a by-product of the MSCTD, it also provides two new benchmarks on multimodal dialogue sentiment analysis. Our work can facilitate research on both multimodal chat translation and multimodal dialogue sentiment analysis.

| Comments: | Accepted at ACL 2022 as a long paper of main conference. Code and data: [this https URL](https://github.com/XL2248/MSCTD) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.13645](https://arxiv.org/abs/2202.13645) [cs.CL]** |
|           | (or **[arXiv:2202.13645v1](https://arxiv.org/abs/2202.13645v1) [cs.CL]** for this version) |





<h2 id="2022-03-01-7">7. Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation
</h2>

Title: [Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation](https://arxiv.org/abs/2202.13663)

Authors: [Chulun Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Hongji Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J)

> Most dominant neural machine translation (NMT) models are restricted to make predictions only according to the local context of preceding words in a left-to-right manner. Although many previous studies try to incorporate global information into NMT models, there still exist limitations on how to effectively exploit bidirectional global context. In this paper, we propose a Confidence Based Bidirectional Global Context Aware (CBBGCA) training framework for NMT, where the NMT model is jointly trained with an auxiliary conditional masked language model (CMLM). The training consists of two stages: (1) multi-task joint training; (2) confidence based knowledge distillation. At the first stage, by sharing encoder parameters, the NMT model is additionally supervised by the signal from the CMLM decoder that contains bidirectional global contexts. Moreover, at the second stage, using the CMLM as teacher, we further pertinently incorporate bidirectional global context to the NMT model on its unconfidently-predicted target words via knowledge distillation. Experimental results show that our proposed CBBGCA training framework significantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on three large-scale translation datasets, namely WMT'14 English-to-German, WMT'19 Chinese-to-English and WMT'14 English-to-French, respectively.

| Comments: | Pre-print version; Accepted at ACL 2022 as a long paper of main conference |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2202.13663](https://arxiv.org/abs/2202.13663) [cs.CL]** |
|           | (or **[arXiv:2202.13663v1](https://arxiv.org/abs/2202.13663v1) [cs.CL]** for this version) |





<h2 id="2022-03-01-8">8. LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding
</h2>

Title: [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669)

Authors: [Jiapeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Lianwen Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+L), [Kai Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+K)

> Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure. Code and model are publicly available at [this https URL](https://github.com/jpWang/LiLT).

| Comments: | ACL 2022 Main conference                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.13669](https://arxiv.org/abs/2202.13669) [cs.CL]** |
|           | (or **[arXiv:2202.13669v1](https://arxiv.org/abs/2202.13669v1) [cs.CL]** for this version) |





# 2022-02-28

[Return to Index](#Index)



<h2 id="2022-02-28-1">1. Screening Gender Transfer in Neural Machine Translation
</h2>

Title: [Screening Gender Transfer in Neural Machine Translation](https://arxiv.org/abs/2202.12568)

Authors: [Guillaume Wisniewski](https://arxiv.org/search/cs?searchtype=author&query=Wisniewski%2C+G), [Lichao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+L), [Nicolas Ballier](https://arxiv.org/search/cs?searchtype=author&query=Ballier%2C+N), [François Yvon](https://arxiv.org/search/cs?searchtype=author&query=Yvon%2C+F)

> This paper aims at identifying the information flow in state-of-the-art machine translation systems, taking as example the transfer of gender when translating from French into English. Using a controlled set of examples, we experiment several ways to investigate how gender information circulates in a encoder-decoder architecture considering both probing techniques as well as interventions on the internal representations used in the MT system. Our results show that gender information can be found in all token representations built by the encoder and the decoder and lead us to conclude that there are multiple pathways for gender transfer.

| Comments:    | Accepted at BlackBoxNLP'2021                                 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| Cite as:     | **[arXiv:2202.12568](https://arxiv.org/abs/2202.12568) [cs.CL]** |
|              | (or **[arXiv:2202.12568v1](https://arxiv.org/abs/2202.12568v1) [cs.CL]** for this version) |
| Related DOI: | https://doi.org/10.18653/v1/2021.blackboxnlp-1.24Focus to learn more |




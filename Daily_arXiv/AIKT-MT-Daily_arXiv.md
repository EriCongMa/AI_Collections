# Daily arXiv: Machine Translation - August, 2021

# Index


- [2021-08-31](#2021-08-31)

  - [1. Goal-driven text descriptions for images](#2021-08-31-1)
  - [2. VTLayout: Fusion of Visual and Text Features for Document Layout Analysis](#2021-08-31-2)
  - [3. Layer-wise Model Pruning based on Mutual Information](#2021-08-31-3)
  - [4. Span Fine-tuning for Pre-trained Language Models](#2021-08-31-4)
  - [5. LOT: A Benchmark for Evaluating Chinese Long Text Understanding and Generation](#2021-08-31-5)
  - [6. Scheduled Sampling Based on Decoding Steps for Neural Machine Translation](#2021-08-31-6)
  - [7. ASR-GLUE: A New Multi-task Benchmark for ASR-Robust Natural Language Understanding](#2021-08-31-7)
  - [8. Neuron-level Interpretation of Deep NLP Models: A Survey](#2021-08-31-8)
  - [9. Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners](#2021-08-31-9)
  - [10. AEDA: An Easier Data Augmentation Technique for Text Classification](#2021-08-31-10)
  - [11. On the Multilingual Capabilities of Very Large-Scale English Language Models](#2021-08-31-11)
- [2021-08-30](#2021-08-30)

  - [1. A New Sentence Ordering Method Using BERT Pretrained Model](#2021-08-30-1)
  - [2. Secoco: Self-Correcting Encoding for Neural Machine Translation](#2021-08-30-2)
  - [3. Translation Error Detection as Rationale Extraction](#2021-08-30-3)
  - [4. Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors](#2021-08-30-4)
  - [5. Injecting Text in Self-Supervised Speech Pretraining](#2021-08-30-5)
  - [6. Evaluating the Robustness of Neural Language Models to Input Perturbations](#2021-08-30-6)
  - [7. CAPE: Context-Aware Private Embeddings for Private Language Learning](#2021-08-30-7)
  - [8. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](#2021-08-30-8)
- [2021-08-27](#2021-08-27)
  - [1. Similar Scenes arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning](#2021-08-27-1)
  - [2. LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision](#2021-08-27-2)
  - [3. LayoutReader: Pre-training of Text and Layout for Reading Order Detection](#2021-08-27-3)
- [2021-08-26](#2021-08-26)

  - [1. SimVLM: Simple Visual Language Model Pretraining with Weak Supervision](#2021-08-26-1)
  - [2. The State of SLIVAR: What's next for robots, human-robot interaction, and (spoken) dialogue systems?](#2021-08-26-2)
  - [3. YANMTT: Yet Another Neural Machine Translation Toolkit](#2021-08-26-3)
- [2021-08-25](#2021-08-25)
  - [1. Examining Covert Gender Bias: A Case Study in Turkish and English Machine Translation Models](#2021-08-25-1)
  - [2. Recurrent multiple shared layers in Depth for Neural Machine Translation](#2021-08-25-2)
  - [3. More Than Words: Collocation Tokenization for Latent Dirichlet Allocation Models](#2021-08-25-3)
  - [4. Regularizing Transformers With Deep Probabilistic Layers](#2021-08-25-4)
- [2021-08-24](#2021-08-24)
  - [1. Grid-VLP: Revisiting Grid Features for Vision-Language Pre-training](#2021-08-24-1)
  - [2. Metric Learning in Multilingual Sentence Similarity Measurement for Document Alignment](#2021-08-24-2)
  - [3. A Unified Transformer-based Framework for Duplex Text Normalization](#2021-08-24-3)
  - [4. Semantic-Preserving Adversarial Text Attacks](#2021-08-24-4)
- [2021-08-23](#2021-08-23)

  - [1. CIGLI: Conditional Image Generation from Language & Image](#2021-08-23-1)
  - [2. Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling](#2021-08-23-2)
  - [3. Airbert: In-domain Pretraining for Vision-and-Language Navigation](#2021-08-23-3)
  - [4. Group-based Distinctive Image Captioning with Memory Attention](#2021-08-23-4)
  - [5. Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models](#2021-08-23-5)
  - [6. Fastformer: Additive Attention is All You Need](#2021-08-23-6)
  - [7. Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer](#2021-08-23-7)
- [2021-08-20](#2021-08-20)
  - [1. Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks](#2021-08-20-1)
  - [2. MvSR-NAT: Multi-view Subset Regularization for Non-Autoregressive Machine Translation](#2021-08-20-2)
  - [3. Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021](#2021-08-20-3)
  - [4. Contrastive Language-Image Pre-training for the Italian Language](#2021-08-20-4)
- [2021-08-19](#2021-08-19)

  - [1. X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics](#2021-08-19-1)
  - [2. GGP: A Graph-based Grouping Planner for Explicit Control of Long Text Generation](#2021-08-19-2)
  - [3. Table Caption Generation in Scholarly Documents Leveraging Pre-trained Language Models](#2021-08-19-3)
  - [4. Deep Natural Language Processing for LinkedIn Search Systems](#2021-08-19-4)
- [2021-08-18](#2021-08-18)

  - [1. Modeling Protein Using Large-scale Pretrain Language Model](#2021-08-18-1)
  - [2. A Light-weight contextual spelling correction model for customizing transducer-based speech recognition systems](#2021-08-18-2)
  - [3. A Game Interface to Study Semantic Grounding in Text-Based Models](#2021-08-18-3)
  - [4. Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition](#2021-08-18-4)
- [2021-08-17](#2021-08-17)

  - [1. ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration](#2021-08-17-1)
  - [2. Who's Waldo? Linking People Across Text and Images](#2021-08-17-2)
  - [3. Findings of the LoResMT 2021 Shared Task on COVID and Sign Language for Low-resource Languages](#2021-08-17-3)
  - [4. Maps Search Misspelling Detection Leveraging Domain-Augmented Contextual Representations](#2021-08-17-4)
  - [5. A Single Example Can Improve Zero-Shot Data Generation](#2021-08-17-5)
  - [6. Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages](#2021-08-17-6)
  - [7. MTG: A Benchmarking Suite for Multilingual Text Generation](#2021-08-17-7)
- [2021-08-16](#2021-08-16)
- [1. FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning](#2021-08-16-1)
- [2021-08-13](#2021-08-13)
  - [1. AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing](#2021-08-13-1)
  - [2. The paradox of the compositionality of natural language: a neural machine translation case study](#2021-08-13-2)
- [2021-08-12](#2021-08-12)
  - [1. Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion](#2021-08-12-1)
  - [2. Post-hoc Interpretability for Neural NLP: A Survey](#2021-08-12-2)
  - [3. A Transformer-based Math Language Model for Handwritten Math Expression Recognition](#2021-08-12-3)
- [2021-08-11](#2021-08-11)

  - [1. FairyTailor: A Multimodal Generative Framework for Storytelling](#2021-08-11-1)
  - [2. BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents](#2021-08-11-2)
  - [3. CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model](#2021-08-11-3)
  - [4. Differentiable Subset Pruning of Transformer Heads](#2021-08-11-4)
  - [5. How Commonsense Knowledge Helps with Natural Language Tasks: A Survey of Recent Resources and Methodologies](#2021-08-11-5)
  - [6. Sampling-Based Minimum Bayes Risk Decoding for Neural Machine Translation](#2021-08-11-6)
- [2021-08-10](#2021-08-10)

  - [1. Improving Similar Language Translation With Transfer Learning](#2021-08-10-1)
  - [2. Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models](#2021-08-10-2)
  - [3. Facebook AI WMT21 News Translation Task Submission](#2021-08-10-3)
  - [4. Towards Zero-shot Language Modeling](#2021-08-10-4)
  - [5. Generating Personalized Dialogue via Multi-Task Meta-Learning](#2021-08-10-5)
  - [6. Language Model Evaluation in Open-ended Text Generation](#2021-08-10-6)
  - [7. Machine Translation of Low-Resource Indo-European Languages](#2021-08-10-7)
  - [8. The HW-TSC's Offline Speech Translation Systems for IWSLT 2021 Evaluation](#2021-08-10-8)
  - [9. Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models](#2021-08-10-9)
- [2021-08-09](#2021-08-09)

  - [1. Sentence Semantic Regression for Text Generation](#2021-08-09-1)
  - [2. Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents](#2021-08-09-2)
  - [3. StrucTexT: Structured Text Understanding with Multi-Modal Transformers](#2021-08-09-3)
- [2021-08-06](#2021-08-06)
  - [1. Sentence-level Online Handwritten Chinese Character Recognition](#2021-08-06-1)
  - [2. Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models](#2021-08-06-2)
  - [3. WeChat Neural Machine Translation Systems for WMT21](#2021-08-06-3)
  - [4. Finetuning Pretrained Transformers into Variational Autoencoders](#2021-08-06-4)
  - [5. VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search](#2021-08-06-5)
- [2021-08-05](#2021-08-05)

  - [1. Improving Distinction between ASR Errors and Speech Disfluencies with Feature Space Interpolation](#2021-08-05-1)
  - [2. PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining](#2021-08-05-2)
  - [3. How to Query Language Models?](#2021-08-05-3)
  - [4. Curriculum learning for language modeling](#2021-08-05-4)
- [2021-08-04](#2021-08-04)

  - [1. Knowledge-intensive Language Understanding for Explainable AI](#2021-08-04-1)
  - [2. Underreporting of errors in NLG output, and what to do about it](#2021-08-04-2)
  - [3. A Dynamic Head Importance Computation Mechanism for Neural Machine Translation](#2021-08-04-3)
- [2021-08-03](#2021-08-03)

  - [1. Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding](#2021-08-03-1)
  - [2. StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators](#2021-08-03-2)
  - [3. Structural Guidance for Transformer Language Models](#2021-08-03-3)
  - [4. LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization](#2021-08-03-4)
- [2021-08-02](#2021-08-02)

  - [1. Difficulty-Aware Machine Translation Evaluation](#2021-08-02-1)
  - [2. Residual Tree Aggregation of Layers for Neural Machine Translation](#2021-08-02-2)
  - [3. Neural Variational Learning for Grounded Language Acquisition](#2021-08-02-3)
  - [4. Multi-stage Pre-training over Simplified Multimodal Pre-training Models](#2021-08-02-4)
  - [5. MDQE: A More Accurate Direct Pretraining for Machine Translation Quality Estimation](#2021-08-02-5)
  - [6. Towards Universality in Multilingual Text Rewriting](#2021-08-02-6)
  - [7. ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback](#2021-08-02-7)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-08-31

[Return to Index](#Index)



<h2 id="2021-08-31-1">1. Goal-driven text descriptions for images
</h2>

Title: [Goal-driven text descriptions for images](https://arxiv.org/abs/2108.12575)

Authors: [Ruotian Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R)

> A big part of achieving Artificial General Intelligence(AGI) is to build a machine that can see and listen like humans. Much work has focused on designing models for image classification, video classification, object detection, pose estimation, speech recognition, etc., and has achieved significant progress in recent years thanks to deep learning. However, understanding the world is not enough. An AI agent also needs to know how to talk, especially how to communicate with a human. While perception (vision, for example) is more common across animal species, the use of complicated language is unique to humans and is one of the most important aspects of intelligence.
> In this thesis, we focus on generating textual output given visual input. In Chapter 3, we focus on generating the referring expression, a text description for an object in the image so that a receiver can infer which object is being described. We use a comprehension machine to directly guide the generated referring expressions to be more discriminative. In Chapter 4, we introduce a method that encourages discriminability in image caption generation. We show that more discriminative captioning models generate more descriptive captions. In Chapter 5, we study how training objectives and sampling methods affect the models' ability to generate diverse captions. We find that a popular captioning training strategy will be detrimental to the diversity of generated captions. In Chapter 6, we propose a model that can control the length of generated captions. By changing the desired length, one can influence the style and descriptiveness of the captions. Finally, in Chapter 7, we rank/generate informative image tags according to their information utility. The proposed method better matches what humans think are the most important tags for the images.

| Comments: | Ph.D. thesis                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.12575](https://arxiv.org/abs/2108.12575) [cs.CV]** |
|           | (or **[arXiv:2108.12575v1](https://arxiv.org/abs/2108.12575v1) [cs.CV]** for this version) |





<h2 id="2021-08-31-2">2. VTLayout: Fusion of Visual and Text Features for Document Layout Analysis
</h2>

Title: [VTLayout: Fusion of Visual and Text Features for Document Layout Analysis](https://arxiv.org/abs/2108.13297)

Authors: [Shoubin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Xuyan Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Shuaiqun Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+S), [Jun Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Lin Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+L), [Qing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q)

> Documents often contain complex physical structures, which make the Document Layout Analysis (DLA) task challenging. As a pre-processing step for content extraction, DLA has the potential to capture rich information in historical or scientific documents on a large scale. Although many deep-learning-based methods from computer vision have already achieved excellent performance in detecting \emph{Figure} from documents, they are still unsatisfactory in recognizing the \emph{List}, \emph{Table}, \emph{Text} and \emph{Title} category blocks in DLA. This paper proposes a VTLayout model fusing the documents' deep visual, shallow visual, and text features to localize and identify different category blocks. The model mainly includes two stages, and the three feature extractors are built in the second stage. In the first stage, the Cascade Mask R-CNN model is applied directly to localize all category blocks of the documents. In the second stage, the deep visual, shallow visual, and text features are extracted for fusion to identify the category blocks of documents. As a result, we strengthen the classification power of different category blocks based on the existing localization technique. The experimental results show that the identification capability of the VTLayout is superior to the most advanced method of DLA based on the PubLayNet dataset, and the F1 score is as high as 0.9599.

| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.13297](https://arxiv.org/abs/2108.13297) [cs.IR]** |
|           | (or **[arXiv:2108.13297v1](https://arxiv.org/abs/2108.13297v1) [cs.IR]** for this version) |





<h2 id="2021-08-31-3">3. Layer-wise Model Pruning based on Mutual Information
</h2>

Title: [Layer-wise Model Pruning based on Mutual Information](https://arxiv.org/abs/2108.12594)

Authors: [Chun Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+C), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Xiang Ao](https://arxiv.org/search/cs?searchtype=author&query=Ao%2C+X), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X)

> The proposed pruning strategy offers merits over weight-based pruning techniques: (1) it avoids irregular memory access since representations and matrices can be squeezed into their smaller but dense counterparts, leading to greater speedup; (2) in a manner of top-down pruning, the proposed method operates from a more global perspective based on training signals in the top layer, and prunes each layer by propagating the effect of global signals through layers, leading to better performances at the same sparsity level. Extensive experiments show that at the same sparsity level, the proposed strategy offers both greater speedup and higher performances than weight-based pruning methods (e.g., magnitude pruning, movement pruning).

| Comments: | To appear at EMNLP2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2108.12594](https://arxiv.org/abs/2108.12594) [cs.CL]** |
|           | (or **[arXiv:2108.12594v1](https://arxiv.org/abs/2108.12594v1) [cs.CL]** for this version) |





<h2 id="2021-08-31-4">4. Span Fine-tuning for Pre-trained Language Models
</h2>

Title: [Span Fine-tuning for Pre-trained Language Models](https://arxiv.org/abs/2108.12848)

Authors: [Rongzhou Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+R), [Zhuosheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> Pre-trained language models (PrLM) have to carefully manage input units when training on a very large text with a vocabulary consisting of millions of words. Previous works have shown that incorporating span-level information over consecutive words in pre-training could further improve the performance of PrLMs. However, given that span-level clues are introduced and fixed in pre-training, previous methods are time-consuming and lack of flexibility. To alleviate the inconvenience, this paper presents a novel span fine-tuning method for PrLMs, which facilitates the span setting to be adaptively determined by specific downstream tasks during the fine-tuning phase. In detail, any sentences processed by the PrLM will be segmented into multiple spans according to a pre-sampled dictionary. Then the segmentation information will be sent through a hierarchical CNN module together with the representation outputs of the PrLM and ultimately generate a span-enhanced representation. Experiments on GLUE benchmark show that the proposed span fine-tuning method significantly enhances the PrLM, and at the same time, offer more flexibility in an efficient way.

| Comments: | Accepted by EMNLP 2021 Finding(early version)                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.12848](https://arxiv.org/abs/2108.12848) [cs.CL]** |
|           | (or **[arXiv:2108.12848v1](https://arxiv.org/abs/2108.12848v1) [cs.CL]** for this version) |





<h2 id="2021-08-31-5">5. LOT: A Benchmark for Evaluating Chinese Long Text Understanding and Generation
</h2>

Title: [LOT: A Benchmark for Evaluating Chinese Long Text Understanding and Generation](https://arxiv.org/abs/2108.12960)

Authors: [Jian Guan](https://arxiv.org/search/cs?searchtype=author&query=Guan%2C+J), [Zhuoer Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Z), [Yamei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Ruilin He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+R), [Xiaoxi Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+X), [Changjie Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+C), [Minlie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M)

> Standard multi-task benchmarks are essential for driving the progress of general pretraining models to generalize to various downstream tasks. However, existing benchmarks such as GLUE and GLGE tend to focus on short text understanding and generation tasks, without considering long text modeling, which requires many distinct capabilities such as modeling long-range commonsense and discourse relations, as well as the coherence and controllability of generation. The lack of standardized benchmarks makes it difficult to fully evaluate these capabilities of a model and fairly compare different models, especially Chinese pretraining models. Therefore, we propose LOT, a benchmark including two understanding and two generation tasks for Chinese long text modeling evaluation. We construct the datasets for the tasks based on various kinds of human-written Chinese stories. Besides, we release an encoder-decoder Chinese long text pretraining model named LongLM with up to 1 billion parameters. We pretrain LongLM on 120G Chinese novels with two generative tasks including text infilling and conditional continuation. Extensive experiments on LOT demonstrate that LongLM matches the performance of similar-sized pretraining models on the understanding tasks and outperforms strong baselines substantially on the generation tasks.

| Comments: | 11 pages. Benchmark datasets, pretraining data and pretraining models url: [this https URL](https://github.com/thu-coai/LOT-Benchmark) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.12960](https://arxiv.org/abs/2108.12960) [cs.CL]** |
|           | (or **[arXiv:2108.12960v1](https://arxiv.org/abs/2108.12960v1) [cs.CL]** for this version) |





<h2 id="2021-08-31-6">6. Scheduled Sampling Based on Decoding Steps for Neural Machine Translation
</h2>

Title: [Scheduled Sampling Based on Decoding Steps for Neural Machine Translation](https://arxiv.org/abs/2108.12963)

Authors: [Yijin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yufeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Jinan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Scheduled sampling is widely used to mitigate the exposure bias problem for neural machine translation. Its core motivation is to simulate the inference scene during training by replacing ground-truth tokens with predicted tokens, thus bridging the gap between training and inference. However, vanilla scheduled sampling is merely based on training steps and equally treats all decoding steps. Namely, it simulates an inference scene with uniform error rates, which disobeys the real inference scene, where larger decoding steps usually have higher error rates due to error accumulations. To alleviate the above discrepancy, we propose scheduled sampling methods based on decoding steps, increasing the selection chance of predicted tokens with the growth of decoding steps. Consequently, we can more realistically simulate the inference scene during training, thus better bridging the gap between training and inference. Moreover, we investigate scheduled sampling based on both training steps and decoding steps for further improvements. Experimentally, our approaches significantly outperform the Transformer baseline and vanilla scheduled sampling on three large-scale WMT tasks. Additionally, our approaches also generalize well to the text summarization task on two popular benchmarks.

| Comments: | To appear in EMNLP-2021 main conference, code is at [this https URL](https://github.com/Adaxry/ss_on_decoding_steps). arXiv admin note: text overlap with [arXiv:2107.10427](https://arxiv.org/abs/2107.10427) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.12963](https://arxiv.org/abs/2108.12963) [cs.CL]** |
|           | (or **[arXiv:2108.12963v1](https://arxiv.org/abs/2108.12963v1) [cs.CL]** for this version) |





<h2 id="2021-08-31-7">7. ASR-GLUE: A New Multi-task Benchmark for ASR-Robust Natural Language Understanding
</h2>

Title: [ASR-GLUE: A New Multi-task Benchmark for ASR-Robust Natural Language Understanding](https://arxiv.org/abs/2108.13048)

Authors: [Lingyun Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+L), [Jianwei Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J), [Deng Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D), [Songxiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Haitao Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H), [Yan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y)

> Language understanding in speech-based systems have attracted much attention in recent years with the growing demand for voice interface applications. However, the robustness of natural language understanding (NLU) systems to errors introduced by automatic speech recognition (ASR) is under-examined. %To facilitate the research on ASR-robust general language understanding, In this paper, we propose ASR-GLUE benchmark, a new collection of 6 different NLU tasks for evaluating the performance of models under ASR error across 3 different levels of background noise and 6 speakers with various voice characteristics. Based on the proposed benchmark, we systematically investigate the effect of ASR error on NLU tasks in terms of noise intensity, error type and speaker variants. We further purpose two ways, correction-based method and data augmentation-based method to improve robustness of the NLU systems. Extensive experimental results and analysises show that the proposed methods are effective to some extent, but still far from human performance, demonstrating that NLU under ASR error is still very challenging and requires further research.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.13048](https://arxiv.org/abs/2108.13048) [cs.CL]** |
|           | (or **[arXiv:2108.13048v1](https://arxiv.org/abs/2108.13048v1) [cs.CL]** for this version) |





<h2 id="2021-08-31-8">8. Neuron-level Interpretation of Deep NLP Models: A Survey
</h2>

Title: [Neuron-level Interpretation of Deep NLP Models: A Survey](https://arxiv.org/abs/2108.13138)

Authors: [Hassan Sajjad](https://arxiv.org/search/cs?searchtype=author&query=Sajjad%2C+H), [Nadir Durrani](https://arxiv.org/search/cs?searchtype=author&query=Durrani%2C+N), [Fahim Dalvi](https://arxiv.org/search/cs?searchtype=author&query=Dalvi%2C+F)

> The proliferation of deep neural networks in various domains has seen an increased need for interpretability of these methods. A plethora of research has been carried out to analyze and understand components of the deep neural network models. Preliminary work done along these lines and papers that surveyed such, were focused on a more high-level representation analysis. However, a recent branch of work has concentrated on interpretability at a more granular level, analyzing neurons and groups of neurons in these large models. In this paper, we survey work done on fine-grained neuron analysis including: i) methods developed to discover and understand neurons in a network, ii) their limitations and evaluation, iii) major findings including cross architectural comparison that such analyses unravel and iv) direct applications of neuron analysis such as model behavior control and domain adaptation along with potential directions for future work.

| Comments: | 13 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.13138](https://arxiv.org/abs/2108.13138) [cs.CL]** |
|           | (or **[arXiv:2108.13138v1](https://arxiv.org/abs/2108.13138v1) [cs.CL]** for this version) |





<h2 id="2021-08-31-9">9. Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners
</h2>

Title: [Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/abs/2108.13161)

Authors: [Ningyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+N), [Luoqiu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Xiang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Shumin Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+S), [Zhen Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+Z), [Chuanqi Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+C), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F), [Huajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H)

> Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners without any prompt engineering. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.13161](https://arxiv.org/abs/2108.13161) [cs.CL]** |
|           | (or **[arXiv:2108.13161v1](https://arxiv.org/abs/2108.13161v1) [cs.CL]** for this version) |





<h2 id="2021-08-31-10">10. AEDA: An Easier Data Augmentation Technique for Text Classification
</h2>

Title: [AEDA: An Easier Data Augmentation Technique for Text Classification](https://arxiv.org/abs/2108.13230)

Authors: [Akbar Karimi](https://arxiv.org/search/cs?searchtype=author&query=Karimi%2C+A), [Leonardo Rossi](https://arxiv.org/search/cs?searchtype=author&query=Rossi%2C+L), [Andrea Prati](https://arxiv.org/search/cs?searchtype=author&query=Prati%2C+A)

> This paper proposes AEDA (An Easier Data Augmentation) technique to help improve the performance on text classification tasks. AEDA includes only random insertion of punctuation marks into the original text. This is an easier technique to implement for data augmentation than EDA method (Wei and Zou, 2019) with which we compare our results. In addition, it keeps the order of the words while changing their positions in the sentence leading to a better generalized performance. Furthermore, the deletion operation in EDA can cause loss of information which, in turn, misleads the network, whereas AEDA preserves all the input information. Following the baseline, we perform experiments on five different datasets for text classification. We show that using the AEDA-augmented data for training, the models show superior performance compared to using the EDA-augmented data in all five datasets. The source code is available for further study and reproduction of the results.

| Comments: | Accepted at EMNLP 2021 Findings                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.13230](https://arxiv.org/abs/2108.13230) [cs.CL]** |
|           | (or **[arXiv:2108.13230v1](https://arxiv.org/abs/2108.13230v1) [cs.CL]** for this version) |





<h2 id="2021-08-31-11">11. On the Multilingual Capabilities of Very Large-Scale English Language Models
</h2>

Title: [On the Multilingual Capabilities of Very Large-Scale English Language Models](https://arxiv.org/abs/2108.13349)

Authors: [Jordi Armengol-Estapé](https://arxiv.org/search/cs?searchtype=author&query=Armengol-Estapé%2C+J), [Ona de Gibert Bonet](https://arxiv.org/search/cs?searchtype=author&query=de+Gibert+Bonet%2C+O), [Maite Melero](https://arxiv.org/search/cs?searchtype=author&query=Melero%2C+M)

> Generative Pre-trained Transformers (GPTs) have recently been scaled to unprecedented sizes in the history of machine learning. These models, solely trained on the language modeling objective, have been shown to exhibit outstanding few-shot learning capabilities in a number of different tasks. Nevertheless, aside from anecdotal experiences, little is known regarding their multilingual capabilities, given the fact that the pre-training corpus is almost entirely composed of English text. In this work, we investigate the multilingual skills of GPT-3, focusing on one language that barely appears in the pre-training corpus, Catalan, which makes the results especially meaningful; we assume that our results may be relevant for other languages as well. We find that the model shows an outstanding performance, particularly in generative tasks, with predictable limitations mostly in language understanding tasks but still with remarkable results given the zero-shot scenario. We investigate its potential and limits in extractive question-answering and natural language generation, as well as the effect of scale in terms of model size.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.13349](https://arxiv.org/abs/2108.13349) [cs.CL]** |
|           | (or **[arXiv:2108.13349v1](https://arxiv.org/abs/2108.13349v1) [cs.CL]** for this version) |





# 2021-08-30

[Return to Index](#Index)



<h2 id="2021-08-30-1">1. A New Sentence Ordering Method Using BERT Pretrained Model
</h2>

Title: [A New Sentence Ordering Method Using BERT Pretrained Model](https://arxiv.org/abs/2108.11994)

Authors: [Melika Golestani](https://arxiv.org/search/cs?searchtype=author&query=Golestani%2C+M), [Seyedeh Zahra Razavi](https://arxiv.org/search/cs?searchtype=author&query=Razavi%2C+S+Z), [Heshaam Faili](https://arxiv.org/search/cs?searchtype=author&query=Faili%2C+H)

> Building systems with capability of natural language understanding (NLU) has been one of the oldest areas of AI. An essential component of NLU is to detect logical succession of events contained in a text. The task of sentence ordering is proposed to learn succession of events with applications in AI tasks. The performance of previous works employing statistical methods is poor, while the neural networks-based approaches are in serious need of large corpora for model learning. In this paper, we propose a method for sentence ordering which does not need a training phase and consequently a large corpus for learning. To this end, we generate sentence embedding using BERT pre-trained model and measure sentence similarity using cosine similarity score. We suggest this score as an indicator of sequential events' level of coherence. We finally sort the sentences through brute-force search to maximize overall similarities of the sequenced sentences. Our proposed method outperformed other baselines on ROCStories, a corpus of 5-sentence human-made stories. The method is specifically more efficient than neural network-based methods when no huge corpus is available. Among other advantages of this method are its interpretability and needlessness to linguistic knowledge.

| Comments: | 7 pages, 4 figures, 2020 11th International Conference on Information and Knowledge Technology (IKT) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2108.11994](https://arxiv.org/abs/2108.11994) [cs.CL]** |
|           | (or **[arXiv:2108.11994v1](https://arxiv.org/abs/2108.11994v1) [cs.CL]** for this version) |





<h2 id="2021-08-30-2">2. Secoco: Self-Correcting Encoding for Neural Machine Translation
</h2>

Title: [Secoco: Self-Correcting Encoding for Neural Machine Translation](https://arxiv.org/abs/2108.12137)

Authors: [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Chengqi Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Hang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Deyi Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+D)

> This paper presents Self-correcting Encoding (Secoco), a framework that effectively deals with input noise for robust neural machine translation by introducing self-correcting predictors. Different from previous robust approaches, Secoco enables NMT to explicitly correct noisy inputs and delete specific errors simultaneously with the translation decoding process. Secoco is able to achieve significant improvements over strong baselines on two real-world test sets and a benchmark WMT dataset with good interpretability. We will make our code and dataset publicly available soon.

| Comments:    | 6 pages, 2 figures, 3 tables                                 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2108.12137](https://arxiv.org/abs/2108.12137) [cs.CL]** |
|              | (or **[arXiv:2108.12137v1](https://arxiv.org/abs/2108.12137v1) [cs.CL]** for this version) |





<h2 id="2021-08-30-3">3. Translation Error Detection as Rationale Extraction
</h2>

Title: [Translation Error Detection as Rationale Extraction](https://arxiv.org/abs/2108.12197)

Authors: [Marina Fomicheva](https://arxiv.org/search/cs?searchtype=author&query=Fomicheva%2C+M), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L), [Nikolaos Aletras](https://arxiv.org/search/cs?searchtype=author&query=Aletras%2C+N)

> Recent Quality Estimation (QE) models based on multilingual pre-trained representations have achieved very competitive results when predicting the overall quality of translated sentences. Predicting translation errors, i.e. detecting specifically which words are incorrect, is a more challenging task, especially with limited amounts of training data. We hypothesize that, not unlike humans, successful QE models rely on translation errors to predict overall sentence quality. By exploring a set of feature attribution methods that assign relevance scores to the inputs to explain model predictions, we study the behaviour of state-of-the-art sentence-level QE models and show that explanations (i.e. rationales) extracted from these models can indeed be used to detect translation errors. We therefore (i) introduce a novel semi-supervised method for word-level QE and (ii) propose to use the QE task as a new benchmark for evaluating the plausibility of feature attribution, i.e. how interpretable model explanations are to humans.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.12197](https://arxiv.org/abs/2108.12197) [cs.CL]** |
|           | (or **[arXiv:2108.12197v1](https://arxiv.org/abs/2108.12197v1) [cs.CL]** for this version) |





<h2 id="2021-08-30-4">4. Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors
</h2>

Title: [Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors](https://arxiv.org/abs/2108.12216)

Authors: [Ryo Nagata](https://arxiv.org/search/cs?searchtype=author&query=Nagata%2C+R), [Manabu Kimura](https://arxiv.org/search/cs?searchtype=author&query=Kimura%2C+M), [Kazuaki Hanawa](https://arxiv.org/search/cs?searchtype=author&query=Hanawa%2C+K)

> In this paper, we explore the capacity of a language model-based method for grammatical error detection in detail. We first show that 5 to 10% of training data are enough for a BERT-based error detection method to achieve performance equivalent to a non-language model-based method can achieve with the full training data; recall improves much faster with respect to training data size in the BERT-based method than in the non-language model method while precision behaves similarly. These suggest that (i) the BERT-based method should have a good knowledge of grammar required to recognize certain types of error and that (ii) it can transform the knowledge into error detection rules by fine-tuning with a few training samples, which explains its high generalization ability in grammatical error detection. We further show with pseudo error data that it actually exhibits such nice properties in learning rules for recognizing various types of error. Finally, based on these findings, we explore a cost-effective method for detecting grammatical errors with feedback comments explaining relevant grammatical rules to learners.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.12216](https://arxiv.org/abs/2108.12216) [cs.CL]** |
|           | (or **[arXiv:2108.12216v1](https://arxiv.org/abs/2108.12216v1) [cs.CL]** for this version) |





<h2 id="2021-08-30-5">5. Injecting Text in Self-Supervised Speech Pretraining
</h2>

Title: [Injecting Text in Self-Supervised Speech Pretraining](https://arxiv.org/abs/2108.12226)

Authors: [Zhehuai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z), [Yu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Andrew Rosenberg](https://arxiv.org/search/cs?searchtype=author&query=Rosenberg%2C+A), [Bhuvana Ramabhadran](https://arxiv.org/search/cs?searchtype=author&query=Ramabhadran%2C+B), [Gary Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G), [Pedro Moreno](https://arxiv.org/search/cs?searchtype=author&query=Moreno%2C+P)

> Self-supervised pretraining for Automated Speech Recognition (ASR) has shown varied degrees of success. In this paper, we propose to jointly learn representations during pretraining from two different modalities: speech and text. The proposed method, tts4pretrain complements the power of contrastive learning in self-supervision with linguistic/lexical representations derived from synthesized speech, effectively learning from untranscribed speech and unspoken text. Lexical learning in the speech encoder is enforced through an additional sequence loss term that is coupled with contrastive loss during pretraining. We demonstrate that this novel pretraining method yields Word Error Rate (WER) reductions of 10% relative on the well-benchmarked, Librispeech task over a state-of-the-art baseline pretrained with wav2vec2.0 only. The proposed method also serves as an effective strategy to compensate for the lack of transcribed speech, effectively matching the performance of 5000 hours of transcribed speech with just 100 hours of transcribed speech on the AMI meeting transcription task. Finally, we demonstrate WER reductions of up to 15% on an in-house Voice Search task over traditional pretraining. Incorporating text into encoder pretraining is complimentary to rescoring with a larger or in-domain language model, resulting in additional 6% relative reduction in WER.

| Comments:    | submit to ASRU 2021                                          |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| MSC classes: | 68T10                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2108.12226](https://arxiv.org/abs/2108.12226) [cs.CL]** |
|              | (or **[arXiv:2108.12226v1](https://arxiv.org/abs/2108.12226v1) [cs.CL]** for this version) |





<h2 id="2021-08-30-6">6. Evaluating the Robustness of Neural Language Models to Input Perturbations
</h2>

Title: [Evaluating the Robustness of Neural Language Models to Input Perturbations](https://arxiv.org/abs/2108.12237)

Authors: [Milad Moradi](https://arxiv.org/search/cs?searchtype=author&query=Moradi%2C+M), [Matthias Samwald](https://arxiv.org/search/cs?searchtype=author&query=Samwald%2C+M)

> High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we design and implement various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained. Conducting comprehensive experiments on different NLP tasks, we investigate the ability of high-performance language models such as BERT, XLNet, RoBERTa, and ELMo in handling different types of input perturbations. The results suggest that language models are sensitive to input perturbations and their performance can decrease even when small changes are introduced. We highlight that models need to be further improved and that current benchmarks are not reflecting model robustness well. We argue that evaluations on perturbed inputs should routinely complement widely-used benchmarks in order to yield a more realistic understanding of NLP systems robustness.

| Comments: | Accepted by EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2108.12237](https://arxiv.org/abs/2108.12237) [cs.CL]** |
|           | (or **[arXiv:2108.12237v1](https://arxiv.org/abs/2108.12237v1) [cs.CL]** for this version) |





<h2 id="2021-08-30-7">7. CAPE: Context-Aware Private Embeddings for Private Language Learning
</h2>

Title: [CAPE: Context-Aware Private Embeddings for Private Language Learning](https://arxiv.org/abs/2108.12318)

Authors: [Richard Plant](https://arxiv.org/search/cs?searchtype=author&query=Plant%2C+R), [Dimitra Gkatzia](https://arxiv.org/search/cs?searchtype=author&query=Gkatzia%2C+D), [Valerio Giuffrida](https://arxiv.org/search/cs?searchtype=author&query=Giuffrida%2C+V)

> Deep learning-based language models have achieved state-of-the-art results in a number of applications including sentiment analysis, topic labelling, intent classification and others. Obtaining text representations or embeddings using these models presents the possibility of encoding personally identifiable information learned from language and context cues that may present a risk to reputation or privacy. To ameliorate these issues, we propose Context-Aware Private Embeddings (CAPE), a novel approach which preserves privacy during training of embeddings. To maintain the privacy of text representations, CAPE applies calibrated noise through differential privacy, preserving the encoded semantic links while obscuring sensitive information. In addition, CAPE employs an adversarial training regime that obscures identified private variables. Experimental results demonstrate that the proposed approach reduces private information leakage better than either single intervention.

| Comments: | Accepted into EMNLP21 main conference                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2108.12318](https://arxiv.org/abs/2108.12318) [cs.CL]** |
|           | (or **[arXiv:2108.12318v1](https://arxiv.org/abs/2108.12318v1) [cs.CL]** for this version) |





<h2 id="2021-08-30-8">8. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation
</h2>

Title: [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)

Authors: [Ofir Press](https://arxiv.org/search/cs?searchtype=author&query=Press%2C+O), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M)

> Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question remains open: how to achieve extrapolation at inference time to longer sequences than seen during training? We first show that extrapolation can be improved by changing the position representation method, though we find that existing proposals do not allow efficient extrapolation. We introduce a simple and efficient method, Attention with Linear Biases (ALiBi), that allows for extrapolation. ALiBi does not add positional embeddings to the word embeddings; instead, it biases the query-key attention scores with a term that is proportional to their distance. We show that this method allows training a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048, 11% faster and using 11% less memory. ALiBi's inductive bias towards recency allows it to outperform multiple strong position methods on the WikiText-103 benchmark. Finally, we provide analysis of ALiBi to understand why it leads to better performance.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.12409](https://arxiv.org/abs/2108.12409) [cs.CL]** |
|           | (or **[arXiv:2108.12409v1](https://arxiv.org/abs/2108.12409v1) [cs.CL]** for this version) |






# 2021-08-27

[Return to Index](#Index)



<h2 id="2021-08-27-1">1. Similar Scenes arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning
</h2>

Title: [Similar Scenes arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning](https://arxiv.org/abs/2108.11912)

Authors: [Guodun Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Yuchen Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+Y), [Zehao Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Yin Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Stylized image captioning systems aim to generate a caption not only semantically related to a given image but also consistent with a given style description. One of the biggest challenges with this task is the lack of sufficient paired stylized data. Many studies focus on unsupervised approaches, without considering from the perspective of data augmentation. We begin with the observation that people may recall similar emotions when they are in similar scenes, and often express similar emotions with similar style phrases, which underpins our data augmentation idea. In this paper, we propose a novel Extract-Retrieve-Generate data augmentation framework to extract style phrases from small-scale stylized sentences and graft them to large-scale factual captions. First, we design the emotional signal extractor to extract style phrases from small-scale stylized sentences. Second, we construct the plugable multi-modal scene retriever to retrieve scenes represented with pairs of an image and its stylized caption, which are similar to the query image or caption in the large-scale factual data. In the end, based on the style phrases of similar scenes and the factual description of the current scene, we build the emotion-aware caption generator to generate fluent and diversified stylized captions for the current scene. Extensive experimental results show that our framework can alleviate the data scarcity problem effectively. It also significantly boosts the performance of several existing image captioning models in both supervised and unsupervised settings, which outperforms the state-of-the-art stylized image captioning methods in terms of both sentence relevance and stylishness by a substantial margin.

| Comments: | Accepted at ACM Multimedia (ACMMM) 2021                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Multimedia (cs.MM) |
| DOI:      | [10.1145/3474085.3475662](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3474085.3475662&v=2bc30c11) |
| Cite as:  | **[arXiv:2108.11912](https://arxiv.org/abs/2108.11912) [cs.CV]** |
|           | (or **[arXiv:2108.11912v1](https://arxiv.org/abs/2108.11912v1) [cs.CV]** for this version) |





<h2 id="2021-08-27-2">2. LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision
</h2>

Title: [LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision](https://arxiv.org/abs/2108.11950)

Authors: [Zhijian Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Simon Stent](https://arxiv.org/search/cs?searchtype=author&query=Stent%2C+S), [Jie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [John Gideon](https://arxiv.org/search/cs?searchtype=author&query=Gideon%2C+J), [Song Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S)

> Computer vision tasks such as object detection and semantic/instance segmentation rely on the painstaking annotation of large training datasets. In this paper, we propose LocTex that takes advantage of the low-cost localized textual annotations (i.e., captions and synchronized mouse-over gestures) to reduce the annotation effort. We introduce a contrastive pre-training framework between images and captions and propose to supervise the cross-modal attention map with rendered mouse traces to provide coarse localization signals. Our learned visual features capture rich semantics (from free-form captions) and accurate localization (from mouse traces), which are very effective when transferred to various downstream vision tasks. Compared with ImageNet supervised pre-training, LocTex can reduce the size of the pre-training dataset by 10x or the target dataset by 2x while achieving comparable or even improved performance on COCO instance segmentation. When provided with the same amount of annotations, LocTex achieves around 4% higher accuracy than the previous state-of-the-art "vision+language" pre-training approach on the task of PASCAL VOC image classification.

| Comments: | ICCV 2021. Project page: [this https URL](https://loctex.mit.edu/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.11950](https://arxiv.org/abs/2108.11950) [cs.CV]** |
|           | (or **[arXiv:2108.11950v1](https://arxiv.org/abs/2108.11950v1) [cs.CV]** for this version) |





<h2 id="2021-08-27-3">3. LayoutReader: Pre-training of Text and Layout for Reading Order Detection
</h2>

Title: [LayoutReader: Pre-training of Text and Layout for Reading Order Detection](https://arxiv.org/abs/2108.11591)

Authors: [Zilong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Yiheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Lei Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Jingbo Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+J), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Reading order detection is the cornerstone to understanding visually-rich documents (e.g., receipts and forms). Unfortunately, no existing work took advantage of advanced deep learning models because it is too laborious to annotate a large enough dataset. We observe that the reading order of WORD documents is embedded in their XML metadata; meanwhile, it is easy to convert WORD documents to PDFs or images. Therefore, in an automated manner, we construct ReadingBank, a benchmark dataset that contains reading order, text, and layout information for 500,000 document images covering a wide spectrum of document types. This first-ever large-scale dataset unleashes the power of deep neural networks for reading order detection. Specifically, our proposed LayoutReader captures the text and layout information for reading order prediction using the seq2seq model. It performs almost perfectly in reading order detection and significantly improves both open-source and commercial OCR engines in ordering text lines in their results in our experiments. We will release the dataset and model at \url{[this https URL](https://aka.ms/readingbank)}.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.11591](https://arxiv.org/abs/2108.11591) [cs.CL]** |
|           | (or **[arXiv:2108.11591v1](https://arxiv.org/abs/2108.11591v1) [cs.CL]** for this version) |





# 2021-08-26

[Return to Index](#Index)



<h2 id="2021-08-26-1">1. SimVLM: Simple Visual Language Model Pretraining with Weak Supervision
</h2>

Title: [SimVLM: Simple Visual Language Model Pretraining with Weak Supervision](https://arxiv.org/abs/2108.10904)

Authors: [Zirui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Jiahui Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J), [Adams Wei Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+A+W), [Zihang Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+Z), [Yulia Tsvetkov](https://arxiv.org/search/cs?searchtype=author&query=Tsvetkov%2C+Y), [Yuan Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y)

> With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.10904](https://arxiv.org/abs/2108.10904) [cs.CV]** |
|           | (or **[arXiv:2108.10904v1](https://arxiv.org/abs/2108.10904v1) [cs.CV]** for this version) |





<h2 id="2021-08-26-2">2. The State of SLIVAR: What's next for robots, human-robot interaction, and (spoken) dialogue systems?
</h2>

Title: [The State of SLIVAR: What's next for robots, human-robot interaction, and (spoken) dialogue systems?](https://arxiv.org/abs/2108.10931)

Authors: [Casey Kennington](https://arxiv.org/search/cs?searchtype=author&query=Kennington%2C+C)

> We synthesize the reported results and recommendations of recent workshops and seminars that convened to discuss open questions within the important intersection of robotics, human-robot interaction, and spoken dialogue systems research. The goal of this growing area of research interest is to enable people to more effectively and naturally communicate with robots. To carry forward opportunities networking and discussion towards concrete, potentially fundable projects, we encourage interested parties to consider participating in future virtual and in-person discussions and workshops.

| Comments: | 6 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.10931](https://arxiv.org/abs/2108.10931) [cs.CL]** |
|           | (or **[arXiv:2108.10931v1](https://arxiv.org/abs/2108.10931v1) [cs.CL]** for this version) |





<h2 id="2021-08-26-3">3. YANMTT: Yet Another Neural Machine Translation Toolkit
</h2>

Title: [YANMTT: Yet Another Neural Machine Translation Toolkit](https://arxiv.org/abs/2108.11126)

Authors: [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Eiichiro Sumita](https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E)

> In this paper we present our open-source neural machine translation (NMT) toolkit called "Yet Another Neural Machine Translation Toolkit" abbreviated as YANMTT which is built on top of the Transformers library. Despite the growing importance of sequence to sequence pre-training there surprisingly few, if not none, well established toolkits that allow users to easily do pre-training. Toolkits such as Fairseq which do allow pre-training, have very large codebases and thus they are not beginner friendly. With regards to transfer learning via fine-tuning most toolkits do not explicitly allow the user to have control over what parts of the pre-trained models can be transferred. YANMTT aims to address these issues via the minimum amount of code to pre-train large scale NMT models, selectively transfer pre-trained parameters and fine-tune them, perform translation as well as extract representations and attentions for visualization and analyses. Apart from these core features our toolkit also provides other advanced functionalities such as but not limited to document/multi-source NMT, simultaneous NMT and model compression via distillation which we believe are relevant to the purpose behind our toolkit.

| Comments: | Submitted to EMNLP 2021 Demo Track                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2108.11126](https://arxiv.org/abs/2108.11126) [cs.CL]** |
|           | (or **[arXiv:2108.11126v1](https://arxiv.org/abs/2108.11126v1) [cs.CL]** for this version) |








# 2021-08-25

[Return to Index](#Index)



<h2 id="2021-08-25-1">1. Examining Covert Gender Bias: A Case Study in Turkish and English Machine Translation Models
</h2>

Title: [Examining Covert Gender Bias: A Case Study in Turkish and English Machine Translation Models](https://arxiv.org/abs/2108.10379)

Authors: [Chloe Ciora](https://arxiv.org/search/cs?searchtype=author&query=Ciora%2C+C), [Nur Iren](https://arxiv.org/search/cs?searchtype=author&query=Iren%2C+N), [Malihe Alikhani](https://arxiv.org/search/cs?searchtype=author&query=Alikhani%2C+M)

> As Machine Translation (MT) has become increasingly more powerful, accessible, and widespread, the potential for the perpetuation of bias has grown alongside its advances. While overt indicators of bias have been studied in machine translation, we argue that covert biases expose a problem that is further entrenched. Through the use of the gender-neutral language Turkish and the gendered language English, we examine cases of both overt and covert gender bias in MT models. Specifically, we introduce a method to investigate asymmetrical gender markings. We also assess bias in the attribution of personhood and examine occupational and personality stereotypes through overt bias indicators in MT models. Our work explores a deeper layer of bias in MT models and demonstrates the continued need for language-specific, interdisciplinary methodology in MT model development.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.10379](https://arxiv.org/abs/2108.10379) [cs.CL]** |
|           | (or **[arXiv:2108.10379v1](https://arxiv.org/abs/2108.10379v1) [cs.CL]** for this version) |





<h2 id="2021-08-25-2">2. Recurrent multiple shared layers in Depth for Neural Machine Translation
</h2>

Title: [Recurrent multiple shared layers in Depth for Neural Machine Translation](https://arxiv.org/abs/2108.10417)

Authors: [GuoLiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Yiyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y)

> Learning deeper models is usually a simple and effective approach to improve model performance, but deeper models have larger model parameters and are more difficult to train. To get a deeper model, simply stacking more layers of the model seems to work well, but previous works have claimed that it cannot benefit the model. We propose to train a deeper model with recurrent mechanism, which loops the encoder and decoder blocks of Transformer in the depth direction. To address the increasing of model parameters, we choose to share parameters in different recursive moments. We conduct our experiments on WMT16 English-to-German and WMT14 English-to-France translation tasks, our model outperforms the shallow Transformer-Base/Big baseline by 0.35, 1.45 BLEU points, which is 27.23% of Transformer-Big model parameters. Compared to the deep Transformer(20-layer encoder, 6-layer decoder), our model has similar model performance and infer speed, but our model parameters are 54.72% of the former.

| Comments: | 8 pages, 2 figures. arXiv admin note: substantial text overlap with [arXiv:2107.14590](https://arxiv.org/abs/2107.14590) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2108.10417](https://arxiv.org/abs/2108.10417) [cs.CL]** |
|           | (or **[arXiv:2108.10417v1](https://arxiv.org/abs/2108.10417v1) [cs.CL]** for this version) |





<h2 id="2021-08-25-3">3. More Than Words: Collocation Tokenization for Latent Dirichlet Allocation Models
</h2>

Title: [More Than Words: Collocation Tokenization for Latent Dirichlet Allocation Models](https://arxiv.org/abs/2108.10755)

Authors: [Jin Cheevaprawatdomrong](https://arxiv.org/search/cs?searchtype=author&query=Cheevaprawatdomrong%2C+J), [Alexandra Schofield](https://arxiv.org/search/cs?searchtype=author&query=Schofield%2C+A), [Attapol T. Rutherford](https://arxiv.org/search/cs?searchtype=author&query=Rutherford%2C+A+T)

> Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences. However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai. Here, we explore the use of Pearson's chi-squared test, t-statistics, and Word Pair Encoding (WPE) to produce tokens as input to the LDA model. The Chi-squared, t, and WPE tokenizers are trained on Wikipedia text to look for words that should be grouped together, such as compound nouns, proper nouns, and complex event verbs. We propose a new metric for measuring the clustering quality in settings where the vocabularies of the models differ. Based on this metric and other established metrics, we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those unmerged models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.10755](https://arxiv.org/abs/2108.10755) [cs.CL]** |
|           | (or **[arXiv:2108.10755v1](https://arxiv.org/abs/2108.10755v1) [cs.CL]** for this version) |





<h2 id="2021-08-25-4">4. Regularizing Transformers With Deep Probabilistic Layers
</h2>

Title: [Regularizing Transformers With Deep Probabilistic Layers](https://arxiv.org/abs/2108.10764)

Authors: [Aurora Cobo Aguilera](https://arxiv.org/search/cs?searchtype=author&query=Aguilera%2C+A+C), [Pablo Martínez Olmos](https://arxiv.org/search/cs?searchtype=author&query=Olmos%2C+P+M), [Antonio Artés-Rodríguez](https://arxiv.org/search/cs?searchtype=author&query=Artés-Rodríguez%2C+A), [Fernando Pérez-Cruz](https://arxiv.org/search/cs?searchtype=author&query=Pérez-Cruz%2C+F)

> Language models (LM) have grown with non-stop in the last decade, from sequence-to-sequence architectures to the state-of-the-art and utter attention-based Transformers. In this work, we demonstrate how the inclusion of deep generative models within BERT can bring more versatile models, able to impute missing/noisy words with richer text or even improve BLEU score. More precisely, we use a Gaussian Mixture Variational Autoencoder (GMVAE) as a regularizer layer and prove its effectiveness not only in Transformers but also in the most relevant encoder-decoder based LM, seq2seq with and without attention.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.10764](https://arxiv.org/abs/2108.10764) [cs.CL]** |
|           | (or **[arXiv:2108.10764v1](https://arxiv.org/abs/2108.10764v1) [cs.CL]** for this version) |






# 2021-08-24

[Return to Index](#Index)



<h2 id="2021-08-24-1">1. Grid-VLP: Revisiting Grid Features for Vision-Language Pre-training
</h2>

Title: [Grid-VLP: Revisiting Grid Features for Vision-Language Pre-training](https://arxiv.org/abs/2108.09479)

Authors: [Ming Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+M), [Haiyang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Chenliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Bin Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+B), [Junfeng Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+J), [Min Gui](https://arxiv.org/search/cs?searchtype=author&query=Gui%2C+M), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W)

> Existing approaches to vision-language pre-training (VLP) heavily rely on an object detector based on bounding boxes (regions), where salient objects are first detected from images and then a Transformer-based model is used for cross-modal fusion. Despite their superior performance, these approaches are bounded by the capability of the object detector in terms of both effectiveness and efficiency. Besides, the presence of object detection imposes unnecessary constraints on model designs and makes it difficult to support end-to-end training. In this paper, we revisit grid-based convolutional features for vision-language pre-training, skipping the expensive region-related steps. We propose a simple yet effective grid-based VLP method that works surprisingly well with the grid features. By pre-training only with in-domain datasets, the proposed Grid-VLP method can outperform most competitive region-based VLP methods on three examined vision-language understanding tasks. We hope that our findings help to further advance the state of the art of vision-language pre-training, and provide a new direction towards effective and efficient VLP.

| Subjects: | **Multimedia (cs.MM)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.09479](https://arxiv.org/abs/2108.09479) [cs.MM]** |
|           | (or **[arXiv:2108.09479v1](https://arxiv.org/abs/2108.09479v1) [cs.MM]** for this version) |





<h2 id="2021-08-24-2">2. Metric Learning in Multilingual Sentence Similarity Measurement for Document Alignment
</h2>

Title: [Metric Learning in Multilingual Sentence Similarity Measurement for Document Alignment](https://arxiv.org/abs/2108.09495)

Authors: [Charith Rajitha](https://arxiv.org/search/cs?searchtype=author&query=Rajitha%2C+C), [Lakmali Piyarathne](https://arxiv.org/search/cs?searchtype=author&query=Piyarathne%2C+L), [Dilan Sachintha](https://arxiv.org/search/cs?searchtype=author&query=Sachintha%2C+D), [Surangika Ranathunga](https://arxiv.org/search/cs?searchtype=author&query=Ranathunga%2C+S)

> Document alignment techniques based on multilingual sentence representations have recently shown state of the art results. However, these techniques rely on unsupervised distance measurement techniques, which cannot be fined-tuned to the task at hand. In this paper, instead of these unsupervised distance measurement techniques, we employ Metric Learning to derive task-specific distance measurements. These measurements are supervised, meaning that the distance measurement metric is trained using a parallel dataset. Using a dataset belonging to English, Sinhala, and Tamil, which belong to three different language families, we show that these task-specific supervised distance learning metrics outperform their unsupervised counterparts, for document alignment.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.09495](https://arxiv.org/abs/2108.09495) [cs.CL]** |
|           | (or **[arXiv:2108.09495v1](https://arxiv.org/abs/2108.09495v1) [cs.CL]** for this version) |







<h2 id="2021-08-24-3">3. A Unified Transformer-based Framework for Duplex Text Normalization
</h2>

Title: [A Unified Transformer-based Framework for Duplex Text Normalization](https://arxiv.org/abs/2108.09889)

Authors: [Tuan Manh Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+T+M), [Yang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Evelina Bakhturina](https://arxiv.org/search/cs?searchtype=author&query=Bakhturina%2C+E), [Boris Ginsburg](https://arxiv.org/search/cs?searchtype=author&query=Ginsburg%2C+B), [Heng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H)

> Text normalization (TN) and inverse text normalization (ITN) are essential preprocessing and postprocessing steps for text-to-speech synthesis and automatic speech recognition, respectively. Many methods have been proposed for either TN or ITN, ranging from weighted finite-state transducers to neural networks. Despite their impressive performance, these methods aim to tackle only one of the two tasks but not both. As a result, in a complete spoken dialog system, two separate models for TN and ITN need to be built. This heterogeneity increases the technical complexity of the system, which in turn increases the cost of maintenance in a production setting. Motivated by this observation, we propose a unified framework for building a single neural duplex system that can simultaneously handle TN and ITN. Combined with a simple but effective data augmentation method, our systems achieve state-of-the-art results on the Google TN dataset for English and Russian. They can also reach over 95% sentence-level accuracy on an internal English TN dataset without any additional fine-tuning. In addition, we also create a cleaned dataset from the Spoken Wikipedia Corpora for German and report the performance of our systems on the dataset. Overall, experimental results demonstrate the proposed duplex text normalization framework is highly effective and applicable to a range of domains and languages

| Comments: | Under Review                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.09889](https://arxiv.org/abs/2108.09889) [cs.CL]** |
|           | (or **[arXiv:2108.09889v1](https://arxiv.org/abs/2108.09889v1) [cs.CL]** for this version) |







<h2 id="2021-08-24-4">4. Semantic-Preserving Adversarial Text Attacks
</h2>

Title: [Semantic-Preserving Adversarial Text Attacks](https://arxiv.org/abs/2108.10015)

Authors: [Xinghao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+X), [Weifeng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W), [James Bailey](https://arxiv.org/search/cs?searchtype=author&query=Bailey%2C+J), [Tianqing Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+T), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D), [Wei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W)

> Deep neural networks (DNNs) are known to be vulnerable to adversarial images, while their robustness in text classification is rarely studied. Several lines of text attack methods have been proposed in the literature, including character-level, word-level, and sentence-level attacks. However, it is still a challenge to minimize the number of word changes necessary to induce misclassification, while simultaneously ensuring lexical correctness, syntactic soundness, and semantic similarity. In this paper, we propose a Bigram and Unigram based adaptive Semantic Preservation Optimization (BU-SPO) method to examine the vulnerability of deep models. Our method has four major merits. Firstly, we propose to attack text documents not only at the unigram word level but also at the bigram level which better keeps semantics and avoids producing meaningless outputs. Secondly, we propose a hybrid method to replace the input words with options among both their synonyms candidates and sememe candidates, which greatly enriches the potential substitutions compared to only using synonyms. Thirdly, we design an optimization algorithm, i.e., Semantic Preservation Optimization (SPO), to determine the priority of word replacements, aiming to reduce the modification cost. Finally, we further improve the SPO with a semantic Filter (named SPOF) to find the adversarial example with the highest semantic similarity. We evaluate the effectiveness of our BU-SPO and BU-SPOF on IMDB, AG's News, and Yahoo! Answers text datasets by attacking four popular DNNs models. Results show that our methods achieve the highest attack success rates and semantics rates by changing the smallest number of words compared with existing methods.

| Comments: | 12 pages, 3 figures, 10 tables                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2108.10015](https://arxiv.org/abs/2108.10015) [cs.CL]** |
|           | (or **[arXiv:2108.10015v1](https://arxiv.org/abs/2108.10015v1) [cs.CL]** for this version) |







# 2021-08-23

[Return to Index](#Index)



<h2 id="2021-08-23-1">1. CIGLI: Conditional Image Generation from Language & Image
</h2>

Title: [CIGLI: Conditional Image Generation from Language & Image](https://arxiv.org/abs/2108.08955)

Authors: [Xiaopeng Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+X), [Lynnette Ng](https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+L), [Jared Fernandez](https://arxiv.org/search/cs?searchtype=author&query=Fernandez%2C+J), [Hao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+H)

> Multi-modal generation has been widely explored in recent years. Current research directions involve generating text based on an image or vice versa. In this paper, we propose a new task called CIGLI: Conditional Image Generation from Language and Image. Instead of generating an image based on text as in text-image generation, this task requires the generation of an image from a textual description and an image prompt. We designed a new dataset to ensure that the text description describes information from both images, and that solely analyzing the description is insufficient to generate an image. We then propose a novel language-image fusion model which improves the performance over two established baseline methods, as evaluated by quantitative (automatic) and qualitative (human) evaluations. The code and dataset is available at [this https URL](https://github.com/vincentlux/CIGLI).

| Comments: | 5 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.08955](https://arxiv.org/abs/2108.08955) [cs.CV]** |
|           | (or **[arXiv:2108.08955v1](https://arxiv.org/abs/2108.08955v1) [cs.CV]** for this version) |





<h2 id="2021-08-23-2">2. Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling
</h2>

Title: [Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling](https://arxiv.org/abs/2108.08965)

Authors: [Xiaopeng Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+X), [Zhen Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+Z), [Yansen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Jean Oh](https://arxiv.org/search/cs?searchtype=author&query=Oh%2C+J), [Carolyn P. Rose](https://arxiv.org/search/cs?searchtype=author&query=Rose%2C+C+P)

> As an important task in multimodal context understanding, Text-VQA (Visual Question Answering) aims at question answering through reading text information in images. It differentiates from the original VQA task as Text-VQA requires large amounts of scene-text relationship understanding, in addition to the cross-modal grounding capability. In this paper, we propose Localize, Group, and Select (LOGOS), a novel model which attempts to tackle this problem from multiple aspects. LOGOS leverages two grounding tasks to better localize the key information of the image, utilizes scene text clustering to group individual OCR tokens, and learns to select the best answer from different sources of OCR (Optical Character Recognition) texts. Experiments show that LOGOS outperforms previous state-of-the-art methods on two Text-VQA benchmarks without using additional OCR annotation data. Ablation studies and analysis demonstrate the capability of LOGOS to bridge different modalities and better understand scene text.

| Comments: | 9 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.08965](https://arxiv.org/abs/2108.08965) [cs.CV]** |
|           | (or **[arXiv:2108.08965v1](https://arxiv.org/abs/2108.08965v1) [cs.CV]** for this version) |





<h2 id="2021-08-23-3">3. Airbert: In-domain Pretraining for Vision-and-Language Navigation
</h2>

Title: [Airbert: In-domain Pretraining for Vision-and-Language Navigation](https://arxiv.org/abs/2108.09105)

Authors: [Pierre-Louis Guhur](https://arxiv.org/search/cs?searchtype=author&query=Guhur%2C+P), [Makarand Tapaswi](https://arxiv.org/search/cs?searchtype=author&query=Tapaswi%2C+M), [Shizhe Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Ivan Laptev](https://arxiv.org/search/cs?searchtype=author&query=Laptev%2C+I), [Cordelia Schmid](https://arxiv.org/search/cs?searchtype=author&query=Schmid%2C+C)

> Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments using natural language instructions. Given the scarcity of domain-specific training data and the high diversity of image and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent methods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing small-scale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB, a large-scale and diverse in-domain VLN dataset. We first collect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal order inside PI pairs. We use BnB pretrain our Airbert model that can be adapted to discriminative and generative settings and show that it outperforms state of the art for Room-to-Room (R2R) navigation and Remote Referring Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining significantly increases performance on a challenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses.

| Comments: | To be published on ICCV 2021. Webpage is at [this https URL](https://airbert-vln.github.io/) linking to our dataset, codes and models |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2108.09105](https://arxiv.org/abs/2108.09105) [cs.CV]** |
|           | (or **[arXiv:2108.09105v1](https://arxiv.org/abs/2108.09105v1) [cs.CV]** for this version) |





<h2 id="2021-08-23-4">4. Group-based Distinctive Image Captioning with Memory Attention
</h2>

Title: [Group-based Distinctive Image Captioning with Memory Attention](https://arxiv.org/abs/2108.09151)

Authors: [Jiuniu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Wenjia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Qingzhong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Antoni B. Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+A+B)

> Describing images using natural language is widely known as image captioning, which has made consistent progress due to the development of computer vision and natural language generation techniques. Though conventional captioning models achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and SPICE, the ability of captions to distinguish the target image from other similar images is under-explored. To generate distinctive captions, a few pioneers employ contrastive learning or re-weighted the ground-truth captions, which focuses on one single input image. However, the relationships between objects in a similar image group (e.g., items or properties within the same album or fine-grained events) are neglected. In this paper, we improve the distinctiveness of image captions using a Group-based Distinctive Captioning Model (GdisCap), which compares each image with other images in one similar group and highlights the uniqueness of each image. In particular, we propose a group-based memory attention (GMA) module, which stores object features that are unique among the image group (i.e., with low similarity to objects in other images). These unique object features are highlighted when generating captions, resulting in more distinctive captions. Furthermore, the distinctive words in the ground-truth captions are selected to supervise the language decoder and GMA. Finally, we propose a new evaluation metric, distinctive word rate (DisWordRate) to measure the distinctiveness of captions. Quantitative results indicate that the proposed method significantly improves the distinctiveness of several baseline models, and achieves the state-of-the-art performance on both accuracy and distinctiveness. Results of a user study agree with the quantitative evaluation and demonstrate the rationality of the new metric DisWordRate.

| Comments: | Accepted at ACM MM 2021 (oral)                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2108.09151](https://arxiv.org/abs/2108.09151) [cs.CV]** |
|           | (or **[arXiv:2108.09151v1](https://arxiv.org/abs/2108.09151v1) [cs.CV]** for this version) |





<h2 id="2021-08-23-5">5. Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models
</h2>

Title: [Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models](https://arxiv.org/abs/2108.08877)

Authors: [Jianmo Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+J), [Gustavo Hernández {Á}brego](https://arxiv.org/search/cs?searchtype=author&query={Á}brego%2C+G+H), [Noah Constant](https://arxiv.org/search/cs?searchtype=author&query=Constant%2C+N), [Ji Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+J), [Keith B. Hall](https://arxiv.org/search/cs?searchtype=author&query=Hall%2C+K+B), [Daniel Cer](https://arxiv.org/search/cs?searchtype=author&query=Cer%2C+D), [Yinfei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y)

> We provide the first exploration of text-to-text transformers (T5) sentence embeddings. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks cast as sequence-to-sequence mapping problems, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods for extracting T5 sentence embeddings: two utilize only the T5 encoder and one uses the full T5 encoder-decoder model. Our encoder-only models outperforms BERT-based sentence embeddings on both transfer tasks and semantic textual similarity (STS). Our encoder-decoder method achieves further improvement on STS. Scaling up T5 from millions to billions of parameters is found to produce consistent improvements on downstream tasks. Finally, we introduce a two-stage contrastive learning approach that achieves a new state-of-art on STS using sentence embeddings, outperforming both Sentence BERT and SimCSE.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.08877](https://arxiv.org/abs/2108.08877) [cs.CL]** |
|           | (or **[arXiv:2108.08877v1](https://arxiv.org/abs/2108.08877v1) [cs.CL]** for this version) |





<h2 id="2021-08-23-6">6. Fastformer: Additive Attention is All You Need
</h2>

Title: [Fastformer: Additive Attention is All You Need](https://arxiv.org/abs/2108.09084)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Tao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+T), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Transformer is a powerful model for text understanding. However, it is inefficient due to its quadratic complexity to input sequence length. Although there are many methods on Transformer acceleration, they are still either inefficient on long sequences or not effective enough. In this paper, we propose Fastformer, which is an efficient Transformer model based on additive attention. In Fastformer, instead of modeling the pair-wise interactions between tokens, we first use additive attention mechanism to model global contexts, and then further transform each token representation based on its interaction with global context representations. In this way, Fastformer can achieve effective context modeling with linear complexity. Extensive experiments on five datasets show that Fastformer is much more efficient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.09084](https://arxiv.org/abs/2108.09084) [cs.CL]** |
|           | (or **[arXiv:2108.09084v1](https://arxiv.org/abs/2108.09084v1) [cs.CL]** for this version) |





<h2 id="2021-08-23-7">7. Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer
</h2>

Title: [Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer](https://arxiv.org/abs/2108.09193)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Tao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+T), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Transformer has achieved great success in NLP. However, the quadratic complexity of the self-attention mechanism in Transformer makes it inefficient in handling long sequences. Many existing works explore to accelerate Transformers by computing sparse self-attention instead of a dense one, which usually attends to tokens at certain positions or randomly selected tokens. However, manually selected or random tokens may be uninformative for context modeling. In this paper, we propose Smart Bird, which is an efficient and effective Transformer with learnable sparse attention. In Smart Bird, we first compute a sketched attention matrix with a single-head low-dimensional Transformer, which aims to find potential important interactions between tokens. We then sample token pairs based on their probability scores derived from the sketched attention matrix to generate different sparse attention index matrices for different attention heads. Finally, we select token embeddings according to the index matrices to form the input of sparse attention networks. Extensive experiments on six benchmark datasets for different tasks validate the efficiency and effectiveness of Smart Bird in text modeling.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.09193](https://arxiv.org/abs/2108.09193) [cs.CL]** |
|           | (or **[arXiv:2108.09193v1](https://arxiv.org/abs/2108.09193v1) [cs.CL]** for this version) |






# 2021-08-20

[Return to Index](#Index)



<h2 id="2021-08-20-1">1. Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks
</h2>

Title: [Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks](https://arxiv.org/abs/2108.08375)

Authors: [Weicheng Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+W), [Kai Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+K), [Renze Lou](https://arxiv.org/search/cs?searchtype=author&query=Lou%2C+R), [Lili Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Soroush Vosoughi](https://arxiv.org/search/cs?searchtype=author&query=Vosoughi%2C+S)

> This paper studies the relative importance of attention heads in Transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks. Prior research has found that only a few attention heads are important in each mono-lingual Natural Language Processing (NLP) task and pruning the remaining heads leads to comparable or improved performance of the model. However, the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks. Through extensive experiments, we show that (1) pruning a number of attention heads in a multi-lingual Transformer-based model has, in general, positive effects on its performance in cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned can be ranked using gradients and identified with a few trial experiments. Our experiments focus on sequence labeling tasks, with potential applicability on other cross-lingual and multi-lingual tasks. For comprehensiveness, we examine two pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and XLM-R, on three tasks across 9 languages each. We also discuss the validity of our findings and their extensibility to truly resource-scarce languages and other task settings.

| Comments: | In ACL 2021                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| DOI:      | [10.18653/v1/2021.acl-long.152](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2F2021.acl-long.152&v=ee32058a) |
| Cite as:  | **[arXiv:2108.08375](https://arxiv.org/abs/2108.08375) [cs.CL]** |
|           | (or **[arXiv:2108.08375v1](https://arxiv.org/abs/2108.08375v1) [cs.CL]** for this version) |





<h2 id="2021-08-20-2">2. MvSR-NAT: Multi-view Subset Regularization for Non-Autoregressive Machine Translation
</h2>

Title: [MvSR-NAT: Multi-view Subset Regularization for Non-Autoregressive Machine Translation](https://arxiv.org/abs/2108.08447)

Authors: [Pan Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+P), [Zexian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Xiaohui Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X)

> Conditional masked language models (CMLM) have shown impressive progress in non-autoregressive machine translation (NAT). They learn the conditional translation model by predicting the random masked subset in the target sentence. Based on the CMLM framework, we introduce Multi-view Subset Regularization (MvSR), a novel regularization method to improve the performance of the NAT model. Specifically, MvSR consists of two parts: (1) \textit{shared mask consistency}: we forward the same target with different mask strategies, and encourage the predictions of shared mask positions to be consistent with each other. (2) \textit{model consistency}, we maintain an exponential moving average of the model weights, and enforce the predictions to be consistent between the average model and the online model. Without changing the CMLM-based architecture, our approach achieves remarkable performance on three public benchmarks with 0.36-1.14 BLEU gains over previous NAT models. Moreover, compared with the stronger Transformer baseline, we reduce the gap to 0.01-0.44 BLEU scores on small datasets (WMT16 RO↔EN and IWSLT DE→EN).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.08447](https://arxiv.org/abs/2108.08447) [cs.CL]** |
|           | (or **[arXiv:2108.08447v1](https://arxiv.org/abs/2108.08447v1) [cs.CL]** for this version) |





<h2 id="2021-08-20-3">3. Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021
</h2>

Title: [Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021](https://arxiv.org/abs/2108.08556)

Authors: [Karthik Puranik](https://arxiv.org/search/cs?searchtype=author&query=Puranik%2C+K), [Adeep Hande](https://arxiv.org/search/cs?searchtype=author&query=Hande%2C+A), [Ruba Priyadharshini](https://arxiv.org/search/cs?searchtype=author&query=Priyadharshini%2C+R), [Thenmozi Durairaj](https://arxiv.org/search/cs?searchtype=author&query=Durairaj%2C+T), [Anbukkarasi Sampath](https://arxiv.org/search/cs?searchtype=author&query=Sampath%2C+A), [Kingston Pal Thamburaj](https://arxiv.org/search/cs?searchtype=author&query=Thamburaj%2C+K+P), [Bharathi Raja Chakravarthi](https://arxiv.org/search/cs?searchtype=author&query=Chakravarthi%2C+B+R)

> This paper reports the Machine Translation (MT) systems submitted by the IIITT team for the English->Marathi and English->Irish language pairs LoResMT 2021 shared task. The task focuses on getting exceptional translations for rather low-resourced languages like Irish and Marathi. We fine-tune IndicTrans, a pretrained multilingual NMT model for English->Marathi, using external parallel corpus as input for additional training. We have used a pretrained Helsinki-NLP Opus MT English->Irish model for the latter language pair. Our approaches yield relatively promising results on the BLEU metrics. Under the team name IIITT, our systems ranked 1, 1, and 2 in English->Marathi, Irish->English, and English->Irish, respectively.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.08556](https://arxiv.org/abs/2108.08556) [cs.CL]** |
|           | (or **[arXiv:2108.08556v1](https://arxiv.org/abs/2108.08556v1) [cs.CL]** for this version) |





<h2 id="2021-08-20-4">4. Contrastive Language-Image Pre-training for the Italian Language
</h2>

Title: [Contrastive Language-Image Pre-training for the Italian Language](https://arxiv.org/abs/2108.08688)

Authors: [Federico Bianchi](https://arxiv.org/search/cs?searchtype=author&query=Bianchi%2C+F), [Giuseppe Attanasio](https://arxiv.org/search/cs?searchtype=author&query=Attanasio%2C+G), [Raphael Pisoni](https://arxiv.org/search/cs?searchtype=author&query=Pisoni%2C+R), [Silvia Terragni](https://arxiv.org/search/cs?searchtype=author&query=Terragni%2C+S), [Gabriele Sarti](https://arxiv.org/search/cs?searchtype=author&query=Sarti%2C+G), [Sri Lakshmi](https://arxiv.org/search/cs?searchtype=author&query=Lakshmi%2C+S)

> CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal model that jointly learns representations of images and texts. The model is trained on a massive amount of English data and shows impressive performance on zero-shot classification tasks. Training the same model on a different language is not trivial, since data in other languages might be not enough and the model needs high-quality translations of the texts to guarantee a good performance. In this paper, we present the first CLIP model for the Italian Language (CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show that CLIP-Italian outperforms the multilingual CLIP model on the tasks of image retrieval and zero-shot classification.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.08688](https://arxiv.org/abs/2108.08688) [cs.CL]** |
|           | (or **[arXiv:2108.08688v1](https://arxiv.org/abs/2108.08688v1) [cs.CL]** for this version) |





# 2021-08-19

[Return to Index](#Index)



<h2 id="2021-08-19-1">1. X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics
</h2>

Title: [X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics](https://arxiv.org/abs/2108.08217)

Authors: [Yehao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yingwei Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+Y), [Jingwen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Ting Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+T), [Tao Mei](https://arxiv.org/search/cs?searchtype=author&query=Mei%2C+T)

> With the rise and development of deep learning over the past decade, there has been a steady momentum of innovation and breakthroughs that convincingly push the state-of-the-art of cross-modal analytics between vision and language in multimedia field. Nevertheless, there has not been an open-source codebase in support of training and deploying numerous neural network models for cross-modal analytics in a unified and modular fashion. In this work, we propose X-modaler -- a versatile and high-performance codebase that encapsulates the state-of-the-art cross-modal analytics into several general-purpose stages (e.g., pre-processing, encoder, cross-modal interaction, decoder, and decode strategy). Each stage is empowered with the functionality that covers a series of modules widely adopted in state-of-the-arts and allows seamless switching in between. This way naturally enables a flexible implementation of state-of-the-art algorithms for image captioning, video captioning, and vision-language pre-training, aiming to facilitate the rapid development of research community. Meanwhile, since the effective modular designs in several stages (e.g., cross-modal interaction) are shared across different vision-language tasks, X-modaler can be simply extended to power startup prototypes for other tasks in cross-modal analytics, including visual question answering, visual commonsense reasoning, and cross-modal retrieval. X-modaler is an Apache-licensed codebase, and its source codes, sample projects and pre-trained models are available on-line: [this https URL](https://github.com/YehLi/xmodaler).

| Comments: | Accepted by 2021 ACMMM Open Source Software Competition. Source code: [this https URL](https://github.com/YehLi/xmodaler) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM) |
| Cite as:  | **[arXiv:2108.08217](https://arxiv.org/abs/2108.08217) [cs.CV]** |
|           | (or **[arXiv:2108.08217v1](https://arxiv.org/abs/2108.08217v1) [cs.CV]** for this version) |





<h2 id="2021-08-19-2">2. GGP: A Graph-based Grouping Planner for Explicit Control of Long Text Generation
</h2>

Title: [GGP: A Graph-based Grouping Planner for Explicit Control of Long Text Generation](https://arxiv.org/abs/2108.07998)

Authors: [Xuming Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+X), [Shaobo Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+S), [Zhongzhou Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Wei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Ji Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Haiqing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H)

> Existing data-driven methods can well handle short text generation. However, when applied to the long-text generation scenarios such as story generation or advertising text generation in the commercial scenario, these methods may generate illogical and uncontrollable texts. To address these aforementioned issues, we propose a graph-based grouping planner(GGP) following the idea of first-plan-then-generate. Specifically, given a collection of key phrases, GGP firstly encodes these phrases into an instance-level sequential representation and a corpus-level graph-based representation separately. With these two synergic representations, we then regroup these phrases into a fine-grained plan, based on which we generate the final long text. We conduct our experiments on three long text generation datasets and the experimental results reveal that GGP significantly outperforms baselines, which proves that GGP can control the long text generation by knowing how to say and in what order.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.07998](https://arxiv.org/abs/2108.07998) [cs.CL]** |
|           | (or **[arXiv:2108.07998v1](https://arxiv.org/abs/2108.07998v1) [cs.CL]** for this version) |





<h2 id="2021-08-19-3">3. Table Caption Generation in Scholarly Documents Leveraging Pre-trained Language Models
</h2>

Title: [Table Caption Generation in Scholarly Documents Leveraging Pre-trained Language Models](https://arxiv.org/abs/2108.08111)

Authors: [Junjie H. Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J+H), [Kohei Shinden](https://arxiv.org/search/cs?searchtype=author&query=Shinden%2C+K), [Makoto P. Kato](https://arxiv.org/search/cs?searchtype=author&query=Kato%2C+M+P)

> This paper addresses the problem of generating table captions for scholarly documents, which often require additional information outside the table. To this end, we propose a method of retrieving relevant sentences from the paper body, and feeding the table content as well as the retrieved sentences into pre-trained language models (e.g. T5 and GPT-2) for generating table captions. The contributions of this paper are: (1) discussion on the challenges in table captioning for scholarly documents; (2) development of a dataset DocBank-TB, which is publicly available; and (3) comparison of caption generation methods for scholarly documents with different strategies to retrieve relevant sentences from the paper body. Our experimental results showed that T5 is the better generation model for this task, as it outperformed GPT-2 in BLEU and METEOR implying that the generated text are clearer and more precise. Moreover, inputting relevant sentences matching the row header or whole table is effective.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | 2021 IEEE 10th Global Conference on Consumer Electronics (GCCE 2021) |
| Cite as:           | **[arXiv:2108.08111](https://arxiv.org/abs/2108.08111) [cs.CL]** |
|                    | (or **[arXiv:2108.08111v1](https://arxiv.org/abs/2108.08111v1) [cs.CL]** for this version) |





<h2 id="2021-08-19-4">4. Deep Natural Language Processing for LinkedIn Search Systems
</h2>

Title: [Deep Natural Language Processing for LinkedIn Search Systems](https://arxiv.org/abs/2108.08252)

Authors: [Weiwei Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+W), [Xiaowei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Sida Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Michaeel Kazi](https://arxiv.org/search/cs?searchtype=author&query=Kazi%2C+M), [Zhoutong Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Z), [Huiji Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+H), [Jun Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+J), [Liang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Bo Long](https://arxiv.org/search/cs?searchtype=author&query=Long%2C+B)

> Many search systems work with large amounts of natural language data, e.g., search queries, user profiles and documents, where deep learning based natural language processing techniques (deep NLP) can be of great help. In this paper, we introduce a comprehensive study of applying deep NLP techniques to five representative tasks in search engines. Through the model design and experiments of the five tasks, readers can find answers to three important questions: (1) When is deep NLP helpful/not helpful in search systems? (2) How to address latency challenges? (3) How to ensure model robustness? This work builds on existing efforts of LinkedIn search, and is tested at scale on a commercial search engine. We believe our experiences can provide useful insights for the industry and research communities.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.08252](https://arxiv.org/abs/2108.08252) [cs.CL]** |
|           | (or **[arXiv:2108.08252v1](https://arxiv.org/abs/2108.08252v1) [cs.CL]** for this version) |





# 2021-08-18

[Return to Index](#Index)



<h2 id="2021-08-18-1">1. Modeling Protein Using Large-scale Pretrain Language Model
</h2>

Title: [Modeling Protein Using Large-scale Pretrain Language Model](https://arxiv.org/abs/2108.07435)

Authors: [Yijia Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+Y), [Jiezhong Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+J), [Ziang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Chang-Yu Hsieh](https://arxiv.org/search/cs?searchtype=author&query=Hsieh%2C+C), [Jie Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J)

> Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token-level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at [this https URL](https://github.com/THUDM/ProteinLM).

| Comments: | Accepted paper in Pretrain@KDD 2021 (The International Workshop on Pretraining: Algorithms, Architectures, and Applications) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Biomolecules (q-bio.BM) |
| Cite as:  | **[arXiv:2108.07435](https://arxiv.org/abs/2108.07435) [cs.LG]** |
|           | (or **[arXiv:2108.07435v1](https://arxiv.org/abs/2108.07435v1) [cs.LG]** for this version) |





<h2 id="2021-08-18-2">2. A Light-weight contextual spelling correction model for customizing transducer-based speech recognition systems
</h2>

Title: [A Light-weight contextual spelling correction model for customizing transducer-based speech recognition systems](https://arxiv.org/abs/2108.07493)

Authors: [Xiaoqiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yanqing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Sheng Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+S), [Jinyu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> It's challenging to customize transducer-based automatic speech recognition (ASR) system with context information which is dynamic and unavailable during model training. In this work, we introduce a light-weight contextual spelling correction model to correct context-related recognition errors in transducer-based ASR systems. We incorporate the context information into the spelling correction model with a shared context encoder and use a filtering algorithm to handle large-size context lists. Experiments show that the model improves baseline ASR model performance with about 50% relative word error rate reduction, which also significantly outperforms the baseline method such as contextual LM biasing. The model also shows excellent performance for out-of-vocabulary terms not seen during training.

| Comments: | This paper has been accepted by Interspeech 2021             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2108.07493](https://arxiv.org/abs/2108.07493) [cs.CL]** |
|           | (or **[arXiv:2108.07493v1](https://arxiv.org/abs/2108.07493v1) [cs.CL]** for this version) |





<h2 id="2021-08-18-3">3. A Game Interface to Study Semantic Grounding in Text-Based Models
</h2>

Title: [A Game Interface to Study Semantic Grounding in Text-Based Models](https://arxiv.org/abs/2108.07708)

Authors: [Timothee Mickus](https://arxiv.org/search/cs?searchtype=author&query=Mickus%2C+T), [Mathieu Constant](https://arxiv.org/search/cs?searchtype=author&query=Constant%2C+M), [Denis Paperno](https://arxiv.org/search/cs?searchtype=author&query=Paperno%2C+D)

> Can language models learn grounded representations from text distribution alone? This question is both central and recurrent in natural language processing; authors generally agree that grounding requires more than textual distribution. We propose to experimentally test this claim: if any two words have different meanings and yet cannot be distinguished from distribution alone, then grounding is out of the reach of text-based models. To that end, we present early work on an online game for the collection of human judgments on the distributional similarity of word pairs in five languages. We further report early results of our data collection campaign.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.07708](https://arxiv.org/abs/2108.07708) [cs.CL]** |
|           | (or **[arXiv:2108.07708v1](https://arxiv.org/abs/2108.07708v1) [cs.CL]** for this version) |





<h2 id="2021-08-18-4">4. Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition
</h2>

Title: [Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition](https://arxiv.org/abs/2108.07789)

Authors: [Xianrui Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X), [Chao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Philip C. Woodland](https://arxiv.org/search/cs?searchtype=author&query=Woodland%2C+P+C)

> Language models (LMs) pre-trained on massive amounts of text, in particular bidirectional encoder representations from Transformers (BERT), generative pre-training (GPT), and GPT-2, have become a key technology for many natural language processing tasks. In this paper, we present results using fine-tuned GPT, GPT-2, and their combination for automatic speech recognition (ASR). Unlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct product of the output probabilities is no longer a valid language prior probability. A conversion method is proposed to compute the correct language prior probability based on bidirectional LM outputs in a mathematically exact way. Experimental results on the widely used AMI and Switchboard ASR tasks showed that the combination of the fine-tuned GPT and GPT-2 outperformed the combination of three neural LMs with different architectures trained from scratch on the in-domain text by up to a 12% relative word error rate reduction (WERR). Furthermore, the proposed conversion for language prior probabilities enables BERT to receive an extra 3% relative WERR, and the combination of BERT, GPT and GPT-2 results in further improvements.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.07789](https://arxiv.org/abs/2108.07789) [cs.CL]** |
|           | (or **[arXiv:2108.07789v1](https://arxiv.org/abs/2108.07789v1) [cs.CL]** for this version) |







# 2021-08-17

[Return to Index](#Index)



<h2 id="2021-08-17-1">1. ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration
</h2>

Title: [ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration](https://arxiv.org/abs/2108.07073)

Authors: [Yuhao Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y), [Zhou Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z), [Chunqi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Zhongzhou Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Ji Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Meng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Jun Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J)

> Vision-and-language pretraining (VLP) aims to learn generic multimodal representations from massive image-text pairs. While various successful attempts have been proposed, learning fine-grained semantic alignments between image-text pairs plays a key role in their approaches. Nevertheless, most existing VLP approaches have not fully utilized the intrinsic knowledge within the image-text pairs, which limits the effectiveness of the learned alignments and further restricts the performance of their models. To this end, we introduce a new VLP method called ROSITA, which integrates the cross- and intra-modal knowledge in a unified scene graph to enhance the semantic alignments. Specifically, we introduce a novel structural knowledge masking (SKM) strategy to use the scene graph structure as a priori to perform masked language (region) modeling, which enhances the semantic alignments by eliminating the interference information within and across modalities. Extensive ablation studies and comprehensive analysis verifies the effectiveness of ROSITA in semantic alignments. Pretrained with both in-domain and out-of-domain datasets, ROSITA significantly outperforms existing state-of-the-art VLP methods on three typical vision-and-language tasks over six benchmark datasets.

| Comments: | Accepted at ACM Multimedia 2021. Code available at [this https URL](https://github.com/MILVLG/rosita) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.07073](https://arxiv.org/abs/2108.07073) [cs.CV]** |
|           | (or **[arXiv:2108.07073v1](https://arxiv.org/abs/2108.07073v1) [cs.CV]** for this version) |





<h2 id="2021-08-17-2">2. Who's Waldo? Linking People Across Text and Images
</h2>

Title: [Who's Waldo? Linking People Across Text and Images](https://arxiv.org/abs/2108.07253)

Authors: [Claire Yuqing Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+C+Y), [Apoorv Khandelwal](https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+A), [Yoav Artzi](https://arxiv.org/search/cs?searchtype=author&query=Artzi%2C+Y), [Noah Snavely](https://arxiv.org/search/cs?searchtype=author&query=Snavely%2C+N), [Hadar Averbuch-Elor](https://arxiv.org/search/cs?searchtype=author&query=Averbuch-Elor%2C+H)

> We present a task and benchmark dataset for person-centric visual grounding, the problem of linking between people named in a caption and people pictured in an image. In contrast to prior work in visual grounding, which is predominantly object-based, our new task masks out the names of people in captions in order to encourage methods trained on such image-caption pairs to focus on contextual cues (such as rich interactions between multiple people), rather than learning associations between names and appearances. To facilitate this task, we introduce a new dataset, Who's Waldo, mined automatically from image-caption data on Wikimedia Commons. We propose a Transformer-based method that outperforms several strong baselines on this task, and are releasing our data to the research community to spur work on contextual models that consider both vision and language.

| Comments: | Published in ICCV 2021 (Oral). Project webpage: [this https URL](https://whoswaldo.github.io/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2108.07253](https://arxiv.org/abs/2108.07253) [cs.CV]** |
|           | (or **[arXiv:2108.07253v1](https://arxiv.org/abs/2108.07253v1) [cs.CV]** for this version) |





<h2 id="2021-08-17-3">3. Findings of the LoResMT 2021 Shared Task on COVID and Sign Language for Low-resource Languages
</h2>

Title: [Findings of the LoResMT 2021 Shared Task on COVID and Sign Language for Low-resource Languages](https://arxiv.org/abs/2108.06598)

Authors: [Atul Kr. Ojha](https://arxiv.org/search/cs?searchtype=author&query=Ojha%2C+A+K), [Chao-Hong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Katharina Kann](https://arxiv.org/search/cs?searchtype=author&query=Kann%2C+K), [John Ortega](https://arxiv.org/search/cs?searchtype=author&query=Ortega%2C+J), [Sheetal Shatam](https://arxiv.org/search/cs?searchtype=author&query=Shatam%2C+S), [Theodorus Fransen](https://arxiv.org/search/cs?searchtype=author&query=Fransen%2C+T)

> We present the findings of the LoResMT 2021 shared task which focuses on machine translation (MT) of COVID-19 data for both low-resource spoken and sign languages. The organization of this task was conducted as part of the fourth workshop on technologies for machine translation of low resource languages (LoResMT). Parallel corpora is presented and publicly available which includes the following directions: English↔Irish, English↔Marathi, and Taiwanese Sign language↔Traditional Chinese. Training data consists of 8112, 20933 and 128608 segments, respectively. There are additional monolingual data sets for Marathi and English that consist of 21901 segments. The results presented here are based on entries from a total of eight teams. Three teams submitted systems for English↔Irish while five teams submitted systems for English↔Marathi. Unfortunately, there were no systems submissions for the Taiwanese Sign language↔Traditional Chinese task. Maximum system performance was computed using BLEU and follow as 36.0 for English--Irish, 34.6 for Irish--English, 24.2 for English--Marathi, and 31.3 for Marathi--English.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.06598](https://arxiv.org/abs/2108.06598) [cs.CL]** |
|           | (or **[arXiv:2108.06598v1](https://arxiv.org/abs/2108.06598v1) [cs.CL]** for this version) |





<h2 id="2021-08-17-4">4. Maps Search Misspelling Detection Leveraging Domain-Augmented Contextual Representations
</h2>

Title: [Maps Search Misspelling Detection Leveraging Domain-Augmented Contextual Representations](https://arxiv.org/abs/2108.06842)

Authors: [Yutong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y)

> Building an independent misspelling detector and serve it before correction can bring multiple benefits to speller and other search components, which is particularly true for the most commonly deployed noisy-channel based speller systems. With rapid development of deep learning and substantial advancement in contextual representation learning such as BERTology, building a decent misspelling detector without having to rely on hand-crafted features associated with noisy-channel architecture becomes more-than-ever accessible. However BERTolgy models are trained with natural language corpus but Maps Search is highly domain specific, would BERTology continue its success. In this paper we design 4 stages of models for misspeling detection ranging from the most basic LSTM to single-domain augmented fine-tuned BERT. We found for Maps Search in our case, other advanced BERTology family model such as RoBERTa does not necessarily outperform BERT, and a classic cross-domain fine-tuned full BERT even underperforms a smaller single-domain fine-tuned BERT. We share more findings through comprehensive modeling experiments and analysis, we also briefly cover the data generation algorithm breakthrough.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.06842](https://arxiv.org/abs/2108.06842) [cs.CL]** |
|           | (or **[arXiv:2108.06842v1](https://arxiv.org/abs/2108.06842v1) [cs.CL]** for this version) |





<h2 id="2021-08-17-5">5. A Single Example Can Improve Zero-Shot Data Generation
</h2>

Title: [A Single Example Can Improve Zero-Shot Data Generation](https://arxiv.org/abs/2108.06991)

Authors: [Pavel Burnyshev](https://arxiv.org/search/cs?searchtype=author&query=Burnyshev%2C+P), [Valentin Malykh](https://arxiv.org/search/cs?searchtype=author&query=Malykh%2C+V), [Andrey Bout](https://arxiv.org/search/cs?searchtype=author&query=Bout%2C+A), [Ekaterina Artemova](https://arxiv.org/search/cs?searchtype=author&query=Artemova%2C+E), [Irina Piontkovskaya](https://arxiv.org/search/cs?searchtype=author&query=Piontkovskaya%2C+I)

> Sub-tasks of intent classification, such as robustness to distribution shift, adaptation to specific user groups and personalization, out-of-domain detection, require extensive and flexible datasets for experiments and evaluation. As collecting such datasets is time- and labor-consuming, we propose to use text generation methods to gather datasets. The generator should be trained to generate utterances that belong to the given intent. We explore two approaches to generating task-oriented utterances. In the zero-shot approach, the model is trained to generate utterances from seen intents and is further used to generate utterances for intents unseen during training. In the one-shot approach, the model is presented with a single utterance from a test intent. We perform a thorough automatic, and human evaluation of the dataset generated utilizing two proposed approaches. Our results reveal that the attributes of the generated data are close to original test sets, collected via crowd-sourcing.

| Comments: | To appear in INLG2021 proceedings                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.06991](https://arxiv.org/abs/2108.06991) [cs.CL]** |
|           | (or **[arXiv:2108.06991v1](https://arxiv.org/abs/2108.06991v1) [cs.CL]** for this version) |





<h2 id="2021-08-17-6">6. Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages
</h2>

Title: [Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages](https://arxiv.org/abs/2108.07127)

Authors: [Zhong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Z), [Alex Waibel](https://arxiv.org/search/cs?searchtype=author&query=Waibel%2C+A)

> We translate a closed text that is known in advance and available in many languages into a new and severely low resource language. Most human translation efforts adopt a portion-based approach to translate consecutive pages/chapters in order, which may not suit machine translation. We compare the portion-based approach that optimizes coherence of the text locally with the random sampling approach that increases coverage of the text globally. Our results show that the random sampling approach performs better. When training on a seed corpus of ~1,000 lines from the Bible and testing on the rest of the Bible (~30,000 lines), random sampling gives a performance gain of +11.0 BLEU using English as a simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a Mayan language. Furthermore, we compare three ways of updating machine translation models with increasing amount of human post-edited data through iterations. We find that adding newly post-edited data to training after vocabulary update without self-supervision performs the best. We propose an algorithm for human and machine to work together seamlessly to translate a closed text into a severely low resource language.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | In Proceedings of the LoResMT Workshop of the 18th Biennial Machine Translation Summit in 2021 |
| Cite as:           | **[arXiv:2108.07127](https://arxiv.org/abs/2108.07127) [cs.CL]** |
|                    | (or **[arXiv:2108.07127v1](https://arxiv.org/abs/2108.07127v1) [cs.CL]** for this version) |





<h2 id="2021-08-17-7">7. MTG: A Benchmarking Suite for Multilingual Text Generation
</h2>

Title: [MTG: A Benchmarking Suite for Multilingual Text Generation](https://arxiv.org/abs/2108.07140)

Authors: [Yiran Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Zhenqiao Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Z), [Xianze Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Danqing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Jingjing Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Jiaze Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> We introduce MTG, a new benchmark suite for training and evaluating multilingual text generation. It is the first and largest text generation benchmark with 120k human-annotated multi-way parallel data for three tasks (story generation, question generation, and title generation) across four languages (English, German, French, and Spanish). Based on it, we set various evaluation scenarios and make a deep analysis of several popular multilingual generation models from different aspects. Our benchmark suite will encourage the multilingualism for text generation community with more human-annotated parallel data and more diverse generation scenarios.

| Comments: | 9 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.07140](https://arxiv.org/abs/2108.07140) [cs.CL]** |
|           | (or **[arXiv:2108.07140v1](https://arxiv.org/abs/2108.07140v1) [cs.CL]** for this version) |






# 2021-08-16

[Return to Index](#Index)



<h2 id="2021-08-16-1">1. AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing
</h2>

Title: [FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning](https://arxiv.org/abs/2108.06332)

Authors: [Jing Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Yanan Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y), [Jie Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J), [Jian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Zhilin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z)

> Most previous methods for text data augmentation are limited to simple tasks and weak baselines. We explore data augmentation on hard tasks (i.e., few-shot natural language understanding) and strong baselines (i.e., pretrained models with over one billion parameters). Under this setting, we reproduced a large number of previous augmentation methods and found that these methods bring marginal gains at best and sometimes degrade the performance much. To address this challenge, we propose a novel data augmentation method FlipDA that jointly uses a generative model and a classifier to generate label-flipped data. Central to the idea of FlipDA is the discovery that generating label-flipped data is more crucial to the performance than generating label-preserved data. Experiments show that FlipDA achieves a good tradeoff between effectiveness and robustness---it substantially improves many tasks while not negatively affecting the others.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.06332](https://arxiv.org/abs/2108.06332) [cs.CL]** |
|           | (or **[arXiv:2108.06332v1](https://arxiv.org/abs/2108.06332v1) [cs.CL]** for this version) |










# 2021-08-13

[Return to Index](#Index)



<h2 id="2021-08-13-1">1. AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing
</h2>

Title: [AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing](https://arxiv.org/abs/2108.05542)

Authors: Transformer-based pretrained language models (T-PTLMs) have achieved great success in almost every NLP task. The evolution of these models started with GPT and BERT. These models are built on the top of transformers, self-supervised learning and transfer learning. Transformed-based PTLMs learn universal language representations from large volumes of text data using self-supervised learning and transfer this knowledge to downstream tasks. These models provide good background knowledge to downstream tasks which avoids training of downstream models from scratch. In this comprehensive survey paper, we initially give a brief overview of self-supervised learning. Next, we explain various core concepts like pretraining, pretraining methods, pretraining tasks, embeddings and downstream adaptation methods. Next, we present a new taxonomy of T-PTLMs and then give brief overview of various benchmarks including both intrinsic and extrinsic. We present a summary of various useful libraries to work with T-PTLMs. Finally, we highlight some of the future research directions which will further improve these models. We strongly believe that this comprehensive survey paper will serve as a good reference to learn the core concepts as well as to stay updated with the recent happenings in T-PTLMs.

| Comments: | Preprint under review                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.05542](https://arxiv.org/abs/2108.05542) [cs.CL]** |
|           | (or **[arXiv:2108.05542v1](https://arxiv.org/abs/2108.05542v1) [cs.CL]** for this version) |





<h2 id="2021-08-13-2">2. The paradox of the compositionality of natural language: a neural machine translation case study
</h2>

Title: [The paradox of the compositionality of natural language: a neural machine translation case study](https://arxiv.org/abs/2108.05885)

Authors: Moving towards human-like linguistic performance is often argued to require compositional generalisation. Whether neural networks exhibit this ability is typically studied using artificial languages, for which the compositionality of input fragments can be guaranteed and their meanings algebraically composed. However, compositionality in natural language is vastly more complex than this rigid, arithmetics-like version of compositionality, and as such artificial compositionality tests do not allow us to draw conclusions about how neural models deal with compositionality in more realistic scenarios. In this work, we re-instantiate three compositionality tests from the literature and reformulate them for neural machine translation (NMT). The results highlight two main issues: the inconsistent behaviour of NMT models and their inability to (correctly) modulate between local and global processing. Aside from an empirical study, our work is a call to action: we should rethink the evaluation of compositionality in neural networks of natural language, where composing meaning is not as straightforward as doing the math.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.05885](https://arxiv.org/abs/2108.05885) [cs.CL]** |
|           | (or **[arXiv:2108.05885v1](https://arxiv.org/abs/2108.05885v1) [cs.CL]** for this version) |





# 2021-08-12

[Return to Index](#Index)



<h2 id="2021-08-12-1">1. Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion
</h2>

Title: [Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion](https://arxiv.org/abs/2108.04927)

Authors: [Alessandro Suglia](https://arxiv.org/search/cs?searchtype=author&query=Suglia%2C+A), [Qiaozi Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Q), [Jesse Thomason](https://arxiv.org/search/cs?searchtype=author&query=Thomason%2C+J), [Govind Thattai](https://arxiv.org/search/cs?searchtype=author&query=Thattai%2C+G), [Gaurav Sukhatme](https://arxiv.org/search/cs?searchtype=author&query=Sukhatme%2C+G)

> Language-guided robots performing home and office tasks must navigate in and interact with the world. Grounding language instructions against visual observations and actions to take in an environment is an open challenge. We present Embodied BERT (EmBERT), a transformer-based model which can attend to high-dimensional, multi-modal inputs across long temporal horizons for language-conditioned task completion. Additionally, we bridge the gap between successful object-centric navigation models used for non-interactive agents and the language-guided visual task completion benchmark, ALFRED, by introducing object navigation targets for EmBERT training. We achieve competitive performance on the ALFRED benchmark, and EmBERT marks the first transformer-based model to successfully handle the long-horizon, dense, multi-modal histories of ALFRED, and the first ALFRED model to utilize object-centric navigation targets.

| Comments: | [this https URL](https://github.com/amazon-research/embert)  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2108.04927](https://arxiv.org/abs/2108.04927) [cs.CV]** |
|           | (or **[arXiv:2108.04927v1](https://arxiv.org/abs/2108.04927v1) [cs.CV]** for this version) |





<h2 id="2021-08-12-2">2. Post-hoc Interpretability for Neural NLP: A Survey
</h2>

Title: [Post-hoc Interpretability for Neural NLP: A Survey](https://arxiv.org/abs/2108.04840)

Authors: [Andreas Madsen](https://arxiv.org/search/cs?searchtype=author&query=Madsen%2C+A), [Siva Reddy](https://arxiv.org/search/cs?searchtype=author&query=Reddy%2C+S), [Sarath Chandar](https://arxiv.org/search/cs?searchtype=author&query=Chandar%2C+S)

> Natural Language Processing (NLP) models have become increasingly more complex and widespread. With recent developments in neural networks, a growing concern is whether it is responsible to use these models. Concerns such as safety and ethics can be partially addressed by providing explanations. Furthermore, when models do fail, providing explanations is paramount for accountability purposes. To this end, interpretability serves to provide these explanations in terms that are understandable to humans. Central to what is understandable is how explanations are communicated. Therefore, this survey provides a categorization of how recent interpretability methods communicate explanations and discusses the methods in depth. Furthermore, the survey focuses on post-hoc methods, which provide explanations after a model is learned and generally model-agnostic. A common concern for this class of methods is whether they accurately reflect the model. Hence, how these post-hoc methods are evaluated is discussed throughout the paper.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.04840](https://arxiv.org/abs/2108.04840) [cs.CL]** |
|           | (or **[arXiv:2108.04840v1](https://arxiv.org/abs/2108.04840v1) [cs.CL]** for this version) |





<h2 id="2021-08-12-3">3. A Transformer-based Math Language Model for Handwritten Math Expression Recognition
</h2>

Title: [A Transformer-based Math Language Model for Handwritten Math Expression Recognition](https://arxiv.org/abs/2108.05002)

Authors: [Huy Quang Ung](https://arxiv.org/search/cs?searchtype=author&query=Ung%2C+H+Q), [Cuong Tuan Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+C+T), [Hung Tuan Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+H+T), [Thanh-Nghia Truong](https://arxiv.org/search/cs?searchtype=author&query=Truong%2C+T), [Masaki Nakagawa](https://arxiv.org/search/cs?searchtype=author&query=Nakagawa%2C+M)

> Handwritten mathematical expressions (HMEs) contain ambiguities in their interpretations, even for humans sometimes. Several math symbols are very similar in the writing style, such as dot and comma or 0, O, and o, which is a challenge for HME recognition systems to handle without using contextual information. To address this problem, this paper presents a Transformer-based Math Language Model (TMLM). Based on the self-attention mechanism, the high-level representation of an input token in a sequence of tokens is computed by how it is related to the previous tokens. Thus, TMLM can capture long dependencies and correlations among symbols and relations in a mathematical expression (ME). We trained the proposed language model using a corpus of approximately 70,000 LaTeX sequences provided in CROHME 2016. TMLM achieved the perplexity of 4.42, which outperformed the previous math language models, i.e., the N-gram and recurrent neural network-based language models. In addition, we combine TMLM into a stochastic context-free grammar-based HME recognition system using a weighting parameter to re-rank the top-10 best candidates. The expression rates on the testing sets of CROHME 2016 and CROHME 2019 were improved by 2.97 and 0.83 percentage points, respectively.

| Comments: | 14 pages, accepted in ICDAR-DIL 2021                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2108.05002](https://arxiv.org/abs/2108.05002) [cs.CL]** |
|           | (or **[arXiv:2108.05002v1](https://arxiv.org/abs/2108.05002v1) [cs.CL]** for this version) |





# 2021-08-11

[Return to Index](#Index)



<h2 id="2021-08-11-1">1. FairyTailor: A Multimodal Generative Framework for Storytelling
</h2>

Title: [FairyTailor: A Multimodal Generative Framework for Storytelling](https://arxiv.org/abs/2108.04324)

Authors: [Eden Bensaid](https://arxiv.org/search/cs?searchtype=author&query=Bensaid%2C+E), [Mauro Martino](https://arxiv.org/search/cs?searchtype=author&query=Martino%2C+M), [Benjamin Hoover](https://arxiv.org/search/cs?searchtype=author&query=Hoover%2C+B), [Jacob Andreas](https://arxiv.org/search/cs?searchtype=author&query=Andreas%2C+J), [Hendrik Strobelt](https://arxiv.org/search/cs?searchtype=author&query=Strobelt%2C+H)

> Storytelling is an open-ended task that entails creative thinking and requires a constant flow of ideas. Natural language generation (NLG) for storytelling is especially challenging because it requires the generated text to follow an overall theme while remaining creative and diverse to engage the reader. In this work, we introduce a system and a web-based demo, FairyTailor, for human-in-the-loop visual story co-creation. Users can create a cohesive children's fairytale by weaving generated texts and retrieved images with their input. FairyTailor adds another modality and modifies the text generation process to produce a coherent and creative sequence of text and images. To our knowledge, this is the first dynamic tool for multimodal story generation that allows interactive co-formation of both texts and images. It allows users to give feedback on co-created stories and share their results.

| Comments: | visit [this https URL](https://fairytailor.org/) and [this https URL](https://github.com/EdenBD/MultiModalStory-demo) for web demo and source code |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2108.04324](https://arxiv.org/abs/2108.04324) [cs.CL]** |
|           | (or **[arXiv:2108.04324v1](https://arxiv.org/abs/2108.04324v1) [cs.CL]** for this version) |





<h2 id="2021-08-11-2">2. BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents
</h2>

Title: [BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents](https://arxiv.org/abs/2108.04539)

Authors: [Teakgyu Hong](https://arxiv.org/search/cs?searchtype=author&query=Hong%2C+T), [Donghyun Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D), [Mingi Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+M), [Wonseok Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+W), [Daehyun Nam](https://arxiv.org/search/cs?searchtype=author&query=Nam%2C+D), [Sungrae Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+S)

> Understanding documents from their visual snapshots is an emerging problem that requires both advanced computer vision and NLP methods. The recent advance in OCR enables the accurate recognition of text blocks, yet it is still challenging to extract key information from documents due to the diversity of their layouts. Although recent studies on pre-trained language models show the importance of incorporating layout information on this task, the conjugation of texts and their layouts still follows the style of BERT optimized for understanding the 1D text. This implies there is room for further improvement considering the 2D nature of text layouts. This paper introduces a pre-trained language model, BERT Relying On Spatiality (BROS), which effectively utilizes the information included in individual text blocks and their layouts. Specifically, BROS encodes spatial information by utilizing relative positions and learns spatial dependencies between OCR blocks with a novel area-masking strategy. These two novel approaches lead to an efficient encoding of spatial layout information highlighted by the robust performance of BROS under low-resource environments. We also introduce a general-purpose parser that can be combined with BROS to extract key information even when there is no order information between text blocks. BROS shows its superiority on four public benchmarks---FUNSD, SROIE*, CORD, and SciTSR---and its robustness in practical cases where order information of text blocks is not available. Further experiments with a varying number of training examples demonstrate the high training efficiency of our approach. Our code will be open to the public.

| Comments: | 11 pages, 6 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.04539](https://arxiv.org/abs/2108.04539) [cs.CL]** |
|           | (or **[arXiv:2108.04539v1](https://arxiv.org/abs/2108.04539v1) [cs.CL]** for this version) |





<h2 id="2021-08-11-3">3. CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model
</h2>

Title: [CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model](https://arxiv.org/abs/2108.04556)

Authors: [Xin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yasheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Pingyi Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+P), [Meng Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+M), [Yadao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Li Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Xiao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Hao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Jin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X)

> Pre-trained models for programming languages have proven their significant values in various code-related tasks, such as code search, code clone detection, and code translation. Currently, most pre-trained models treat a code snippet as a sequence of tokens or only focus on the data flow between code identifiers. However, rich code syntax and hierarchy are ignored which can provide important structure information and semantic rules of codes to help enhance code representations. In addition, although the BERT-based code pre-trained models achieve high performance on many downstream tasks, the native derived sequence representations of BERT are proven to be of low-quality, it performs poorly on code matching and similarity tasks. To address these problems, we propose CLSEBERT, a Constrastive Learning Framework for Syntax Enhanced Code Pre-Trained Model, to deal with various code intelligence tasks. In the pre-training stage, we consider the code syntax and hierarchy contained in the Abstract Syntax Tree (AST) and leverage the constrastive learning to learn noise-invariant code representations. Besides the masked language modeling (MLM), we also introduce two novel pre-training objectives. One is to predict the edges between nodes in the abstract syntax tree, and the other is to predict the types of code tokens. Through extensive experiments on four code intelligence tasks, we successfully show the effectiveness of our proposed model.

| Comments: | 10 pages, 3 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Programming Languages (cs.PL) |
| Cite as:  | **[arXiv:2108.04556](https://arxiv.org/abs/2108.04556) [cs.CL]** |
|           | (or **[arXiv:2108.04556v1](https://arxiv.org/abs/2108.04556v1) [cs.CL]** for this version) |





<h2 id="2021-08-11-4">4. Differentiable Subset Pruning of Transformer Heads
</h2>

Title: [Differentiable Subset Pruning of Transformer Heads](https://arxiv.org/abs/2108.04657)

Authors: [Jiaoda Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R), [Mrinmaya Sachan](https://arxiv.org/search/cs?searchtype=author&query=Sachan%2C+M)

> Multi-head attention, a collection of several attention mechanisms that independently attend to different parts of the input, is the key ingredient in the Transformer (Vaswaniet al., 2017). Recent work has shown, however, that a large proportion of the heads in a Transformer's multi-head attention mechanism can be safely pruned away without significantly harming the performance of the model; such pruning leads to models that are noticeably smaller and faster in practice. Our work introduces a new head pruning technique that we term differentiable subset pruning. Intuitively, our method learns per-head importance variables and then enforces a user-specified hard constraint on the number of unpruned heads. The importance variables are learned via stochastic gradient descent. We conduct experiments on natural language inference and machine translation; we show that differentiable subset pruning performs comparably or better than Voita et al. (2019) while offering the same exact control over the number of heads as Michel et al. (2019).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.04657](https://arxiv.org/abs/2108.04657) [cs.CL]** |
|           | (or **[arXiv:2108.04657v1](https://arxiv.org/abs/2108.04657v1) [cs.CL]** for this version) |





<h2 id="2021-08-11-5">5. How Commonsense Knowledge Helps with Natural Language Tasks: A Survey of Recent Resources and Methodologies
</h2>

Title: [How Commonsense Knowledge Helps with Natural Language Tasks: A Survey of Recent Resources and Methodologies](https://arxiv.org/abs/2108.04674)

Authors: [Yubo Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y), [Pearl Pu](https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+P)

> In this paper, we give an overview of commonsense reasoning in natural language processing, which requires a deeper understanding of the contexts and usually involves inference over implicit external knowledge. We first review some popular commonsense knowledge bases and commonsense reasoning benchmarks, but give more emphasis on the methodologies, including recent approaches that aim at solving some general natural language problems that take advantage of external knowledge bases. Finally, we discuss some future directions in pushing the boundary of commonsense reasoning in natural language processing.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.04674](https://arxiv.org/abs/2108.04674) [cs.CL]** |
|           | (or **[arXiv:2108.04674v1](https://arxiv.org/abs/2108.04674v1) [cs.CL]** for this version) |





<h2 id="2021-08-11-6">6. Sampling-Based Minimum Bayes Risk Decoding for Neural Machine Translation
</h2>

Title: [Sampling-Based Minimum Bayes Risk Decoding for Neural Machine Translation](https://arxiv.org/abs/2108.04718)

Authors: [Bryan Eikema](https://arxiv.org/search/cs?searchtype=author&query=Eikema%2C+B), [Wilker Aziz](https://arxiv.org/search/cs?searchtype=author&query=Aziz%2C+W)

> In neural machine translation (NMT), we search for the mode of the model distribution to form predictions. The mode as well as other high probability translations found by beam search have been shown to often be inadequate in a number of ways. This prevents practitioners from improving translation quality through better search, as these idiosyncratic translations end up being selected by the decoding algorithm, a problem known as the beam search curse. Recently, a sampling-based approximation to minimum Bayes risk (MBR) decoding has been proposed as an alternative decision rule for NMT that would likely not suffer from the same problems. We analyse this approximation and establish that it has no equivalent to the beam search curse, i.e. better search always leads to better translations. We also design different approximations aimed at decoupling the cost of exploration from the cost of robust estimation of expected utility. This allows for exploration of much larger hypothesis spaces, which we show to be beneficial. We also show that it can be beneficial to make use of strategies like beam search and nucleus sampling to construct hypothesis spaces efficiently. We show on three language pairs (English into and from German, Romanian, and Nepali) that MBR can improve upon beam search with moderate computation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.04718](https://arxiv.org/abs/2108.04718) [cs.CL]** |
|           | (or **[arXiv:2108.04718v1](https://arxiv.org/abs/2108.04718v1) [cs.CL]** for this version) |









# 2021-08-10

[Return to Index](#Index)



<h2 id="2021-08-10-1">1. Improving Similar Language Translation With Transfer Learning
</h2>

Title: [Improving Similar Language Translation With Transfer Learning](https://arxiv.org/abs/2108.03533)

Authors: [Ife Adebara](https://arxiv.org/search/cs?searchtype=author&query=Adebara%2C+I), [Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+M)

> We investigate transfer learning based on pre-trained neural machine translation models to translate between (low-resource) similar languages. This work is part of our contribution to the WMT 2021 Similar Languages Translation Shared Task where we submitted models for different language pairs, including French-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our models for Catalan-Spanish (82.79 BLEU) and Portuguese-Spanish (87.11 BLEU) rank top 1 in the official shared task evaluation, and we are the only team to submit models for the French-Bambara pairs.

| Comments: | Submitted to WMT 2021 Similar Language Task                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.03533](https://arxiv.org/abs/2108.03533) [cs.AI]** |
|           | (or **[arXiv:2108.03533v1](https://arxiv.org/abs/2108.03533v1) [cs.AI]** for this version) |





<h2 id="2021-08-10-2">2. Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models
</h2>

Title: [Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models](https://arxiv.org/abs/2108.04024)

Authors: [Zheyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Cristian Rodriguez-Opazo](https://arxiv.org/search/cs?searchtype=author&query=Rodriguez-Opazo%2C+C), [Damien Teney](https://arxiv.org/search/cs?searchtype=author&query=Teney%2C+D), [Stephen Gould](https://arxiv.org/search/cs?searchtype=author&query=Gould%2C+S)

> We extend the task of composed image retrieval, where an input query consists of an image and short textual description of how to modify the image. Existing methods have only been applied to non-complex images within narrow domains, such as fashion products, thereby limiting the scope of study on in-depth visual reasoning in rich image and language contexts. To address this issue, we collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with human-generated modifying text. To extend current methods to the open-domain, we propose CIRPLANT, a transformer based model that leverages rich pre-trained vision-and-language (V&L) knowledge for modifying visual features conditioned on natural language. Retrieval is then done by nearest neighbor lookup on the modified features. We demonstrate that with a relatively simple architecture, CIRPLANT outperforms existing methods on open-domain images, while matching state-of-the-art accuracy on the existing narrow datasets, such as fashion. Together with the release of CIRR, we believe this work will inspire further research on composed image retrieval.

| Comments: | ICCV 2021. Dataset, code, and pre-trained models are released at [this https URL](https://cuberick-orion.github.io/CIRR/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| Cite as:  | **[arXiv:2108.04024](https://arxiv.org/abs/2108.04024) [cs.CV]** |
|           | (or **[arXiv:2108.04024v1](https://arxiv.org/abs/2108.04024v1) [cs.CV]** for this version) |





<h2 id="2021-08-10-3">3. Facebook AI WMT21 News Translation Task Submission
</h2>

Title: [Facebook AI WMT21 News Translation Task Submission](https://arxiv.org/abs/2108.03265)

Authors: [Chau Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+C), [Shruti Bhosale](https://arxiv.org/search/cs?searchtype=author&query=Bhosale%2C+S), [James Cross](https://arxiv.org/search/cs?searchtype=author&query=Cross%2C+J), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P), [Sergey Edunov](https://arxiv.org/search/cs?searchtype=author&query=Edunov%2C+S), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A)

> We describe Facebook's multilingual model submission to the WMT2021 shared task on news translation. We participate in 14 language directions: English to and from Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese. To develop systems covering all these directions, we focus on multilingual models. We utilize data from all available sources --- WMT, large-scale data mining, and in-domain backtranslation --- to create high quality bilingual and multilingual baselines. Subsequently, we investigate strategies for scaling multilingual model size, such that one system has sufficient capacity for high quality representations of all eight languages. Our final submission is an ensemble of dense and sparse Mixture-of-Expert multilingual translation models, followed by finetuning on in-domain news data and noisy channel reranking. Compared to previous year's winning submissions, our multilingual system improved the translation quality on all language directions, with an average improvement of 2.0 BLEU. In the WMT2021 task, our system ranks first in 10 directions based on automatic evaluation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.03265](https://arxiv.org/abs/2108.03265) [cs.CL]** |
|           | (or **[arXiv:2108.03265v1](https://arxiv.org/abs/2108.03265v1) [cs.CL]** for this version) |





<h2 id="2021-08-10-4">4. Towards Zero-shot Language Modeling
</h2>

Title: [Towards Zero-shot Language Modeling](https://arxiv.org/abs/2108.03334)

Authors: [Edoardo Maria Ponti](https://arxiv.org/search/cs?searchtype=author&query=Ponti%2C+E+M), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R), [Roi Reichart](https://arxiv.org/search/cs?searchtype=author&query=Reichart%2C+R), [Anna Korhonen](https://arxiv.org/search/cs?searchtype=author&query=Korhonen%2C+A)

> Can we construct a neural model that is inductively biased towards learning human languages? Motivated by this question, we aim at constructing an informative prior over neural weights, in order to adapt quickly to held-out languages in the task of character-level language modeling. We infer this distribution from a sample of typologically diverse training languages via Laplace approximation. The use of such a prior outperforms baseline models with an uninformative prior (so-called "fine-tuning") in both zero-shot and few-shot settings. This shows that the prior is imbued with universal phonological knowledge. Moreover, we harness additional language-specific side information as distant supervision for held-out languages. Specifically, we condition language models on features from typological databases, by concatenating them to hidden states or generating weights with hyper-networks. These features appear beneficial in the few-shot setting, but not in the zero-shot setting. Since the paucity of digital texts affects the majority of the world's languages, we hope that these findings will help broaden the scope of applications for language technology.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.03334](https://arxiv.org/abs/2108.03334) [cs.CL]** |
|           | (or **[arXiv:2108.03334v1](https://arxiv.org/abs/2108.03334v1) [cs.CL]** for this version) |





<h2 id="2021-08-10-5">5. Generating Personalized Dialogue via Multi-Task Meta-Learning
</h2>

Title: [Generating Personalized Dialogue via Multi-Task Meta-Learning](https://arxiv.org/abs/2108.03377)

Authors: [Jing Yang Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J+Y), [Kong Aik Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K+A), [Woon Seng Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+W+S)

> Conventional approaches to personalized dialogue generation typically require a large corpus, as well as predefined persona information. However, in a real-world setting, neither a large corpus of training data nor persona information are readily available. To address these practical limitations, we propose a novel multi-task meta-learning approach which involves training a model to adapt to new personas without relying on a large corpus, or on any predefined persona information. Instead, the model is tasked with generating personalized responses based on only the dialogue context. Unlike prior work, our approach leverages on the provided persona information only during training via the introduction of an auxiliary persona reconstruction task. In this paper, we introduce 2 frameworks that adopt the proposed multi-task meta-learning approach: the Multi-Task Meta-Learning (MTML) framework, and the Alternating Multi-Task Meta-Learning (AMTML) framework. Experimental results show that utilizing MTML and AMTML results in dialogue responses with greater persona consistency.

| Comments: | Accepted at SemDial 2021 (PotsDial 2021)                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2108.03377](https://arxiv.org/abs/2108.03377) [cs.CL]** |
|           | (or **[arXiv:2108.03377v1](https://arxiv.org/abs/2108.03377v1) [cs.CL]** for this version) |





<h2 id="2021-08-10-6">6. Language Model Evaluation in Open-ended Text Generation
</h2>

Title: [Language Model Evaluation in Open-ended Text Generation](https://arxiv.org/abs/2108.03578)

Authors: [An Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+A)

> Although current state-of-the-art language models have achieved impressive results in numerous natural language processing tasks, still they could not solve the problem of producing repetitive, dull and sometimes inconsistent text in open-ended text generation. Studies often attribute this problem to the maximum likelihood training objective, and propose alternative approaches by using stochastic decoding methods or altering the training objective. However, there is still a lack of consistent evaluation metrics to directly compare the efficacy of these solutions. In this work, we study different evaluation metrics that have been proposed to evaluate quality, diversity and consistency of machine-generated text. From there, we propose a practical pipeline to evaluate language models in open-ended generation task, and research on how to improve the model's performance in all dimensions by leveraging different auxiliary training objectives.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.03578](https://arxiv.org/abs/2108.03578) [cs.CL]** |
|           | (or **[arXiv:2108.03578v1](https://arxiv.org/abs/2108.03578v1) [cs.CL]** for this version) |





<h2 id="2021-08-10-7">7. Machine Translation of Low-Resource Indo-European Languages
</h2>

Title: [Machine Translation of Low-Resource Indo-European Languages](https://arxiv.org/abs/2108.03739)

Authors: [Wei-Rui Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+M)

> Transfer learning has been an important technique for low-resource neural machine translation. In this work, we build two systems to study how relatedness can benefit the translation performance. The primary system adopts machine translation model pre-trained on related language pair and the contrastive system adopts that pre-trained on unrelated language pair. We show that relatedness is not required for transfer learning to work but does benefit the performance.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.03739](https://arxiv.org/abs/2108.03739) [cs.CL]** |
|           | (or **[arXiv:2108.03739v1](https://arxiv.org/abs/2108.03739v1) [cs.CL]** for this version) |





<h2 id="2021-08-10-8">8. The HW-TSC's Offline Speech Translation Systems for IWSLT 2021 Evaluation
</h2>

Title: [The HW-TSC's Offline Speech Translation Systems for IWSLT 2021 Evaluation](https://arxiv.org/abs/2108.03845)

Authors: [Minghan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Yuxia Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Chang Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+C), [Jiaxin Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Yingtao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Yujia Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Shimin Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+S), [Xingshan Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Hao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H), [Ying Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y)

> This paper describes our work in participation of the IWSLT-2021 offline speech translation task. Our system was built in a cascade form, including a speaker diarization module, an Automatic Speech Recognition (ASR) module and a Machine Translation (MT) module. We directly use the LIUM SpkDiarization tool as the diarization module. The ASR module is trained with three ASR datasets from different sources, by multi-source training, using a modified Transformer encoder. The MT module is pretrained on the large-scale WMT news translation dataset and fine-tuned on the TED corpus. Our method achieves 24.6 BLEU score on the 2021 test set.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.03845](https://arxiv.org/abs/2108.03845) [cs.CL]** |
|           | (or **[arXiv:2108.03845v1](https://arxiv.org/abs/2108.03845v1) [cs.CL]** for this version) |





<h2 id="2021-08-10-9">9. Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models
</h2>

Title: [Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models](https://arxiv.org/abs/2108.04049)

Authors: [Bogdan Kostić](https://arxiv.org/search/cs?searchtype=author&query=Kostić%2C+B), [Julian Risch](https://arxiv.org/search/cs?searchtype=author&query=Risch%2C+J), [Timo Möller](https://arxiv.org/search/cs?searchtype=author&query=Möller%2C+T)

> Open-domain extractive question answering works well on textual data by first retrieving candidate texts and then extracting the answer from those candidates. However, some questions cannot be answered by text alone but require information stored in tables. In this paper, we present an approach for retrieving both texts and tables relevant to a question by jointly encoding texts, tables and questions into a single vector space. To this end, we create a new multi-modal dataset based on text and table datasets from related work and compare the retrieval performance of different encoding schemata. We find that dense vector embeddings of transformer models outperform sparse embeddings on four out of six evaluation datasets. Comparing different dense embedding models, tri-encoders, with one encoder for each question, text and table, increase retrieval performance compared to bi-encoders with one encoder for the question and one for both text and tables. We release the newly created multi-modal dataset to the community so that it can be used for training and evaluation.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.04049](https://arxiv.org/abs/2108.04049) [cs.CL]** |
|           | (or **[arXiv:2108.04049v1](https://arxiv.org/abs/2108.04049v1) [cs.CL]** for this version) |





# 2021-08-09

[Return to Index](#Index)



<h2 id="2021-08-09-1">1. Sentence Semantic Regression for Text Generation
</h2>

Title: [Sentence Semantic Regression for Text Generation](https://arxiv.org/abs/2108.02984)

Authors: [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Piji Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Hai-Tao Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H)

> Recall the classical text generation works, the generation framework can be briefly divided into two phases: \textbf{idea reasoning} and \textbf{surface realization}. The target of idea reasoning is to figure out the main idea which will be presented in the following talking/writing periods. Surface realization aims to arrange the most appropriate sentence to depict and convey the information distilled from the main idea. However, the current popular token-by-token text generation methods ignore this crucial process and suffer from many serious issues, such as idea/topic drift. To tackle the problems and realize this two-phase paradigm, we propose a new framework named Sentence Semantic Regression (\textbf{SSR}) based on sentence-level language modeling. For idea reasoning, two architectures \textbf{SSR-AR} and \textbf{SSR-NonAR} are designed to conduct sentence semantic regression autoregressively (like GPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a mixed-granularity sentence decoder is designed to generate text with better consistency by jointly incorporating the predicted sentence-level main idea as well as the preceding contextual token-level information. We conduct experiments on four tasks of story ending prediction, story ending generation, dialogue generation, and sentence infilling. The results show that SSR can obtain better performance in terms of automatic metrics and human evaluation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.02984](https://arxiv.org/abs/2108.02984) [cs.CL]** |
|           | (or **[arXiv:2108.02984v1](https://arxiv.org/abs/2108.02984v1) [cs.CL]** for this version) |





<h2 id="2021-08-09-2">2. Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents
</h2>

Title: [Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents](https://arxiv.org/abs/2108.02899)

Authors: [Amit Gupte](https://arxiv.org/search/cs?searchtype=author&query=Gupte%2C+A), [Alexey Romanov](https://arxiv.org/search/cs?searchtype=author&query=Romanov%2C+A), [Sahitya Mantravadi](https://arxiv.org/search/cs?searchtype=author&query=Mantravadi%2C+S), [Dalitso Banda](https://arxiv.org/search/cs?searchtype=author&query=Banda%2C+D), [Jianjie Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Raza Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+R), [Lakshmanan Ramu Meenal](https://arxiv.org/search/cs?searchtype=author&query=Meenal%2C+L+R), [Benjamin Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+B), [Soundar Srinivasan](https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+S)

> Document digitization is essential for the digital transformation of our societies, yet a crucial step in the process, Optical Character Recognition (OCR), is still not perfect. Even commercial OCR systems can produce questionable output depending on the fidelity of the scanned documents. In this paper, we demonstrate an effective framework for mitigating OCR errors for any downstream NLP task, using Named Entity Recognition (NER) as an example. We first address the data scarcity problem for model training by constructing a document synthesis pipeline, generating realistic but degraded data with NER labels. We measure the NER accuracy drop at various degradation levels and show that a text restoration model, trained on the degraded data, significantly closes the NER accuracy gaps caused by OCR errors, including on an out-of-domain dataset. For the benefit of the community, we have made the document synthesis pipeline available as an open-source project.

| Comments: | Accepted to the Document Intelligence Workshop at KDD 2021. The source code of Genalog is available at [this https URL](https://github.com/microsoft/genalog) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2108.02899](https://arxiv.org/abs/2108.02899) [cs.CL]** |
|           | (or **[arXiv:2108.02899v1](https://arxiv.org/abs/2108.02899v1) [cs.CL]** for this version) |





<h2 id="2021-08-09-3">3. StrucTexT: Structured Text Understanding with Multi-Modal Transformers
</h2>

Title: [StrucTexT: Structured Text Understanding with Multi-Modal Transformers](https://arxiv.org/abs/2108.02923)

Authors: [Yulin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yuxi Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+Y), [Yuchen Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y), [Xiameng Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+X), [Chengquan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Kun Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+K), [Junyu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J), [Jingtuo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Errui Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+E)

> Structured text understanding on Visually Rich Documents (VRDs) is a crucial part of Document Intelligence. Due to the complexity of content and layout in VRDs, structured text understanding has been a challenging task. Most existing studies decoupled this problem into two sub-tasks: entity labeling and entity linking, which require an entire understanding of the context of documents at both token and segment levels. However, little work has been concerned with the solutions that efficiently extract the structured data from different levels. This paper proposes a unified framework named StrucTexT, which is flexible and effective for handling both sub-tasks. Specifically, based on the transformer, we introduce a segment-token aligned encoder to deal with the entity labeling and entity linking tasks at different levels of granularity. Moreover, we design a novel pre-training strategy with three self-supervised tasks to learn a richer representation. StrucTexT uses the existing Masked Visual Language Modeling task and the new Sentence Length Prediction and Paired Boxes Direction tasks to incorporate the multi-modal information across text, image, and layout. We evaluate our method for structured text understanding at segment-level and token-level and show it outperforms the state-of-the-art counterparts with significantly superior performance on the FUNSD, SROIE, and EPHOIE datasets.

| Comments: | 9 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.02923](https://arxiv.org/abs/2108.02923) [cs.CV]** |
|           | (or **[arXiv:2108.02923v1](https://arxiv.org/abs/2108.02923v1) [cs.CV]** for this version) |








# 2021-08-06

[Return to Index](#Index)



<h2 id="2021-08-06-1">1. Sentence-level Online Handwritten Chinese Character Recognition
</h2>

Title: [Sentence-level Online Handwritten Chinese Character Recognition](https://arxiv.org/abs/2108.02561)

Authors: [Yunxin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Qian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Q), [Qingcai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Lin Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+L), [Baotian Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+B), [Xiaolong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yuxin Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Y)

> Single online handwritten Chinese character recognition~(single OLHCCR) has achieved prominent performance. However, in real application scenarios, users always write multiple Chinese characters to form one complete sentence and the contextual information within these characters holds the significant potential to improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In this work, we first propose a simple and straightforward end-to-end network, namely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR. It couples convolutional neural network with sequence modeling architecture to exploit the handwritten character's previous contextual information. Although VCN performs much better than the state-of-the-art single OLHCCR model, it exposes high fragility when confronting with not well written characters such as sloppy writing, missing or broken strokes. To improve the robustness of sentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion network~(DSTFN). It utilizes a pre-trained autoregresssive framework as the backbone component, which projects each Chinese character into word embeddings, and integrates the spatial glyph features of handwritten characters and their contextual information multiple times at multi-layer fusion module. We also construct a large-scale sentence-level handwriting dataset, named as CSOHD to evaluate models. Extensive experiment results demonstrate that DSTFN achieves the state-of-the-art performance, which presents strong robustness compared with VCN and exiting single OLHCCR models. The in-depth empirical analysis and case studies indicate that DSTFN can significantly improve the efficiency of handwriting input, with the handwritten Chinese character with incomplete strokes being recognized precisely.

| Comments: | 10 pages, 10 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.02561](https://arxiv.org/abs/2108.02561) [cs.CV]** |
|           | (or **[arXiv:2108.02561v1](https://arxiv.org/abs/2108.02561v1) [cs.CV]** for this version) |





<h2 id="2021-08-06-2">2. Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models
</h2>

Title: [Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models](https://arxiv.org/abs/2108.02562)

Authors: [Khazar Khorrami](https://arxiv.org/search/cs?searchtype=author&query=Khorrami%2C+K), [Okko Räsänen](https://arxiv.org/search/cs?searchtype=author&query=Räsänen%2C+O)

> Systems that can find correspondences between multiple modalities, such as between speech and images, have great potential to solve different recognition and data analysis tasks in an unsupervised manner. This work studies multimodal learning in the context of visually grounded speech (VGS) models, and focuses on their recently demonstrated capability to extract spatiotemporal alignments between spoken words and the corresponding visual objects without ever been explicitly trained for object localization or word recognition. As the main contributions, we formalize the alignment problem in terms of an audiovisual alignment tensor that is based on earlier VGS work, introduce systematic metrics for evaluating model performance in aligning visual objects and spoken words, and propose a new VGS model variant for the alignment task utilizing cross-modal attention layer. We test our model and a previously proposed model in the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We compare the alignment performance using our proposed evaluation metrics to the semantic retrieval task commonly used to evaluate VGS models. We show that cross-modal attention layer not only helps the model to achieve higher semantic cross-modal retrieval performance, but also leads to substantial improvements in the alignment performance between image object and spoken words.

| Comments: | To be published in Proc. Interspeech-2021, Brno, Czech Republic |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2108.02562](https://arxiv.org/abs/2108.02562) [cs.CV]** |
|           | (or **[arXiv:2108.02562v1](https://arxiv.org/abs/2108.02562v1) [cs.CV]** for this version) |





<h2 id="2021-08-06-3">3. WeChat Neural Machine Translation Systems for WMT21
</h2>

Title: [WeChat Neural Machine Translation Systems for WMT21](https://arxiv.org/abs/2108.02401)

Authors: [Xianfeng Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Yijin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Ernan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+E), [Qiu Ran](https://arxiv.org/search/cs?searchtype=author&query=Ran%2C+Q), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Jinan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> This paper introduces WeChat AI's participation in WMT 2021 shared news translation task on English->Chinese, English->Japanese, Japanese->English and English->German. Our systems are based on the Transformer (Vaswani et al., 2017) with several novel and effective variants. In our experiments, we employ data filtering, large-scale synthetic data generation (i.e., back-translation, knowledge distillation, forward-translation, iterative in-domain knowledge transfer), advanced finetuning approaches, and boosted Self-BLEU based model ensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3 case-sensitive BLEU scores on English->Chinese, English->Japanese, Japanese->English and English->German, respectively. The BLEU scores of English->Chinese, English->Japanese and Japanese->English are the highest among all submissions, and that of English->German is the highest among all constrained submissions.

| Comments: | Submitted to WMT 2021 as a system paper                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2108.02401](https://arxiv.org/abs/2108.02401) [cs.CL]** |
|           | (or **[arXiv:2108.02401v1](https://arxiv.org/abs/2108.02401v1) [cs.CL]** for this version) |





<h2 id="2021-08-06-4">4. Finetuning Pretrained Transformers into Variational Autoencoders
</h2>

Title: [Finetuning Pretrained Transformers into Variational Autoencoders](https://arxiv.org/abs/2108.02446)

Authors: [Seongmin Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+S), [Jihwa Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J)

> Text variational autoencoders (VAEs) are notorious for posterior collapse, a phenomenon where the model's decoder learns to ignore signals from the encoder. Because posterior collapse is known to be exacerbated by expressive decoders, Transformers have seen limited adoption as components of text VAEs. Existing studies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et al., 2021) mitigate posterior collapse using massive pretraining, a technique unavailable to most of the research community without extensive computing resources. We present a simple two-phase training scheme to convert a sequence-to-sequence Transformer into a VAE with just finetuning. The resulting language model is competitive with massively pretrained Transformer-based VAEs in some internal metrics while falling short on others. To facilitate training we comprehensively explore the impact of common posterior collapse alleviation techniques in the literature. We release our code for reproducability.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.02446](https://arxiv.org/abs/2108.02446) [cs.CL]** |
|           | (or **[arXiv:2108.02446v1](https://arxiv.org/abs/2108.02446v1) [cs.CL]** for this version) |





<h2 id="2021-08-06-5">5. VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search
</h2>

Title: [VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search](https://arxiv.org/abs/2108.02725)

Authors: [Shaunak Mishra](https://arxiv.org/search/cs?searchtype=author&query=Mishra%2C+S), [Mikhail Kuznetsov](https://arxiv.org/search/cs?searchtype=author&query=Kuznetsov%2C+M), [Gaurav Srivastava](https://arxiv.org/search/cs?searchtype=author&query=Srivastava%2C+G), [Maxim Sviridenko](https://arxiv.org/search/cs?searchtype=author&query=Sviridenko%2C+M)

> Numerous online stock image libraries offer high quality yet copyright free images for use in marketing campaigns. To assist advertisers in navigating such third party libraries, we study the problem of automatically fetching relevant ad images given the ad text (via a short textual query for images). Motivated by our observations in logged data on ad image search queries (given ad text), we formulate a keyword extraction problem, where a keyword extracted from the ad text (or its augmented version) serves as the ad image query. In this context, we propose VisualTextRank: an unsupervised method to (i) augment input ad text using semantically similar ads, and (ii) extract the image query from the augmented ad text. VisualTextRank builds on prior work on graph based context extraction (biased TextRank in particular) by leveraging both the text and image of similar ads for better keyword extraction, and using advertiser category specific biasing with sentence-BERT embeddings. Using data collected from the Verizon Media Native (Yahoo Gemini) ad platform's stock image search feature for onboarding advertisers, we demonstrate the superiority of VisualTextRank compared to competitive keyword extraction baselines (including an 11% accuracy lift over biased TextRank). For the case when the stock image library is restricted to English queries, we show the effectiveness of VisualTextRank on multilingual ads (translated to English) while leveraging semantically similar English ads. Online tests with a simplified version of VisualTextRank led to a 28.7% increase in the usage of stock image search, and a 41.6% increase in the advertiser onboarding rate in the Verizon Media Native ad platform.

| Comments: | Accepted for publication at KDD 2021                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| DOI:      | [10.1145/1122445.1122456](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F1122445.1122456&v=a56ae4b4) |
| Cite as:  | **[arXiv:2108.02725](https://arxiv.org/abs/2108.02725) [cs.CL]** |
|           | (or **[arXiv:2108.02725v1](https://arxiv.org/abs/2108.02725v1) [cs.CL]** for this version) |





# 2021-08-05

[Return to Index](#Index)



<h2 id="2021-08-05-1">1. Improving Distinction between ASR Errors and Speech Disfluencies with Feature Space Interpolation
</h2>

Title: [Improving Distinction between ASR Errors and Speech Disfluencies with Feature Space Interpolation](https://arxiv.org/abs/2108.01812)

Authors: [Seongmin Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+S), [Dongchan Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+D), [Sangyoun Paik](https://arxiv.org/search/cs?searchtype=author&query=Paik%2C+S), [Subong Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+S), [Alena Kazakova](https://arxiv.org/search/cs?searchtype=author&query=Kazakova%2C+A), [Jihwa Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J)

> Fine-tuning pretrained language models (LMs) is a popular approach to automatic speech recognition (ASR) error detection during post-processing. While error detection systems often take advantage of statistical language archetypes captured by LMs, at times the pretrained knowledge can hinder error detection performance. For instance, presence of speech disfluencies might confuse the post-processing system into tagging disfluent but accurate transcriptions as ASR errors. Such confusion occurs because both error detection and disfluency detection tasks attempt to identify tokens at statistically unlikely positions. This paper proposes a scheme to improve existing LM-based ASR error detection systems, both in terms of detection scores and resilience to such distracting auxiliary tasks. Our approach adopts the popular mixup method in text feature space and can be utilized with any black-box ASR output. To demonstrate the effectiveness of our method, we conduct post-processing experiments with both traditional and end-to-end ASR systems (both for English and Korean languages) with 5 different speech corpora. We find that our method improves both ASR error detection F 1 scores and reduces the number of correctly transcribed disfluencies wrongly detected as ASR errors. Finally, we suggest methods to utilize resulting LMs directly in semi-supervised ASR training.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.01812](https://arxiv.org/abs/2108.01812) [cs.CL]** |
|           | (or **[arXiv:2108.01812v1](https://arxiv.org/abs/2108.01812v1) [cs.CL]** for this version) |





<h2 id="2021-08-05-2">2. PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining
</h2>

Title: [PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining](https://arxiv.org/abs/2108.01887)

Authors: [Machel Reid](https://arxiv.org/search/cs?searchtype=author&query=Reid%2C+M), [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M)

> Despite the success of multilingual sequence-to-sequence pretraining, most existing approaches rely on monolingual corpora, and do not make use of the strong cross-lingual signal contained in parallel data. In this paper, we present PARADISE (PARAllel & Denoising Integration in SEquence-to-sequence models), which extends the conventional denoising objective used to train these models by (i) replacing words in the noised sequence according to a multilingual dictionary, and (ii) predicting the reference translation according to a parallel corpus instead of recovering the original sequence. Our experiments on machine translation and cross-lingual natural language inference show an average improvement of 2.0 BLEU points and 6.7 accuracy points from integrating parallel data into pretraining, respectively, obtaining results that are competitive with several popular models at a fraction of their computational cost.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2108.01887](https://arxiv.org/abs/2108.01887) [cs.CL]** |
|           | (or **[arXiv:2108.01887v1](https://arxiv.org/abs/2108.01887v1) [cs.CL]** for this version) |



<h2 id="2021-08-05-3">3. How to Query Language Models?
</h2>

Title: [How to Query Language Models?](https://arxiv.org/abs/2108.01928)

Authors: [Leonard Adolphs](https://arxiv.org/search/cs?searchtype=author&query=Adolphs%2C+L), [Shehzaad Dhuliawala](https://arxiv.org/search/cs?searchtype=author&query=Dhuliawala%2C+S), [Thomas Hofmann](https://arxiv.org/search/cs?searchtype=author&query=Hofmann%2C+T)

> Large pre-trained language models (LMs) are capable of not only recovering linguistic but also factual and commonsense knowledge. To access the knowledge stored in mask-based LMs, we can use cloze-style questions and let the model fill in the blank. The flexibility advantage over structured knowledge bases comes with the drawback of finding the right query for a certain information need. Inspired by human behavior to disambiguate a question, we propose to query LMs by example. To clarify the ambivalent question "Who does Neuer play for?", a successful strategy is to demonstrate the relation using another subject, e.g., "Ronaldo plays for Portugal. Who does Neuer play for?". We apply this approach of querying by example to the LAMA probe and obtain substantial improvements of up to 37.8% for BERT-large on the T-REx data when providing only 10 demonstrations--even outperforming a baseline that queries the model with up to 40 paraphrases of the question. The examples are provided through the model's context and thus require neither fine-tuning nor an additional forward pass. This suggests that LMs contain more factual and commonsense knowledge than previously assumed--if we query the model in the right way.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.01928](https://arxiv.org/abs/2108.01928) [cs.CL]** |
|           | (or **[arXiv:2108.01928v1](https://arxiv.org/abs/2108.01928v1) [cs.CL]** for this version) |





<h2 id="2021-08-05-4">4. Curriculum learning for language modeling
</h2>

Title: [Curriculum learning for language modeling](https://arxiv.org/abs/2108.02170)

Authors: [Daniel Campos](https://arxiv.org/search/cs?searchtype=author&query=Campos%2C+D)

> Language Models like ELMo and BERT have provided robust representations of natural language, which serve as the language understanding component for a diverse range of downstream tasks.Curriculum learning is a method that employs a structured training regime instead, which has been leveraged in computer vision and machine translation to improve model training speed and model performance. While language models have proven transformational for the natural language processing community, these models have proven expensive, energy-intensive, and challenging to train. In this work, we explore the effect of curriculum learning on language model pretraining using various linguistically motivated curricula and evaluate transfer performance on the GLUE Benchmark. Despite a broad variety of training methodologies and experiments we do not find compelling evidence that curriculum learning methods improve language model training.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.02170](https://arxiv.org/abs/2108.02170) [cs.CL]** |
|           | (or **[arXiv:2108.02170v1](https://arxiv.org/abs/2108.02170v1) [cs.CL]** for this version) |







# 2021-08-04

[Return to Index](#Index)



<h2 id="2021-08-04-1">1. Knowledge-intensive Language Understanding for Explainable AI
</h2>

Title: [Knowledge-intensive Language Understanding for Explainable AI](https://arxiv.org/abs/2108.01174)

Authors: [Amit Sheth](https://arxiv.org/search/cs?searchtype=author&query=Sheth%2C+A), [Manas Gaur](https://arxiv.org/search/cs?searchtype=author&query=Gaur%2C+M), [Kaushik Roy](https://arxiv.org/search/cs?searchtype=author&query=Roy%2C+K), [Keyur Faldu](https://arxiv.org/search/cs?searchtype=author&query=Faldu%2C+K)

> AI systems have seen significant adoption in various domains. At the same time, further adoption in some domains is hindered by inability to fully trust an AI system that it will not harm a human. Besides the concerns for fairness, privacy, transparency, and explainability are key to developing trusts in AI systems. As stated in describing trustworthy AI "Trust comes through understanding. How AI-led decisions are made and what determining factors were included are crucial to understand." The subarea of explaining AI systems has come to be known as XAI. Multiple aspects of an AI system can be explained; these include biases that the data might have, lack of data points in a particular region of the example space, fairness of gathering the data, feature importances, etc. However, besides these, it is critical to have human-centered explanations that are directly related to decision-making similar to how a domain expert makes decisions based on "domain knowledge," that also include well-established, peer-validated explicit guidelines. To understand and validate an AI system's outcomes (such as classification, recommendations, predictions), that lead to developing trust in the AI system, it is necessary to involve explicit domain knowledge that humans understand and use.

| Comments: | To appear in IEEE Internet Computing, September/October 2021 Issue |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.01174](https://arxiv.org/abs/2108.01174) [cs.AI]** |
|           | (or **[arXiv:2108.01174v1](https://arxiv.org/abs/2108.01174v1) [cs.AI]** for this version) |





<h2 id="2021-08-04-2">2. Underreporting of errors in NLG output, and what to do about it
</h2>

Title: [Underreporting of errors in NLG output, and what to do about it](https://arxiv.org/abs/2108.01182)

Authors: [Emiel van Miltenburg](https://arxiv.org/search/cs?searchtype=author&query=van+Miltenburg%2C+E), [Miruna-Adriana Clinciu](https://arxiv.org/search/cs?searchtype=author&query=Clinciu%2C+M), [Ondřej Dušek](https://arxiv.org/search/cs?searchtype=author&query=Dušek%2C+O), [Dimitra Gkatzia](https://arxiv.org/search/cs?searchtype=author&query=Gkatzia%2C+D), [Stephanie Inglis](https://arxiv.org/search/cs?searchtype=author&query=Inglis%2C+S), [Leo Leppänen](https://arxiv.org/search/cs?searchtype=author&query=Leppänen%2C+L), [Saad Mahamood](https://arxiv.org/search/cs?searchtype=author&query=Mahamood%2C+S), [Emma Manning](https://arxiv.org/search/cs?searchtype=author&query=Manning%2C+E), [Stephanie Schoch](https://arxiv.org/search/cs?searchtype=author&query=Schoch%2C+S), [Craig Thomson](https://arxiv.org/search/cs?searchtype=author&query=Thomson%2C+C), [Luou Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+L)

> We observe a severe under-reporting of the different kinds of errors that Natural Language Generation systems make. This is a problem, because mistakes are an important indicator of where systems should still be improved. If authors only report overall performance metrics, the research community is left in the dark about the specific weaknesses that are exhibited by `state-of-the-art' research. Next to quantifying the extent of error under-reporting, this position paper provides recommendations for error identification, analysis and reporting.

| Comments: | Prefinal version, accepted for publication in the Proceedings of the 14th International Conference on Natural Language Generation (INLG 2021, Aberdeen). Comments welcome |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.01182](https://arxiv.org/abs/2108.01182) [cs.CL]** |
|           | (or **[arXiv:2108.01182v1](https://arxiv.org/abs/2108.01182v1) [cs.CL]** for this version) |





<h2 id="2021-08-04-3">3. A Dynamic Head Importance Computation Mechanism for Neural Machine Translation
</h2>

Title: [A Dynamic Head Importance Computation Mechanism for Neural Machine Translation](https://arxiv.org/abs/2108.01377)

Authors: [Akshay Goindani](https://arxiv.org/search/cs?searchtype=author&query=Goindani%2C+A), [Manish Shrivastava](https://arxiv.org/search/cs?searchtype=author&query=Shrivastava%2C+M)

> Multiple parallel attention mechanisms that use multiple attention heads facilitate greater performance of the Transformer model for various applications e.g., Neural Machine Translation (NMT), text classification. In multi-head attention mechanism, different heads attend to different parts of the input. However, the limitation is that multiple heads might attend to the same part of the input, resulting in multiple heads being redundant. Thus, the model resources are under-utilized. One approach to avoid this is to prune least important heads based on certain importance score. In this work, we focus on designing a Dynamic Head Importance Computation Mechanism (DHICM) to dynamically calculate the importance of a head with respect to the input. Our insight is to design an additional attention layer together with multi-head attention, and utilize the outputs of the multi-head attention along with the input, to compute the importance for each head. Additionally, we add an extra loss function to prevent the model from assigning same score to all heads, to identify more important heads and improvise performance. We analyzed performance of DHICM for NMT with different languages. Experiments on different datasets show that DHICM outperforms traditional Transformer-based approach by large margin, especially, when less training data is available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.01377](https://arxiv.org/abs/2108.01377) [cs.CL]** |
|           | (or **[arXiv:2108.01377v1](https://arxiv.org/abs/2108.01377v1) [cs.CL]** for this version) |





# 2021-08-03

[Return to Index](#Index)



<h2 id="2021-08-03-1">1. Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding
</h2>

Title: [Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding](https://arxiv.org/abs/2108.00205)

Authors: [Heng Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Joey Tianyi Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J+T), [Yew-Soon Ong](https://arxiv.org/search/cs?searchtype=author&query=Ong%2C+Y)

> Current one-stage methods for visual grounding encode the language query as one holistic sentence embedding before fusion with visual feature. Such a formulation does not treat each word of a query sentence on par when modeling language to visual attention, therefore prone to neglect words which are less important for sentence embedding but critical for visual grounding. In this paper we propose Word2Pix: a one-stage visual grounding network based on encoder-decoder transformer architecture that enables learning for textual to visual feature correspondence via word to pixel attention. The embedding of each word from the query sentence is treated alike by attending to visual pixels individually instead of single holistic sentence embedding. In this way, each word is given equivalent opportunity to adjust the language to vision attention towards the referent target through multiple stacks of transformer decoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg datasets and the proposed Word2Pix outperforms existing one-stage methods by a notable margin. The results obtained also show that Word2Pix surpasses two-stage visual grounding models, while at the same time keeping the merits of one-stage paradigm namely end-to-end training and real-time inference speed intact.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.00205](https://arxiv.org/abs/2108.00205) [cs.CV]** |
|           | (or **[arXiv:2108.00205v1](https://arxiv.org/abs/2108.00205v1) [cs.CV]** for this version) |





<h2 id="2021-08-03-2">2. StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators
</h2>

Title: [StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators](https://arxiv.org/abs/2108.00946)

Authors: [Rinon Gal](https://arxiv.org/search/cs?searchtype=author&query=Gal%2C+R), [Or Patashnik](https://arxiv.org/search/cs?searchtype=author&query=Patashnik%2C+O), [Haggai Maron](https://arxiv.org/search/cs?searchtype=author&query=Maron%2C+H), [Gal Chechik](https://arxiv.org/search/cs?searchtype=author&query=Chechik%2C+G), [Daniel Cohen-Or](https://arxiv.org/search/cs?searchtype=author&query=Cohen-Or%2C+D)

> Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained blindly? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image from those domains. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Graphics (cs.GR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.00946](https://arxiv.org/abs/2108.00946) [cs.CV]** |
|           | (or **[arXiv:2108.00946v1](https://arxiv.org/abs/2108.00946v1) [cs.CV]** for this version) |





<h2 id="2021-08-03-3">3. Structural Guidance for Transformer Language Models
</h2>

Title: [Structural Guidance for Transformer Language Models](https://arxiv.org/abs/2108.00104)

Authors: [Peng Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+P), [Tahira Naseem](https://arxiv.org/search/cs?searchtype=author&query=Naseem%2C+T), [Roger Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+R), [Ramón Fernandez Astudillo](https://arxiv.org/search/cs?searchtype=author&query=Astudillo%2C+R+F)

> Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The "Generative Parsing" idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The "Structural Scaffold" idea guides the language model's representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models' syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.

| Comments: | To be issued as paper revision for ACL 2021                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.00104](https://arxiv.org/abs/2108.00104) [cs.CL]** |
|           | (or **[arXiv:2108.00104v1](https://arxiv.org/abs/2108.00104v1) [cs.CL]** for this version) |





<h2 id="2021-08-03-4">4. LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization
</h2>

Title: [LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization](https://arxiv.org/abs/2108.00801)

Authors: [Weidong Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+W), [Mingjun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+M), [Lusheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Di Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+D), [Jinwen Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J), [Zhenhua Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Zhenyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Jianbo Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J)

> Language model pre-training based on large corpora has achieved tremendous success in terms of constructing enriched contextual representations and has led to significant performance gains on a diverse range of Natural Language Understanding (NLU) tasks. Despite the success, most current pre-trained language models, such as BERT, are trained based on single-grained tokenization, usually with fine-grained characters or sub-words, making it hard for them to learn the precise meaning of coarse-grained words and phrases. In this paper, we propose a simple yet effective pre-training method named LICHEE to efficiently incorporate multi-grained information of input text. Our method can be applied to various pre-trained language models and improve their representation capability. Extensive experiments conducted on CLUE and SuperGLUE demonstrate that our method achieves comprehensive improvements on a wide variety of NLU tasks in both Chinese and English with little extra inference cost incurred, and that our best ensemble model achieves the state-of-the-art performance on CLUE benchmark competition.

| Comments: | Accepted by ACL Findings 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2108.00801](https://arxiv.org/abs/2108.00801) [cs.CL]** |
|           | (or **[arXiv:2108.00801v1](https://arxiv.org/abs/2108.00801v1) [cs.CL]** for this version) |







# 2021-08-02

[Return to Index](#Index)



<h2 id="2021-08-02-1">1. Difficulty-Aware Machine Translation Evaluation
</h2>

Title: [Difficulty-Aware Machine Translation Evaluation](https://arxiv.org/abs/2107.14402)

Authors: [Runzhe Zhan](https://arxiv.org/search/cs?searchtype=author&query=Zhan%2C+R), [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S)

> The high-quality translation results produced by machine translation (MT) systems still pose a huge challenge for automatic evaluation. Current MT evaluation pays the same attention to each sentence component, while the questions of real-world examinations (e.g., university examinations) have different difficulties and weightings. In this paper, we propose a novel difficulty-aware MT evaluation metric, expanding the evaluation dimension by taking translation difficulty into consideration. A translation that fails to be predicted by most MT systems will be treated as a difficult one and assigned a large weight in the final score function, and conversely. Experimental results on the WMT19 English-German Metrics shared tasks show that our proposed method outperforms commonly used MT metrics in terms of human correlation. In particular, our proposed method performs well even when all the MT systems are very competitive, which is when most existing metrics fail to distinguish between them. The source code is freely available at [this https URL](https://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation).

| Comments: | Accepted to ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2107.14402](https://arxiv.org/abs/2107.14402) [cs.CL]** |
|           | (or **[arXiv:2107.14402v1](https://arxiv.org/abs/2107.14402v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-2">2. Residual Tree Aggregation of Layers for Neural Machine Translation
</h2>

Title: [Residual Tree Aggregation of Layers for Neural Machine Translation](https://arxiv.org/abs/2107.14590)

Authors: [GuoLiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Yiyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y)

> Although attention-based Neural Machine Translation has achieved remarkable progress in recent layers, it still suffers from issue of making insufficient use of the output of each layer. In transformer, it only uses the top layer of encoder and decoder in the subsequent process, which makes it impossible to take advantage of the useful information in other layers. To address this issue, we propose a residual tree aggregation of layers for Transformer(RTAL), which helps to fuse information across layers. Specifically, we try to fuse the information across layers by constructing a post-order binary tree. In additional to the last node, we add the residual connection to the process of generating child nodes. Our model is based on the Neural Machine Translation model Transformer and we conduct our experiments on WMT14 English-to-German and WMT17 English-to-France translation tasks. Experimental results across language pairs show that the proposed approach outperforms the strong baseline model significantly

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.14590](https://arxiv.org/abs/2107.14590) [cs.CL]** |
|           | (or **[arXiv:2107.14590v1](https://arxiv.org/abs/2107.14590v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-3">3. Neural Variational Learning for Grounded Language Acquisition
</h2>

Title: [Neural Variational Learning for Grounded Language Acquisition](https://arxiv.org/abs/2107.14593)

Authors: [Nisha Pillai](https://arxiv.org/search/cs?searchtype=author&query=Pillai%2C+N), [Cynthia Matuszek](https://arxiv.org/search/cs?searchtype=author&query=Matuszek%2C+C), [Francis Ferraro](https://arxiv.org/search/cs?searchtype=author&query=Ferraro%2C+F)

> We propose a learning system in which language is grounded in visual percepts without specific pre-defined categories of terms. We present a unified generative method to acquire a shared semantic/visual embedding that enables the learning of language about a wide range of real-world objects. We evaluate the efficacy of this learning by predicting the semantics of objects and comparing the performance with neural and non-neural inputs. We show that this generative approach exhibits promising results in language grounding without pre-specifying visual categories under low resource settings. Our experiments demonstrate that this approach is generalizable to multilingual, highly varied datasets.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | 2021 30th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN) |
| Cite as:           | **[arXiv:2107.14593](https://arxiv.org/abs/2107.14593) [cs.CL]** |
|                    | (or **[arXiv:2107.14593v1](https://arxiv.org/abs/2107.14593v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-4">4. Multi-stage Pre-training over Simplified Multimodal Pre-training Models
</h2>

Title: [Multi-stage Pre-training over Simplified Multimodal Pre-training Models](https://arxiv.org/abs/2107.14596)

Authors: [Tongtong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Fangxiang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+F), [Xiaojie Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X)

> Multimodal pre-training models, such as LXMERT, have achieved excellent results in downstream tasks. However, current pre-trained models require large amounts of training data and have huge model sizes, which make them difficult to apply in low-resource situations. How to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem. In this paper, we propose a new Multi-stage Pre-training (MSP) method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train the model in stages. We also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus. We take a Simplified LXMERT (LXMERT- S), which has only 45.9% parameters of the original LXMERT model and 11.76% of the original pre-training data as the testbed of our MSP method. Experimental results show that our method achieves comparable performance to the original LXMERT model in all downstream tasks, and even outperforms the original model in Image-Text Retrieval task.

| Comments: | 10 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.14596](https://arxiv.org/abs/2107.14596) [cs.CL]** |
|           | (or **[arXiv:2107.14596v1](https://arxiv.org/abs/2107.14596v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-5">5. MDQE: A More Accurate Direct Pretraining for Machine Translation Quality Estimation
</h2>

Title: [MDQE: A More Accurate Direct Pretraining for Machine Translation Quality Estimation](https://arxiv.org/abs/2107.14600)

Authors: [Lei Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+L)

> It is expensive to evaluate the results of Machine Translation(MT), which usually requires manual translation as a reference. Machine Translation Quality Estimation (QE) is a task of predicting the quality of machine translations without relying on any reference. Recently, the emergence of predictor-estimator framework which trains the predictor as a feature extractor and estimator as a QE predictor, and pre-trained language models(PLM) have achieved promising QE performance. However, we argue that there are still gaps between the predictor and the estimator in both data quality and training objectives, which preclude QE models from benefiting from a large number of parallel corpora more directly. Based on previous related work that have alleviated gaps to some extent, we propose a novel framework that provides a more accurate direct pretraining for QE tasks. In this framework, a generator is trained to produce pseudo data that is closer to the real QE data, and a estimator is pretrained on these data with novel objectives that are the same as the QE task. Experiments on widely used benchmarks show that our proposed framework outperforms existing methods, without using any pretraining models such as BERT.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:2105.07149](https://arxiv.org/abs/2105.07149) by other authors |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.14600](https://arxiv.org/abs/2107.14600) [cs.CL]** |
|           | (or **[arXiv:2107.14600v1](https://arxiv.org/abs/2107.14600v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-6">6. Towards Universality in Multilingual Text Rewriting
</h2>

Title: [Towards Universality in Multilingual Text Rewriting](https://arxiv.org/abs/2107.14749)

Authors: [Xavier Garcia](https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+X), [Noah Constant](https://arxiv.org/search/cs?searchtype=author&query=Constant%2C+N), [Mandy Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+M), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

> In this work, we take the first steps towards building a universal rewriter: a model capable of rewriting text in any language to exhibit a wide variety of attributes, including styles and languages, while preserving as much of the original semantics as possible. In addition to obtaining state-of-the-art results on unsupervised translation, we also demonstrate the ability to do zero-shot sentiment transfer in non-English languages using only English exemplars for sentiment. We then show that our model is able to modify multiple attributes at once, for example adjusting both language and sentiment jointly. Finally, we show that our model is capable of performing zero-shot formality-sensitive translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.14749](https://arxiv.org/abs/2107.14749) [cs.CL]** |
|           | (or **[arXiv:2107.14749v1](https://arxiv.org/abs/2107.14749v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-7">7. ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback
</h2>

Title: [ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback](https://arxiv.org/abs/2107.14800)

Authors: [Shiyue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Benjamin Frey](https://arxiv.org/search/cs?searchtype=author&query=Frey%2C+B), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> We introduce ChrEnTranslate, an online machine translation demonstration system for translation between English and an endangered language Cherokee. It supports both statistical and neural translation models as well as provides quality estimation to inform users of reliability, two user feedback interfaces for experts and common users respectively, example inputs to collect human translations for monolingual data, word alignment visualization, and relevant terms from the Cherokee-English dictionary. The quantitative evaluation demonstrates that our backbone translation models achieve state-of-the-art translation performance and our quality estimation well correlates with both BLEU and human judgment. By analyzing 216 pieces of expert feedback, we find that NMT is preferable because it copies less than SMT, and, in general, current models can translate fragments of the source sentence but make major mistakes. When we add these 216 expert-corrected parallel texts into the training set and retrain models, equal or slightly better performance is observed, which demonstrates indicates the potential of human-in-the-loop learning. Our online demo is at [this https URL](https://chren.cs.unc.edu/;) our code is open-sourced at [this https URL](https://github.com/ZhangShiyue/ChrEnTranslate;) and our data is available at [this https URL](https://github.com/ZhangShiyue/ChrEn).

| Comments: | ACL 2021 Demo (8 pages)                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2107.14800](https://arxiv.org/abs/2107.14800) [cs.CL]** |
|           | (or **[arXiv:2107.14800v1](https://arxiv.org/abs/2107.14800v1) [cs.CL]** for this version) |




# Daily arXiv: Machine Translation - July, 2021

# Index


- [2021-07-09](#2021-07-09)

  - [1. Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text](#2021-07-09-1)
  - [2. Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation](#2021-07-09-2)
- [2021-07-08](#2021-07-08)
  - [1. Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking](#2021-07-08-1)
  - [2. Kosp2e: Korean Speech to English Translation Corpus](#2021-07-08-2)
  - [3. Efficient Transformer for Direct Speech Translation](#2021-07-08-3)
  - [4. On Training Instance Selection for Few-Shot Neural Text Generation](#2021-07-08-4)
  - [5. Time-Aware Ancient Chinese Text Translation and Inference](#2021-07-08-5)
- [2021-07-07](#2021-07-07)

  - [1. Long-Short Transformer: Efficient Transformers for Language and Vision](#2021-07-07-1)
  - [2. Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering](#2021-07-07-2)
  - [3. An NLG pipeline for a legal expert system: a work in progress](#2021-07-07-3)
  - [4. The NiuTrans End-to-End Speech Translation System \\for IWSLT 2021 Offline Task](#2021-07-07-4)
  - [5. VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer](#2021-07-07-5)
- [2021-07-06](#2021-07-06)
  - [1. Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition](#2021-07-06-1)
  - [2. IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task](#2021-07-06-2)
  - [3. Packing: Towards 2x NLP BERT Acceleration](#2021-07-06-3)
  - [4. Power Law Graph Transformer for Machine Translation and Representation Learning](#2021-07-06-4)
  - [5. ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](#2021-07-06-5)


- [2021-07-05](#2021-07-05)
  - [1. Transformer-F: A Transformer network with effective methods for learning universal sentence representation](#2021-07-05-1)
  - [2. A Primer on Pretrained Multilingual Language Models](#2021-07-05-2)
  - [3. Interactive decoding of words from visual speech recognition models](#2021-07-05-3)
  - [4. Data Centric Domain Adaptation for Historical Text with OCR Errors](#2021-07-05-4)
- [2021-07-02](#2021-07-02)

  - [1. GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph](#2021-07-02-1)
  - [2. ESPnet-ST IWSLT 2021 Offline Speech Translation System](#2021-07-02-2)
  - [3. Word-Free Spoken Language Understanding for Mandarin-Chinese](#2021-07-02-3)
  - [4. The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021](#2021-07-02-4)
  - [5. Zero-pronoun Data Augmentation for Japanese-to-English Translation](#2021-07-02-5)
  - [6. Modeling Target-side Inflection in Placeholder Translation](#2021-07-02-6)
  - [7. CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding](#2021-07-02-7 )
- [2021-07-01](#2021-07-01)
  - [1. What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?](#2021-07-01-1)
  - [2. Mixed Cross Entropy Loss for Neural Machine Translation](#2021-07-01-2)
  - [3. Cross-lingual alignments of ELMo contextual embeddings](#2021-07-01-3)
  - [4. ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information](#2021-07-01-4)
  - [5. IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task](#2021-07-01-5)
  - [6. XLM-E: Cross-lingual Language Model Pre-training via ELECTRA](#2021-07-01-6)
  - [7. On the Power of Saturated Transformers: A View from Circuit Complexity](#2021-07-01-7)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-07-09

[Return to Index](#Index)



<h2 id="2021-07-09-1">1. Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text
</h2>

Title: [Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text](https://arxiv.org/abs/2107.03444)

Authors: [Philippe Laban](https://arxiv.org/search/cs?searchtype=author&query=Laban%2C+P), [Tobias Schnabel](https://arxiv.org/search/cs?searchtype=author&query=Schnabel%2C+T), [Paul Bennett](https://arxiv.org/search/cs?searchtype=author&query=Bennett%2C+P), [Marti A. Hearst](https://arxiv.org/search/cs?searchtype=author&query=Hearst%2C+M+A)

> This work presents Keep it Simple (KiS), a new approach to unsupervised text simplification which learns to balance a reward across three properties: fluency, salience and simplicity. We train the model with a novel algorithm to optimize the reward (k-SCST), in which the model proposes several candidate simplifications, computes each candidate's reward, and encourages candidates that outperform the mean reward. Finally, we propose a realistic text comprehension task as an evaluation method for text simplification. When tested on the English news domain, the KiS model outperforms strong supervised baselines by more than 4 SARI points, and can help people complete a comprehension task an average of 18% faster while retaining accuracy, when compared to the original text. Code available: [this https URL](https://github.com/tingofurro/keep_it_simple)

| Comments:          | Accepted at ACL-IJCNLP 2021, 14 pages, 7 figures             |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Association for Computational Linguistics (2021)             |
| Cite as:           | **[arXiv:2107.03444](https://arxiv.org/abs/2107.03444) [cs.CL]** |
|                    | (or **[arXiv:2107.03444v1](https://arxiv.org/abs/2107.03444v1) [cs.CL]** for this version) |





<h2 id="2021-07-09-2">2. Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation
</h2>

Title: [Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation](https://arxiv.org/abs/2107.03625)

Authors: [Yves Bestgen](https://arxiv.org/search/cs?searchtype=author&query=Bestgen%2C+Y)

> A comparison of formulaic sequences in human and neural machine translation of quality newspaper articles shows that neural machine translations contain less lower-frequency, but strongly-associated formulaic sequences, and more high-frequency formulaic sequences. These differences were statistically significant and the effect sizes were almost always medium or large. These observations can be related to the differences between second language learners of various levels and between translated and untranslated texts. The comparison between the neural machine translation systems indicates that some systems produce more formulaic sequences of both types than other systems.

| Comments: | Accepted at Translation and Interpreting Technology Online - TRITON 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.03625](https://arxiv.org/abs/2107.03625) [cs.CL]** |
|           | (or **[arXiv:2107.03625v1](https://arxiv.org/abs/2107.03625v1) [cs.CL]** for this version) |






# 2021-07-08

[Return to Index](#Index)



<h2 id="2021-07-08-1">1. Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking
</h2>

Title: [Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking](https://arxiv.org/abs/2107.02865)

Authors: [Daniel Diomedi](https://arxiv.org/search/cs?searchtype=author&query=Diomedi%2C+D), [Aidan Hogan](https://arxiv.org/search/cs?searchtype=author&query=Hogan%2C+A)

> The goal of Question Answering over Knowledge Graphs (KGQA) is to find answers for natural language questions over a knowledge graph. Recent KGQA approaches adopt a neural machine translation (NMT) approach, where the natural language question is translated into a structured query language. However, NMT suffers from the out-of-vocabulary problem, where terms in a question may not have been seen during training, impeding their translation. This issue is particularly problematic for the millions of entities that large knowledge graphs describe. We rather propose a KGQA approach that delegates the processing of entities to entity linking (EL) systems. NMT is then used to create a query template with placeholders that are filled by entities identified in an EL phase. Slot filling is used to decide which entity fills which placeholder. Experiments for QA over Wikidata show that our approach outperforms pure NMT: while there remains a strong dependence on having seen similar query templates during training, errors relating to entities are greatly reduced.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02865](https://arxiv.org/abs/2107.02865) [cs.AI]** |
|           | (or **[arXiv:2107.02865v1](https://arxiv.org/abs/2107.02865v1) [cs.AI]** for this version) |





<h2 id="2021-07-08-2">2. Kosp2e: Korean Speech to English Translation Corpus
</h2>

Title: [Kosp2e: Korean Speech to English Translation Corpus](https://arxiv.org/abs/2107.02875)

Authors: [Won Ik Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+W+I), [Seok Min Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S+M), [Hyunchang Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+H), [Nam Soo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+N+S)

> Most speech-to-text (S2T) translation studies use English speech as a source, which makes it difficult for non-English speakers to take advantage of the S2T technologies. For some languages, this problem was tackled through corpus construction, but the farther linguistically from English or the more under-resourced, this deficiency and underrepresentedness becomes more significant. In this paper, we introduce kosp2e (read as `kospi'), a corpus that allows Korean speech to be translated into English text in an end-to-end manner. We adopt open license speech recognition corpus, translation corpus, and spoken language corpora to make our dataset freely available to the public, and check the performance through the pipeline and training-based approaches. Using pipeline and various end-to-end schemes, we obtain the highest BLEU of 21.3 and 18.0 for each based on the English hypothesis, validating the feasibility of our data. We plan to supplement annotations for other target languages through community contributions in the future.

| Comments: | Interspeech 2021 Camera-ready                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.02875](https://arxiv.org/abs/2107.02875) [cs.CL]** |
|           | (or **[arXiv:2107.02875v1](https://arxiv.org/abs/2107.02875v1) [cs.CL]** for this version) |





<h2 id="2021-07-08-3">3. Efficient Transformer for Direct Speech Translation
</h2>

Title: [Efficient Transformer for Direct Speech Translation](https://arxiv.org/abs/2107.03069)

Authors: [Belen Alastruey](https://arxiv.org/search/cs?searchtype=author&query=Alastruey%2C+B), [Gerard I. Gállego](https://arxiv.org/search/cs?searchtype=author&query=Gállego%2C+G+I), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

> The advent of Transformer-based models has surpassed the barriers of text. When working with speech, we must face a problem: the sequence length of an audio input is not suitable for the Transformer. To bypass this problem, a usual approach is adding strided convolutional layers, to reduce the sequence length before using the Transformer. In this paper, we propose a new approach for direct Speech Translation, where thanks to an efficient Transformer we can work with a spectrogram without having to use convolutional layers before the Transformer. This allows the encoder to learn directly from the spectrogram and no information is lost. We have created an encoder-decoder model, where the encoder is an efficient Transformer -- the Longformer -- and the decoder is a traditional Transformer decoder. Our results, which are close to the ones obtained with the standard approach, show that this is a promising research direction.

| Comments: | (c) 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2107.03069](https://arxiv.org/abs/2107.03069) [cs.CL]** |
|           | (or **[arXiv:2107.03069v1](https://arxiv.org/abs/2107.03069v1) [cs.CL]** for this version) |





<h2 id="2021-07-08-4">4. On Training Instance Selection for Few-Shot Neural Text Generation
</h2>

Title: [On Training Instance Selection for Few-Shot Neural Text Generation](https://arxiv.org/abs/2107.03176)

Authors: [Ernie Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+E), [Xiaoyu Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+X), [Hui-Syuan Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+H), [Vera Demberg](https://arxiv.org/search/cs?searchtype=author&query=Demberg%2C+V)

> Large-scale pretrained language models have led to dramatic improvements in text generation. Impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). Nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. Little to no attention has been paid to the selection strategies and how they would affect model performance. In this work, we present a study on training instance selection in few-shot neural text generation. The selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost. Based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution, we propose a simple selection strategy with K-means clustering. We show that even with the naive clustering-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. We hope that this work will call for more attention on this largely unexplored area.

| Comments: | Accepted at ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.03176](https://arxiv.org/abs/2107.03176) [cs.CL]** |
|           | (or **[arXiv:2107.03176v1](https://arxiv.org/abs/2107.03176v1) [cs.CL]** for this version) |





<h2 id="2021-07-08-5">5. Time-Aware Ancient Chinese Text Translation and Inference
</h2>

Title: [Time-Aware Ancient Chinese Text Translation and Inference](https://arxiv.org/abs/2107.03179)

Authors: [Ernie Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+E), [Yow-Ting Shiue](https://arxiv.org/search/cs?searchtype=author&query=Shiue%2C+Y), [Hui-Syuan Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+H), [Vera Demberg](https://arxiv.org/search/cs?searchtype=author&query=Demberg%2C+V)

> In this paper, we aim to address the challenges surrounding the translation of ancient Chinese text: (1) The linguistic gap due to the difference in eras results in translations that are poor in quality, and (2) most translations are missing the contextual information that is often very crucial to understanding the text. To this end, we improve upon past translation techniques by proposing the following: We reframe the task as a multi-label prediction task where the model predicts both the translation and its particular era. We observe that this helps to bridge the linguistic gap as chronological context is also used as auxiliary information. % As a natural step of generalization, we pivot on the modern Chinese translations to generate multilingual outputs. %We show experimentally the efficacy of our framework in producing quality translation outputs and also validate our framework on a collected task-specific parallel corpus. We validate our framework on a parallel corpus annotated with chronology information and show experimentally its efficacy in producing quality translation outputs. We release both the code and the data [this https URL](https://github.com/orina1123/time-aware-ancient-text-translation) for future research.

| Comments: | Accepted at LChange at ACL 2021                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.03179](https://arxiv.org/abs/2107.03179) [cs.CL]** |
|           | (or **[arXiv:2107.03179v1](https://arxiv.org/abs/2107.03179v1) [cs.CL]** for this version) |





# 2021-07-07

[Return to Index](#Index)



<h2 id="2021-07-07-1">1. Long-Short Transformer: Efficient Transformers for Language and Vision
</h2>

Title: [Long-Short Transformer: Efficient Transformers for Language and Vision](https://arxiv.org/abs/2107.02192)

Authors: [Chen Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Wei Ping](https://arxiv.org/search/cs?searchtype=author&query=Ping%2C+W), [Chaowei Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C), [Mohammad Shoeybi](https://arxiv.org/search/cs?searchtype=author&query=Shoeybi%2C+M), [Tom Goldstein](https://arxiv.org/search/cs?searchtype=author&query=Goldstein%2C+T), [Anima Anandkumar](https://arxiv.org/search/cs?searchtype=author&query=Anandkumar%2C+A), [Bryan Catanzaro](https://arxiv.org/search/cs?searchtype=author&query=Catanzaro%2C+B)

> Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3× as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results~(e.g., Top-1 accuracy 84.1% trained on 224×224 ImageNet-1K only), while being more scalable on high-resolution images. The models and source code will be released soon.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02192](https://arxiv.org/abs/2107.02192) [cs.CV]** |
|           | (or **[arXiv:2107.02192v1](https://arxiv.org/abs/2107.02192v1) [cs.CV]** for this version) |





<h2 id="2021-07-07-2">2. Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering
</h2>

Title: [Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering](https://arxiv.org/abs/2107.02331)

Authors: [Siddharth Karamcheti](https://arxiv.org/search/cs?searchtype=author&query=Karamcheti%2C+S), [Ranjay Krishna](https://arxiv.org/search/cs?searchtype=author&query=Krishna%2C+R), [Li Fei-Fei](https://arxiv.org/search/cs?searchtype=author&query=Fei-Fei%2C+L), [Christopher D. Manning](https://arxiv.org/search/cs?searchtype=author&query=Manning%2C+C+D)

> Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers -- groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.

| Comments: | Accepted at ACL-IJCNLP 2021. 17 pages, 16 Figures            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.02331](https://arxiv.org/abs/2107.02331) [cs.CL]** |
|           | (or **[arXiv:2107.02331v1](https://arxiv.org/abs/2107.02331v1) [cs.CL]** for this version) |





<h2 id="2021-07-07-3">3. An NLG pipeline for a legal expert system: a work in progress
</h2>

Title: [An NLG pipeline for a legal expert system: a work in progress](https://arxiv.org/abs/2107.02421)

Authors: [Inari Listenmaa](https://arxiv.org/search/cs?searchtype=author&query=Listenmaa%2C+I), [Jason Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+J), [Alfred Ang](https://arxiv.org/search/cs?searchtype=author&query=Ang%2C+A), [Maryam Hanafiah](https://arxiv.org/search/cs?searchtype=author&query=Hanafiah%2C+M), [Regina Cheong](https://arxiv.org/search/cs?searchtype=author&query=Cheong%2C+R)

> We present the NLG component for L4, a prototype domain-specific language (DSL) for drafting laws and contracts. As a concrete use case, we describe a pipeline for a legal expert system created from L4 code. The NLG component is used in two steps. The first step is to create an interview, whose answers are processed into a query for an automated reasoner. The second step is to render the answers of the reasoner in natural language.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02421](https://arxiv.org/abs/2107.02421) [cs.CL]** |
|           | (or **[arXiv:2107.02421v1](https://arxiv.org/abs/2107.02421v1) [cs.CL]** for this version) |





<h2 id="2021-07-07-4">4. The NiuTrans End-to-End Speech Translation System \\for IWSLT 2021 Offline Task
</h2>

Title: [The NiuTrans End-to-End Speech Translation System \\for IWSLT 2021 Offline Task](https://arxiv.org/abs/2107.02444)

Authors: [Chen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Xiaoqian Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Xiaowen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Laohu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Canan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> This paper describes the submission of the NiuTrans end-to-end speech translation system for the IWSLT 2021 offline task, which translates from the English audio to German text directly without intermediate transcription. We use the Transformer-based model architecture and enhance it by Conformer, relative position encoding, and stacked acoustic and textual encoding. To augment the training data, the English transcriptions are translated to German translations. Finally, we employ ensemble decoding to integrate the predictions from several models trained with the different datasets. Combining these techniques, we achieve 33.84 BLEU points on the MuST-C En-De test set, which shows the enormous potential of the end-to-end model.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.02444](https://arxiv.org/abs/2107.02444) [cs.CL]** |
|           | (or **[arXiv:2107.02444v1](https://arxiv.org/abs/2107.02444v1) [cs.CL]** for this version) |





<h2 id="2021-07-07-5">5. VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer
</h2>

Title: [VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer](https://arxiv.org/abs/2107.02681)

Authors: [Zineng Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Z), [Jaemin Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+J), [Hao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Since visual perception can give rich information beyond text descriptions for world understanding, there has been increasing interest in leveraging visual grounding for language learning. Recently, vokenization has attracted attention by using the predictions of a text-to-image retrieval model as labels for language model supervision. Despite its success, the method suffers from approximation error of using finite image labels and the lack of vocabulary diversity of a small image-text dataset. To overcome these limitations, we present VidLanKD, a video-language knowledge distillation method for improving language understanding. We train a multi-modal teacher model on a video-text dataset, and then transfer its knowledge to a student language model with a text dataset. To avoid approximation error, we propose to use different knowledge distillation objectives. In addition, the use of a large-scale video-text dataset helps learn diverse and richer vocabularies. In our experiments, VidLanKD achieves consistent improvements over text-only language models and vokenization models, on several downstream language understanding tasks including GLUE, SQuAD, and SWAG. We also demonstrate the improved world knowledge, physical reasoning, and temporal reasoning capabilities of our model by evaluating on the GLUE-diagnostics, PIQA, and TRACIE datasets. Lastly, we present comprehensive ablation studies as well as visualizations of the learned text-to-video grounding results of our teacher and student language models. Our code and models are available at: [this https URL](https://github.com/zinengtang/VidLanKD)

| Comments: | 18 pages (5 figures, 10 tables)                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.02681](https://arxiv.org/abs/2107.02681) [cs.CL]** |
|           | (or **[arXiv:2107.02681v1](https://arxiv.org/abs/2107.02681v1) [cs.CL]** for this version) |










# 2021-07-06

[Return to Index](#Index)



<h2 id="2021-07-06-1">1. Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition
</h2>

Title: [Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition](https://arxiv.org/abs/2107.01569)

Authors: [Tomohiro Tanaka](https://arxiv.org/search/cs?searchtype=author&query=Tanaka%2C+T), [Ryo Masumura](https://arxiv.org/search/cs?searchtype=author&query=Masumura%2C+R), [Mana Ihori](https://arxiv.org/search/cs?searchtype=author&query=Ihori%2C+M), [Akihiko Takashima](https://arxiv.org/search/cs?searchtype=author&query=Takashima%2C+A), [Takafumi Moriya](https://arxiv.org/search/cs?searchtype=author&query=Moriya%2C+T), [Takanori Ashihara](https://arxiv.org/search/cs?searchtype=author&query=Ashihara%2C+T), [Shota Orihashi](https://arxiv.org/search/cs?searchtype=author&query=Orihashi%2C+S), [Naoki Makishima](https://arxiv.org/search/cs?searchtype=author&query=Makishima%2C+N)

> We propose a cross-modal transformer-based neural correction models that refines the output of an automatic speech recognition (ASR) system so as to exclude ASR errors. Generally, neural correction models are composed of encoder-decoder networks, which can directly model sequence-to-sequence mapping problems. The most successful method is to use both input speech and its ASR output text as the input contexts for the encoder-decoder networks. However, the conventional method cannot take into account the relationships between these two different modal inputs because the input contexts are separately encoded for each modal. To effectively leverage the correlated information between the two different modal inputs, our proposed models encode two different contexts jointly on the basis of cross-modal self-attention using a transformer. We expect that cross-modal self-attention can effectively capture the relationships between two different modals for refining ASR hypotheses. We also introduce a shallow fusion technique to efficiently integrate the first-pass ASR model and our proposed neural correction model. Experiments on Japanese natural language ASR tasks demonstrated that our proposed models achieve better ASR performance than conventional neural correction models.

| Comments: | Accepted to Interspeech 2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.01569](https://arxiv.org/abs/2107.01569) [cs.CL]** |
|           | (or **[arXiv:2107.01569v1](https://arxiv.org/abs/2107.01569v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-2">2. IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task
</h2>

Title: [IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task](https://arxiv.org/abs/2107.01656)

Authors: [Baban Gain](https://arxiv.org/search/cs?searchtype=author&query=Gain%2C+B), [Dibyanayan Bandyopadhyay](https://arxiv.org/search/cs?searchtype=author&query=Bandyopadhyay%2C+D), [Asif Ekbal](https://arxiv.org/search/cs?searchtype=author&query=Ekbal%2C+A)

> Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the translation by removing ambiguity on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT - 2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU points for Evaluation and Challenge subset, respectively.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.01656](https://arxiv.org/abs/2107.01656) [cs.CL]** |
|           | (or **[arXiv:2107.01656v1](https://arxiv.org/abs/2107.01656v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-3">3. Packing: Towards 2x NLP BERT Acceleration
</h2>

Title: [Packing: Towards 2x NLP BERT Acceleration](https://arxiv.org/abs/2107.02027)

Authors: [Matej Kosec](https://arxiv.org/search/cs?searchtype=author&query=Kosec%2C+M), [Sheng Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+S), [Mario Michael Krell](https://arxiv.org/search/cs?searchtype=author&query=Krell%2C+M+M)

> We find that at sequence length 512 padding tokens represent in excess of 50% of the Wikipedia dataset used for pretraining BERT (Bidirectional Encoder Representations from Transformers). Therefore by removing all padding we achieve a 2x speed-up in terms of sequences/sec. To exploit this characteristic of the dataset, we develop and contrast two deterministic packing algorithms. Both algorithms rely on the assumption that sequences are interchangeable and therefore packing can be performed on the histogram of sequence lengths, rather than per sample. This transformation of the problem leads to algorithms which are fast and have linear complexity in dataset size. The shortest-pack-first histogram-packing (SPFHP) algorithm determines the packing order for the Wikipedia dataset of over 16M sequences in 0.02 seconds. The non-negative least-squares histogram-packing (NNLSHP) algorithm converges in 28.4 seconds but produces solutions which are more depth efficient, managing to get near optimal packing by combining a maximum of 3 sequences in one sample. Using the dataset with multiple sequences per sample requires additional masking in the attention layer and a modification of the MLM loss function. We demonstrate that both of these changes are straightforward to implement and have relatively little impact on the achievable performance gain on modern hardware. Finally, we pretrain BERT-Large using the packed dataset, demonstrating no loss of convergence and the desired 2x speed-up.

| Subjects:    | **Computation and Language (cs.CL)**; Computational Complexity (cs.CC); Information Theory (cs.IT); Machine Learning (cs.LG) |
| ------------ | ------------------------------------------------------------ |
| MSC classes: | 05-08                                                        |
| ACM classes: | I.2.7; G.2.1                                                 |
| Cite as:     | **[arXiv:2107.02027](https://arxiv.org/abs/2107.02027) [cs.CL]** |
|              | (or **[arXiv:2107.02027v1](https://arxiv.org/abs/2107.02027v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-4">4. Power Law Graph Transformer for Machine Translation and Representation Learning
</h2>

Title: [Power Law Graph Transformer for Machine Translation and Representation Learning](https://arxiv.org/abs/2107.02039)

Authors: [Burc Gokden](https://arxiv.org/search/cs?searchtype=author&query=Gokden%2C+B)

> We present the Power Law Graph Transformer, a transformer model with well defined deductive and inductive tasks for prediction and representation learning. The deductive task learns the dataset level (global) and instance level (local) graph structures in terms of learnable power law distribution parameters. The inductive task outputs the prediction probabilities using the deductive task output, similar to a transductive model. We trained our model with Turkish-English and Portuguese-English datasets from TED talk transcripts for machine translation and compared the model performance and characteristics to a transformer model with scaled dot product attention trained on the same experimental setup. We report BLEU scores of 17.79 and 28.33 on the Turkish-English and Portuguese-English translation tasks with our model, respectively. We also show how a duality between a quantization set and N-dimensional manifold representation can be leveraged to transform between local and global deductive-inductive outputs using successive application of linear and non-linear transformations end-to-end.

| Comments: | 55 pages, 39 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.02039](https://arxiv.org/abs/2107.02039) [cs.CL]** |
|           | (or **[arXiv:2107.02039v1](https://arxiv.org/abs/2107.02039v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-5">5. ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
</h2>

Title: [ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2107.02137)

Authors: [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Shuohuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Shikun Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S), [Siyu Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+S), [Chao Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+C), [Junyuan Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+J), [Jiaxiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Xuyi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Yanbin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Yuxiang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Weixin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W), [Zhihua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Weibao Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+W), [Jianzhong Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+J), [Zhizhou Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+Z), [Peng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+P), [Wei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W), [Xuan Ouyang](https://arxiv.org/search/cs?searchtype=author&query=Ouyang%2C+X), [Dianhai Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D), [Hao Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+H), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02137](https://arxiv.org/abs/2107.02137) [cs.CL]** |
|           | (or **[arXiv:2107.02137v1](https://arxiv.org/abs/2107.02137v1) [cs.CL]** for this version) |








# 2021-07-05

[Return to Index](#Index)



<h2 id="2021-07-05-1">1. Transformer-F: A Transformer network with effective methods for learning universal sentence representation
</h2>

Title: [Transformer-F: A Transformer network with effective methods for learning universal sentence representation](https://arxiv.org/abs/2107.00653)

Authors: [Yu Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y)

> The Transformer model is widely used in natural language processing for sentence representation. However, the previous Transformer-based models focus on function words that have limited meaning in most cases and could merely extract high-level semantic abstraction features. In this paper, two approaches are introduced to improve the performance of Transformers. We calculated the attention score by multiplying the part-of-speech weight vector with the correlation coefficient, which helps extract the words with more practical meaning. The weight vector is obtained by the input text sequence based on the importance of the part-of-speech. Furthermore, we fuse the features of each layer to make the sentence representation results more comprehensive and accurate. In experiments, we demonstrate the effectiveness of our model Transformer-F on three standard text classification datasets. Experimental results show that our proposed model significantly boosts the performance of text classification as compared to the baseline model. Specifically, we obtain a 5.28% relative improvement over the vanilla Transformer on the simple tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00653](https://arxiv.org/abs/2107.00653) [cs.CL]** |
|           | (or **[arXiv:2107.00653v1](https://arxiv.org/abs/2107.00653v1) [cs.CL]** for this version) |





<h2 id="2021-07-05-2">2. A Primer on Pretrained Multilingual Language Models
</h2>

Title: [A Primer on Pretrained Multilingual Language Models](https://arxiv.org/abs/2107.00676)

Authors: [Sumanth Doddapaneni](https://arxiv.org/search/cs?searchtype=author&query=Doddapaneni%2C+S), [Gowtham Ramesh](https://arxiv.org/search/cs?searchtype=author&query=Ramesh%2C+G), [Anoop Kunchukuttan](https://arxiv.org/search/cs?searchtype=author&query=Kunchukuttan%2C+A), [Pratyush Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+P), [Mitesh M. Khapra](https://arxiv.org/search/cs?searchtype=author&query=Khapra%2C+M+M)

> Multilingual Language Models (MLLMs) such as mBERT, XLM, XLM-R, \textit{etc.} have emerged as a viable option for bringing the power of pretraining to a large number of languages. Given their success in zero shot transfer learning, there has emerged a large body of work in (i) building bigger MLLMs covering a large number of languages (ii) creating exhaustive benchmarks covering a wider variety of tasks and languages for evaluating MLLMs (iii) analysing the performance of MLLMs on monolingual, zero shot crosslingual and bilingual tasks (iv) understanding the universal language patterns (if any) learnt by MLLMs and (v) augmenting the (often) limited capacity of MLLMs to improve their performance on seen or even unseen languages. In this survey, we review the existing literature covering the above broad areas of research pertaining to MLLMs. Based on our survey, we recommend some promising directions of future research.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00676](https://arxiv.org/abs/2107.00676) [cs.CL]** |
|           | (or **[arXiv:2107.00676v1](https://arxiv.org/abs/2107.00676v1) [cs.CL]** for this version) |







<h2 id="2021-07-05-3">3. Interactive decoding of words from visual speech recognition models
</h2>

Title: [Interactive decoding of words from visual speech recognition models](https://arxiv.org/abs/2107.00692)

Authors: [Brendan Shillingford](https://arxiv.org/search/cs?searchtype=author&query=Shillingford%2C+B), [Yannis Assael](https://arxiv.org/search/cs?searchtype=author&query=Assael%2C+Y), [Misha Denil](https://arxiv.org/search/cs?searchtype=author&query=Denil%2C+M)

> This work describes an interactive decoding method to improve the performance of visual speech recognition systems using user input to compensate for the inherent ambiguity of the task. Unlike most phoneme-to-word decoding pipelines, which produce phonemes and feed these through a finite state transducer, our method instead expands words in lockstep, facilitating the insertion of interaction points at each word position. Interaction points enable us to solicit input during decoding, allowing users to interactively direct the decoding process. We simulate the behavior of user input using an oracle to give an automated evaluation, and show promise for the use of this method for text input.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.00692](https://arxiv.org/abs/2107.00692) [cs.CL]** |
|           | (or **[arXiv:2107.00692v1](https://arxiv.org/abs/2107.00692v1) [cs.CL]** for this version) |







<h2 id="2021-07-05-4">4. Data Centric Domain Adaptation for Historical Text with OCR Errors
</h2>

Title: [Data Centric Domain Adaptation for Historical Text with OCR Errors](https://arxiv.org/abs/2107.00927)

Authors: [Luisa März](https://arxiv.org/search/cs?searchtype=author&query=März%2C+L), [Stefan Schweter](https://arxiv.org/search/cs?searchtype=author&query=Schweter%2C+S), [Nina Poerner](https://arxiv.org/search/cs?searchtype=author&query=Poerner%2C+N), [Benjamin Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+B), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> We propose new methods for in-domain and cross-domain Named Entity Recognition (NER) on historical data for Dutch and French. For the cross-domain case, we address domain shift by integrating unsupervised in-domain data via contextualized string embeddings; and OCR errors by injecting synthetic OCR errors into the source domain and address data centric domain adaptation. We propose a general approach to imitate OCR errors in arbitrary input data. Our cross-domain as well as our in-domain results outperform several strong baselines and establish state-of-the-art results. We publish preprocessed versions of the French and Dutch Europeana NER corpora.

| Comments: | 14 pages, 2 figures, 6 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.00927](https://arxiv.org/abs/2107.00927) [cs.CL]** |
|           | (or **[arXiv:2107.00927v1](https://arxiv.org/abs/2107.00927v1) [cs.CL]** for this version) |







# 2021-07-02

[Return to Index](#Index)



<h2 id="2021-07-02-1">1. GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph
</h2>

Title: [GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph](https://arxiv.org/abs/2107.00395)

Authors: [Yunxin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yu Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Baotian Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+B), [Qingcai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Yang Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+Y), [Xiaolong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yuxin Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Y), [Lin Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+L)

> Previous works indicate that the glyph of Chinese characters contains rich semantic information and has the potential to enhance the representation of Chinese characters. The typical method to utilize the glyph features is by incorporating them into the character embedding space. Inspired by previous methods, we innovatively propose a Chinese pre-trained representation model named as GlyphCRM, which abandons the ID-based character embedding method yet solely based on sequential character images. We render each character into a binary grayscale image and design two-channel position feature maps for it. Formally, we first design a two-layer residual convolutional neural network, namely HanGlyph to generate the initial glyph representation of Chinese characters, and subsequently adopt multiple bidirectional encoder Transformer blocks as the superstructure to capture the context-sensitive information. Meanwhile, we feed the glyph features extracted from each layer of the HanGlyph module into the underlying Transformer blocks by skip-connection method to fully exploit the glyph features of Chinese characters. As the HanGlyph module can obtain a sufficient glyph representation of any Chinese character, the long-standing out-of-vocabulary problem could be effectively solved. Extensive experimental results indicate that GlyphCRM substantially outperforms the previous BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has strong transferability and generalization on specialized fields and low-resource tasks. We hope this work could spark further research beyond the realms of well-established representation of Chinese texts.

| Comments: | 11 pages, 7 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2107.00395](https://arxiv.org/abs/2107.00395) [cs.AI]** |
|           | (or **[arXiv:2107.00395v1](https://arxiv.org/abs/2107.00395v1) [cs.AI]** for this version) |





<h2 id="2021-07-02-2">2. ESPnet-ST IWSLT 2021 Offline Speech Translation System
</h2>

Title: [ESPnet-ST IWSLT 2021 Offline Speech Translation System](https://arxiv.org/abs/2107.00636)

Authors: [Hirofumi Inaguma](https://arxiv.org/search/eess?searchtype=author&query=Inaguma%2C+H), [Brian Yan](https://arxiv.org/search/eess?searchtype=author&query=Yan%2C+B), [Siddharth Dalmia](https://arxiv.org/search/eess?searchtype=author&query=Dalmia%2C+S), [Pengcheng Gu](https://arxiv.org/search/eess?searchtype=author&query=Gu%2C+P), [Jiatong Shi](https://arxiv.org/search/eess?searchtype=author&query=Shi%2C+J), [Kevin Duh](https://arxiv.org/search/eess?searchtype=author&query=Duh%2C+K), [Shinji Watanabe](https://arxiv.org/search/eess?searchtype=author&query=Watanabe%2C+S)

> This paper describes the ESPnet-ST group's IWSLT 2021 submission in the offline speech translation track. This year we made various efforts on training data, architecture, and audio segmentation. On the data side, we investigated sequence-level knowledge distillation (SeqKD) for end-to-end (E2E) speech translation. Specifically, we used multi-referenced SeqKD from multiple teachers trained on different amounts of bitext. On the architecture side, we adopted the Conformer encoder and the Multi-Decoder architecture, which equips dedicated decoders for speech recognition and translation tasks in a unified encoder-decoder model and enables search in both source and target language spaces during inference. We also significantly improved audio segmentation by using the pyannote.audio toolkit and merging multiple short segments for long context modeling. Experimental evaluations showed that each of them contributed to large improvements in translation performance. Our best E2E system combined all the above techniques with model ensembling and achieved 31.4 BLEU on the 2-ref of tst2021 and 21.2 BLEU and 19.3 BLEU on the two single references of tst2021.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2107.00636](https://arxiv.org/abs/2107.00636) [eess.AS]** |
|           | (or **[arXiv:2107.00636v1](https://arxiv.org/abs/2107.00636v1) [eess.AS]** for this version) |





<h2 id="2021-07-02-3">3. Word-Free Spoken Language Understanding for Mandarin-Chinese
</h2>

Title: [Word-Free Spoken Language Understanding for Mandarin-Chinese](https://arxiv.org/abs/2107.00186)

Authors: [Zhiyuan Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Z), [Yuexin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Guo Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Xingyu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Akshat Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A)

> Spoken dialogue systems such as Siri and Alexa provide great convenience to people's everyday life. However, current spoken language understanding (SLU) pipelines largely depend on automatic speech recognition (ASR) modules, which require a large amount of language-specific training data. In this paper, we propose a Transformer-based SLU system that works directly on phones. This acoustic-based SLU system consists of only two blocks and does not require the presence of ASR module. The first block is a universal phone recognition system, and the second block is a Transformer-based language model for phones. We verify the effectiveness of the system on an intent classification dataset in Mandarin Chinese.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00186](https://arxiv.org/abs/2107.00186) [cs.CL]** |
|           | (or **[arXiv:2107.00186v1](https://arxiv.org/abs/2107.00186v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-4">4. The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021 </h2>



Title: [The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021]()

Authors: [Dan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Mengge Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+M), [Xiaoxi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Yuchen Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Lirong Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+L)

> This paper describes USTC-NELSLIP's submissions to the IWSLT2021 Simultaneous Speech Translation task. We proposed a novel simultaneous translation model, Cross Attention Augmented Transducer (CAAT), which extends conventional RNN-T to sequence-to-sequence tasks without monotonic constraints, e.g., simultaneous translation. Experiments on speech-to-text (S2T) and text-to-text (T2T) simultaneous translation tasks shows CAAT achieves better quality-latency trade-offs compared to \textit{wait-k}, one of the previous state-of-the-art approaches. Based on CAAT architecture and data augmentation, we build S2T and T2T simultaneous translation systems in this evaluation campaign. Compared to last year's optimal systems, our S2T simultaneous translation system improves by an average of 11.3 BLEU for all latency regimes, and our T2T simultaneous translation system improves by an average of 4.6 BLEU.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00279](https://arxiv.org/abs/2107.00279) [cs.CL]** |
|           | (or **[arXiv:2107.00279v1](https://arxiv.org/abs/2107.00279v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-5">5. Zero-pronoun Data Augmentation for Japanese-to-English Translation
</h2>

Title: [Zero-pronoun Data Augmentation for Japanese-to-English Translation](https://arxiv.org/abs/2107.00318)

Authors: [Ryokan Ri](https://arxiv.org/search/cs?searchtype=author&query=Ri%2C+R), [Toshiaki Nakazawa](https://arxiv.org/search/cs?searchtype=author&query=Nakazawa%2C+T), [Yoshimasa Tsuruoka](https://arxiv.org/search/cs?searchtype=author&query=Tsuruoka%2C+Y)

> For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding pronoun in the target side of the English sentence. However, although fully resolving zero pronouns often needs discourse context, in some cases, the local context within a sentence gives clues to the inference of the zero pronoun. In this study, we propose a data augmentation method that provides additional training signals for the translation model to learn correlations between local context and zero pronouns. We show that the proposed method significantly improves the accuracy of zero pronoun translation with machine translation experiments in the conversational domain.

| Comments: | WAT2021                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.00318](https://arxiv.org/abs/2107.00318) [cs.CL]** |
|           | (or **[arXiv:2107.00318v1](https://arxiv.org/abs/2107.00318v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-6">6. Modeling Target-side Inflection in Placeholder Translation
</h2>

Title: [Modeling Target-side Inflection in Placeholder Translation](https://arxiv.org/abs/2107.00334)

Authors: [Ryokan Ri](https://arxiv.org/search/cs?searchtype=author&query=Ri%2C+R), [Toshiaki Nakazawa](https://arxiv.org/search/cs?searchtype=author&query=Nakazawa%2C+T), [Yoshimasa Tsuruoka](https://arxiv.org/search/cs?searchtype=author&query=Tsuruoka%2C+Y)

> Placeholder translation systems enable the users to specify how a specific phrase is translated in the output sentence. The system is trained to output special placeholder tokens, and the user-specified term is injected into the output through the context-free replacement of the placeholder token. However, this approach could result in ungrammatical sentences because it is often the case that the specified term needs to be inflected according to the context of the output, which is unknown before the translation. To address this problem, we propose a novel method of placeholder translation that can inflect specified terms according to the grammatical construction of the output sentence. We extend the sequence-to-sequence architecture with a character-level decoder that takes the lemma of a user-specified term and the words generated from the word-level decoder to output the correct inflected form of the lemma. We evaluate our approach with a Japanese-to-English translation task in the scientific writing domain, and show that our model can incorporate specified terms in the correct form more successfully than other comparable models.

| Comments: | MT Summit 2021                                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.00334](https://arxiv.org/abs/2107.00334) [cs.CL]** |
|           | (or **[arXiv:2107.00334v1](https://arxiv.org/abs/2107.00334v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-7">7. CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding
</h2>

Title: [CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding](https://arxiv.org/abs/2107.00440)

Authors: [Dong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Ning Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Piji Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Hai-Tao Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H)

> Despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations. Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics. Different from the image processing field, the text is discrete and few word substitutions can cause significant semantic changes. To study the impact of semantics caused by small perturbations, we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes. To address this problem, we propose Contrastive Learning with semantIc Negative Examples (CLINE), which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking. By comparing with similar and opposite semantic examples, the model can effectively perceive the semantic changes caused by small perturbations. Empirical results show that our approach yields substantial improvements on a range of sentiment analysis, reasoning, and reading comprehension tasks. And CLINE also ensures the compactness within the same semantics and separability across different semantics in sentence-level.

| Comments: | ACL 2021, Main Conference, Long Paper                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2107.00440](https://arxiv.org/abs/2107.00440) [cs.CL]** |
|           | (or **[arXiv:2107.00440v1](https://arxiv.org/abs/2107.00440v1) [cs.CL]** for this version) |








# 2021-07-01

[Return to Index](#Index)



<h2 id="2021-07-01-1">1. What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?
</h2>

Title: [What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?](https://arxiv.org/abs/2106.15818)

Authors: [Kelly Marchisio](https://arxiv.org/search/cs?searchtype=author&query=Marchisio%2C+K), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [David Grangier](https://arxiv.org/search/cs?searchtype=author&query=Grangier%2C+D)

> Whereas existing literature on unsupervised machine translation (MT) focuses on exploiting unsupervised techniques for low-resource language pairs where bilingual training data is scare or unavailable, we investigate whether unsupervised MT can also improve translation quality of high-resource language pairs where sufficient bitext does exist. We compare the style of correct translations generated by either supervised or unsupervised MT and find that the unsupervised output is less monotonic and more natural than supervised output. We demonstrate a way to combine the benefits of unsupervised and supervised MT into a single system, resulting in better human evaluation of quality and fluency. Our results open the door to discussions about the potential contributions of unsupervised MT in high-resource settings, and how supervised and unsupervised systems might be mutually-beneficial.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.15818](https://arxiv.org/abs/2106.15818) [cs.CL]** |
|           | (or **[arXiv:2106.15818v1](https://arxiv.org/abs/2106.15818v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-2">2. Mixed Cross Entropy Loss for Neural Machine Translation
</h2>

Title: [Mixed Cross Entropy Loss for Neural Machine Translation](https://arxiv.org/abs/2106.15880)

Authors: [Haoran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Wei Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+W)

> In neural machine translation, cross entropy (CE) is the standard loss function in two training methods of auto-regressive models, i.e., teacher forcing and scheduled sampling. In this paper, we propose mixed cross entropy loss (mixed CE) as a substitute for CE in both training approaches. In teacher forcing, the model trained with CE regards the translation problem as a one-to-one mapping process, while in mixed CE this process can be relaxed to one-to-many. In scheduled sampling, we show that mixed CE has the potential to encourage the training and testing behaviours to be similar to each other, more effectively mitigating the exposure bias problem. We demonstrate the superiority of mixed CE over CE on several machine translation datasets, WMT'16 Ro-En, WMT'16 Ru-En, and WMT'14 En-De in both teacher forcing and scheduled sampling setups. Furthermore, in WMT'14 En-De, we also find mixed CE consistently outperforms CE on a multi-reference set as well as a challenging paraphrased reference set. We also found the model trained with mixed CE is able to provide a better probability distribution defined over the translation output space. Our code is available at [this https URL](https://github.com/haorannlp/mix).

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | ICML2021                                                     |
| Cite as:           | **[arXiv:2106.15880](https://arxiv.org/abs/2106.15880) [cs.CL]** |
|                    | (or **[arXiv:2106.15880v1](https://arxiv.org/abs/2106.15880v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-3">3. Cross-lingual alignments of ELMo contextual embeddings
</h2>

Title: [Cross-lingual alignments of ELMo contextual embeddings](https://arxiv.org/abs/2106.15986)

Authors: [Matej Ulčar](https://arxiv.org/search/cs?searchtype=author&query=Ulčar%2C+M), [Marko Robnik-Šikonja](https://arxiv.org/search/cs?searchtype=author&query=Robnik-Šikonja%2C+M)

> Building machine learning prediction models for a specific NLP task requires sufficient training data, which can be difficult to obtain for low-resource languages. Cross-lingual embeddings map word embeddings from a low-resource language to a high-resource language so that a prediction model trained on data from the high-resource language can also be used in the low-resource language. To produce cross-lingual mappings of recent contextual embeddings, anchor points between the embedding spaces have to be words in the same context. We address this issue with a new method for creating datasets for cross-lingual contextual alignments. Based on that, we propose novel cross-lingual mapping methods for ELMo embeddings. Our linear mapping methods use existing vecmap and MUSE alignments on contextual ELMo embeddings. Our new nonlinear ELMoGAN mapping method is based on GANs and does not assume isomorphic embedding spaces. We evaluate the proposed mapping methods on nine languages, using two downstream tasks, NER and dependency parsing. The ELMoGAN method performs well on the NER task, with low cross-lingual loss compared to direct training on some languages. In the dependency parsing, linear alignment variants are more successful.

| Comments: | 26 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.15986](https://arxiv.org/abs/2106.15986) [cs.CL]** |
|           | (or **[arXiv:2106.15986v1](https://arxiv.org/abs/2106.15986v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-4">4. ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information
</h2>

Title: [ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information](https://arxiv.org/abs/2106.16038)

Authors: [Zijun Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Xiang Ao](https://arxiv.org/search/cs?searchtype=author&query=Ao%2C+X), [Qing He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Q), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding. In this work, we propose ChineseBERT, which incorporates both the {\it glyph} and {\it pinyin} information of Chinese characters into language model pretraining. The glyph embedding is obtained based on different fonts of a Chinese character, being able to capture character semantics from the visual features, and the pinyin embedding characterizes the pronunciation of Chinese characters, which handles the highly prevalent heteronym phenomenon in Chinese (the same character has different pronunciations with different meanings). Pretrained on large-scale unlabeled Chinese corpus, the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps. The porpsoed model achieves new SOTA performances on a wide range of Chinese NLP tasks, including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition. Code and pretrained models are publicly available at [this https URL](https://github.com/ShannonAI/ChineseBert).

| Comments: | To appear at ACL2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.16038](https://arxiv.org/abs/2106.16038) [cs.CL]** |
|           | (or **[arXiv:2106.16038v1](https://arxiv.org/abs/2106.16038v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-5">5. IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task
</h2>

Title: [IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task](https://arxiv.org/abs/2106.16055)

Authors: [Pavel Denisov](https://arxiv.org/search/cs?searchtype=author&query=Denisov%2C+P), [Manuel Mager](https://arxiv.org/search/cs?searchtype=author&query=Mager%2C+M), [Ngoc Thang Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+N+T)

> This paper describes the submission to the IWSLT 2021 Low-Resource Speech Translation Shared Task by IMS team. We utilize state-of-the-art models combined with several data augmentation, multi-task and transfer learning approaches for the automatic speech recognition (ASR) and machine translation (MT) steps of our cascaded system. Moreover, we also explore the feasibility of a full end-to-end speech translation (ST) model in the case of very constrained amount of ground truth labeled data. Our best system achieves the best performance among all submitted systems for Congolese Swahili to English and French with BLEU scores 7.7 and 13.7 respectively, and the second best result for Coastal Swahili to English with BLEU score 14.9.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2106.16055](https://arxiv.org/abs/2106.16055) [cs.CL]** |
|           | (or **[arXiv:2106.16055v1](https://arxiv.org/abs/2106.16055v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-6">6. XLM-E: Cross-lingual Language Model Pre-training via ELECTRA
</h2>

Title: [XLM-E: Cross-lingual Language Model Pre-training via ELECTRA](https://arxiv.org/abs/2106.16138)

Authors: [Zewen Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+Z), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Saksham Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal%2C+S), [Payal Bajaj](https://arxiv.org/search/cs?searchtype=author&query=Bajaj%2C+P), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> In this paper, we introduce ELECTRA-style tasks to cross-lingual language model pre-training. Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora. Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost. Moreover, analysis shows that XLM-E tends to obtain better cross-lingual transferability.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.16138](https://arxiv.org/abs/2106.16138) [cs.CL]** |
|           | (or **[arXiv:2106.16138v1](https://arxiv.org/abs/2106.16138v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-7">7. On the Power of Saturated Transformers: A View from Circuit Complexity
</h2>

Title: [On the Power of Saturated Transformers: A View from Circuit Complexity](https://arxiv.org/abs/2106.16213)

Authors: [William Merrill](https://arxiv.org/search/cs?searchtype=author&query=Merrill%2C+W), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y), [Roy Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+R), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> Transformers have become a standard architecture for many NLP problems. This has motivated theoretically analyzing their capabilities as models of language, in order to understand what makes them successful, and what their potential weaknesses might be. Recent work has shown that transformers with hard attention are quite limited in capacity, and in fact can be simulated by constant-depth circuits. However, hard attention is a restrictive assumption, which may complicate the relevance of these results for practical transformers. In this work, we analyze the circuit complexity of transformers with saturated attention: a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We show that saturated transformers transcend the limitations of hard-attention transformers. With some minor assumptions, we prove that the number of bits needed to represent a saturated transformer memory vector is O(logn), which implies saturated transformers can be simulated by log-depth circuits. Thus, the jump from hard to saturated attention can be understood as increasing the transformer's effective circuit depth by a factor of O(logn).

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computational Complexity (cs.CC); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.16213](https://arxiv.org/abs/2106.16213) [cs.CL]** |
|           | (or **[arXiv:2106.16213v1](https://arxiv.org/abs/2106.16213v1) [cs.CL]** for this version) |




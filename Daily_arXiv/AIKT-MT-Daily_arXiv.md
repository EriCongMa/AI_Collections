# Daily arXiv: Machine Translation - July, 2021

# Index


- [2021-07-26](#2021-07-26)

  - [1. Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation](#2021-07-26-1)
  - [2. OLR 2021 Challenge: Datasets, Rules and Baselines](#2021-07-26-2)
  - [3. Modeling Bilingual Conversational Characteristics for Neural Chat Translation](#2021-07-26-3)
- [2021-07-23](#2021-07-23)
  - [1. Multi-Stream Transformers](#2021-07-23-1)
  - [2. Confidence-Aware Scheduled Sampling for Neural Machine Translation](#2021-07-23-2)
  - [3. To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation](#2021-07-23-3)
- [2021-07-22](#2021-07-22)
  - [1. TLA: Twitter Linguistic Analysis](#2021-07-22-1)
  - [2. What Do You Get When You Cross Beam Search with Nucleus Sampling?](#2021-07-22-2)
  - [3. Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer](#2021-07-22-3)
  - [4. CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision](#2021-07-22-4)
  - [5. Improved Text Classification via Contrastive Adversarial Training](#2021-07-22-5)
- [2021-07-21](#2021-07-21)
  - [1. Generative Video Transformer: Can Objects be the Words?](#2021-07-21-1)
  - [2. Token-Level Supervised Contrastive Learning for Punctuation Restoration](#2021-07-21-2)
  - [3. Seed Words Based Data Selection for Language Model Adaptation](#2021-07-21-3)
  - [4. More Parameters? No Thanks!](#2021-07-21-4)
- [2021-07-20](#2021-07-20)

  - [1. The Law of Large Documents: Understanding the Structure of Legal Contracts Using Visual Cues](#2021-07-20-1)
  - [2. Darmok and Jalad at Tanagra: A Dataset and Model for English-to-Tamarian Translation](#2021-07-20-2)
  - [3. Dynamic Transformer for Efficient Machine Translation on Embedded Devices](#2021-07-20-3)
  - [4. On the Copying Behaviors of Pre-Training for Neural Machine Translation](#2021-07-20-4)
  - [5. As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical Translation](#2021-07-20-5)
  - [6. Pre-trained Language Models as Prior Knowledge for Playing Text-based Games](#2021-07-20-6)
  - [7. Translatotron 2: Robust direct speech-to-speech translation](#2021-07-20-7)
  - [8. Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images](#2021-07-20-8)
  - [9. Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages](#2021-07-20-9)
  - [10. Simultaneous Speech Translation for Live Subtitling: from Delay to Display](#2021-07-20-10)
- [2021-07-19](#2021-07-19)

  - [1. Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models](#2021-07-19-1)
- [2021-07-16](#2021-07-16)

  - [1. From Show to Tell: A Survey on Image Captioning](#2021-07-16-1)
  - [2. MultiBench: Multiscale Benchmarks for Multimodal Representation Learning](#2021-07-16-2)
  - [3. FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task](#2021-07-16-3)
- [2021-07-15](#2021-07-15)
  - [1. How Much Can CLIP Benefit Vision-and-Language Tasks?](#2021-07-15-1)
  - [2. From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text](#2021-07-15-2)
  - [3. Deduplicating Training Data Makes Language Models Better](#2021-07-15-3)
  - [4. Importance-based Neuron Allocation for Multilingual Neural Machine Translation](#2021-07-15-4)
  - [5. ParCourE: A Parallel Corpus Explorer fora Massively Multilingual Corpus](#2021-07-15-5)
- [2021-07-14](#2021-07-14)
  - [1. A Configurable Multilingual Model is All You Need to Recognize All Languages](#2021-07-14-1)
  - [2. Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task](#2021-07-14-2)
  - [3. Zero-shot Speech Translation](#2021-07-14-3)
  - [4. The IWSLT 2021 BUT Speech Translation Systems](#2021-07-14-4)
  - [5. Between Flexibility and Consistency: Joint Generation of Captions and Subtitles](#2021-07-14-5)
- [2021-07-13](#2021-07-13)

  - [1. Oriental Language Recognition (OLR) 2020: Summary and Analysis](#2021-07-13-1)
  - [2. Parameter Selection: Why We Should Pay More Attention to It](#2021-07-13-2)
  - [3. Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning](#2021-07-13-3)
  - [4. Direct speech-to-speech translation with discrete units](#2021-07-13-4)
- [2021-07-12](#2021-07-21)
  - [1. Improved Language Identification Through Cross-Lingual Self-Supervised Learning](#2021-07-12-1)
  - [2. A Systematic Survey of Text Worlds as Embodied Natural Language Environments](#2021-07-12-2)
  - [3. A Survey on Low-Resource Neural Machine Translation](#2021-07-12-3)
  - [4. Using Machine Translation to Localize Task Oriented NLG Output](#2021-07-12-4)
- [2021-07-09](#2021-07-09)
  - [1. Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text](#2021-07-09-1)
  - [2. Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation](#2021-07-09-2)
- [2021-07-08](#2021-07-08)
  - [1. Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking](#2021-07-08-1)
  - [2. Kosp2e: Korean Speech to English Translation Corpus](#2021-07-08-2)
  - [3. Efficient Transformer for Direct Speech Translation](#2021-07-08-3)
  - [4. On Training Instance Selection for Few-Shot Neural Text Generation](#2021-07-08-4)
  - [5. Time-Aware Ancient Chinese Text Translation and Inference](#2021-07-08-5)
- [2021-07-07](#2021-07-07)

  - [1. Long-Short Transformer: Efficient Transformers for Language and Vision](#2021-07-07-1)
  - [2. Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering](#2021-07-07-2)
  - [3. An NLG pipeline for a legal expert system: a work in progress](#2021-07-07-3)
  - [4. The NiuTrans End-to-End Speech Translation System \\for IWSLT 2021 Offline Task](#2021-07-07-4)
  - [5. VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer](#2021-07-07-5)
- [2021-07-06](#2021-07-06)
  - [1. Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition](#2021-07-06-1)
  - [2. IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task](#2021-07-06-2)
  - [3. Packing: Towards 2x NLP BERT Acceleration](#2021-07-06-3)
  - [4. Power Law Graph Transformer for Machine Translation and Representation Learning](#2021-07-06-4)
  - [5. ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](#2021-07-06-5)


- [2021-07-05](#2021-07-05)
  - [1. Transformer-F: A Transformer network with effective methods for learning universal sentence representation](#2021-07-05-1)
  - [2. A Primer on Pretrained Multilingual Language Models](#2021-07-05-2)
  - [3. Interactive decoding of words from visual speech recognition models](#2021-07-05-3)
  - [4. Data Centric Domain Adaptation for Historical Text with OCR Errors](#2021-07-05-4)
- [2021-07-02](#2021-07-02)

  - [1. GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph](#2021-07-02-1)
  - [2. ESPnet-ST IWSLT 2021 Offline Speech Translation System](#2021-07-02-2)
  - [3. Word-Free Spoken Language Understanding for Mandarin-Chinese](#2021-07-02-3)
  - [4. The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021](#2021-07-02-4)
  - [5. Zero-pronoun Data Augmentation for Japanese-to-English Translation](#2021-07-02-5)
  - [6. Modeling Target-side Inflection in Placeholder Translation](#2021-07-02-6)
  - [7. CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding](#2021-07-02-7 )
- [2021-07-01](#2021-07-01)
  - [1. What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?](#2021-07-01-1)
  - [2. Mixed Cross Entropy Loss for Neural Machine Translation](#2021-07-01-2)
  - [3. Cross-lingual alignments of ELMo contextual embeddings](#2021-07-01-3)
  - [4. ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information](#2021-07-01-4)
  - [5. IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task](#2021-07-01-5)
  - [6. XLM-E: Cross-lingual Language Model Pre-training via ELECTRA](#2021-07-01-6)
  - [7. On the Power of Saturated Transformers: A View from Circuit Complexity](#2021-07-01-7)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-07-26

[Return to Index](#Index)



<h2 id="2021-07-26-1">1. Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation
</h2>

Title: [Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation](https://arxiv.org/abs/2107.11252)

Authors: [Bingqian Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+B), [Yi Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Y), [Yanxin Long](https://arxiv.org/search/cs?searchtype=author&query=Long%2C+Y), [Xiaodan Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+X), [Qixiang Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Q), [Liang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+L)

> Language instruction plays an essential role in the natural language grounded navigation tasks. However, navigators trained with limited human-annotated instructions may have difficulties in accurately capturing key information from the complicated instruction at different timesteps, leading to poor navigation performance. In this paper, we exploit to train a more robust navigator which is capable of dynamically extracting crucial factors from the long instruction, by using an adversarial attacking paradigm. Specifically, we propose a Dynamic Reinforced Instruction Attacker (DR-Attacker), which learns to mislead the navigator to move to the wrong target by destroying the most instructive information in instructions at different timesteps. By formulating the perturbation generation as a Markov Decision Process, DR-Attacker is optimized by the reinforcement learning algorithm to generate perturbed instructions sequentially during the navigation, according to a learnable attack score. Then, the perturbed instructions, which serve as hard samples, are used for improving the robustness of the navigator with an effective adversarial training strategy and an auxiliary self-supervised reasoning task. Experimental results on both Vision-and-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks show the superiority of our proposed method over state-of-the-art methods. Moreover, the visualization analysis shows the effectiveness of the proposed DR-Attacker, which can successfully attack crucial information in the instructions at different timesteps. Code is available at [this https URL](https://github.com/expectorlin/DR-Attacker).

| Comments:          | Accepted by TPAMI 2021                                       |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Journal reference: | IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021 |
| DOI:               | [10.1109/TPAMI.2021.3097435](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1109%2FTPAMI.2021.3097435&v=14d29106) |
| Cite as:           | **[arXiv:2107.11252](https://arxiv.org/abs/2107.11252) [cs.CV]** |
|                    | (or **[arXiv:2107.11252v1](https://arxiv.org/abs/2107.11252v1) [cs.CV]** for this version) |





<h2 id="2021-07-26-2">2. OLR 2021 Challenge: Datasets, Rules and Baselines
</h2>

Title: [OLR 2021 Challenge: Datasets, Rules and Baselines](https://arxiv.org/abs/2107.11113)

Authors: [Binling Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Wenxuan Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+W), [Jing Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Yiming Zhi](https://arxiv.org/search/cs?searchtype=author&query=Zhi%2C+Y), [Zheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Qingyang Hong](https://arxiv.org/search/cs?searchtype=author&query=Hong%2C+Q), [Lin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Dong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Liming Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+L), [Cheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+C)

> This paper introduces the sixth Oriental Language Recognition (OLR) 2021 Challenge, which intends to improve the performance of language recognition systems and speech recognition systems within multilingual scenarios. The data profile, four tasks, two baselines, and the evaluation principles are introduced in this paper. In addition to the Language Identification (LID) tasks, multilingual Automatic Speech Recognition (ASR) tasks are introduced to OLR 2021 Challenge for the first time. The challenge this year focuses on more practical and challenging problems, with four tasks: (1) constrained LID, (2) unconstrained LID, (3) constrained multilingual ASR, (4) unconstrained multilingual ASR. Baselines for LID tasks and multilingual ASR tasks are provided, respectively. The LID baseline system is an extended TDNN x-vector model constructed with Pytorch. A transformer-based end-to-end model is provided as the multilingual ASR baseline system. These recipes will be online published, and available for participants to construct their own LID or ASR systems. The baseline results demonstrate that those tasks are rather challenging and deserve more effort to achieve better performance.

| Comments: | arXiv admin note: text overlap with [arXiv:2006.03473](https://arxiv.org/abs/2006.03473), [arXiv:1907.07626](https://arxiv.org/abs/1907.07626), [arXiv:1806.00616](https://arxiv.org/abs/1806.00616), [arXiv:1706.09742](https://arxiv.org/abs/1706.09742) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2107.11113](https://arxiv.org/abs/2107.11113) [cs.CL]** |
|           | (or **[arXiv:2107.11113v1](https://arxiv.org/abs/2107.11113v1) [cs.CL]** for this version) |





<h2 id="2021-07-26-3">3. Modeling Bilingual Conversational Characteristics for Neural Chat Translation
</h2>

Title: [Modeling Bilingual Conversational Characteristics for Neural Chat Translation](https://arxiv.org/abs/2107.11164)

Authors: [Yunlong Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yufeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Jinan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Neural chat translation aims to translate bilingual conversational text, which has a broad application in international exchanges and cooperation. Despite the impressive performance of sentence-level and context-aware Neural Machine Translation (NMT), there still remain challenges to translate bilingual conversational text due to its inherent characteristics such as role preference, dialogue coherence, and translation consistency. In this paper, we aim to promote the translation quality of conversational text by modeling the above properties. Specifically, we design three latent variational modules to learn the distributions of bilingual conversational characteristics. Through sampling from these learned distributions, the latent variables, tailored for role preference, dialogue coherence, and translation consistency, are incorporated into the NMT model for better translation. We evaluate our approach on the benchmark dataset BConTrasT (English-German) and a self-collected bilingual dialogue corpus, named BMELD (English-Chinese). Extensive experiments show that our approach notably boosts the performance over strong baselines by a large margin and significantly surpasses some state-of-the-art context-aware NMT models in terms of BLEU and TER. Additionally, we make the BMELD dataset publicly available for the research community.

| Comments: | Accepted as a long paper at ACL 2021. Code and data are available at [this https URL](https://github.com/XL2248/CPCC) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.11164](https://arxiv.org/abs/2107.11164) [cs.CL]** |
|           | (or **[arXiv:2107.11164v1](https://arxiv.org/abs/2107.11164v1) [cs.CL]** for this version) |






# 2021-07-23

[Return to Index](#Index)



<h2 id="2021-07-23-1">1. Multi-Stream Transformers
</h2>

Title: [Multi-Stream Transformers](https://arxiv.org/abs/2107.10342)

Authors: [Mikhail Burtsev](https://arxiv.org/search/cs?searchtype=author&query=Burtsev%2C+M), [Anna Rumshisky](https://arxiv.org/search/cs?searchtype=author&query=Rumshisky%2C+A)

> Transformer-based encoder-decoder models produce a fused token-wise representation after every encoder layer. We investigate the effects of allowing the encoder to preserve and explore alternative hypotheses, combined at the end of the encoding process. To that end, we design and examine a Multi-stream Transformer architecture and find that splitting the Transformer encoder into multiple encoder streams and allowing the model to merge multiple representational hypotheses improves performance, with further improvement obtained by adding a skip connection between the first and the final encoder layer.

| Subjects: | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.10342](https://arxiv.org/abs/2107.10342) [cs.CL]** |
|           | (or **[arXiv:2107.10342v1](https://arxiv.org/abs/2107.10342v1) [cs.CL]** for this version) |





<h2 id="2021-07-23-2">2. Confidence-Aware Scheduled Sampling for Neural Machine Translation
</h2>

Title: [Confidence-Aware Scheduled Sampling for Neural Machine Translation](https://arxiv.org/abs/2107.10427)

Authors: [Yijin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yufeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Jinan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Scheduled sampling is an effective method to alleviate the exposure bias problem of neural machine translation. It simulates the inference scene by randomly replacing ground-truth target input tokens with predicted ones during training. Despite its success, its critical schedule strategies are merely based on training steps, ignoring the real-time model competence, which limits its potential performance and convergence speed. To address this issue, we propose confidence-aware scheduled sampling. Specifically, we quantify real-time model competence by the confidence of model predictions, based on which we design fine-grained schedule strategies. In this way, the model is exactly exposed to predicted tokens for high-confidence positions and still ground-truth tokens for low-confidence positions. Moreover, we observe vanilla scheduled sampling suffers from degenerating into the original teacher forcing mode since most predicted tokens are the same as ground-truth tokens. Therefore, under the above confidence-aware strategy, we further expose more noisy tokens (e.g., wordy and incorrect word order) instead of predicted ones for high-confidence token positions. We evaluate our approach on the Transformer and conduct experiments on large-scale WMT 2014 English-German, WMT 2014 English-French, and WMT 2019 Chinese-English. Results show that our approach significantly outperforms the Transformer and vanilla scheduled sampling on both translation quality and convergence speed.

| Comments: | Findings of ACL-2021, code at [this https URL](https://github.com/Adaxry/conf_aware_ss4nmt) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.10427](https://arxiv.org/abs/2107.10427) [cs.CL]** |
|           | (or **[arXiv:2107.10427v1](https://arxiv.org/abs/2107.10427v1) [cs.CL]** for this version) |





<h2 id="2021-07-23-3">3. To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation
</h2>

Title: [To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation](https://arxiv.org/abs/2107.10821)

Authors: [Tom Kocmi](https://arxiv.org/search/cs?searchtype=author&query=Kocmi%2C+T), [Christian Federmann](https://arxiv.org/search/cs?searchtype=author&query=Federmann%2C+C), [Roman Grundkiewicz](https://arxiv.org/search/cs?searchtype=author&query=Grundkiewicz%2C+R), [Marcin Junczys-Dowmunt](https://arxiv.org/search/cs?searchtype=author&query=Junczys-Dowmunt%2C+M), [Hitokazu Matsushita](https://arxiv.org/search/cs?searchtype=author&query=Matsushita%2C+H), [Arul Menezes](https://arxiv.org/search/cs?searchtype=author&query=Menezes%2C+A)

> Automatic metrics are commonly used as the exclusive tool for declaring the superiority of one machine translation system's quality over another. The community choice of automatic metric guides research directions and industrial developments by deciding which models are deemed better. Evaluating metrics correlations has been limited to a small collection of human judgements. In this paper, we corroborate how reliable metrics are in contrast to human judgements on - to the best of our knowledge - the largest collection of human judgements. We investigate which metrics have the highest accuracy to make system-level quality rankings for pairs of systems, taking human judgement as a gold standard, which is the closest scenario to the real metric usage. Furthermore, we evaluate the performance of various metrics across different language pairs and domains. Lastly, we show that the sole use of BLEU negatively affected the past development of improved models. We release the collection of human judgements of 4380 systems, and 2.3 M annotated sentences for further analysis and replication of our work.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.10821](https://arxiv.org/abs/2107.10821) [cs.CL]** |
|           | (or **[arXiv:2107.10821v1](https://arxiv.org/abs/2107.10821v1) [cs.CL]** for this version) |






# 2021-07-22

[Return to Index](#Index)



<h2 id="2021-07-22-1">1. TLA: Twitter Linguistic Analysis
</h2>

Title: [TLA: Twitter Linguistic Analysis](https://arxiv.org/abs/2107.09710)

Authors: [Tushar Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+T), [Nishant Rajadhyaksha](https://arxiv.org/search/cs?searchtype=author&query=Rajadhyaksha%2C+N)

> Linguistics has been instrumental in developing a deeper understanding of human nature. Words are indispensable to bequeath the thoughts, emotions, and purpose of any human interaction, and critically analyzing these words can elucidate the social and psychological behavior and characteristics of these social animals. Social media has become a platform for human interaction on a large scale and thus gives us scope for collecting and using that data for our study. However, this entire process of collecting, labeling, and analyzing this data iteratively makes the entire procedure cumbersome. To make this entire process easier and structured, we would like to introduce TLA(Twitter Linguistic Analysis). In this paper, we describe TLA and provide a basic understanding of the framework and discuss the process of collecting, labeling, and analyzing data from Twitter for a corpus of languages while providing detailed labeled datasets for all the languages and the models are trained on these datasets. The analysis provided by TLA will also go a long way in understanding the sentiments of different linguistic communities and come up with new and innovative solutions for their problems based on the analysis.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.09710](https://arxiv.org/abs/2107.09710) [cs.CL]** |
|           | (or **[arXiv:2107.09710v1](https://arxiv.org/abs/2107.09710v1) [cs.CL]** for this version) |



<h2 id="2021-07-22-2">2. What Do You Get When You Cross Beam Search with Nucleus Sampling?
</h2>

Title: [What Do You Get When You Cross Beam Search with Nucleus Sampling?](https://arxiv.org/abs/2107.09729)

Authors: [Uri Shaham](https://arxiv.org/search/cs?searchtype=author&query=Shaham%2C+U), [Omer Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+O)

> We combine beam search with the probabilistic pruning technique of nucleus sampling to create two deterministic nucleus search algorithms for natural language generation. The first algorithm, p-exact search, locally prunes the next-token distribution and performs an exact search over the remaining space. The second algorithm, dynamic beam search, shrinks and expands the beam size according to the entropy of the candidate's probability distribution. Despite the probabilistic intuition behind nucleus search, experiments on machine translation and summarization benchmarks show that both algorithms reach the same performance levels as standard beam search.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.09729](https://arxiv.org/abs/2107.09729) [cs.CL]** |
|           | (or **[arXiv:2107.09729v1](https://arxiv.org/abs/2107.09729v1) [cs.CL]** for this version) |





<h2 id="2021-07-22-3">3. Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer
</h2>

Title: [Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2107.09840)

Authors: [Weijia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Batool Haider](https://arxiv.org/search/cs?searchtype=author&query=Haider%2C+B), [Jason Krone](https://arxiv.org/search/cs?searchtype=author&query=Krone%2C+J), [Saab Mansour](https://arxiv.org/search/cs?searchtype=author&query=Mansour%2C+S)

> Multilingual pre-trained contextual embedding models (Devlin et al., 2019) have achieved impressive performance on zero-shot cross-lingual transfer tasks. Finding the most effective fine-tuning strategy to fine-tune these models on high-resource languages so that it transfers well to the zero-shot languages is a non-trivial task. In this paper, we propose a novel meta-optimizer to soft-select which layers of the pre-trained model to freeze during fine-tuning. We train the meta-optimizer by simulating the zero-shot transfer scenario. Results on cross-lingual natural language inference show that our approach improves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al., 2020).

| Comments: | MetaNLP at ACL 2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.09840](https://arxiv.org/abs/2107.09840) [cs.CL]** |
|           | (or **[arXiv:2107.09840v1](https://arxiv.org/abs/2107.09840v1) [cs.CL]** for this version) |





<h2 id="2021-07-22-4">4. CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision
</h2>

Title: [CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision](https://arxiv.org/abs/2107.09852)

Authors: [Zhongyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Xiao Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+X), [Kuo Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+K), [Ting Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Bing Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+B)

> Recent work has shown success in incorporating pre-trained models like BERT to improve NLP systems. However, existing pre-trained models lack of causal knowledge which prevents today's NLP systems from thinking like humans. In this paper, we investigate the problem of injecting causal knowledge into pre-trained models. There are two fundamental problems: 1) how to collect a large-scale causal resource from unstructured texts; 2) how to effectively inject causal knowledge into pre-trained models. To address these issues, we propose CausalBERT, which collects the largest scale of causal resource using precise causal patterns and causal embedding techniques. In addition, we adopt a regularization-based method to preserve the already learned knowledge with an extra regularization term while injecting causal knowledge. Extensive experiments on 7 datasets, including four causal pair classification tasks, two causal QA tasks and a causal inference task, demonstrate that CausalBERT captures rich causal knowledge and outperforms all pre-trained models-based state-of-the-art methods, achieving a new causal inference benchmark.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.09852](https://arxiv.org/abs/2107.09852) [cs.CL]** |
|           | (or **[arXiv:2107.09852v1](https://arxiv.org/abs/2107.09852v1) [cs.CL]** for this version) |





<h2 id="2021-07-22-5">5. Improved Text Classification via Contrastive Adversarial Training
</h2>

Title: [Improved Text Classification via Contrastive Adversarial Training](https://arxiv.org/abs/2107.10137)

Authors: [Lin Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+L), [Chung-Wei Hang](https://arxiv.org/search/cs?searchtype=author&query=Hang%2C+C), [Avirup Sil](https://arxiv.org/search/cs?searchtype=author&query=Sil%2C+A), [Saloni Potdar](https://arxiv.org/search/cs?searchtype=author&query=Potdar%2C+S), [Mo Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+M)

> We propose a simple and general method to regularize the fine-tuning of Transformer-based encoders for text classification tasks. Specifically, during fine-tuning we generate adversarial examples by perturbing the word embeddings of the model and perform contrastive learning on clean and adversarial examples in order to teach the model to learn noise-invariant representations. By training on both clean and adversarial examples along with the additional contrastive objective, we observe consistent improvement over standard fine-tuning on clean examples. On several GLUE benchmark tasks, our fine-tuned BERT Large model outperforms BERT Large baseline by 1.7% on average, and our fine-tuned RoBERTa Large improves over RoBERTa Large baseline by 1.3%. We additionally validate our method in different domains using three intent classification datasets, where our fine-tuned RoBERTa Large outperforms RoBERTa Large baseline by 1-2% on average.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.10137](https://arxiv.org/abs/2107.10137) [cs.CL]** |
|           | (or **[arXiv:2107.10137v1](https://arxiv.org/abs/2107.10137v1) [cs.CL]** for this version) |






# 2021-07-21

[Return to Index](#Index)



<h2 id="2021-07-21-1">1. Generative Video Transformer: Can Objects be the Words?
</h2>

Title: [Generative Video Transformer: Can Objects be the Words?](https://arxiv.org/abs/2107.09240)

Authors: [Yi-Fu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Jaesik Yoon](https://arxiv.org/search/cs?searchtype=author&query=Yoon%2C+J), [Sungjin Ahn](https://arxiv.org/search/cs?searchtype=author&query=Ahn%2C+S)

> Transformers have been successful for many natural language processing tasks. However, applying transformers to the video domain for tasks such as long-term video generation and scene understanding has remained elusive due to the high computational complexity and the lack of natural tokenization. In this paper, we propose the Object-Centric Video Transformer (OCVT) which utilizes an object-centric approach for decomposing scenes into tokens suitable for use in a generative video transformer. By factoring the video into objects, our fully unsupervised model is able to learn complex spatio-temporal dynamics of multiple interacting objects in a scene and generate future frames of the video. Our model is also significantly more memory-efficient than pixel-based models and thus able to train on videos of length up to 70 frames with a single 48GB GPU. We compare our model with previous RNN-based approaches as well as other possible video transformer baselines. We demonstrate OCVT performs well when compared to baselines in generating future frames. OCVT also develops useful representations for video reasoning, achieving start-of-the-art performance on the CATER task.

| Comments: | Published in ICML 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2107.09240](https://arxiv.org/abs/2107.09240) [cs.LG]** |
|           | (or **[arXiv:2107.09240v1](https://arxiv.org/abs/2107.09240v1) [cs.LG]** for this version) |





<h2 id="2021-07-21-2">2. Token-Level Supervised Contrastive Learning for Punctuation Restoration
</h2>

Title: [Token-Level Supervised Contrastive Learning for Punctuation Restoration](https://arxiv.org/abs/2107.09099)

Authors: [Qiushi Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Q), [Tom Ko](https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+T), [H Lilian Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+H+L), [Xubo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Bo Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+B)

> Punctuation is critical in understanding natural language text. Currently, most automatic speech recognition (ASR) systems do not generate punctuation, which affects the performance of downstream tasks, such as intent detection and slot filling. This gives rise to the need for punctuation restoration. Recent work in punctuation restoration heavily utilizes pre-trained language models without considering data imbalance when predicting punctuation classes. In this work, we address this problem by proposing a token-level supervised contrastive learning method that aims at maximizing the distance of representation of different punctuation marks in the embedding space. The result shows that training with token-level supervised contrastive learning obtains up to 3.2% absolute F1 improvement on the test set.

| Comments: | 5 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.09099](https://arxiv.org/abs/2107.09099) [cs.CL]** |
|           | (or **[arXiv:2107.09099v1](https://arxiv.org/abs/2107.09099v1) [cs.CL]** for this version) |





<h2 id="2021-07-21-3">3. Seed Words Based Data Selection for Language Model Adaptation
</h2>

Title: [Seed Words Based Data Selection for Language Model Adaptation](https://arxiv.org/abs/2107.09433)

Authors: [Roberto Gretter](https://arxiv.org/search/cs?searchtype=author&query=Gretter%2C+R), [Marco Matassoni](https://arxiv.org/search/cs?searchtype=author&query=Matassoni%2C+M), [Daniele Falavigna](https://arxiv.org/search/cs?searchtype=author&query=Falavigna%2C+D)

> We address the problem of language model customization in applications where the ASR component needs to manage domain-specific terminology; although current state-of-the-art speech recognition technology provides excellent results for generic domains, the adaptation to specialized dictionaries or glossaries is still an open issue. In this work we present an approach for automatically selecting sentences, from a text corpus, that match, both semantically and morphologically, a glossary of terms (words or composite words) furnished by the user. The final goal is to rapidly adapt the language model of an hybrid ASR system with a limited amount of in-domain text data in order to successfully cope with the linguistic domain at hand; the vocabulary of the baseline model is expanded and tailored, reducing the resulting OOV rate. Data selection strategies based on shallow morphological seeds and semantic similarity viaword2vec are introduced and discussed; the experimental setting consists in a simultaneous interpreting scenario, where ASRs in three languages are designed to recognize the domain-specific terms (i.e. dentistry). Results using different metrics (OOV rate, WER, precision and recall) show the effectiveness of the proposed techniques.

| Comments: | 11 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.09433](https://arxiv.org/abs/2107.09433) [cs.CL]** |
|           | (or **[arXiv:2107.09433v1](https://arxiv.org/abs/2107.09433v1) [cs.CL]** for this version) |





<h2 id="2021-07-21-4">4. More Parameters? No Thanks!
</h2>

Title: [More Parameters? No Thanks!](https://arxiv.org/abs/2107.09622)

Authors: [Zeeshan Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+Z), [Kartheek Akella](https://arxiv.org/search/cs?searchtype=author&query=Akella%2C+K), [Vinay P. Namboodiri](https://arxiv.org/search/cs?searchtype=author&query=Namboodiri%2C+V+P), [C V Jawahar](https://arxiv.org/search/cs?searchtype=author&query=Jawahar%2C+C+V)

> This work studies the long-standing problems of model capacity and negative interference in multilingual neural machine translation MNMT. We use network pruning techniques and observe that pruning 50-70% of the parameters from a trained MNMT model results only in a 0.29-1.98 drop in the BLEU score. Suggesting that there exist large redundancies even in MNMT models. These observations motivate us to use the redundant parameters and counter the interference problem efficiently. We propose a novel adaptation strategy, where we iteratively prune and retrain the redundant parameters of an MNMT to improve bilingual representations while retaining the multilinguality. Negative interference severely affects high resource languages, and our method alleviates it without any additional adapter modules. Hence, we call it parameter-free adaptation strategy, paving way for the efficient adaptation of MNMT. We demonstrate the effectiveness of our method on a 9 language MNMT trained on TED talks, and report an average improvement of +1.36 BLEU on high resource pairs. Code will be released here.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.09622](https://arxiv.org/abs/2107.09622) [cs.CL]** |
|           | (or **[arXiv:2107.09622v1](https://arxiv.org/abs/2107.09622v1) [cs.CL]** for this version) |







# 2021-07-20

[Return to Index](#Index)



<h2 id="2021-07-20-1">1. The Law of Large Documents: Understanding the Structure of Legal Contracts Using Visual Cues
</h2>

Title: [The Law of Large Documents: Understanding the Structure of Legal Contracts Using Visual Cues](https://arxiv.org/abs/2107.08128)

Authors: [Allison Hegel](https://arxiv.org/search/cs?searchtype=author&query=Hegel%2C+A), [Marina Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+M), [Genevieve Peaslee](https://arxiv.org/search/cs?searchtype=author&query=Peaslee%2C+G), [Brendan Roof](https://arxiv.org/search/cs?searchtype=author&query=Roof%2C+B), [Emad Elwany](https://arxiv.org/search/cs?searchtype=author&query=Elwany%2C+E)

> Large, pre-trained transformer models like BERT have achieved state-of-the-art results on document understanding tasks, but most implementations can only consider 512 tokens at a time. For many real-world applications, documents can be much longer, and the segmentation strategies typically used on longer documents miss out on document structure and contextual information, hurting their results on downstream tasks. In our work on legal agreements, we find that visual cues such as layout, style, and placement of text in a document are strong features that are crucial to achieving an acceptable level of accuracy on long documents. We measure the impact of incorporating such visual cues, obtained via computer vision methods, on the accuracy of document understanding tasks including document segmentation, entity extraction, and attribute classification. Our method of segmenting documents based on structural metadata out-performs existing methods on four long-document understanding tasks as measured on the Contract Understanding Atticus Dataset.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Document Intelligence Workshop at KDD, 2021                  |
| Cite as:           | **[arXiv:2107.08128](https://arxiv.org/abs/2107.08128) [cs.CL]** |
|                    | (or **[arXiv:2107.08128v1](https://arxiv.org/abs/2107.08128v1) [cs.CL]** for this version) |





<h2 id="2021-07-20-2">2. Darmok and Jalad at Tanagra: A Dataset and Model for English-to-Tamarian Translation
</h2>

Title: [Darmok and Jalad at Tanagra: A Dataset and Model for English-to-Tamarian Translation](https://arxiv.org/abs/2107.08146)

Authors: [Peter Jansen](https://arxiv.org/search/cs?searchtype=author&query=Jansen%2C+P)

> Tamarian, a fictional language introduced in the Star Trek episode Darmok, communicates meaning through utterances of metaphorical references, such as "Darmok and Jalad at Tanagra" instead of "We should work together." This work assembles a Tamarian-English dictionary of utterances from the original episode and several follow-on novels, and uses this to construct a parallel corpus of 456 English-Tamarian utterances. A machine translation system based on a large language model (T5) is trained using this parallel corpus, and is shown to produce an accuracy of 76% when translating from English to Tamarian on known utterances.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.08146](https://arxiv.org/abs/2107.08146) [cs.CL]** |
|           | (or **[arXiv:2107.08146v1](https://arxiv.org/abs/2107.08146v1) [cs.CL]** for this version) |





<h2 id="2021-07-20-3">3. Dynamic Transformer for Efficient Machine Translation on Embedded Devices
</h2>

Title: [Dynamic Transformer for Efficient Machine Translation on Embedded Devices](https://arxiv.org/abs/2107.08199)

Authors: [Hishan Parry](https://arxiv.org/search/cs?searchtype=author&query=Parry%2C+H), [Lei Xun](https://arxiv.org/search/cs?searchtype=author&query=Xun%2C+L), [Amin Sabet](https://arxiv.org/search/cs?searchtype=author&query=Sabet%2C+A), [Jia Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+J), [Jonathon Hare](https://arxiv.org/search/cs?searchtype=author&query=Hare%2C+J), [Geoff V. Merrett](https://arxiv.org/search/cs?searchtype=author&query=Merrett%2C+G+V)

> The Transformer architecture is widely used for machine translation tasks. However, its resource-intensive nature makes it challenging to implement on constrained embedded devices, particularly where available hardware resources can vary at run-time. We propose a dynamic machine translation model that scales the Transformer architecture based on the available resources at any particular time. The proposed approach, 'Dynamic-HAT', uses a HAT SuperTransformer as the backbone to search for SubTransformers with different accuracy-latency trade-offs at design time. The optimal SubTransformers are sampled from the SuperTransformer at run-time, depending on latency constraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses inherited SubTransformers sampled directly from the SuperTransformer with a switching time of <1s. Using inherited SubTransformers results in a BLEU score loss of <1.5% because the SubTransformer configuration is not retrained from scratch after sampling. However, to recover this loss in performance, the dimensions of the design space can be reduced to tailor it to a family of target hardware. The new reduced design space results in a BLEU score increase of approximately 1% for sub-optimal models from the original design space, with a wide range for performance scaling between 0.356s - 1.526s for the GPU and 2.9s - 7.31s for the CPU.

| Comments: | Accepted at MLCAD 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.08199](https://arxiv.org/abs/2107.08199) [cs.CL]** |
|           | (or **[arXiv:2107.08199v1](https://arxiv.org/abs/2107.08199v1) [cs.CL]** for this version) |





<h2 id="2021-07-20-4">4. On the Copying Behaviors of Pre-Training for Neural Machine Translation
</h2>

Title: [On the Copying Behaviors of Pre-Training for Neural Machine Translation](https://arxiv.org/abs/2107.08212)

Authors: [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Previous studies have shown that initializing neural machine translation (NMT) models with the pre-trained language models (LM) can speed up the model training and boost the model performance. In this work, we identify a critical side-effect of pre-training for NMT, which is due to the discrepancy between the training objectives of LM-based pre-training and NMT. Since the LM objective learns to reconstruct a few source tokens and copy most of them, the pre-training initialization would affect the copying behaviors of NMT models. We provide a quantitative analysis of copying behaviors by introducing a metric called copying ratio, which empirically shows that pre-training based NMT models have a larger copying ratio than the standard one. In response to this problem, we propose a simple and effective method named copying penalty to control the copying behaviors in decoding. Extensive experiments on both in-domain and out-of-domain benchmarks show that the copying penalty method consistently improves translation performance by controlling copying behaviors for pre-training based NMT models. Source code is freely available at [this https URL](https://github.com/SunbowLiu/CopyingPenalty).

| Comments: | Accepted to Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.08212](https://arxiv.org/abs/2107.08212) [cs.CL]** |
|           | (or **[arXiv:2107.08212v1](https://arxiv.org/abs/2107.08212v1) [cs.CL]** for this version) |





<h2 id="2021-07-20-5">5. As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical Translation
</h2>

Title: [As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical Translation](https://arxiv.org/abs/2107.08357)

Authors: [Jun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Chang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Francisco Guzman](https://arxiv.org/search/cs?searchtype=author&query=Guzman%2C+F), [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Benjamin I. P. Rubinstein](https://arxiv.org/search/cs?searchtype=author&query=Rubinstein%2C+B+I+P), [Trevor Cohn](https://arxiv.org/search/cs?searchtype=author&query=Cohn%2C+T)

> Mistranslated numbers have the potential to cause serious effects, such as financial loss or medical misinformation. In this work we develop comprehensive assessments of the robustness of neural machine translation systems to numerical text via behavioural testing. We explore a variety of numerical translation capabilities a system is expected to exhibit and design effective test examples to expose system underperformance. We find that numerical mistranslation is a general issue: major commercial systems and state-of-the-art research models fail on many of our test examples, for high- and low-resource languages. Our tests reveal novel errors that have not previously been reported in NMT systems, to the best of our knowledge. Lastly, we discuss strategies to mitigate numerical mistranslation.

| Comments: | Findings of ACL, to appear                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Cryptography and Security (cs.CR) |
| Cite as:  | **[arXiv:2107.08357](https://arxiv.org/abs/2107.08357) [cs.CL]** |
|           | (or **[arXiv:2107.08357v1](https://arxiv.org/abs/2107.08357v1) [cs.CL]** for this version) |





<h2 id="2021-07-20-6">6. Pre-trained Language Models as Prior Knowledge for Playing Text-based Games
</h2>

Title: [Pre-trained Language Models as Prior Knowledge for Playing Text-based Games](https://arxiv.org/abs/2107.08408)

Authors: [Ishika Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+I), [Gargi Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+G), [Ashutosh Modi](https://arxiv.org/search/cs?searchtype=author&query=Modi%2C+A)

> Recently, text world games have been proposed to enable artificial agents to understand and reason about real-world scenarios. These text-based games are challenging for artificial agents, as it requires understanding and interaction using natural language in a partially observable environment. In this paper, we improve the semantic understanding of the agent by proposing a simple RL with LM framework where we use transformer-based language models with Deep RL models. We perform a detailed study of our framework to demonstrate how our model outperforms all existing agents on the popular game, Zork1, to achieve a score of 44.7, which is 1.6 higher than the state-of-the-art model. Our proposed approach also performs comparably to the state-of-the-art models on the other set of text games.

| Comments: | 55 Pages (8 Pages main content + 2 Pages references + 45 Pages Appendix) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Robotics (cs.RO) |
| Cite as:  | **[arXiv:2107.08408](https://arxiv.org/abs/2107.08408) [cs.CL]** |
|           | (or **[arXiv:2107.08408v1](https://arxiv.org/abs/2107.08408v1) [cs.CL]** for this version) |





<h2 id="2021-07-20-7">7. Translatotron 2: Robust direct speech-to-speech translation
</h2>

Title: [Translatotron 2: Robust direct speech-to-speech translation](https://arxiv.org/abs/2107.08661)

Authors: [Ye Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+Y), [Michelle Tadmor Ramanovich](https://arxiv.org/search/cs?searchtype=author&query=Ramanovich%2C+M+T), [Tal Remez](https://arxiv.org/search/cs?searchtype=author&query=Remez%2C+T), [Roi Pomerantz](https://arxiv.org/search/cs?searchtype=author&query=Pomerantz%2C+R)

> We present Translatotron 2, a neural direct speech-to-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention module that connects all the previous three components. Experimental results suggest that Translatotron 2 outperforms the original Translatotron by a large margin in terms of translation quality and predicted speech naturalness, and drastically improves the robustness of the predicted speech by mitigating over-generation, such as babbling or long pause. We also propose a new method for retaining the source speaker's voice in the translated speech. The trained model is restricted to retain the source speaker's voice, and unlike the original Translatotron, it is not able to generate speech in a different speaker's voice, making the model more robust for production deployment, by mitigating potential misuse for creating spoofing audio artifacts. When the new method is used together with a simple concatenation-based data augmentation, the trained Translatotron 2 model is able to retain each speaker's voice for input with speaker turns.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.08661](https://arxiv.org/abs/2107.08661) [cs.CL]** |
|           | (or **[arXiv:2107.08661v1](https://arxiv.org/abs/2107.08661v1) [cs.CL]** for this version) |





<h2 id="2021-07-20-8">8. Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images
</h2>

Title: [Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images](https://arxiv.org/abs/2107.08685)

Authors: [Nyoungwoo Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+N), [Suwon Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+S), [Jaegul Choo](https://arxiv.org/search/cs?searchtype=author&query=Choo%2C+J), [Ho-Jin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+H), [Sung-Hyun Myaeng](https://arxiv.org/search/cs?searchtype=author&query=Myaeng%2C+S)

> In multi-modal dialogue systems, it is important to allow the use of images as part of a multi-turn conversation. Training such dialogue systems generally requires a large-scale dataset consisting of multi-turn dialogues that involve images, but such datasets rarely exist. In response, this paper proposes a 45k multi-modal dialogue dataset created with minimal human intervention. Our method to create such a dataset consists of (1) preparing and pre-processing text dialogue datasets, (2) creating image-mixed dialogues by using a text-to-image replacement technique, and (3) employing a contextual-similarity-based filtering step to ensure the contextual coherence of the dataset. To evaluate the validity of our dataset, we devise a simple retrieval model for dialogue sentence prediction tasks. Automatic metrics and human evaluation results on such tasks show that our dataset can be effectively used as training data for multi-modal dialogue systems which require an understanding of images and text in a context-aware manner. Our dataset and generation code is available at [this https URL](https://github.com/shh1574/multi-modal-dialogue-dataset).

| Comments: | Accepted by ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.08685](https://arxiv.org/abs/2107.08685) [cs.CL]** |
|           | (or **[arXiv:2107.08685v1](https://arxiv.org/abs/2107.08685v1) [cs.CL]** for this version) |





<h2 id="2021-07-20-9">9. Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages
</h2>

Title: [Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages](https://arxiv.org/abs/2107.08772)

Authors: [Dana Ruiter](https://arxiv.org/search/cs?searchtype=author&query=Ruiter%2C+D), [Dietrich Klakow](https://arxiv.org/search/cs?searchtype=author&query=Klakow%2C+D), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J), [Cristina Espaa-Bonet](https://arxiv.org/search/cs?searchtype=author&query=Espaa-Bonet%2C+C)

> For most language combinations, parallel data is either scarce or simply unavailable. To address this, unsupervised machine translation (UMT) exploits large amounts of monolingual data by using synthetic data generation techniques such as back-translation and noising, while self-supervised NMT (SSNMT) identifies parallel sentences in smaller comparable data and trains on them. To date, the inclusion of UMT data generation techniques in SSNMT has not been investigated. We show that including UMT techniques into SSNMT significantly outperforms SSNMT and UMT on all tested language pairs, with improvements of up to +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT, respectively, on Afrikaans to English. We further show that the combination of multilingual denoising autoencoding, SSNMT with backtranslation and bilingual finetuning enables us to learn machine translation even for distant language pairs for which only small amounts of monolingual data are available, e.g. yielding BLEU scores of 11.6 (English to Swahili).

| Comments: | 11 pages, 8 figures, accepted at MT-Summit 2021 (Research Track) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.08772](https://arxiv.org/abs/2107.08772) [cs.CL]** |
|           | (or **[arXiv:2107.08772v1](https://arxiv.org/abs/2107.08772v1) [cs.CL]** for this version) |





<h2 id="2021-07-20-10">10. Simultaneous Speech Translation for Live Subtitling: from Delay to Display
</h2>

Title: [Simultaneous Speech Translation for Live Subtitling: from Delay to Display](https://arxiv.org/abs/2107.08807)

Authors: [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Sara Papi](https://arxiv.org/search/cs?searchtype=author&query=Papi%2C+S), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> With the increased audiovisualisation of communication, the need for live subtitles in multilingual events is more relevant than ever. In an attempt to automatise the process, we aim at exploring the feasibility of simultaneous speech translation (SimulST) for live subtitling. However, the word-for-word rate of generation of SimulST systems is not optimal for displaying the subtitles in a comprehensible and readable way. In this work, we adapt SimulST systems to predict subtitle breaks along with the translation. We then propose a display mode that exploits the predicted break structure by presenting the subtitles in scrolling lines. We compare our proposed mode with a display 1) word-for-word and 2) in blocks, in terms of reading speed and delay. Experiments on three language pairs (enit, de, fr) show that scrolling lines is the only mode achieving an acceptable reading speed while keeping delay close to a 4-second threshold. We argue that simultaneous translation for readable live subtitles still faces challenges, the main one being poor translation quality, and propose directions for steering future research.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of MT Summit 2021 at Automatic Spoken Language Translation in Real-World Settings |
| Cite as:           | **[arXiv:2107.08807](https://arxiv.org/abs/2107.08807) [cs.CL]** |
|                    | (or **[arXiv:2107.08807v1](https://arxiv.org/abs/2107.08807v1) [cs.CL]** for this version) |



# 2021-07-19

[Return to Index](#Index)



<h2 id="2021-07-19-1">1. Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models
</h2>

Title: [Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models](https://arxiv.org/abs/2107.07610)

Authors: [Zhao Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Z), [Yihan Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Y), [Mrinmaya Sachan](https://arxiv.org/search/cs?searchtype=author&query=Sachan%2C+M), [Roger Wattenhofer](https://arxiv.org/search/cs?searchtype=author&query=Wattenhofer%2C+R)

> This paper improves the robustness of the pretrained language model BERT against word substitution-based adversarial attacks by leveraging self-supervised contrastive learning with adversarial perturbations. One advantage of our method compared to previous works is that it is capable of improving model robustness without using any labels. Additionally, we also create an adversarial attack for word-level adversarial training on BERT. The attack is efficient, allowing adversarial training for BERT on adversarial examples generated on the fly during training. Experimental results on four datasets show that our method improves the robustness of BERT against four different word substitution-based adversarial attacks. Furthermore, to understand why our method can improve the model robustness against adversarial attacks, we study vector representations of clean examples and their corresponding adversarial examples before and after applying our method. As our method improves model robustness with unlabeled raw data, it opens up the possibility of using large text datasets to train robust language models.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.07610](https://arxiv.org/abs/2107.07610) [cs.CL]** |
|           | (or **[arXiv:2107.07610v1](https://arxiv.org/abs/2107.07610v1) [cs.CL]** for this version) |









# 2021-07-16

[Return to Index](#Index)



<h2 id="2021-07-16-1">1. How Much Can CLIP Benefit Vision-and-Language Tasks?
</h2>

Title: [From Show to Tell: A Survey on Image Captioning](https://arxiv.org/abs/2107.06912)

Authors: [Matteo Stefanini](https://arxiv.org/search/cs?searchtype=author&query=Stefanini%2C+M), [Marcella Cornia](https://arxiv.org/search/cs?searchtype=author&query=Cornia%2C+M), [Lorenzo Baraldi](https://arxiv.org/search/cs?searchtype=author&query=Baraldi%2C+L), [Silvia Cascianelli](https://arxiv.org/search/cs?searchtype=author&query=Cascianelli%2C+S), [Giuseppe Fiameni](https://arxiv.org/search/cs?searchtype=author&query=Fiameni%2C+G), [Rita Cucchiara](https://arxiv.org/search/cs?searchtype=author&query=Cucchiara%2C+R)

> Connecting Vision and Language plays an essential role in Generative Intelligence. For this reason, in the last few years, a large research effort has been devoted to image captioning, i.e. the task of describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoding step and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, and relationships and the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results obtained, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview and categorization of image captioning approaches, from visual encoding and text generation to training strategies, used datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in image captioning architectures and training strategies. Moreover, many variants of the problem and its open challenges are analyzed and discussed. The final goal of this work is to serve as a tool for understanding the existing state-of-the-art and highlighting the future directions for an area of research where Computer Vision and Natural Language Processing can find an optimal synergy.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.06912](https://arxiv.org/abs/2107.06912) [cs.CV]** |
|           | (or **[arXiv:2107.06912v1](https://arxiv.org/abs/2107.06912v1) [cs.CV]** for this version) |





<h2 id="2021-07-16-2">2. MultiBench: Multiscale Benchmarks for Multimodal Representation Learning
</h2>

Title: [MultiBench: Multiscale Benchmarks for Multimodal Representation Learning](https://arxiv.org/abs/2107.07502)

Authors: [Paul Pu Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P+P), [Yiwei Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+Y), [Xiang Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+X), [Zetian Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Yun Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Jason Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+J), [Leslie Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L), [Peter Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+P), [Michelle A. Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+M+A), [Yuke Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Y), [Ruslan Salakhutdinov](https://arxiv.org/search/cs?searchtype=author&query=Salakhutdinov%2C+R), [Louis-Philippe Morency](https://arxiv.org/search/cs?searchtype=author&query=Morency%2C+L)

> Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized code, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.

| Comments: | Code: [this https URL](https://github.com/pliang279/MultiBench) and Website: [this https URL](https://cmu-multicomp-lab.github.io/multibench/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM) |
| Cite as:  | **[arXiv:2107.07502](https://arxiv.org/abs/2107.07502) [cs.LG]** |
|           | (or **[arXiv:2107.07502v1](https://arxiv.org/abs/2107.07502v1) [cs.LG]** for this version) |





<h2 id="2021-07-16-3">3. FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task
</h2>

Title: [FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task](https://arxiv.org/abs/2107.06959)

Authors: [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Hongyu Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Holger Schwenk](https://arxiv.org/search/cs?searchtype=author&query=Schwenk%2C+H), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N)

> In this paper, we describe our end-to-end multilingual speech translation system submitted to the IWSLT 2021 evaluation campaign on the Multilingual Speech Translation shared task. Our system is built by leveraging transfer learning across modalities, tasks and languages. First, we leverage general-purpose multilingual modules pretrained with large amounts of unlabelled and labelled data. We further enable knowledge transfer from the text task to the speech task by training two tasks jointly. Finally, our multilingual model is finetuned on speech translation task-specific data to achieve the best translation results. Experimental results show our system outperforms the reported systems, including both end-to-end and cascaded based approaches, by a large margin.
> In some translation directions, our speech translation results evaluated on the public Multilingual TEDx test set are even comparable with the ones from a strong text-to-text translation system, which uses the oracle speech transcripts as input.

| Comments: | Accepted by IWSLT 2021 as a system paper                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2107.06959](https://arxiv.org/abs/2107.06959) [cs.CL]** |
|           | (or **[arXiv:2107.06959v1](https://arxiv.org/abs/2107.06959v1) [cs.CL]** for this version) |






# 2021-07-15

[Return to Index](#Index)



<h2 id="2021-07-15-1">1. How Much Can CLIP Benefit Vision-and-Language Tasks?
</h2>

Title: [How Much Can CLIP Benefit Vision-and-Language Tasks?](https://arxiv.org/abs/2107.06383)

Authors: [Sheng Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+S), [Liunian Harold Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L+H), [Hao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M), [Anna Rohrbach](https://arxiv.org/search/cs?searchtype=author&query=Rohrbach%2C+A), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K), [Zhewei Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Z), [Kurt Keutzer](https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K)

> Most existing Vision-and-Language (V&L) models rely on pre-trained visual encoders, using a relatively small set of manually-annotated data (as compared to web-crawled data), to perceive the visual world. However, it has been observed that large-scale pretraining usually can result in better generalization performance, e.g., CLIP (Contrastive Language-Image Pre-training), trained on a massive amount of image-caption pairs, has shown a strong zero-shot capability on various vision tasks. To further study the advantage brought by CLIP, we propose to use CLIP as the visual encoder in various V&L models in two typical scenarios: 1) plugging CLIP into task-specific fine-tuning; 2) combining CLIP with V&L pre-training and transferring to downstream tasks. We show that CLIP significantly outperforms widely-used visual encoders trained with in-domain annotated data, such as BottomUp-TopDown. We achieve competitive or better results on diverse V&L tasks, while establishing new state-of-the-art results on Visual Question Answering, Visual Entailment, and V&L Navigation tasks. We release our code at [this https URL](https://github.com/clip-vil/CLIP-ViL).

| Comments: | 14 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.06383](https://arxiv.org/abs/2107.06383) [cs.CV]** |
|           | (or **[arXiv:2107.06383v1](https://arxiv.org/abs/2107.06383v1) [cs.CV]** for this version) |





<h2 id="2021-07-15-2">2. From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text
</h2>

Title: [From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text](https://arxiv.org/abs/2107.06483)

Authors: [Ishan Tarunesh](https://arxiv.org/search/cs?searchtype=author&query=Tarunesh%2C+I), [Syamantak Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S), [Preethi Jyothi](https://arxiv.org/search/cs?searchtype=author&query=Jyothi%2C+P)

> Generating code-switched text is a problem of growing interest, especially given the scarcity of corpora containing large volumes of real code-switched text. In this work, we adapt a state-of-the-art neural machine translation model to generate Hindi-English code-switched sentences starting from monolingual Hindi sentences. We outline a carefully designed curriculum of pretraining steps, including the use of synthetic code-switched text, that enable the model to generate high-quality code-switched text. Using text generated from our model as data augmentation, we show significant reductions in perplexity on a language modeling task, compared to using text from other generative models of CS text. We also show improvements using our text for a downstream code-switched natural language inference task. Our generated text is further subjected to a rigorous evaluation using a human evaluation study and a range of objective metrics, where we show performance comparable (and sometimes even superior) to code-switched text obtained via crowd workers who are native Hindi speakers.

| Comments: | In Proceedings of The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.06483](https://arxiv.org/abs/2107.06483) [cs.CL]** |
|           | (or **[arXiv:2107.06483v1](https://arxiv.org/abs/2107.06483v1) [cs.CL]** for this version) |





<h2 id="2021-07-15-3">3. Deduplicating Training Data Makes Language Models Better
</h2>

Title: [Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499)

Authors: [Katherine Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K), [Daphne Ippolito](https://arxiv.org/search/cs?searchtype=author&query=Ippolito%2C+D), [Andrew Nystrom](https://arxiv.org/search/cs?searchtype=author&query=Nystrom%2C+A), [Chiyuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Douglas Eck](https://arxiv.org/search/cs?searchtype=author&query=Eck%2C+D), [Chris Callison-Burch](https://arxiv.org/search/cs?searchtype=author&query=Callison-Burch%2C+C), [Nicholas Carlini](https://arxiv.org/search/cs?searchtype=author&query=Carlini%2C+N)

> We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at [this https URL](https://github.com/google-research/deduplicate-text-datasets).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.06499](https://arxiv.org/abs/2107.06499) [cs.CL]** |
|           | (or **[arXiv:2107.06499v1](https://arxiv.org/abs/2107.06499v1) [cs.CL]** for this version) |





<h2 id="2021-07-15-4">4. Importance-based Neuron Allocation for Multilingual Neural Machine Translation
</h2>

Title: [Importance-based Neuron Allocation for Multilingual Neural Machine Translation](https://arxiv.org/abs/2107.06569)

Authors: [Wanying Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+W), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Shuhao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+S), [Dong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D)

> Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages. However, the current multilingual translation paradigm often makes the model tend to preserve the general knowledge, but ignore the language-specific knowledge. Some previous works try to solve this problem by adding various kinds of language-specific modules to the model, but they suffer from the parameter explosion problem and require specialized manual design. To solve these problems, we propose to divide the model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the general knowledge and participating in the translation of all the languages, while the language-specific part is responsible for preserving the language-specific knowledge and participating in the translation of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.06569](https://arxiv.org/abs/2107.06569) [cs.CL]** |
|           | (or **[arXiv:2107.06569v1](https://arxiv.org/abs/2107.06569v1) [cs.CL]** for this version) |





<h2 id="2021-07-15-5">5. ParCourE: A Parallel Corpus Explorer fora Massively Multilingual Corpus
</h2>

Title: [ParCourE: A Parallel Corpus Explorer fora Massively Multilingual Corpus](https://arxiv.org/abs/2107.06632)

Authors: [Ayyoob Imani](https://arxiv.org/search/cs?searchtype=author&query=Imani%2C+A), [Masoud Jalili Sabet](https://arxiv.org/search/cs?searchtype=author&query=Sabet%2C+M+J), [Philipp Dufter](https://arxiv.org/search/cs?searchtype=author&query=Dufter%2C+P), [Michael Cysouw](https://arxiv.org/search/cs?searchtype=author&query=Cysouw%2C+M), [Hinrich Schtze](https://arxiv.org/search/cs?searchtype=author&query=Schtze%2C+H)

> With more than 7000 languages worldwide, multilingual natural language processing (NLP) is essential both from an academic and commercial perspective. Researching typological properties of languages is fundamental for progress in multilingual NLP. Examples include assessing language similarity for effective transfer learning, injecting inductive biases into machine learning models or creating resources such as dictionaries and inflection tables. We provide ParCourE, an online tool that allows to browse a word-aligned parallel corpus, covering 1334 languages. We give evidence that this is useful for typological research. ParCourE can be set up for any parallel corpus and can thus be used for typological research on other corpora as well as for exploring their quality and properties.

| Comments: | The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.06632](https://arxiv.org/abs/2107.06632) [cs.CL]** |
|           | (or **[arXiv:2107.06632v1](https://arxiv.org/abs/2107.06632v1) [cs.CL]** for this version) |








# 2021-07-14

[Return to Index](#Index)



<h2 id="2021-07-14-1">1. A Configurable Multilingual Model is All You Need to Recognize All Languages
</h2>

Title: [A Configurable Multilingual Model is All You Need to Recognize All Languages](https://arxiv.org/abs/2107.05876)

Authors: [Long Zhou](https://arxiv.org/search/eess?searchtype=author&query=Zhou%2C+L), [Jinyu Li](https://arxiv.org/search/eess?searchtype=author&query=Li%2C+J), [Eric Sun](https://arxiv.org/search/eess?searchtype=author&query=Sun%2C+E), [Shujie Liu](https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+S)

> Multilingual automatic speech recognition (ASR) models have shown great promise in recent years because of the simplified model training and deployment process. Conventional methods either train a universal multilingual model without taking any language information or with a 1-hot language ID (LID) vector to guide the recognition of the target language. In practice, the user can be prompted to pre-select several languages he/she can speak. The multilingual model without LID cannot well utilize the language information set by the user while the multilingual model with LID can only handle one pre-selected language. In this paper, we propose a novel configurable multilingual model (CMM) which is trained only once but can be configured as different models based on users' choices by extracting language-specific modules together with a universal model from the trained CMM. Particularly, a single CMM can be deployed to any user scenario where the users can pre-select any combination of languages. Trained with 75K hours of transcribed anonymized Microsoft multilingual data and evaluated with 10-language test sets, the proposed CMM improves from the universal multilingual model by 26.0%, 16.9%, and 10.4% relative word error reduction when the user selects 1, 2, or 3 languages, respectively. CMM also performs significantly better on code-switching test sets.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.05876](https://arxiv.org/abs/2107.05876) [eess.AS]** |
|           | (or **[arXiv:2107.05876v1](https://arxiv.org/abs/2107.05876v1) [eess.AS]** for this version) |





<h2 id="2021-07-14-2">2. Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task
</h2>

Title: [Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task](https://arxiv.org/abs/2107.05782)

Authors: [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Dmitriy Genzel](https://arxiv.org/search/cs?searchtype=author&query=Genzel%2C+D)

> Pretraining and multitask learning are widely used to improve the speech to text translation performance. In this study, we are interested in training a speech to text translation model along with an auxiliary text to text translation task. We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework. Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more information from the pretrained text translation modules. We observe minimal negative transfer effect between the two tasks and sharing more parameters is helpful to transfer knowledge from the text task to the speech task. The analysis also reveals that the modality representation difference at the top decoder layers is still not negligible, and those layers are critical for the translation quality. Inspired by these findings, we propose three methods to improve translation quality. First, a parameter sharing and initialization strategy is proposed to enhance information sharing between the tasks. Second, a novel attention-based regularization is proposed for the encoders and pulls the representations from different modalities closer. Third, an online knowledge distillation is proposed to enhance the knowledge transfer from the text to the speech task. Our experiments show that the proposed approach improves translation performance by more than 2 BLEU over a strong baseline and achieves state-of-the-art results on the \textsc{MuST-C} English-German, English-French and English-Spanish language pairs.

| Comments: | Accepted by ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2107.05782](https://arxiv.org/abs/2107.05782) [cs.CL]** |
|           | (or **[arXiv:2107.05782v1](https://arxiv.org/abs/2107.05782v1) [cs.CL]** for this version) |





<h2 id="2021-07-14-3">3. Zero-shot Speech Translation
</h2>

Title: [Zero-shot Speech Translation](https://arxiv.org/abs/2107.06010)

Authors: [Tu Anh Dinh](https://arxiv.org/search/cs?searchtype=author&query=Dinh%2C+T+A)

> Speech Translation (ST) is the task of translating speech in one language into text in another language. Traditional cascaded approaches for ST, using Automatic Speech Recognition (ASR) and Machine Translation (MT) systems, are prone to error propagation. End-to-end approaches use only one system to avoid propagating error, yet are difficult to employ due to data scarcity. We explore zero-shot translation, which enables translating a pair of languages that is unseen during training, thus avoid the use of end-to-end ST data. Zero-shot translation has been shown to work for multilingual machine translation, yet has not been studied for speech translation. We attempt to build zero-shot ST models that are trained only on ASR and MT tasks but can do ST task during inference. The challenge is that the representation of text and audio is significantly different, thus the models learn ASR and MT tasks in different ways, making it non-trivial to perform zero-shot. These models tend to output the wrong language when performing zero-shot ST. We tackle the issues by including additional training data and an auxiliary loss function that minimizes the text-audio difference. Our experiment results and analysis show that the methods are promising for zero-shot ST. Moreover, our methods are particularly useful in the few-shot settings where a limited amount of ST data is available, with improvements of up to +11.8 BLEU points compared to direct end-to-end ST models and +3.9 BLEU points compared to ST models fine-tuned from pre-trained ASR model.

| Subjects:    | **Computation and Language (cs.CL)**                         |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2107.06010](https://arxiv.org/abs/2107.06010) [cs.CL]** |
|              | (or **[arXiv:2107.06010v1](https://arxiv.org/abs/2107.06010v1) [cs.CL]** for this version) |





<h2 id="2021-07-14-4">4. The IWSLT 2021 BUT Speech Translation Systems
</h2>

Title: [The IWSLT 2021 BUT Speech Translation Systems](https://arxiv.org/abs/2107.06155)

Authors: [Hari Krishna Vydana](https://arxiv.org/search/cs?searchtype=author&query=Vydana%2C+H+K), [Martin Karafi'at](https://arxiv.org/search/cs?searchtype=author&query=Karafi'at%2C+M), [Luk'as Burget](https://arxiv.org/search/cs?searchtype=author&query=Burget%2C+L), ["Honza" Cernock'y](https://arxiv.org/search/cs?searchtype=author&query=Cernock'y%2C+")

> The paper describes BUT's English to German offline speech translation(ST) systems developed for IWSLT2021. They are based on jointly trained Automatic Speech Recognition-Machine Translation models. Their performances is evaluated on MustC-Common test set. In this work, we study their efficiency from the perspective of having a large amount of separate ASR training data and MT training data, and a smaller amount of speech-translation training data. Large amounts of ASR and MT training data are utilized for pre-training the ASR and MT models. Speech-translation data is used to jointly optimize ASR-MT models by defining an end-to-end differentiable path from speech to translations. For this purpose, we use the internal continuous representations from the ASR-decoder as the input to MT module. We show that speech translation can be further improved by training the ASR-decoder jointly with the MT-module using large amount of text-only MT training data. We also show significant improvements by training an ASR module capable of generating punctuated text, rather than leaving the punctuation task to the MT module.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.06155](https://arxiv.org/abs/2107.06155) [cs.CL]** |
|           | (or **[arXiv:2107.06155v1](https://arxiv.org/abs/2107.06155v1) [cs.CL]** for this version) |





<h2 id="2021-07-14-5">5. Between Flexibility and Consistency: Joint Generation of Captions and Subtitles
</h2>

Title: [Between Flexibility and Consistency: Joint Generation of Captions and Subtitles](https://arxiv.org/abs/2107.06246)

Authors: [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Speech translation (ST) has lately received growing interest for the generation of subtitles without the need for an intermediate source language transcription and timing (i.e. captions). However, the joint generation of source captions and target subtitles does not only bring potential output quality advantages when the two decoding processes inform each other, but it is also often required in multilingual scenarios. In this work, we focus on ST models which generate consistent captions-subtitles in terms of structure and lexical content. We further introduce new metrics for evaluating subtitling consistency. Our findings show that joint decoding leads to increased performance and consistency between the generated captions and subtitles while still allowing for sufficient flexibility to produce subtitles conforming to language-specific needs and norms.

| Comments: | Accepted at IWSLT 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.06246](https://arxiv.org/abs/2107.06246) [cs.CL]** |
|           | (or **[arXiv:2107.06246v1](https://arxiv.org/abs/2107.06246v1) [cs.CL]** for this version) |







# 2021-07-13

[Return to Index](#Index)



<h2 id="2021-07-13-1">1. Oriental Language Recognition (OLR) 2020: Summary and Analysis
</h2>

Title: [Oriental Language Recognition (OLR) 2020: Summary and Analysis](https://arxiv.org/abs/2107.05365)

Authors: [Jing Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Binling Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Yiming Zhi](https://arxiv.org/search/cs?searchtype=author&query=Zhi%2C+Y), [Zheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Lin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Qingyang Hong](https://arxiv.org/search/cs?searchtype=author&query=Hong%2C+Q), [Dong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D)

> The fifth Oriental Language Recognition (OLR) Challenge focuses on language recognition in a variety of complex environments to promote its development. The OLR 2020 Challenge includes three tasks: (1) cross-channel language identification, (2) dialect identification, and (3) noisy language identification. We choose Cavg as the principle evaluation metric, and the Equal Error Rate (EER) as the secondary metric. There were 58 teams participating in this challenge and one third of the teams submitted valid results. Compared with the best baseline, the Cavg values of Top 1 system for the three tasks were relatively reduced by 82%, 62% and 48%, respectively. This paper describes the three tasks, the database profile, and the final results. We also outline the novel approaches that improve the performance of language recognition systems most significantly, such as the utilization of auxiliary information.

| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL)          |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.05365](https://arxiv.org/abs/2107.05365) [cs.SD]** |
|           | (or **[arXiv:2107.05365v1](https://arxiv.org/abs/2107.05365v1) [cs.SD]** for this version) |





<h2 id="2021-07-13-2">2. Parameter Selection: Why We Should Pay More Attention to It
</h2>

Title: [Parameter Selection: Why We Should Pay More Attention to It](https://arxiv.org/abs/2107.05393)

Authors: [Jie-Jyun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Tsung-Han Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+T), [Si-An Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Chih-Jen Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+C)

> The importance of parameter selection in supervised learning is well known. However, due to the many parameter combinations, an incomplete or an insufficient procedure is often applied. This situation may cause misleading or confusing conclusions. In this opinion paper, through an intriguing example we point out that the seriousness goes beyond what is generally recognized. In the topic of multi-label classification for medical code prediction, one influential paper conducted a proper parameter selection on a set, but when moving to a subset of frequently occurring labels, the authors used the same parameters without a separate tuning. The set of frequent labels became a popular benchmark in subsequent studies, which kept pushing the state of the art. However, we discovered that most of the results in these studies cannot surpass the approach in the original paper if a parameter tuning had been conducted at the time. Thus it is unclear how much progress the subsequent developments have actually brought. The lesson clearly indicates that without enough attention on parameter selection, the research progress in our field can be uncertain or even illusive.

| Comments: | Accepted by ACL-IJCNLP 2021                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2107.05393](https://arxiv.org/abs/2107.05393) [cs.LG]** |
|           | (or **[arXiv:2107.05393v1](https://arxiv.org/abs/2107.05393v1) [cs.LG]** for this version) |





<h2 id="2021-07-13-3">3. Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning
</h2>

Title: [Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning](https://arxiv.org/abs/2107.05243)

Authors: [Jun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Chang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Francisco Guzman](https://arxiv.org/search/cs?searchtype=author&query=Guzman%2C+F), [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Yuqing Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Benjamin I. P. Rubinstein](https://arxiv.org/search/cs?searchtype=author&query=Rubinstein%2C+B+I+P), [Trevor Cohn](https://arxiv.org/search/cs?searchtype=author&query=Cohn%2C+T)

> Neural machine translation systems are known to be vulnerable to adversarial test inputs, however, as we show in this paper, these systems are also vulnerable to training attacks. Specifically, we propose a poisoning attack in which a malicious adversary inserts a small poisoned sample of monolingual text into the training set of a system trained using back-translation. This sample is designed to induce a specific, targeted translation behaviour, such as peddling misinformation. We present two methods for crafting poisoned examples, and show that only a tiny handful of instances, amounting to only 0.02% of the training set, is sufficient to enact a successful attack. We outline a defence method against said attacks, which partly ameliorates the problem. However, we stress that this is a blind-spot in modern NMT, demanding immediate attention.

| Comments: | Findings of ACL, to appear                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Cryptography and Security (cs.CR) |
| Cite as:  | **[arXiv:2107.05243](https://arxiv.org/abs/2107.05243) [cs.CL]** |
|           | (or **[arXiv:2107.05243v1](https://arxiv.org/abs/2107.05243v1) [cs.CL]** for this version) |





<h2 id="2021-07-13-4">4. Direct speech-to-speech translation with discrete units
</h2>

Title: [Direct speech-to-speech translation with discrete units](https://arxiv.org/abs/2107.05604)

Authors: [Ann Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+A), [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Adam Polyak](https://arxiv.org/search/cs?searchtype=author&query=Polyak%2C+A), [Yossi Adi](https://arxiv.org/search/cs?searchtype=author&query=Adi%2C+Y), [Qing He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Q), [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Wei-Ning Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+W)

> We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation. Previous work addresses the problem by training an attention-based sequence-to-sequence model that maps source speech spectrograms into target spectrograms. To tackle the challenge of modeling continuous spectrogram features of the target speech, we propose to predict the self-supervised discrete representations learned from an unlabeled speech corpus instead. When target text transcripts are available, we design a multitask learning framework with joint speech and text training that enables the model to generate dual mode output (speech and text) simultaneously in the same inference pass. Experiments on the Fisher Spanish-English dataset show that predicting discrete units and joint speech and text training improve model performance by 11 BLEU compared with a baseline that predicts spectrograms and bridges 83% of the performance gap towards a cascaded system. When trained without any text transcripts, our model achieves similar performance as a baseline that predicts spectrograms and is trained with text data.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.05604](https://arxiv.org/abs/2107.05604) [cs.CL]** |
|           | (or **[arXiv:2107.05604v1](https://arxiv.org/abs/2107.05604v1) [cs.CL]** for this version) |








# 2021-07-12

[Return to Index](#Index)



<h2 id="2021-07-12-1">1. Improved Language Identification Through Cross-Lingual Self-Supervised Learning
</h2>

Title: [Improved Language Identification Through Cross-Lingual Self-Supervised Learning](https://arxiv.org/abs/2107.04082)

Authors: [Andros Tjandra](https://arxiv.org/search/cs?searchtype=author&query=Tjandra%2C+A), [Diptanu Gon Choudhury](https://arxiv.org/search/cs?searchtype=author&query=Choudhury%2C+D+G), [Frank Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+F), [Kritika Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+K), [Alexei Baevski](https://arxiv.org/search/cs?searchtype=author&query=Baevski%2C+A), [Assaf Sela](https://arxiv.org/search/cs?searchtype=author&query=Sela%2C+A), [Yatharth Saraf](https://arxiv.org/search/cs?searchtype=author&query=Saraf%2C+Y), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M)

> Language identification greatly impacts the success of downstream tasks such as automatic speech recognition. Recently, self-supervised speech representations learned by wav2vec 2.0 have been shown to be very effective for a range of speech tasks. We extend previous self-supervised work on language identification by experimenting with pre-trained models which were learned on real-world unconstrained speech in multiple languages and not just on English. We show that models pre-trained on many languages perform better and enable language identification systems that require very little labeled data to perform well. Results on a 25 languages setup show that with only 10 minutes of labeled data per language, a cross-lingually pre-trained model can achieve over 93% accuracy.

| Comments: | Submitted to ASRU 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2107.04082](https://arxiv.org/abs/2107.04082) [cs.CL]** |
|           | (or **[arXiv:2107.04082v1](https://arxiv.org/abs/2107.04082v1) [cs.CL]** for this version) |





<h2 id="2021-07-12-2">2. A Systematic Survey of Text Worlds as Embodied Natural Language Environments
</h2>

Title: [A Systematic Survey of Text Worlds as Embodied Natural Language Environments](https://arxiv.org/abs/2107.04132)

Authors: [Peter A Jansen](https://arxiv.org/search/cs?searchtype=author&query=Jansen%2C+P+A)

> Text Worlds are virtual environments for embodied agents that, unlike 2D or 3D environments, are rendered exclusively using textual descriptions. These environments offer an alternative to higher-fidelity 3D environments due to their low barrier to entry, providing the ability to study semantics, compositional inference, and other high-level tasks with rich high-level action spaces while controlling for perceptual input. This systematic survey outlines recent developments in tooling, environments, and agent modeling for Text Worlds, while examining recent trends in knowledge graphs, common sense reasoning, transfer learning of Text World performance to higher-fidelity environments, as well as near-term development targets that, once achieved, make Text Worlds an attractive general research paradigm for natural language processing.

| Comments: | 18 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2107.04132](https://arxiv.org/abs/2107.04132) [cs.CL]** |
|           | (or **[arXiv:2107.04132v1](https://arxiv.org/abs/2107.04132v1) [cs.CL]** for this version) |





<h2 id="2021-07-12-3">3. A Survey on Low-Resource Neural Machine Translation
</h2>

Title: [A Survey on Low-Resource Neural Machine Translation](https://arxiv.org/abs/2107.04239)

Authors: [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Renqian Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> Neural approaches have achieved state-of-the-art accuracy on machine translation but suffer from the high cost of collecting large scale parallel data. Thus, a lot of research has been conducted for neural machine translation (NMT) with very limited parallel data, i.e., the low-resource setting. In this paper, we provide a survey for low-resource NMT and classify related works into three categories according to the auxiliary data they used: (1) exploiting monolingual data of source and/or target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.

| Comments: | A short version has been submitted to IJCAI2021 Survey Track on Feb. 26th, 2021, accepted on Apr. 16th, 2021. 14 pages, 4 figures |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.04239](https://arxiv.org/abs/2107.04239) [cs.CL]** |
|           | (or **[arXiv:2107.04239v1](https://arxiv.org/abs/2107.04239v1) [cs.CL]** for this version) |





<h2 id="2021-07-12-4">4. Using Machine Translation to Localize Task Oriented NLG Output
</h2>

Title: [Using Machine Translation to Localize Task Oriented NLG Output](https://arxiv.org/abs/2107.04512)

Authors: [Scott Roy](https://arxiv.org/search/cs?searchtype=author&query=Roy%2C+S), [Cliff Brunk](https://arxiv.org/search/cs?searchtype=author&query=Brunk%2C+C), [Kyu-Young Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+K), [Justin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [Mihir Kale](https://arxiv.org/search/cs?searchtype=author&query=Kale%2C+M), [Gagan Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+G), [Sidharth Mudgal](https://arxiv.org/search/cs?searchtype=author&query=Mudgal%2C+S), [Chris Varano](https://arxiv.org/search/cs?searchtype=author&query=Varano%2C+C)

> One of the challenges in a task oriented natural language application like the Google Assistant, Siri, or Alexa is to localize the output to many languages. This paper explores doing this by applying machine translation to the English output. Using machine translation is very scalable, as it can work with any English output and can handle dynamic text, but otherwise the problem is a poor fit. The required quality bar is close to perfection, the range of sentences is extremely narrow, and the sentences are often very different than the ones in the machine translation training data. This combination of requirements is novel in the field of domain adaptation for machine translation. We are able to reach the required quality bar by building on existing ideas and adding new ones: finetuning on in-domain translations, adding sentences from the Web, adding semantic annotations, and using automatic error detection. The paper shares our approach and results, together with a distillation model to serve the translation models at scale.

| Comments: | 12 pages, 10 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.04512](https://arxiv.org/abs/2107.04512) [cs.CL]** |
|           | (or **[arXiv:2107.04512v1](https://arxiv.org/abs/2107.04512v1) [cs.CL]** for this version) |






# 2021-07-09

[Return to Index](#Index)



<h2 id="2021-07-09-1">1. Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text
</h2>

Title: [Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text](https://arxiv.org/abs/2107.03444)

Authors: [Philippe Laban](https://arxiv.org/search/cs?searchtype=author&query=Laban%2C+P), [Tobias Schnabel](https://arxiv.org/search/cs?searchtype=author&query=Schnabel%2C+T), [Paul Bennett](https://arxiv.org/search/cs?searchtype=author&query=Bennett%2C+P), [Marti A. Hearst](https://arxiv.org/search/cs?searchtype=author&query=Hearst%2C+M+A)

> This work presents Keep it Simple (KiS), a new approach to unsupervised text simplification which learns to balance a reward across three properties: fluency, salience and simplicity. We train the model with a novel algorithm to optimize the reward (k-SCST), in which the model proposes several candidate simplifications, computes each candidate's reward, and encourages candidates that outperform the mean reward. Finally, we propose a realistic text comprehension task as an evaluation method for text simplification. When tested on the English news domain, the KiS model outperforms strong supervised baselines by more than 4 SARI points, and can help people complete a comprehension task an average of 18% faster while retaining accuracy, when compared to the original text. Code available: [this https URL](https://github.com/tingofurro/keep_it_simple)

| Comments:          | Accepted at ACL-IJCNLP 2021, 14 pages, 7 figures             |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Association for Computational Linguistics (2021)             |
| Cite as:           | **[arXiv:2107.03444](https://arxiv.org/abs/2107.03444) [cs.CL]** |
|                    | (or **[arXiv:2107.03444v1](https://arxiv.org/abs/2107.03444v1) [cs.CL]** for this version) |





<h2 id="2021-07-09-2">2. Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation
</h2>

Title: [Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation](https://arxiv.org/abs/2107.03625)

Authors: [Yves Bestgen](https://arxiv.org/search/cs?searchtype=author&query=Bestgen%2C+Y)

> A comparison of formulaic sequences in human and neural machine translation of quality newspaper articles shows that neural machine translations contain less lower-frequency, but strongly-associated formulaic sequences, and more high-frequency formulaic sequences. These differences were statistically significant and the effect sizes were almost always medium or large. These observations can be related to the differences between second language learners of various levels and between translated and untranslated texts. The comparison between the neural machine translation systems indicates that some systems produce more formulaic sequences of both types than other systems.

| Comments: | Accepted at Translation and Interpreting Technology Online - TRITON 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.03625](https://arxiv.org/abs/2107.03625) [cs.CL]** |
|           | (or **[arXiv:2107.03625v1](https://arxiv.org/abs/2107.03625v1) [cs.CL]** for this version) |






# 2021-07-08

[Return to Index](#Index)



<h2 id="2021-07-08-1">1. Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking
</h2>

Title: [Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking](https://arxiv.org/abs/2107.02865)

Authors: [Daniel Diomedi](https://arxiv.org/search/cs?searchtype=author&query=Diomedi%2C+D), [Aidan Hogan](https://arxiv.org/search/cs?searchtype=author&query=Hogan%2C+A)

> The goal of Question Answering over Knowledge Graphs (KGQA) is to find answers for natural language questions over a knowledge graph. Recent KGQA approaches adopt a neural machine translation (NMT) approach, where the natural language question is translated into a structured query language. However, NMT suffers from the out-of-vocabulary problem, where terms in a question may not have been seen during training, impeding their translation. This issue is particularly problematic for the millions of entities that large knowledge graphs describe. We rather propose a KGQA approach that delegates the processing of entities to entity linking (EL) systems. NMT is then used to create a query template with placeholders that are filled by entities identified in an EL phase. Slot filling is used to decide which entity fills which placeholder. Experiments for QA over Wikidata show that our approach outperforms pure NMT: while there remains a strong dependence on having seen similar query templates during training, errors relating to entities are greatly reduced.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02865](https://arxiv.org/abs/2107.02865) [cs.AI]** |
|           | (or **[arXiv:2107.02865v1](https://arxiv.org/abs/2107.02865v1) [cs.AI]** for this version) |





<h2 id="2021-07-08-2">2. Kosp2e: Korean Speech to English Translation Corpus
</h2>

Title: [Kosp2e: Korean Speech to English Translation Corpus](https://arxiv.org/abs/2107.02875)

Authors: [Won Ik Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+W+I), [Seok Min Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S+M), [Hyunchang Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+H), [Nam Soo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+N+S)

> Most speech-to-text (S2T) translation studies use English speech as a source, which makes it difficult for non-English speakers to take advantage of the S2T technologies. For some languages, this problem was tackled through corpus construction, but the farther linguistically from English or the more under-resourced, this deficiency and underrepresentedness becomes more significant. In this paper, we introduce kosp2e (read as `kospi'), a corpus that allows Korean speech to be translated into English text in an end-to-end manner. We adopt open license speech recognition corpus, translation corpus, and spoken language corpora to make our dataset freely available to the public, and check the performance through the pipeline and training-based approaches. Using pipeline and various end-to-end schemes, we obtain the highest BLEU of 21.3 and 18.0 for each based on the English hypothesis, validating the feasibility of our data. We plan to supplement annotations for other target languages through community contributions in the future.

| Comments: | Interspeech 2021 Camera-ready                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.02875](https://arxiv.org/abs/2107.02875) [cs.CL]** |
|           | (or **[arXiv:2107.02875v1](https://arxiv.org/abs/2107.02875v1) [cs.CL]** for this version) |





<h2 id="2021-07-08-3">3. Efficient Transformer for Direct Speech Translation
</h2>

Title: [Efficient Transformer for Direct Speech Translation](https://arxiv.org/abs/2107.03069)

Authors: [Belen Alastruey](https://arxiv.org/search/cs?searchtype=author&query=Alastruey%2C+B), [Gerard I. Gllego](https://arxiv.org/search/cs?searchtype=author&query=Gllego%2C+G+I), [Marta R. Costa-juss](https://arxiv.org/search/cs?searchtype=author&query=Costa-juss%2C+M+R)

> The advent of Transformer-based models has surpassed the barriers of text. When working with speech, we must face a problem: the sequence length of an audio input is not suitable for the Transformer. To bypass this problem, a usual approach is adding strided convolutional layers, to reduce the sequence length before using the Transformer. In this paper, we propose a new approach for direct Speech Translation, where thanks to an efficient Transformer we can work with a spectrogram without having to use convolutional layers before the Transformer. This allows the encoder to learn directly from the spectrogram and no information is lost. We have created an encoder-decoder model, where the encoder is an efficient Transformer -- the Longformer -- and the decoder is a traditional Transformer decoder. Our results, which are close to the ones obtained with the standard approach, show that this is a promising research direction.

| Comments: | (c) 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2107.03069](https://arxiv.org/abs/2107.03069) [cs.CL]** |
|           | (or **[arXiv:2107.03069v1](https://arxiv.org/abs/2107.03069v1) [cs.CL]** for this version) |





<h2 id="2021-07-08-4">4. On Training Instance Selection for Few-Shot Neural Text Generation
</h2>

Title: [On Training Instance Selection for Few-Shot Neural Text Generation](https://arxiv.org/abs/2107.03176)

Authors: [Ernie Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+E), [Xiaoyu Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+X), [Hui-Syuan Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+H), [Vera Demberg](https://arxiv.org/search/cs?searchtype=author&query=Demberg%2C+V)

> Large-scale pretrained language models have led to dramatic improvements in text generation. Impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). Nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. Little to no attention has been paid to the selection strategies and how they would affect model performance. In this work, we present a study on training instance selection in few-shot neural text generation. The selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost. Based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution, we propose a simple selection strategy with K-means clustering. We show that even with the naive clustering-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. We hope that this work will call for more attention on this largely unexplored area.

| Comments: | Accepted at ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.03176](https://arxiv.org/abs/2107.03176) [cs.CL]** |
|           | (or **[arXiv:2107.03176v1](https://arxiv.org/abs/2107.03176v1) [cs.CL]** for this version) |





<h2 id="2021-07-08-5">5. Time-Aware Ancient Chinese Text Translation and Inference
</h2>

Title: [Time-Aware Ancient Chinese Text Translation and Inference](https://arxiv.org/abs/2107.03179)

Authors: [Ernie Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+E), [Yow-Ting Shiue](https://arxiv.org/search/cs?searchtype=author&query=Shiue%2C+Y), [Hui-Syuan Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+H), [Vera Demberg](https://arxiv.org/search/cs?searchtype=author&query=Demberg%2C+V)

> In this paper, we aim to address the challenges surrounding the translation of ancient Chinese text: (1) The linguistic gap due to the difference in eras results in translations that are poor in quality, and (2) most translations are missing the contextual information that is often very crucial to understanding the text. To this end, we improve upon past translation techniques by proposing the following: We reframe the task as a multi-label prediction task where the model predicts both the translation and its particular era. We observe that this helps to bridge the linguistic gap as chronological context is also used as auxiliary information. % As a natural step of generalization, we pivot on the modern Chinese translations to generate multilingual outputs. %We show experimentally the efficacy of our framework in producing quality translation outputs and also validate our framework on a collected task-specific parallel corpus. We validate our framework on a parallel corpus annotated with chronology information and show experimentally its efficacy in producing quality translation outputs. We release both the code and the data [this https URL](https://github.com/orina1123/time-aware-ancient-text-translation) for future research.

| Comments: | Accepted at LChange at ACL 2021                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.03179](https://arxiv.org/abs/2107.03179) [cs.CL]** |
|           | (or **[arXiv:2107.03179v1](https://arxiv.org/abs/2107.03179v1) [cs.CL]** for this version) |





# 2021-07-07

[Return to Index](#Index)



<h2 id="2021-07-07-1">1. Long-Short Transformer: Efficient Transformers for Language and Vision
</h2>

Title: [Long-Short Transformer: Efficient Transformers for Language and Vision](https://arxiv.org/abs/2107.02192)

Authors: [Chen Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Wei Ping](https://arxiv.org/search/cs?searchtype=author&query=Ping%2C+W), [Chaowei Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C), [Mohammad Shoeybi](https://arxiv.org/search/cs?searchtype=author&query=Shoeybi%2C+M), [Tom Goldstein](https://arxiv.org/search/cs?searchtype=author&query=Goldstein%2C+T), [Anima Anandkumar](https://arxiv.org/search/cs?searchtype=author&query=Anandkumar%2C+A), [Bryan Catanzaro](https://arxiv.org/search/cs?searchtype=author&query=Catanzaro%2C+B)

> Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3 as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results~(e.g., Top-1 accuracy 84.1% trained on 224224 ImageNet-1K only), while being more scalable on high-resolution images. The models and source code will be released soon.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02192](https://arxiv.org/abs/2107.02192) [cs.CV]** |
|           | (or **[arXiv:2107.02192v1](https://arxiv.org/abs/2107.02192v1) [cs.CV]** for this version) |





<h2 id="2021-07-07-2">2. Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering
</h2>

Title: [Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering](https://arxiv.org/abs/2107.02331)

Authors: [Siddharth Karamcheti](https://arxiv.org/search/cs?searchtype=author&query=Karamcheti%2C+S), [Ranjay Krishna](https://arxiv.org/search/cs?searchtype=author&query=Krishna%2C+R), [Li Fei-Fei](https://arxiv.org/search/cs?searchtype=author&query=Fei-Fei%2C+L), [Christopher D. Manning](https://arxiv.org/search/cs?searchtype=author&query=Manning%2C+C+D)

> Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers -- groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.

| Comments: | Accepted at ACL-IJCNLP 2021. 17 pages, 16 Figures            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.02331](https://arxiv.org/abs/2107.02331) [cs.CL]** |
|           | (or **[arXiv:2107.02331v1](https://arxiv.org/abs/2107.02331v1) [cs.CL]** for this version) |





<h2 id="2021-07-07-3">3. An NLG pipeline for a legal expert system: a work in progress
</h2>

Title: [An NLG pipeline for a legal expert system: a work in progress](https://arxiv.org/abs/2107.02421)

Authors: [Inari Listenmaa](https://arxiv.org/search/cs?searchtype=author&query=Listenmaa%2C+I), [Jason Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+J), [Alfred Ang](https://arxiv.org/search/cs?searchtype=author&query=Ang%2C+A), [Maryam Hanafiah](https://arxiv.org/search/cs?searchtype=author&query=Hanafiah%2C+M), [Regina Cheong](https://arxiv.org/search/cs?searchtype=author&query=Cheong%2C+R)

> We present the NLG component for L4, a prototype domain-specific language (DSL) for drafting laws and contracts. As a concrete use case, we describe a pipeline for a legal expert system created from L4 code. The NLG component is used in two steps. The first step is to create an interview, whose answers are processed into a query for an automated reasoner. The second step is to render the answers of the reasoner in natural language.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02421](https://arxiv.org/abs/2107.02421) [cs.CL]** |
|           | (or **[arXiv:2107.02421v1](https://arxiv.org/abs/2107.02421v1) [cs.CL]** for this version) |





<h2 id="2021-07-07-4">4. The NiuTrans End-to-End Speech Translation System \\for IWSLT 2021 Offline Task
</h2>

Title: [The NiuTrans End-to-End Speech Translation System \\for IWSLT 2021 Offline Task](https://arxiv.org/abs/2107.02444)

Authors: [Chen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Xiaoqian Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Xiaowen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Laohu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Canan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> This paper describes the submission of the NiuTrans end-to-end speech translation system for the IWSLT 2021 offline task, which translates from the English audio to German text directly without intermediate transcription. We use the Transformer-based model architecture and enhance it by Conformer, relative position encoding, and stacked acoustic and textual encoding. To augment the training data, the English transcriptions are translated to German translations. Finally, we employ ensemble decoding to integrate the predictions from several models trained with the different datasets. Combining these techniques, we achieve 33.84 BLEU points on the MuST-C En-De test set, which shows the enormous potential of the end-to-end model.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.02444](https://arxiv.org/abs/2107.02444) [cs.CL]** |
|           | (or **[arXiv:2107.02444v1](https://arxiv.org/abs/2107.02444v1) [cs.CL]** for this version) |





<h2 id="2021-07-07-5">5. VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer
</h2>

Title: [VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer](https://arxiv.org/abs/2107.02681)

Authors: [Zineng Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Z), [Jaemin Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+J), [Hao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Since visual perception can give rich information beyond text descriptions for world understanding, there has been increasing interest in leveraging visual grounding for language learning. Recently, vokenization has attracted attention by using the predictions of a text-to-image retrieval model as labels for language model supervision. Despite its success, the method suffers from approximation error of using finite image labels and the lack of vocabulary diversity of a small image-text dataset. To overcome these limitations, we present VidLanKD, a video-language knowledge distillation method for improving language understanding. We train a multi-modal teacher model on a video-text dataset, and then transfer its knowledge to a student language model with a text dataset. To avoid approximation error, we propose to use different knowledge distillation objectives. In addition, the use of a large-scale video-text dataset helps learn diverse and richer vocabularies. In our experiments, VidLanKD achieves consistent improvements over text-only language models and vokenization models, on several downstream language understanding tasks including GLUE, SQuAD, and SWAG. We also demonstrate the improved world knowledge, physical reasoning, and temporal reasoning capabilities of our model by evaluating on the GLUE-diagnostics, PIQA, and TRACIE datasets. Lastly, we present comprehensive ablation studies as well as visualizations of the learned text-to-video grounding results of our teacher and student language models. Our code and models are available at: [this https URL](https://github.com/zinengtang/VidLanKD)

| Comments: | 18 pages (5 figures, 10 tables)                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.02681](https://arxiv.org/abs/2107.02681) [cs.CL]** |
|           | (or **[arXiv:2107.02681v1](https://arxiv.org/abs/2107.02681v1) [cs.CL]** for this version) |










# 2021-07-06

[Return to Index](#Index)



<h2 id="2021-07-06-1">1. Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition
</h2>

Title: [Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition](https://arxiv.org/abs/2107.01569)

Authors: [Tomohiro Tanaka](https://arxiv.org/search/cs?searchtype=author&query=Tanaka%2C+T), [Ryo Masumura](https://arxiv.org/search/cs?searchtype=author&query=Masumura%2C+R), [Mana Ihori](https://arxiv.org/search/cs?searchtype=author&query=Ihori%2C+M), [Akihiko Takashima](https://arxiv.org/search/cs?searchtype=author&query=Takashima%2C+A), [Takafumi Moriya](https://arxiv.org/search/cs?searchtype=author&query=Moriya%2C+T), [Takanori Ashihara](https://arxiv.org/search/cs?searchtype=author&query=Ashihara%2C+T), [Shota Orihashi](https://arxiv.org/search/cs?searchtype=author&query=Orihashi%2C+S), [Naoki Makishima](https://arxiv.org/search/cs?searchtype=author&query=Makishima%2C+N)

> We propose a cross-modal transformer-based neural correction models that refines the output of an automatic speech recognition (ASR) system so as to exclude ASR errors. Generally, neural correction models are composed of encoder-decoder networks, which can directly model sequence-to-sequence mapping problems. The most successful method is to use both input speech and its ASR output text as the input contexts for the encoder-decoder networks. However, the conventional method cannot take into account the relationships between these two different modal inputs because the input contexts are separately encoded for each modal. To effectively leverage the correlated information between the two different modal inputs, our proposed models encode two different contexts jointly on the basis of cross-modal self-attention using a transformer. We expect that cross-modal self-attention can effectively capture the relationships between two different modals for refining ASR hypotheses. We also introduce a shallow fusion technique to efficiently integrate the first-pass ASR model and our proposed neural correction model. Experiments on Japanese natural language ASR tasks demonstrated that our proposed models achieve better ASR performance than conventional neural correction models.

| Comments: | Accepted to Interspeech 2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.01569](https://arxiv.org/abs/2107.01569) [cs.CL]** |
|           | (or **[arXiv:2107.01569v1](https://arxiv.org/abs/2107.01569v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-2">2. IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task
</h2>

Title: [IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task](https://arxiv.org/abs/2107.01656)

Authors: [Baban Gain](https://arxiv.org/search/cs?searchtype=author&query=Gain%2C+B), [Dibyanayan Bandyopadhyay](https://arxiv.org/search/cs?searchtype=author&query=Bandyopadhyay%2C+D), [Asif Ekbal](https://arxiv.org/search/cs?searchtype=author&query=Ekbal%2C+A)

> Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the translation by removing ambiguity on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT - 2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU points for Evaluation and Challenge subset, respectively.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.01656](https://arxiv.org/abs/2107.01656) [cs.CL]** |
|           | (or **[arXiv:2107.01656v1](https://arxiv.org/abs/2107.01656v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-3">3. Packing: Towards 2x NLP BERT Acceleration
</h2>

Title: [Packing: Towards 2x NLP BERT Acceleration](https://arxiv.org/abs/2107.02027)

Authors: [Matej Kosec](https://arxiv.org/search/cs?searchtype=author&query=Kosec%2C+M), [Sheng Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+S), [Mario Michael Krell](https://arxiv.org/search/cs?searchtype=author&query=Krell%2C+M+M)

> We find that at sequence length 512 padding tokens represent in excess of 50% of the Wikipedia dataset used for pretraining BERT (Bidirectional Encoder Representations from Transformers). Therefore by removing all padding we achieve a 2x speed-up in terms of sequences/sec. To exploit this characteristic of the dataset, we develop and contrast two deterministic packing algorithms. Both algorithms rely on the assumption that sequences are interchangeable and therefore packing can be performed on the histogram of sequence lengths, rather than per sample. This transformation of the problem leads to algorithms which are fast and have linear complexity in dataset size. The shortest-pack-first histogram-packing (SPFHP) algorithm determines the packing order for the Wikipedia dataset of over 16M sequences in 0.02 seconds. The non-negative least-squares histogram-packing (NNLSHP) algorithm converges in 28.4 seconds but produces solutions which are more depth efficient, managing to get near optimal packing by combining a maximum of 3 sequences in one sample. Using the dataset with multiple sequences per sample requires additional masking in the attention layer and a modification of the MLM loss function. We demonstrate that both of these changes are straightforward to implement and have relatively little impact on the achievable performance gain on modern hardware. Finally, we pretrain BERT-Large using the packed dataset, demonstrating no loss of convergence and the desired 2x speed-up.

| Subjects:    | **Computation and Language (cs.CL)**; Computational Complexity (cs.CC); Information Theory (cs.IT); Machine Learning (cs.LG) |
| ------------ | ------------------------------------------------------------ |
| MSC classes: | 05-08                                                        |
| ACM classes: | I.2.7; G.2.1                                                 |
| Cite as:     | **[arXiv:2107.02027](https://arxiv.org/abs/2107.02027) [cs.CL]** |
|              | (or **[arXiv:2107.02027v1](https://arxiv.org/abs/2107.02027v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-4">4. Power Law Graph Transformer for Machine Translation and Representation Learning
</h2>

Title: [Power Law Graph Transformer for Machine Translation and Representation Learning](https://arxiv.org/abs/2107.02039)

Authors: [Burc Gokden](https://arxiv.org/search/cs?searchtype=author&query=Gokden%2C+B)

> We present the Power Law Graph Transformer, a transformer model with well defined deductive and inductive tasks for prediction and representation learning. The deductive task learns the dataset level (global) and instance level (local) graph structures in terms of learnable power law distribution parameters. The inductive task outputs the prediction probabilities using the deductive task output, similar to a transductive model. We trained our model with Turkish-English and Portuguese-English datasets from TED talk transcripts for machine translation and compared the model performance and characteristics to a transformer model with scaled dot product attention trained on the same experimental setup. We report BLEU scores of 17.79 and 28.33 on the Turkish-English and Portuguese-English translation tasks with our model, respectively. We also show how a duality between a quantization set and N-dimensional manifold representation can be leveraged to transform between local and global deductive-inductive outputs using successive application of linear and non-linear transformations end-to-end.

| Comments: | 55 pages, 39 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.02039](https://arxiv.org/abs/2107.02039) [cs.CL]** |
|           | (or **[arXiv:2107.02039v1](https://arxiv.org/abs/2107.02039v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-5">5. ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
</h2>

Title: [ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2107.02137)

Authors: [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Shuohuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Shikun Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S), [Siyu Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+S), [Chao Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+C), [Junyuan Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+J), [Jiaxiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Xuyi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Yanbin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Yuxiang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Weixin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W), [Zhihua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Weibao Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+W), [Jianzhong Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+J), [Zhizhou Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+Z), [Peng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+P), [Wei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W), [Xuan Ouyang](https://arxiv.org/search/cs?searchtype=author&query=Ouyang%2C+X), [Dianhai Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D), [Hao Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+H), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02137](https://arxiv.org/abs/2107.02137) [cs.CL]** |
|           | (or **[arXiv:2107.02137v1](https://arxiv.org/abs/2107.02137v1) [cs.CL]** for this version) |








# 2021-07-05

[Return to Index](#Index)



<h2 id="2021-07-05-1">1. Transformer-F: A Transformer network with effective methods for learning universal sentence representation
</h2>

Title: [Transformer-F: A Transformer network with effective methods for learning universal sentence representation](https://arxiv.org/abs/2107.00653)

Authors: [Yu Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y)

> The Transformer model is widely used in natural language processing for sentence representation. However, the previous Transformer-based models focus on function words that have limited meaning in most cases and could merely extract high-level semantic abstraction features. In this paper, two approaches are introduced to improve the performance of Transformers. We calculated the attention score by multiplying the part-of-speech weight vector with the correlation coefficient, which helps extract the words with more practical meaning. The weight vector is obtained by the input text sequence based on the importance of the part-of-speech. Furthermore, we fuse the features of each layer to make the sentence representation results more comprehensive and accurate. In experiments, we demonstrate the effectiveness of our model Transformer-F on three standard text classification datasets. Experimental results show that our proposed model significantly boosts the performance of text classification as compared to the baseline model. Specifically, we obtain a 5.28% relative improvement over the vanilla Transformer on the simple tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00653](https://arxiv.org/abs/2107.00653) [cs.CL]** |
|           | (or **[arXiv:2107.00653v1](https://arxiv.org/abs/2107.00653v1) [cs.CL]** for this version) |





<h2 id="2021-07-05-2">2. A Primer on Pretrained Multilingual Language Models
</h2>

Title: [A Primer on Pretrained Multilingual Language Models](https://arxiv.org/abs/2107.00676)

Authors: [Sumanth Doddapaneni](https://arxiv.org/search/cs?searchtype=author&query=Doddapaneni%2C+S), [Gowtham Ramesh](https://arxiv.org/search/cs?searchtype=author&query=Ramesh%2C+G), [Anoop Kunchukuttan](https://arxiv.org/search/cs?searchtype=author&query=Kunchukuttan%2C+A), [Pratyush Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+P), [Mitesh M. Khapra](https://arxiv.org/search/cs?searchtype=author&query=Khapra%2C+M+M)

> Multilingual Language Models (MLLMs) such as mBERT, XLM, XLM-R, \textit{etc.} have emerged as a viable option for bringing the power of pretraining to a large number of languages. Given their success in zero shot transfer learning, there has emerged a large body of work in (i) building bigger MLLMs covering a large number of languages (ii) creating exhaustive benchmarks covering a wider variety of tasks and languages for evaluating MLLMs (iii) analysing the performance of MLLMs on monolingual, zero shot crosslingual and bilingual tasks (iv) understanding the universal language patterns (if any) learnt by MLLMs and (v) augmenting the (often) limited capacity of MLLMs to improve their performance on seen or even unseen languages. In this survey, we review the existing literature covering the above broad areas of research pertaining to MLLMs. Based on our survey, we recommend some promising directions of future research.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00676](https://arxiv.org/abs/2107.00676) [cs.CL]** |
|           | (or **[arXiv:2107.00676v1](https://arxiv.org/abs/2107.00676v1) [cs.CL]** for this version) |







<h2 id="2021-07-05-3">3. Interactive decoding of words from visual speech recognition models
</h2>

Title: [Interactive decoding of words from visual speech recognition models](https://arxiv.org/abs/2107.00692)

Authors: [Brendan Shillingford](https://arxiv.org/search/cs?searchtype=author&query=Shillingford%2C+B), [Yannis Assael](https://arxiv.org/search/cs?searchtype=author&query=Assael%2C+Y), [Misha Denil](https://arxiv.org/search/cs?searchtype=author&query=Denil%2C+M)

> This work describes an interactive decoding method to improve the performance of visual speech recognition systems using user input to compensate for the inherent ambiguity of the task. Unlike most phoneme-to-word decoding pipelines, which produce phonemes and feed these through a finite state transducer, our method instead expands words in lockstep, facilitating the insertion of interaction points at each word position. Interaction points enable us to solicit input during decoding, allowing users to interactively direct the decoding process. We simulate the behavior of user input using an oracle to give an automated evaluation, and show promise for the use of this method for text input.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.00692](https://arxiv.org/abs/2107.00692) [cs.CL]** |
|           | (or **[arXiv:2107.00692v1](https://arxiv.org/abs/2107.00692v1) [cs.CL]** for this version) |







<h2 id="2021-07-05-4">4. Data Centric Domain Adaptation for Historical Text with OCR Errors
</h2>

Title: [Data Centric Domain Adaptation for Historical Text with OCR Errors](https://arxiv.org/abs/2107.00927)

Authors: [Luisa Mrz](https://arxiv.org/search/cs?searchtype=author&query=Mrz%2C+L), [Stefan Schweter](https://arxiv.org/search/cs?searchtype=author&query=Schweter%2C+S), [Nina Poerner](https://arxiv.org/search/cs?searchtype=author&query=Poerner%2C+N), [Benjamin Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+B), [Hinrich Schtze](https://arxiv.org/search/cs?searchtype=author&query=Schtze%2C+H)

> We propose new methods for in-domain and cross-domain Named Entity Recognition (NER) on historical data for Dutch and French. For the cross-domain case, we address domain shift by integrating unsupervised in-domain data via contextualized string embeddings; and OCR errors by injecting synthetic OCR errors into the source domain and address data centric domain adaptation. We propose a general approach to imitate OCR errors in arbitrary input data. Our cross-domain as well as our in-domain results outperform several strong baselines and establish state-of-the-art results. We publish preprocessed versions of the French and Dutch Europeana NER corpora.

| Comments: | 14 pages, 2 figures, 6 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.00927](https://arxiv.org/abs/2107.00927) [cs.CL]** |
|           | (or **[arXiv:2107.00927v1](https://arxiv.org/abs/2107.00927v1) [cs.CL]** for this version) |







# 2021-07-02

[Return to Index](#Index)



<h2 id="2021-07-02-1">1. GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph
</h2>

Title: [GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph](https://arxiv.org/abs/2107.00395)

Authors: [Yunxin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yu Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Baotian Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+B), [Qingcai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Yang Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+Y), [Xiaolong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yuxin Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Y), [Lin Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+L)

> Previous works indicate that the glyph of Chinese characters contains rich semantic information and has the potential to enhance the representation of Chinese characters. The typical method to utilize the glyph features is by incorporating them into the character embedding space. Inspired by previous methods, we innovatively propose a Chinese pre-trained representation model named as GlyphCRM, which abandons the ID-based character embedding method yet solely based on sequential character images. We render each character into a binary grayscale image and design two-channel position feature maps for it. Formally, we first design a two-layer residual convolutional neural network, namely HanGlyph to generate the initial glyph representation of Chinese characters, and subsequently adopt multiple bidirectional encoder Transformer blocks as the superstructure to capture the context-sensitive information. Meanwhile, we feed the glyph features extracted from each layer of the HanGlyph module into the underlying Transformer blocks by skip-connection method to fully exploit the glyph features of Chinese characters. As the HanGlyph module can obtain a sufficient glyph representation of any Chinese character, the long-standing out-of-vocabulary problem could be effectively solved. Extensive experimental results indicate that GlyphCRM substantially outperforms the previous BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has strong transferability and generalization on specialized fields and low-resource tasks. We hope this work could spark further research beyond the realms of well-established representation of Chinese texts.

| Comments: | 11 pages, 7 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2107.00395](https://arxiv.org/abs/2107.00395) [cs.AI]** |
|           | (or **[arXiv:2107.00395v1](https://arxiv.org/abs/2107.00395v1) [cs.AI]** for this version) |





<h2 id="2021-07-02-2">2. ESPnet-ST IWSLT 2021 Offline Speech Translation System
</h2>

Title: [ESPnet-ST IWSLT 2021 Offline Speech Translation System](https://arxiv.org/abs/2107.00636)

Authors: [Hirofumi Inaguma](https://arxiv.org/search/eess?searchtype=author&query=Inaguma%2C+H), [Brian Yan](https://arxiv.org/search/eess?searchtype=author&query=Yan%2C+B), [Siddharth Dalmia](https://arxiv.org/search/eess?searchtype=author&query=Dalmia%2C+S), [Pengcheng Gu](https://arxiv.org/search/eess?searchtype=author&query=Gu%2C+P), [Jiatong Shi](https://arxiv.org/search/eess?searchtype=author&query=Shi%2C+J), [Kevin Duh](https://arxiv.org/search/eess?searchtype=author&query=Duh%2C+K), [Shinji Watanabe](https://arxiv.org/search/eess?searchtype=author&query=Watanabe%2C+S)

> This paper describes the ESPnet-ST group's IWSLT 2021 submission in the offline speech translation track. This year we made various efforts on training data, architecture, and audio segmentation. On the data side, we investigated sequence-level knowledge distillation (SeqKD) for end-to-end (E2E) speech translation. Specifically, we used multi-referenced SeqKD from multiple teachers trained on different amounts of bitext. On the architecture side, we adopted the Conformer encoder and the Multi-Decoder architecture, which equips dedicated decoders for speech recognition and translation tasks in a unified encoder-decoder model and enables search in both source and target language spaces during inference. We also significantly improved audio segmentation by using the pyannote.audio toolkit and merging multiple short segments for long context modeling. Experimental evaluations showed that each of them contributed to large improvements in translation performance. Our best E2E system combined all the above techniques with model ensembling and achieved 31.4 BLEU on the 2-ref of tst2021 and 21.2 BLEU and 19.3 BLEU on the two single references of tst2021.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2107.00636](https://arxiv.org/abs/2107.00636) [eess.AS]** |
|           | (or **[arXiv:2107.00636v1](https://arxiv.org/abs/2107.00636v1) [eess.AS]** for this version) |





<h2 id="2021-07-02-3">3. Word-Free Spoken Language Understanding for Mandarin-Chinese
</h2>

Title: [Word-Free Spoken Language Understanding for Mandarin-Chinese](https://arxiv.org/abs/2107.00186)

Authors: [Zhiyuan Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Z), [Yuexin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Guo Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Xingyu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Akshat Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A)

> Spoken dialogue systems such as Siri and Alexa provide great convenience to people's everyday life. However, current spoken language understanding (SLU) pipelines largely depend on automatic speech recognition (ASR) modules, which require a large amount of language-specific training data. In this paper, we propose a Transformer-based SLU system that works directly on phones. This acoustic-based SLU system consists of only two blocks and does not require the presence of ASR module. The first block is a universal phone recognition system, and the second block is a Transformer-based language model for phones. We verify the effectiveness of the system on an intent classification dataset in Mandarin Chinese.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00186](https://arxiv.org/abs/2107.00186) [cs.CL]** |
|           | (or **[arXiv:2107.00186v1](https://arxiv.org/abs/2107.00186v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-4">4. The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021 </h2>



Title: [The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021]()

Authors: [Dan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Mengge Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+M), [Xiaoxi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Yuchen Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Lirong Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+L)

> This paper describes USTC-NELSLIP's submissions to the IWSLT2021 Simultaneous Speech Translation task. We proposed a novel simultaneous translation model, Cross Attention Augmented Transducer (CAAT), which extends conventional RNN-T to sequence-to-sequence tasks without monotonic constraints, e.g., simultaneous translation. Experiments on speech-to-text (S2T) and text-to-text (T2T) simultaneous translation tasks shows CAAT achieves better quality-latency trade-offs compared to \textit{wait-k}, one of the previous state-of-the-art approaches. Based on CAAT architecture and data augmentation, we build S2T and T2T simultaneous translation systems in this evaluation campaign. Compared to last year's optimal systems, our S2T simultaneous translation system improves by an average of 11.3 BLEU for all latency regimes, and our T2T simultaneous translation system improves by an average of 4.6 BLEU.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00279](https://arxiv.org/abs/2107.00279) [cs.CL]** |
|           | (or **[arXiv:2107.00279v1](https://arxiv.org/abs/2107.00279v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-5">5. Zero-pronoun Data Augmentation for Japanese-to-English Translation
</h2>

Title: [Zero-pronoun Data Augmentation for Japanese-to-English Translation](https://arxiv.org/abs/2107.00318)

Authors: [Ryokan Ri](https://arxiv.org/search/cs?searchtype=author&query=Ri%2C+R), [Toshiaki Nakazawa](https://arxiv.org/search/cs?searchtype=author&query=Nakazawa%2C+T), [Yoshimasa Tsuruoka](https://arxiv.org/search/cs?searchtype=author&query=Tsuruoka%2C+Y)

> For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding pronoun in the target side of the English sentence. However, although fully resolving zero pronouns often needs discourse context, in some cases, the local context within a sentence gives clues to the inference of the zero pronoun. In this study, we propose a data augmentation method that provides additional training signals for the translation model to learn correlations between local context and zero pronouns. We show that the proposed method significantly improves the accuracy of zero pronoun translation with machine translation experiments in the conversational domain.

| Comments: | WAT2021                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.00318](https://arxiv.org/abs/2107.00318) [cs.CL]** |
|           | (or **[arXiv:2107.00318v1](https://arxiv.org/abs/2107.00318v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-6">6. Modeling Target-side Inflection in Placeholder Translation
</h2>

Title: [Modeling Target-side Inflection in Placeholder Translation](https://arxiv.org/abs/2107.00334)

Authors: [Ryokan Ri](https://arxiv.org/search/cs?searchtype=author&query=Ri%2C+R), [Toshiaki Nakazawa](https://arxiv.org/search/cs?searchtype=author&query=Nakazawa%2C+T), [Yoshimasa Tsuruoka](https://arxiv.org/search/cs?searchtype=author&query=Tsuruoka%2C+Y)

> Placeholder translation systems enable the users to specify how a specific phrase is translated in the output sentence. The system is trained to output special placeholder tokens, and the user-specified term is injected into the output through the context-free replacement of the placeholder token. However, this approach could result in ungrammatical sentences because it is often the case that the specified term needs to be inflected according to the context of the output, which is unknown before the translation. To address this problem, we propose a novel method of placeholder translation that can inflect specified terms according to the grammatical construction of the output sentence. We extend the sequence-to-sequence architecture with a character-level decoder that takes the lemma of a user-specified term and the words generated from the word-level decoder to output the correct inflected form of the lemma. We evaluate our approach with a Japanese-to-English translation task in the scientific writing domain, and show that our model can incorporate specified terms in the correct form more successfully than other comparable models.

| Comments: | MT Summit 2021                                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.00334](https://arxiv.org/abs/2107.00334) [cs.CL]** |
|           | (or **[arXiv:2107.00334v1](https://arxiv.org/abs/2107.00334v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-7">7. CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding
</h2>

Title: [CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding](https://arxiv.org/abs/2107.00440)

Authors: [Dong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Ning Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Piji Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Hai-Tao Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H)

> Despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations. Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics. Different from the image processing field, the text is discrete and few word substitutions can cause significant semantic changes. To study the impact of semantics caused by small perturbations, we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes. To address this problem, we propose Contrastive Learning with semantIc Negative Examples (CLINE), which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking. By comparing with similar and opposite semantic examples, the model can effectively perceive the semantic changes caused by small perturbations. Empirical results show that our approach yields substantial improvements on a range of sentiment analysis, reasoning, and reading comprehension tasks. And CLINE also ensures the compactness within the same semantics and separability across different semantics in sentence-level.

| Comments: | ACL 2021, Main Conference, Long Paper                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2107.00440](https://arxiv.org/abs/2107.00440) [cs.CL]** |
|           | (or **[arXiv:2107.00440v1](https://arxiv.org/abs/2107.00440v1) [cs.CL]** for this version) |








# 2021-07-01

[Return to Index](#Index)



<h2 id="2021-07-01-1">1. What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?
</h2>

Title: [What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?](https://arxiv.org/abs/2106.15818)

Authors: [Kelly Marchisio](https://arxiv.org/search/cs?searchtype=author&query=Marchisio%2C+K), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [David Grangier](https://arxiv.org/search/cs?searchtype=author&query=Grangier%2C+D)

> Whereas existing literature on unsupervised machine translation (MT) focuses on exploiting unsupervised techniques for low-resource language pairs where bilingual training data is scare or unavailable, we investigate whether unsupervised MT can also improve translation quality of high-resource language pairs where sufficient bitext does exist. We compare the style of correct translations generated by either supervised or unsupervised MT and find that the unsupervised output is less monotonic and more natural than supervised output. We demonstrate a way to combine the benefits of unsupervised and supervised MT into a single system, resulting in better human evaluation of quality and fluency. Our results open the door to discussions about the potential contributions of unsupervised MT in high-resource settings, and how supervised and unsupervised systems might be mutually-beneficial.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.15818](https://arxiv.org/abs/2106.15818) [cs.CL]** |
|           | (or **[arXiv:2106.15818v1](https://arxiv.org/abs/2106.15818v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-2">2. Mixed Cross Entropy Loss for Neural Machine Translation
</h2>

Title: [Mixed Cross Entropy Loss for Neural Machine Translation](https://arxiv.org/abs/2106.15880)

Authors: [Haoran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Wei Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+W)

> In neural machine translation, cross entropy (CE) is the standard loss function in two training methods of auto-regressive models, i.e., teacher forcing and scheduled sampling. In this paper, we propose mixed cross entropy loss (mixed CE) as a substitute for CE in both training approaches. In teacher forcing, the model trained with CE regards the translation problem as a one-to-one mapping process, while in mixed CE this process can be relaxed to one-to-many. In scheduled sampling, we show that mixed CE has the potential to encourage the training and testing behaviours to be similar to each other, more effectively mitigating the exposure bias problem. We demonstrate the superiority of mixed CE over CE on several machine translation datasets, WMT'16 Ro-En, WMT'16 Ru-En, and WMT'14 En-De in both teacher forcing and scheduled sampling setups. Furthermore, in WMT'14 En-De, we also find mixed CE consistently outperforms CE on a multi-reference set as well as a challenging paraphrased reference set. We also found the model trained with mixed CE is able to provide a better probability distribution defined over the translation output space. Our code is available at [this https URL](https://github.com/haorannlp/mix).

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | ICML2021                                                     |
| Cite as:           | **[arXiv:2106.15880](https://arxiv.org/abs/2106.15880) [cs.CL]** |
|                    | (or **[arXiv:2106.15880v1](https://arxiv.org/abs/2106.15880v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-3">3. Cross-lingual alignments of ELMo contextual embeddings
</h2>

Title: [Cross-lingual alignments of ELMo contextual embeddings](https://arxiv.org/abs/2106.15986)

Authors: [Matej Ular](https://arxiv.org/search/cs?searchtype=author&query=Ular%2C+M), [Marko Robnik-ikonja](https://arxiv.org/search/cs?searchtype=author&query=Robnik-ikonja%2C+M)

> Building machine learning prediction models for a specific NLP task requires sufficient training data, which can be difficult to obtain for low-resource languages. Cross-lingual embeddings map word embeddings from a low-resource language to a high-resource language so that a prediction model trained on data from the high-resource language can also be used in the low-resource language. To produce cross-lingual mappings of recent contextual embeddings, anchor points between the embedding spaces have to be words in the same context. We address this issue with a new method for creating datasets for cross-lingual contextual alignments. Based on that, we propose novel cross-lingual mapping methods for ELMo embeddings. Our linear mapping methods use existing vecmap and MUSE alignments on contextual ELMo embeddings. Our new nonlinear ELMoGAN mapping method is based on GANs and does not assume isomorphic embedding spaces. We evaluate the proposed mapping methods on nine languages, using two downstream tasks, NER and dependency parsing. The ELMoGAN method performs well on the NER task, with low cross-lingual loss compared to direct training on some languages. In the dependency parsing, linear alignment variants are more successful.

| Comments: | 26 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.15986](https://arxiv.org/abs/2106.15986) [cs.CL]** |
|           | (or **[arXiv:2106.15986v1](https://arxiv.org/abs/2106.15986v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-4">4. ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information
</h2>

Title: [ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information](https://arxiv.org/abs/2106.16038)

Authors: [Zijun Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Xiang Ao](https://arxiv.org/search/cs?searchtype=author&query=Ao%2C+X), [Qing He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Q), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding. In this work, we propose ChineseBERT, which incorporates both the {\it glyph} and {\it pinyin} information of Chinese characters into language model pretraining. The glyph embedding is obtained based on different fonts of a Chinese character, being able to capture character semantics from the visual features, and the pinyin embedding characterizes the pronunciation of Chinese characters, which handles the highly prevalent heteronym phenomenon in Chinese (the same character has different pronunciations with different meanings). Pretrained on large-scale unlabeled Chinese corpus, the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps. The porpsoed model achieves new SOTA performances on a wide range of Chinese NLP tasks, including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition. Code and pretrained models are publicly available at [this https URL](https://github.com/ShannonAI/ChineseBert).

| Comments: | To appear at ACL2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.16038](https://arxiv.org/abs/2106.16038) [cs.CL]** |
|           | (or **[arXiv:2106.16038v1](https://arxiv.org/abs/2106.16038v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-5">5. IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task
</h2>

Title: [IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task](https://arxiv.org/abs/2106.16055)

Authors: [Pavel Denisov](https://arxiv.org/search/cs?searchtype=author&query=Denisov%2C+P), [Manuel Mager](https://arxiv.org/search/cs?searchtype=author&query=Mager%2C+M), [Ngoc Thang Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+N+T)

> This paper describes the submission to the IWSLT 2021 Low-Resource Speech Translation Shared Task by IMS team. We utilize state-of-the-art models combined with several data augmentation, multi-task and transfer learning approaches for the automatic speech recognition (ASR) and machine translation (MT) steps of our cascaded system. Moreover, we also explore the feasibility of a full end-to-end speech translation (ST) model in the case of very constrained amount of ground truth labeled data. Our best system achieves the best performance among all submitted systems for Congolese Swahili to English and French with BLEU scores 7.7 and 13.7 respectively, and the second best result for Coastal Swahili to English with BLEU score 14.9.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2106.16055](https://arxiv.org/abs/2106.16055) [cs.CL]** |
|           | (or **[arXiv:2106.16055v1](https://arxiv.org/abs/2106.16055v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-6">6. XLM-E: Cross-lingual Language Model Pre-training via ELECTRA
</h2>

Title: [XLM-E: Cross-lingual Language Model Pre-training via ELECTRA](https://arxiv.org/abs/2106.16138)

Authors: [Zewen Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+Z), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Saksham Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal%2C+S), [Payal Bajaj](https://arxiv.org/search/cs?searchtype=author&query=Bajaj%2C+P), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> In this paper, we introduce ELECTRA-style tasks to cross-lingual language model pre-training. Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora. Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost. Moreover, analysis shows that XLM-E tends to obtain better cross-lingual transferability.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.16138](https://arxiv.org/abs/2106.16138) [cs.CL]** |
|           | (or **[arXiv:2106.16138v1](https://arxiv.org/abs/2106.16138v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-7">7. On the Power of Saturated Transformers: A View from Circuit Complexity
</h2>

Title: [On the Power of Saturated Transformers: A View from Circuit Complexity](https://arxiv.org/abs/2106.16213)

Authors: [William Merrill](https://arxiv.org/search/cs?searchtype=author&query=Merrill%2C+W), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y), [Roy Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+R), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> Transformers have become a standard architecture for many NLP problems. This has motivated theoretically analyzing their capabilities as models of language, in order to understand what makes them successful, and what their potential weaknesses might be. Recent work has shown that transformers with hard attention are quite limited in capacity, and in fact can be simulated by constant-depth circuits. However, hard attention is a restrictive assumption, which may complicate the relevance of these results for practical transformers. In this work, we analyze the circuit complexity of transformers with saturated attention: a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We show that saturated transformers transcend the limitations of hard-attention transformers. With some minor assumptions, we prove that the number of bits needed to represent a saturated transformer memory vector is O(logn), which implies saturated transformers can be simulated by log-depth circuits. Thus, the jump from hard to saturated attention can be understood as increasing the transformer's effective circuit depth by a factor of O(logn).

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computational Complexity (cs.CC); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.16213](https://arxiv.org/abs/2106.16213) [cs.CL]** |
|           | (or **[arXiv:2106.16213v1](https://arxiv.org/abs/2106.16213v1) [cs.CL]** for this version) |




# Daily arXiv: Machine Translation - August, 2021

# Index


- [2021-08-04](#2021-08-04)

  - [1. Knowledge-intensive Language Understanding for Explainable AI](#2021-08-04-1)
  - [2. Underreporting of errors in NLG output, and what to do about it](#2021-08-04-2)
  - [3. A Dynamic Head Importance Computation Mechanism for Neural Machine Translation](#2021-08-04-3)
- [2021-08-03](#2021-08-03)

  - [1. Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding](#2021-08-03-1)
  - [2. StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators](#2021-08-03-2)
  - [3. Structural Guidance for Transformer Language Models](#2021-08-03-3)
  - [4. LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization](#2021-08-03-4)
- [2021-08-02](#2021-08-02)

  - [1. Difficulty-Aware Machine Translation Evaluation](#2021-08-02-1)
  - [2. Residual Tree Aggregation of Layers for Neural Machine Translation](#2021-08-02-2)
  - [3. Neural Variational Learning for Grounded Language Acquisition](#2021-08-02-3)
  - [4. Multi-stage Pre-training over Simplified Multimodal Pre-training Models](#2021-08-02-4)
  - [5. MDQE: A More Accurate Direct Pretraining for Machine Translation Quality Estimation](#2021-08-02-5)
  - [6. Towards Universality in Multilingual Text Rewriting](#2021-08-02-6)
  - [7. ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback](#2021-08-02-7)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-08-04

[Return to Index](#Index)



<h2 id="2021-08-04-1">1. Knowledge-intensive Language Understanding for Explainable AI
</h2>

Title: [Knowledge-intensive Language Understanding for Explainable AI](https://arxiv.org/abs/2108.01174)

Authors: [Amit Sheth](https://arxiv.org/search/cs?searchtype=author&query=Sheth%2C+A), [Manas Gaur](https://arxiv.org/search/cs?searchtype=author&query=Gaur%2C+M), [Kaushik Roy](https://arxiv.org/search/cs?searchtype=author&query=Roy%2C+K), [Keyur Faldu](https://arxiv.org/search/cs?searchtype=author&query=Faldu%2C+K)

> AI systems have seen significant adoption in various domains. At the same time, further adoption in some domains is hindered by inability to fully trust an AI system that it will not harm a human. Besides the concerns for fairness, privacy, transparency, and explainability are key to developing trusts in AI systems. As stated in describing trustworthy AI "Trust comes through understanding. How AI-led decisions are made and what determining factors were included are crucial to understand." The subarea of explaining AI systems has come to be known as XAI. Multiple aspects of an AI system can be explained; these include biases that the data might have, lack of data points in a particular region of the example space, fairness of gathering the data, feature importances, etc. However, besides these, it is critical to have human-centered explanations that are directly related to decision-making similar to how a domain expert makes decisions based on "domain knowledge," that also include well-established, peer-validated explicit guidelines. To understand and validate an AI system's outcomes (such as classification, recommendations, predictions), that lead to developing trust in the AI system, it is necessary to involve explicit domain knowledge that humans understand and use.

| Comments: | To appear in IEEE Internet Computing, September/October 2021 Issue |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.01174](https://arxiv.org/abs/2108.01174) [cs.AI]** |
|           | (or **[arXiv:2108.01174v1](https://arxiv.org/abs/2108.01174v1) [cs.AI]** for this version) |





<h2 id="2021-08-04-2">2. Underreporting of errors in NLG output, and what to do about it
</h2>

Title: [Underreporting of errors in NLG output, and what to do about it](https://arxiv.org/abs/2108.01182)

Authors: [Emiel van Miltenburg](https://arxiv.org/search/cs?searchtype=author&query=van+Miltenburg%2C+E), [Miruna-Adriana Clinciu](https://arxiv.org/search/cs?searchtype=author&query=Clinciu%2C+M), [Ondřej Dušek](https://arxiv.org/search/cs?searchtype=author&query=Dušek%2C+O), [Dimitra Gkatzia](https://arxiv.org/search/cs?searchtype=author&query=Gkatzia%2C+D), [Stephanie Inglis](https://arxiv.org/search/cs?searchtype=author&query=Inglis%2C+S), [Leo Leppänen](https://arxiv.org/search/cs?searchtype=author&query=Leppänen%2C+L), [Saad Mahamood](https://arxiv.org/search/cs?searchtype=author&query=Mahamood%2C+S), [Emma Manning](https://arxiv.org/search/cs?searchtype=author&query=Manning%2C+E), [Stephanie Schoch](https://arxiv.org/search/cs?searchtype=author&query=Schoch%2C+S), [Craig Thomson](https://arxiv.org/search/cs?searchtype=author&query=Thomson%2C+C), [Luou Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+L)

> We observe a severe under-reporting of the different kinds of errors that Natural Language Generation systems make. This is a problem, because mistakes are an important indicator of where systems should still be improved. If authors only report overall performance metrics, the research community is left in the dark about the specific weaknesses that are exhibited by `state-of-the-art' research. Next to quantifying the extent of error under-reporting, this position paper provides recommendations for error identification, analysis and reporting.

| Comments: | Prefinal version, accepted for publication in the Proceedings of the 14th International Conference on Natural Language Generation (INLG 2021, Aberdeen). Comments welcome |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.01182](https://arxiv.org/abs/2108.01182) [cs.CL]** |
|           | (or **[arXiv:2108.01182v1](https://arxiv.org/abs/2108.01182v1) [cs.CL]** for this version) |





<h2 id="2021-08-04-3">3. A Dynamic Head Importance Computation Mechanism for Neural Machine Translation
</h2>

Title: [A Dynamic Head Importance Computation Mechanism for Neural Machine Translation](https://arxiv.org/abs/2108.01377)

Authors: [Akshay Goindani](https://arxiv.org/search/cs?searchtype=author&query=Goindani%2C+A), [Manish Shrivastava](https://arxiv.org/search/cs?searchtype=author&query=Shrivastava%2C+M)

> Multiple parallel attention mechanisms that use multiple attention heads facilitate greater performance of the Transformer model for various applications e.g., Neural Machine Translation (NMT), text classification. In multi-head attention mechanism, different heads attend to different parts of the input. However, the limitation is that multiple heads might attend to the same part of the input, resulting in multiple heads being redundant. Thus, the model resources are under-utilized. One approach to avoid this is to prune least important heads based on certain importance score. In this work, we focus on designing a Dynamic Head Importance Computation Mechanism (DHICM) to dynamically calculate the importance of a head with respect to the input. Our insight is to design an additional attention layer together with multi-head attention, and utilize the outputs of the multi-head attention along with the input, to compute the importance for each head. Additionally, we add an extra loss function to prevent the model from assigning same score to all heads, to identify more important heads and improvise performance. We analyzed performance of DHICM for NMT with different languages. Experiments on different datasets show that DHICM outperforms traditional Transformer-based approach by large margin, especially, when less training data is available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.01377](https://arxiv.org/abs/2108.01377) [cs.CL]** |
|           | (or **[arXiv:2108.01377v1](https://arxiv.org/abs/2108.01377v1) [cs.CL]** for this version) |





# 2021-08-03

[Return to Index](#Index)



<h2 id="2021-08-03-1">1. Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding
</h2>

Title: [Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding](https://arxiv.org/abs/2108.00205)

Authors: [Heng Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Joey Tianyi Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J+T), [Yew-Soon Ong](https://arxiv.org/search/cs?searchtype=author&query=Ong%2C+Y)

> Current one-stage methods for visual grounding encode the language query as one holistic sentence embedding before fusion with visual feature. Such a formulation does not treat each word of a query sentence on par when modeling language to visual attention, therefore prone to neglect words which are less important for sentence embedding but critical for visual grounding. In this paper we propose Word2Pix: a one-stage visual grounding network based on encoder-decoder transformer architecture that enables learning for textual to visual feature correspondence via word to pixel attention. The embedding of each word from the query sentence is treated alike by attending to visual pixels individually instead of single holistic sentence embedding. In this way, each word is given equivalent opportunity to adjust the language to vision attention towards the referent target through multiple stacks of transformer decoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg datasets and the proposed Word2Pix outperforms existing one-stage methods by a notable margin. The results obtained also show that Word2Pix surpasses two-stage visual grounding models, while at the same time keeping the merits of one-stage paradigm namely end-to-end training and real-time inference speed intact.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.00205](https://arxiv.org/abs/2108.00205) [cs.CV]** |
|           | (or **[arXiv:2108.00205v1](https://arxiv.org/abs/2108.00205v1) [cs.CV]** for this version) |





<h2 id="2021-08-03-2">2. StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators
</h2>

Title: [StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators](https://arxiv.org/abs/2108.00946)

Authors: [Rinon Gal](https://arxiv.org/search/cs?searchtype=author&query=Gal%2C+R), [Or Patashnik](https://arxiv.org/search/cs?searchtype=author&query=Patashnik%2C+O), [Haggai Maron](https://arxiv.org/search/cs?searchtype=author&query=Maron%2C+H), [Gal Chechik](https://arxiv.org/search/cs?searchtype=author&query=Chechik%2C+G), [Daniel Cohen-Or](https://arxiv.org/search/cs?searchtype=author&query=Cohen-Or%2C+D)

> Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained blindly? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image from those domains. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Graphics (cs.GR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2108.00946](https://arxiv.org/abs/2108.00946) [cs.CV]** |
|           | (or **[arXiv:2108.00946v1](https://arxiv.org/abs/2108.00946v1) [cs.CV]** for this version) |





<h2 id="2021-08-03-3">3. Structural Guidance for Transformer Language Models
</h2>

Title: [Structural Guidance for Transformer Language Models](https://arxiv.org/abs/2108.00104)

Authors: [Peng Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+P), [Tahira Naseem](https://arxiv.org/search/cs?searchtype=author&query=Naseem%2C+T), [Roger Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+R), [Ramón Fernandez Astudillo](https://arxiv.org/search/cs?searchtype=author&query=Astudillo%2C+R+F)

> Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The "Generative Parsing" idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The "Structural Scaffold" idea guides the language model's representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models' syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.

| Comments: | To be issued as paper revision for ACL 2021                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.00104](https://arxiv.org/abs/2108.00104) [cs.CL]** |
|           | (or **[arXiv:2108.00104v1](https://arxiv.org/abs/2108.00104v1) [cs.CL]** for this version) |





<h2 id="2021-08-03-4">4. LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization
</h2>

Title: [LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization](https://arxiv.org/abs/2108.00801)

Authors: [Weidong Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+W), [Mingjun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+M), [Lusheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Di Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+D), [Jinwen Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J), [Zhenhua Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Zhenyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Jianbo Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J)

> Language model pre-training based on large corpora has achieved tremendous success in terms of constructing enriched contextual representations and has led to significant performance gains on a diverse range of Natural Language Understanding (NLU) tasks. Despite the success, most current pre-trained language models, such as BERT, are trained based on single-grained tokenization, usually with fine-grained characters or sub-words, making it hard for them to learn the precise meaning of coarse-grained words and phrases. In this paper, we propose a simple yet effective pre-training method named LICHEE to efficiently incorporate multi-grained information of input text. Our method can be applied to various pre-trained language models and improve their representation capability. Extensive experiments conducted on CLUE and SuperGLUE demonstrate that our method achieves comprehensive improvements on a wide variety of NLU tasks in both Chinese and English with little extra inference cost incurred, and that our best ensemble model achieves the state-of-the-art performance on CLUE benchmark competition.

| Comments: | Accepted by ACL Findings 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2108.00801](https://arxiv.org/abs/2108.00801) [cs.CL]** |
|           | (or **[arXiv:2108.00801v1](https://arxiv.org/abs/2108.00801v1) [cs.CL]** for this version) |







# 2021-08-02

[Return to Index](#Index)



<h2 id="2021-08-02-1">1. Difficulty-Aware Machine Translation Evaluation
</h2>

Title: [Difficulty-Aware Machine Translation Evaluation](https://arxiv.org/abs/2107.14402)

Authors: [Runzhe Zhan](https://arxiv.org/search/cs?searchtype=author&query=Zhan%2C+R), [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S)

> The high-quality translation results produced by machine translation (MT) systems still pose a huge challenge for automatic evaluation. Current MT evaluation pays the same attention to each sentence component, while the questions of real-world examinations (e.g., university examinations) have different difficulties and weightings. In this paper, we propose a novel difficulty-aware MT evaluation metric, expanding the evaluation dimension by taking translation difficulty into consideration. A translation that fails to be predicted by most MT systems will be treated as a difficult one and assigned a large weight in the final score function, and conversely. Experimental results on the WMT19 English-German Metrics shared tasks show that our proposed method outperforms commonly used MT metrics in terms of human correlation. In particular, our proposed method performs well even when all the MT systems are very competitive, which is when most existing metrics fail to distinguish between them. The source code is freely available at [this https URL](https://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation).

| Comments: | Accepted to ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2107.14402](https://arxiv.org/abs/2107.14402) [cs.CL]** |
|           | (or **[arXiv:2107.14402v1](https://arxiv.org/abs/2107.14402v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-2">2. Residual Tree Aggregation of Layers for Neural Machine Translation
</h2>

Title: [Residual Tree Aggregation of Layers for Neural Machine Translation](https://arxiv.org/abs/2107.14590)

Authors: [GuoLiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Yiyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y)

> Although attention-based Neural Machine Translation has achieved remarkable progress in recent layers, it still suffers from issue of making insufficient use of the output of each layer. In transformer, it only uses the top layer of encoder and decoder in the subsequent process, which makes it impossible to take advantage of the useful information in other layers. To address this issue, we propose a residual tree aggregation of layers for Transformer(RTAL), which helps to fuse information across layers. Specifically, we try to fuse the information across layers by constructing a post-order binary tree. In additional to the last node, we add the residual connection to the process of generating child nodes. Our model is based on the Neural Machine Translation model Transformer and we conduct our experiments on WMT14 English-to-German and WMT17 English-to-France translation tasks. Experimental results across language pairs show that the proposed approach outperforms the strong baseline model significantly

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.14590](https://arxiv.org/abs/2107.14590) [cs.CL]** |
|           | (or **[arXiv:2107.14590v1](https://arxiv.org/abs/2107.14590v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-3">3. Neural Variational Learning for Grounded Language Acquisition
</h2>

Title: [Neural Variational Learning for Grounded Language Acquisition](https://arxiv.org/abs/2107.14593)

Authors: [Nisha Pillai](https://arxiv.org/search/cs?searchtype=author&query=Pillai%2C+N), [Cynthia Matuszek](https://arxiv.org/search/cs?searchtype=author&query=Matuszek%2C+C), [Francis Ferraro](https://arxiv.org/search/cs?searchtype=author&query=Ferraro%2C+F)

> We propose a learning system in which language is grounded in visual percepts without specific pre-defined categories of terms. We present a unified generative method to acquire a shared semantic/visual embedding that enables the learning of language about a wide range of real-world objects. We evaluate the efficacy of this learning by predicting the semantics of objects and comparing the performance with neural and non-neural inputs. We show that this generative approach exhibits promising results in language grounding without pre-specifying visual categories under low resource settings. Our experiments demonstrate that this approach is generalizable to multilingual, highly varied datasets.

| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | 2021 30th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN) |
| Cite as:           | **[arXiv:2107.14593](https://arxiv.org/abs/2107.14593) [cs.CL]** |
|                    | (or **[arXiv:2107.14593v1](https://arxiv.org/abs/2107.14593v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-4">4. Multi-stage Pre-training over Simplified Multimodal Pre-training Models
</h2>

Title: [Multi-stage Pre-training over Simplified Multimodal Pre-training Models](https://arxiv.org/abs/2107.14596)

Authors: [Tongtong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Fangxiang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+F), [Xiaojie Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X)

> Multimodal pre-training models, such as LXMERT, have achieved excellent results in downstream tasks. However, current pre-trained models require large amounts of training data and have huge model sizes, which make them difficult to apply in low-resource situations. How to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem. In this paper, we propose a new Multi-stage Pre-training (MSP) method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train the model in stages. We also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus. We take a Simplified LXMERT (LXMERT- S), which has only 45.9% parameters of the original LXMERT model and 11.76% of the original pre-training data as the testbed of our MSP method. Experimental results show that our method achieves comparable performance to the original LXMERT model in all downstream tasks, and even outperforms the original model in Image-Text Retrieval task.

| Comments: | 10 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.14596](https://arxiv.org/abs/2107.14596) [cs.CL]** |
|           | (or **[arXiv:2107.14596v1](https://arxiv.org/abs/2107.14596v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-5">5. MDQE: A More Accurate Direct Pretraining for Machine Translation Quality Estimation
</h2>

Title: [MDQE: A More Accurate Direct Pretraining for Machine Translation Quality Estimation](https://arxiv.org/abs/2107.14600)

Authors: [Lei Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+L)

> It is expensive to evaluate the results of Machine Translation(MT), which usually requires manual translation as a reference. Machine Translation Quality Estimation (QE) is a task of predicting the quality of machine translations without relying on any reference. Recently, the emergence of predictor-estimator framework which trains the predictor as a feature extractor and estimator as a QE predictor, and pre-trained language models(PLM) have achieved promising QE performance. However, we argue that there are still gaps between the predictor and the estimator in both data quality and training objectives, which preclude QE models from benefiting from a large number of parallel corpora more directly. Based on previous related work that have alleviated gaps to some extent, we propose a novel framework that provides a more accurate direct pretraining for QE tasks. In this framework, a generator is trained to produce pseudo data that is closer to the real QE data, and a estimator is pretrained on these data with novel objectives that are the same as the QE task. Experiments on widely used benchmarks show that our proposed framework outperforms existing methods, without using any pretraining models such as BERT.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:2105.07149](https://arxiv.org/abs/2105.07149) by other authors |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.14600](https://arxiv.org/abs/2107.14600) [cs.CL]** |
|           | (or **[arXiv:2107.14600v1](https://arxiv.org/abs/2107.14600v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-6">6. Towards Universality in Multilingual Text Rewriting
</h2>

Title: [Towards Universality in Multilingual Text Rewriting](https://arxiv.org/abs/2107.14749)

Authors: [Xavier Garcia](https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+X), [Noah Constant](https://arxiv.org/search/cs?searchtype=author&query=Constant%2C+N), [Mandy Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+M), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

> In this work, we take the first steps towards building a universal rewriter: a model capable of rewriting text in any language to exhibit a wide variety of attributes, including styles and languages, while preserving as much of the original semantics as possible. In addition to obtaining state-of-the-art results on unsupervised translation, we also demonstrate the ability to do zero-shot sentiment transfer in non-English languages using only English exemplars for sentiment. We then show that our model is able to modify multiple attributes at once, for example adjusting both language and sentiment jointly. Finally, we show that our model is capable of performing zero-shot formality-sensitive translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.14749](https://arxiv.org/abs/2107.14749) [cs.CL]** |
|           | (or **[arXiv:2107.14749v1](https://arxiv.org/abs/2107.14749v1) [cs.CL]** for this version) |





<h2 id="2021-08-02-7">7. ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback
</h2>

Title: [ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback](https://arxiv.org/abs/2107.14800)

Authors: [Shiyue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Benjamin Frey](https://arxiv.org/search/cs?searchtype=author&query=Frey%2C+B), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> We introduce ChrEnTranslate, an online machine translation demonstration system for translation between English and an endangered language Cherokee. It supports both statistical and neural translation models as well as provides quality estimation to inform users of reliability, two user feedback interfaces for experts and common users respectively, example inputs to collect human translations for monolingual data, word alignment visualization, and relevant terms from the Cherokee-English dictionary. The quantitative evaluation demonstrates that our backbone translation models achieve state-of-the-art translation performance and our quality estimation well correlates with both BLEU and human judgment. By analyzing 216 pieces of expert feedback, we find that NMT is preferable because it copies less than SMT, and, in general, current models can translate fragments of the source sentence but make major mistakes. When we add these 216 expert-corrected parallel texts into the training set and retrain models, equal or slightly better performance is observed, which demonstrates indicates the potential of human-in-the-loop learning. Our online demo is at [this https URL](https://chren.cs.unc.edu/;) our code is open-sourced at [this https URL](https://github.com/ZhangShiyue/ChrEnTranslate;) and our data is available at [this https URL](https://github.com/ZhangShiyue/ChrEn).

| Comments: | ACL 2021 Demo (8 pages)                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2107.14800](https://arxiv.org/abs/2107.14800) [cs.CL]** |
|           | (or **[arXiv:2107.14800v1](https://arxiv.org/abs/2107.14800v1) [cs.CL]** for this version) |





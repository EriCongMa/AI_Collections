# Daily arXiv: Machine Translation - June, 2021

# Index


- [2021-06-28](#2021-06-28)

  - [1. ParaLaw Nets -- Cross-lingual Sentence-level Pretraining for Legal Text Processing](#2021-06-28-1)
  - [2. Manually Annotated Spelling Error Corpus for Amharic](#2021-06-28-2)
  - [3. Language Models are Good Translators](#2021-06-28-3)
  - [4. Learning to Sample Replacements for ELECTRA Pre-Training](#2021-06-28-4)
  - [5. DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders](#2021-06-28-5)
- [2021-06-25](#2021-06-25)
  - [1. A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021](#2021-06-25-1)
  - [2. Charformer: Fast Character Transformers via Gradient-based Subword Tokenization](#2021-06-25-2)
  - [3. On the Influence of Machine Translation on Language Origin Obfuscation](#2021-06-25-3)
  - [4. AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry](#2021-06-25-4)
- [2021-06-24](#2021-06-24)
  - [1. A Simple and Practical Approach to Improve Misspellings in OCR Text](#2021-06-24-1)
  - [2. End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages](#2021-06-24-2)
  - [3. Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations](#2021-06-24-3)
- [2021-06-23](#2021-06-23)

  - [1. On the Evaluation of Machine Translation for Terminology Consistency](#2021-06-23-1)
  - [2. Dive into Deep Learning](#2021-06-23-2)
  - [3. Incremental Deep Neural Network Learning using Classification Confidence Thresholding](#2021-06-23-3)
  - [4. Phrase-level Active Learning for Neural Machine Translation](#2021-06-23-4)
  - [5. BARTScore: Evaluating Generated Text as Text Generation](#2021-06-23-5)
  - [6. Do Language Models Perform Generalizable Commonsense Inference?](#2021-06-23-6)
  - [7. LV-BERT: Exploiting Layer Variety for BERT](#2021-06-23-7)
- [2021-06-22](#2021-06-22)
  - [1. TCIC: Theme Concepts Learning Cross Language and Vision for Image Captioning](#2021-06-22-1)
  - [2. Interventional Video Grounding with Dual Contrastive Learning](#2021-06-22-2)
  - [3. CPM-2: Large-scale Cost-effective Pre-trained Language Models](#2021-06-22-3)
  - [4. Challenges in Translation of Emotions in Multilingual User-Generated Content: Twitter as a Case Study](#2021-06-22-4)
  - [5. Pay Better Attention to Attention: Head Selection in Multilingual and Multi-Domain Sequence Modeling](#2021-06-22-5)
- [2021-06-21](#2021-06-21)

  - [1. Multi-mode Transformer Transducer with Stochastic Future Context](#2021-06-21-1)
  - [2. Investigating the Role of Negatives in Contrastive Representation Learning](#2021-06-21-2)
  - [3. BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](#2021-06-21-3)
  - [4. GEM: A General Evaluation Benchmark for Multimodal Tasks](#2021-06-21-4)
  - [5. Bad Characters: Imperceptible NLP Attacks](#2021-06-21-5)
  - [6. Recurrent Stacking of Layers in Neural Networks: An Application to Neural Machine Translation](#2021-06-21-6)
  - [7. Challenges and Limitations with the Metrics Measuring the Complexity of Code-Mixed Text](#2021-06-21-7)
- [2021-06-18](#2021-06-18)
  - [1. Specializing Multilingual Language Models: An Empirical Study](#2021-06-18-1)
  - [2. Probing Image-Language Transformers for Verb Understanding](#2021-06-18-2)
  - [3. An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](#2021-06-18-3)
  - [4. Lost in Interpreting: Speech Translation from Source or Interpreter?](#2021-06-18-4)
  - [5. Modeling Worlds in Text](#2021-06-18-5)
  - [6. Multi-head or Single-head? An Empirical Comparison for Transformer Training](#2021-06-18-6)
- [2021-06-17](#2021-06-17)
  - [1. Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors](#2021-06-17-1)
  - [2. What Context Features Can Transformer Language Models Use?](#2021-06-17-2)
  - [3. Alternated Training with Synthetic and Authentic Data for Neural Machine Translation](#2021-06-17-3)
  - [4. Semantic sentence similarity: size does not always matter](#2021-06-17-4)
  - [5. Evaluating Gender Bias in Hindi-English Machine Translation](#2021-06-17-5)
  - [6. Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study](#2021-06-17-6)
  - [7. RefBERT: Compressing BERT by Referencing to Pre-computed Representations](#2021-06-17-7)
  - [8. Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation](#2021-06-17-8)
  - [9. Collaborative Training of Acoustic Encoders for Speech Recognition](#2021-06-17-9)
- [2021-06-16](#2021-06-16)
  - [1. Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup](#2021-06-16-1)
  - [2. Targeted Data Acquisition for Evolving Negotiation Agents](#2021-06-16-2)
  - [3. Language Tags Matter for Zero-Shot Neural Machine Translation](#2021-06-16-3)
  - [4. Semantic Representation and Inference for NLP](#2021-06-16-4)
  - [5. Sequence-Level Training for Non-Autoregressive Neural Machine Translation](#2021-06-16-5)
  - [6. Consistency Regularization for Cross-Lingual Fine-Tuning](#2021-06-16-6)
- [2021-06-15](#2021-06-15)
  - [1. Thinking Like Transformers](#2021-06-15-1)
  - [2. Pre-Trained Models: Past, Present and Future](#2021-06-15-2)
  - [3. Model Explainability in Deep Learning Based Natural Language Processing](#2021-06-15-3)
  - [4. Direct Simultaneous Speech-to-Text Translation Assisted by Synchronized Streaming ASR](#2021-06-15-4)
  - [5. Assessing Multilingual Fairness in Pre-trained Multimodal Representations](#2021-06-15-5)
  - [6. Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation](#2021-06-15-6)
  - [7. Machine Translation into Low-resource Language Varieties](#2021-06-15-7)
  - [8. Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data](#2021-06-15-8)
  - [9. Memory-efficient Transformers via Top-k Attention](#2021-06-15-9)
  - [10. SASICM A Multi-Task Benchmark For Subtext Recognition](#2021-06-15-10)
  - [11. Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces](#2021-06-15-11)
  - [12. SAS: Self-Augmented Strategy for Language Model Pre-training](#2021-06-15-12)
  - [13. Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation](#2021-06-15-13)
  - [14.English to Bangla Machine Translation Using Recurrent Neural Network ](#2021-06-15-14)
  - [15. MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education](#2021-06-15-15)
  - [16. Self-Guided Contrastive Learning for BERT Sentence Representations](#2021-06-15-16)
  - [17. Is it a click bait? Let's predict using Machine Learning](#2021-06-15-17)
  - [18. Using Integrated Gradients to explain Linguistic Acceptability learnt by BERT](#2021-06-15-18)
  - [19. Determinantal Beam Search](#2021-06-15-19)
  - [20. Grammar Equations](#2021-06-15-20)
  - [21. An Empirical Survey of Data Augmentation for Limited Data Learning in NLP](#2021-06-15-21)
- [2021-06-14](#2021-06-14)
  - [1. One Sense Per Translation](#2021-06-14-1)
  - [2. Graph Neural Networks for Natural Language Processing: A Survey](#2021-06-14-2)
  - [3. Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation](#2021-06-14-3)
  - [4. Towards User-Driven Neural Machine Translation](#2021-06-14-4)
  - [5. A Discussion on Building Practical NLP Leaderboards: The Case of Machine Translation](#2021-06-14-5)
  - [6. Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment](#2021-06-14-6)
  - [7. Zero-Shot Controlled Generation with Encoder-Decoder Transformers](#2021-06-14-7)
  - [8. Semi-Supervised and Unsupervised Sense Annotation via Translations](#2021-06-14-8)
- [2021-06-11](#2021-06-11)
  - [1. Data augmentation to improve robustness of image captioning solutions](#2021-06-11-1)
  - [2. Input Augmentation Improves Constrained Beam Search for Neural Machine Translation: NTT at WAT 2021](#2021-06-11-2)
  - [3. Progressive Multi-Granularity Training for Non-Autoregressive Translation](#2021-06-11-3)
  - [4. AUGNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation](#2021-06-11-4)
  - [5. Exploring Unsupervised Pretraining Objectives for Machine Translation](#2021-06-11-5)
  - [6. Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation](#2021-06-11-6)
  - [7. GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures](#2021-06-11-7)
  - [8. ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation](#2021-06-11-8)
- [2021-06-10](#2021-06-10)

  - [1. PAM: Understanding Product Images in Cross Product Category Attribute Extraction](#2021-06-10-1)
  - [2. VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation](#2021-06-10-2)
  - [3. FastSeq: Make Sequence Generation Faster](#2021-06-10-3)
  - [4. Sentence Embeddings using Supervised Contrastive Learning](#2021-06-10-4)
  - [5. Probing Multilingual Language Models for Discourse](#2021-06-10-5)
  - [6. RealTranS: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer](#2021-06-10-6)
  - [7. Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding](#2021-06-10-7)
  - [8. Crosslingual Embeddings are Essential in UNMT for Distant Languages: An English to IndoAryan Case Study](#2021-06-10-8)
  - [9. Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation](#2021-06-10-9)
  - [10. AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT](#2021-06-10-10)
- [2021-06-09](#2021-06-09)

  - [1. A Survey of Transformers](#2021-06-09-1)
  - [2. Meta Learning for Knowledge Distillation](#2021-06-09-2)
  - [3. Lexicon Learning for Few-Shot Neural Sequence Modeling](#2021-06-09-3)
  - [4. Self-supervised and Supervised Joint Training for Resource-rich Machine Translation](#2021-06-09-4)
  - [5. Obtaining Better Static Word Embeddings Using Contextual Embedding Models](#2021-06-09-5)
  - [6. CLTR: An End-to-End, Transformer-Based System for Cell Level TableRetrieval and Table Question Answering](#2021-06-09-6)
  - [7. Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks](#2021-06-09-7)
- [2021-06-08](#2021-06-08)
  - [1. CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings](#2021-06-08-1)
  - [2. SelfDoc: Self-Supervised Document Representation Learning](#2021-06-08-2)
  - [3. Do Grammatical Error Correction Models Realize Grammatical Generalization?](#2021-06-08-3)
  - [4. The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation](#2021-06-08-4)
  - [5. Itihasa: A large-scale corpus for Sanskrit to English translation](#2021-06-08-5)
  - [6. On the Language Coverage Bias for Neural Machine Translation](#2021-06-08-6)
  - [7. BERTGEN: Multi-task Generation through BERT](#2021-06-08-7)
  - [8. RoSearch: Search for Robust Student Architectures When Distilling Pre-trained Language Models](#2021-06-08-8)
  - [9. Diverse Pretrained Context Encodings Improve Document Translation](#2021-06-08-9)
  - [10. Encouraging Neural Machine Translation to Satisfy Terminology Constraints](#2021-06-08-10)
  - [11. A Simple Recipe for Multilingual Grammatical Error Correction](#2021-06-08-11)
- [2021-06-07](#2021-06-07)

  - [1. Neural semi-Markov CRF for Monolingual Word Alignment](#2021-06-07-1)
  - [2. Human-Adversarial Visual Question Answering](#2021-06-07-2)
  - [3. How to Adapt Your Pretrained Multilingual Model to 1600 Languages](#2021-06-07-3)
  - [4. Syntax-augmented Multilingual BERT for Cross-lingual Transfer](#2021-06-07-4)
  - [5. Grounding 'Grounding' in NLP](#2021-06-07-5)
  - [6. BERTTune: Fine-Tuning Neural Machine Translation with BERTScore](#2021-06-07-6)
  - [7. Scalable Transformers for Neural Machine Translation](#2021-06-07-7)
  - [8. Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene](#2021-06-07-8)
  - [9. Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings](#2021-06-07-9)
- [2021-06-04](#2021-06-04)
  - [1. TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data](#2021-06-04-1)
  - [2. Representing Syntax and Composition with Geometric Transformations](#2021-06-04-2)
  - [3. The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models](#2021-06-04-3)
  - [4. A Dataset and Baselines for Multilingual Reply Suggestion](#2021-06-04-4)
  - [5. E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning](#2021-06-04-5)
  - [6. Lightweight Adapter Tuning for Multilingual Speech Translation](#2021-06-04-6)
  - [7. Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?](#2021-06-04-7)
  - [8. Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction](#2021-06-04-8)
  - [9. Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models](#2021-06-04-9)
  - [10. Fingerprinting Fine-tuned Language Models in the Wild](#2021-06-04-10 )
  - [11. Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer](#2021-06-04-11)
- [2021-06-03](#2021-06-03)

  - [1. Part of Speech and Universal Dependency effects on English Arabic Machine Translation](#2021-06-03-1)
  - [2. Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation](#2021-06-03-2)
  - [3. Discrete Cosine Transform as Universal Sentence Encoder](#2021-06-03-3)
  - [4. Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation](#2021-06-03-4)
  - [5. One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers](#2021-06-03-5)
  - [6. Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling](#2021-06-03-6)
  - [7. Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?](#2021-06-03-7)
  - [8. Evidence-based Factual Error Correction](#2021-06-03-8)
  - [9. Is Sparse Attention more Interpretable?](#2021-06-03-9)
  - [10. End-to-End NLP Knowledge Graph Construction](#2021-06-03-10)
  - [11. IrEne: Interpretable Energy Prediction for Transformers](#2021-06-03-11)
  - [12. Lower Perplexity is Not Always Human-Like](#2021-06-03-12)
  - [13. On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers](#2021-06-03-13)
- [2021-06-02](#2021-06-02)

  - [1. Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models](#2021-06-02-1)
  - [2. Language Model Evaluation Beyond Perplexity](#2021-06-02-2)
  - [3. An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers](#2021-06-02-3)
  - [4. Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation](#2021-06-02-4)
  - [5. Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives](#2021-06-02-5)
  - [6. Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021](#2021-06-02-6)
  - [7. ViTA: Visual-Linguistic Translation by Aligning Object Tags](#2021-06-02-7)
  - [8. An In-depth Study on Internal Structure of Chinese Words](#2021-06-02-8)
  - [9. SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining](#2021-06-02-9)
  - [10. DoT: An efficient Double Transformer for NLP tasks with tables](#2021-06-02-10)
  - [11. NewsEmbed: Modeling News through Pre-trained DocumentRepresentations](#2021-06-02-11)
  - [12. Incorporating Visual Layout Structures for Scientific Text Classification](#2021-06-02-12)
- [2021-06-01](#2021-06-01)
  - [1. An Attention Free Transformer](#2021-06-01-1)
  - [2. LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering](#2021-06-01-2)
  - [3. Re-evaluating Word Mover's Distance](#2021-06-01-3)
  - [4. Memory-Efficient Differentiable Transformer Architecture Search](#2021-06-01-4)
  - [5. Why does CTC result in peaky behavior?](#2021-06-01-5)
  - [6. Grammatical Error Correction as GAN-like Sequence Labeling](#2021-06-01-6)
  - [7. Predictive Representation Learning for Language Modeling](#2021-06-01-7)
  - [8. Korean-English Machine Translation with Multiple Tokenization Strategy](#2021-06-01-8)
  - [9. Grammar Accuracy Evaluation (GAE): Quantifiable Intrinsic Evaluation of Machine Translation Models](#2021-06-01-9)
  - [10. NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search](#2021-06-01-10)
  - [11. Pre-training Universal Language Representation](#2021-06-01-11)
  - [12. Fast Nearest Neighbor Machine Translation](#2021-06-01-12)
  - [13. HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation](#2021-06-01-13)
  - [14. Attention Flows are Shapley Value Explanations](#2021-06-01-14)
  - [15. G-Transformer for Document-level Machine Translation](#2021-06-01-15)
  - [16. On Compositional Generalization of Neural Machine Translation](#2021-06-01-16)
  - [17. Transfer Learning for Sequence Generation: from Single-source to Multi-source](#2021-06-01-17)
  - [18. Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models](#2021-06-01-18)
  - [19. Effective Batching for Recurrent Neural Network Grammars](#2021-06-01-19)
  - [20. Greedy Layer Pruning: Decreasing Inference Time of Transformer Models](#2021-06-01-20)
  - [21. Verdi: Quality Estimation and Error Detection for Bilingual](#2021-06-01-21)
  - [22. GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation](#2021-06-01-22)
  - [23. Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?](#2021-06-01-23)
  - [24. Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data](#2021-06-01-24)
  - [25. Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation](#2021-06-01-25)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-06-28

[Return to Index](#Index)



<h2 id="2021-06-28-1">1. ParaLaw Nets -- Cross-lingual Sentence-level Pretraining for Legal Text Processing
</h2>

Title: [ParaLaw Nets -- Cross-lingual Sentence-level Pretraining for Legal Text Processing](https://arxiv.org/abs/2106.13403)

Authors: [Ha-Thanh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+H), [Vu Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+V), [Phuong Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+P+M), [Thi-Hai-Yen Vuong](https://arxiv.org/search/cs?searchtype=author&query=Vuong%2C+T), [Quan Minh Bui](https://arxiv.org/search/cs?searchtype=author&query=Bui%2C+Q+M), [Chau Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+C+M), [Binh Tran Dang](https://arxiv.org/search/cs?searchtype=author&query=Dang%2C+B+T), [Minh Le Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+M+L), [Ken Satoh](https://arxiv.org/search/cs?searchtype=author&query=Satoh%2C+K)

> Ambiguity is a characteristic of natural language, which makes expression ideas flexible. However, in a domain that requires accurate statements, it becomes a barrier. Specifically, a single word can have many meanings and multiple words can have the same meaning. When translating a text into a foreign language, the translator needs to determine the exact meaning of each element in the original sentence to produce the correct translation sentence. From that observation, in this paper, we propose ParaLaw Nets, a pretrained model family using sentence-level cross-lingual information to reduce ambiguity and increase the performance in legal text processing. This approach achieved the best result in the Question Answering task of COLIEE-2021.

| Comments: | Also published in COLIEE 2021's Proceeding                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2106.13403](https://arxiv.org/abs/2106.13403) [cs.CL]** |
|           | (or **[arXiv:2106.13403v1](https://arxiv.org/abs/2106.13403v1) [cs.CL]** for this version) |





<h2 id="2021-06-28-2">2. Manually Annotated Spelling Error Corpus for Amharic
</h2>

Title: [Manually Annotated Spelling Error Corpus for Amharic](https://arxiv.org/abs/2106.13521)

Authors: [Andargachew Mekonnen Gezmu](https://arxiv.org/search/cs?searchtype=author&query=Gezmu%2C+A+M), [Tirufat Tesifaye Lema](https://arxiv.org/search/cs?searchtype=author&query=Lema%2C+T+T), [Binyam Ephrem Seyoum](https://arxiv.org/search/cs?searchtype=author&query=Seyoum%2C+B+E), [Andreas Nürnberger](https://arxiv.org/search/cs?searchtype=author&query=Nürnberger%2C+A)

> This paper presents a manually annotated spelling error corpus for Amharic, lingua franca in Ethiopia. The corpus is designed to be used for the evaluation of spelling error detection and correction. The misspellings are tagged as non-word and real-word errors. In addition, the contextual information available in the corpus makes it useful in dealing with both types of spelling errors.

| Comments: | Accepted to 2nd AfricaNLP Workshop at EACL 2021              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.13521](https://arxiv.org/abs/2106.13521) [cs.CL]** |
|           | (or **[arXiv:2106.13521v1](https://arxiv.org/abs/2106.13521v1) [cs.CL]** for this version) |





<h2 id="2021-06-28-3">3. Language Models are Good Translators
</h2>

Title: [Language Models are Good Translators](https://arxiv.org/abs/2106.13627)

Authors: [Shuo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Wenxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Recent years have witnessed the rapid advance in neural machine translation (NMT), the core of which lies in the encoder-decoder architecture. Inspired by the recent progress of large-scale pre-trained language models on machine translation in a limited scenario, we firstly demonstrate that a single language model (LM4MT) can achieve comparable performance with strong encoder-decoder NMT models on standard machine translation benchmarks, using the same training data and similar amount of model parameters. LM4MT can also easily utilize source-side texts as additional supervision. Though modeling the source- and target-language texts with the same mechanism, LM4MT can provide unified representations for both source and target sentences, which can better transfer knowledge across languages. Extensive experiments on pivot-based and zero-shot translation tasks show that LM4MT can outperform the encoder-decoder NMT model by a large margin.

| Comments: | 12 pages. Work in progress. An earlier verison of this manuscript is under review |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.13627](https://arxiv.org/abs/2106.13627) [cs.CL]** |
|           | (or **[arXiv:2106.13627v1](https://arxiv.org/abs/2106.13627v1) [cs.CL]** for this version) |





<h2 id="2021-06-28-4">4. Learning to Sample Replacements for ELECTRA Pre-Training
</h2>

Title: [Learning to Sample Replacements for ELECTRA Pre-Training](https://arxiv.org/abs/2106.13715)

Authors: [Yaru Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+Y), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Hangbo Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+H), [Ke Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> ELECTRA pretrains a discriminator to detect replaced tokens, where the replacements are sampled from a generator trained with masked language modeling. Despite the compelling performance, ELECTRA suffers from the following two issues. First, there is no direct feedback loop from discriminator to generator, which renders replacement sampling inefficient. Second, the generator's prediction tends to be over-confident along with training, making replacements biased to correct tokens. In this paper, we propose two methods to improve replacement sampling for ELECTRA pre-training. Specifically, we augment sampling with a hardness prediction mechanism, so that the generator can encourage the discriminator to learn what it has not acquired. We also prove that efficient sampling reduces the training variance of the discriminator. Moreover, we propose to use a focal loss for the generator in order to relieve oversampling of correct tokens as replacements. Experimental results show that our method improves ELECTRA pre-training on various downstream tasks.

| Comments: | Accepted by Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.13715](https://arxiv.org/abs/2106.13715) [cs.CL]** |
|           | (or **[arXiv:2106.13715v1](https://arxiv.org/abs/2106.13715v1) [cs.CL]** for this version) |





<h2 id="2021-06-28-5">5. DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders
</h2>

Title: [DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders](https://arxiv.org/abs/2106.13736)

Authors: [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Alexandre Muzio](https://arxiv.org/search/cs?searchtype=author&query=Muzio%2C+A), [Saksham Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal%2C+S), [Hany Hassan Awadalla](https://arxiv.org/search/cs?searchtype=author&query=Awadalla%2C+H+H), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> While pretrained encoders have achieved success in various natural language understanding (NLU) tasks, there is a gap between these pretrained encoders and natural language generation (NLG). NLG tasks are often based on the encoder-decoder framework, where the pretrained encoders can only benefit part of it. To reduce this gap, we introduce DeltaLM, a pretrained multilingual encoder-decoder model that regards the decoder as the task layer of off-the-shelf pretrained encoders. Specifically, we augment the pretrained multilingual encoder with a decoder and pre-train it in a self-supervised way. To take advantage of both the large-scale monolingual data and bilingual data, we adopt the span corruption and translation span corruption as the pre-training tasks. Experiments show that DeltaLM outperforms various strong baselines on both natural language generation and translation tasks, including machine translation, abstractive text summarization, data-to-text, and question generation.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.13736](https://arxiv.org/abs/2106.13736) [cs.CL]** |
|           | (or **[arXiv:2106.13736v1](https://arxiv.org/abs/2106.13736v1) [cs.CL]** for this version) |





# 2021-06-25

[Return to Index](#Index)



<h2 id="2021-06-25-1">1. A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021
</h2>

Title: [A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021](https://arxiv.org/abs/2106.13033)

Authors: [Ke-Han Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+K), [Bo-Han Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+B), [Kuan-Yu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K)

> In this paper, inspired by the successes of visionlanguage pre-trained models and the benefits from training with adversarial attacks, we present a novel transformerbased cross-modal fusion modeling by incorporating the both notions for VQA challenge 2021. Specifically, the proposed model is on top of the architecture of VinVL model [19], and the adversarial training strategy [4] is applied to make the model robust and generalized. Moreover, two implementation tricks are also used in our system to obtain better results. The experiments demonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.

| Comments: | CVPR 2021 Workshop: Visual Question Answering (VQA) Challenge |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.13033](https://arxiv.org/abs/2106.13033) [cs.CV]** |
|           | (or **[arXiv:2106.13033v1](https://arxiv.org/abs/2106.13033v1) [cs.CV]** for this version) |





<h2 id="2021-06-25-2">2. Charformer: Fast Character Transformers via Gradient-based Subword Tokenization
</h2>

Title: [Charformer: Fast Character Transformers via Gradient-based Subword Tokenization](https://arxiv.org/abs/2106.12672)

Authors: [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Vinh Q. Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+V+Q), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Jai Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+J), [Hyung Won Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+H+W), [Dara Bahri](https://arxiv.org/search/cs?searchtype=author&query=Bahri%2C+D), [Zhen Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Z), [Simon Baumgartner](https://arxiv.org/search/cs?searchtype=author&query=Baumgartner%2C+S), [Cong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+C), [Donald Metzler](https://arxiv.org/search/cs?searchtype=author&query=Metzler%2C+D)

> State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adaptation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. We additionally introduce Charformer, a deep Transformer model that integrates GBST and operates on the byte level. Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that Charformer outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, Charformer is fast, improving the speed of both vanilla byte-level and subword-level Transformers by 28%-100% while maintaining competitive quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.12672](https://arxiv.org/abs/2106.12672) [cs.CL]** |
|           | (or **[arXiv:2106.12672v1](https://arxiv.org/abs/2106.12672v1) [cs.CL]** for this version) |





<h2 id="2021-06-25-3">3. On the Influence of Machine Translation on Language Origin Obfuscation
</h2>

Title: [On the Influence of Machine Translation on Language Origin Obfuscation](https://arxiv.org/abs/2106.12830)

Authors: [Benjamin Murauer](https://arxiv.org/search/cs?searchtype=author&query=Murauer%2C+B), [Michael Tschuggnall](https://arxiv.org/search/cs?searchtype=author&query=Tschuggnall%2C+M), [Günther Specht](https://arxiv.org/search/cs?searchtype=author&query=Specht%2C+G)

> In the last decade, machine translation has become a popular means to deal with multilingual digital content. By providing higher quality translations, obfuscating the source language of a text becomes more attractive. In this paper, we analyze the ability to detect the source language from the translated output of two widely used commercial machine translation systems by utilizing machine-learning algorithms with basic textual features like n-grams. Evaluations show that the source language can be reconstructed with high accuracy for documents that contain a sufficient amount of translated text. In addition, we analyze how the document size influences the performance of the prediction, as well as how limiting the set of possible source languages improves the classification accuracy.

| Comments: | This was peer-reviewed, accepted and presented at [this https URL](https://www.cicling.org/2018/), but the organizer somehow failed to publish the proceedings |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.12830](https://arxiv.org/abs/2106.12830) [cs.CL]** |
|           | (or **[arXiv:2106.12830v1](https://arxiv.org/abs/2106.12830v1) [cs.CL]** for this version) |





<h2 id="2021-06-25-4">4. AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry
</h2>

Title: [AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry](https://arxiv.org/abs/2106.12944)

Authors: [Yannis Katsis](https://arxiv.org/search/cs?searchtype=author&query=Katsis%2C+Y), [Saneem Chemmengath](https://arxiv.org/search/cs?searchtype=author&query=Chemmengath%2C+S), [Vishwajeet Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+V), [Samarth Bharadwaj](https://arxiv.org/search/cs?searchtype=author&query=Bharadwaj%2C+S), [Mustafa Canim](https://arxiv.org/search/cs?searchtype=author&query=Canim%2C+M), [Michael Glass](https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+M), [Alfio Gliozzo](https://arxiv.org/search/cs?searchtype=author&query=Gliozzo%2C+A), [Feifei Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+F), [Jaydeep Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen%2C+J), [Karthik Sankaranarayanan](https://arxiv.org/search/cs?searchtype=author&query=Sankaranarayanan%2C+K), [Soumen Chakrabarti](https://arxiv.org/search/cs?searchtype=author&query=Chakrabarti%2C+S)

> Recent advances in transformers have enabled Table Question Answering (Table QA) systems to achieve high accuracy and SOTA results on open domain datasets like WikiTableQuestions and WikiSQL. Such transformers are frequently pre-trained on open-domain content such as Wikipedia, where they effectively encode questions and corresponding tables from Wikipedia as seen in Table QA dataset. However, web tables in Wikipedia are notably flat in their layout, with the first row as the sole column header. The layout lends to a relational view of tables where each row is a tuple. Whereas, tables in domain-specific business or scientific documents often have a much more complex layout, including hierarchical row and column headers, in addition to having specialized vocabulary terms from that domain.
> To address this problem, we introduce the domain-specific Table QA dataset AIT-QA (Airline Industry Table QA). The dataset consists of 515 questions authored by human annotators on 116 tables extracted from public U.S. SEC filings (publicly available at: [this https URL](https://www.sec.gov/edgar.shtml)) of major airline companies for the fiscal years 2017-2019. We also provide annotations pertaining to the nature of questions, marking those that require hierarchical headers, domain-specific terminology, and paraphrased forms. Our zero-shot baseline evaluation of three transformer-based SOTA Table QA methods - TaPAS (end-to-end), TaBERT (semantic parsing-based), and RCI (row-column encoding-based) - clearly exposes the limitation of these methods in this practical setting, with the best accuracy at just 51.8\% (RCI). We also present pragmatic table preprocessing steps used to pivot and project these complex tables into a layout suitable for the SOTA Table QA models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.12944](https://arxiv.org/abs/2106.12944) [cs.CL]** |
|           | (or **[arXiv:2106.12944v1](https://arxiv.org/abs/2106.12944v1) [cs.CL]** for this version) |






# 2021-06-24

[Return to Index](#Index)



<h2 id="2021-06-24-1">1. A Simple and Practical Approach to Improve Misspellings in OCR Text
</h2>

Title: [A Simple and Practical Approach to Improve Misspellings in OCR Text](https://arxiv.org/abs/2106.12030)

Authors: [Junxia Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J) (1), [Johannes Ledolter](https://arxiv.org/search/cs?searchtype=author&query=Ledolter%2C+J) (2) ((1) Georgetown University Medical Center, Georgetown University, (2) Tippie College of Business, University of Iowa)

> The focus of our paper is the identification and correction of non-word errors in OCR text. Such errors may be the result of incorrect insertion, deletion, or substitution of a character, or the transposition of two adjacent characters within a single word. Or, it can be the result of word boundary problems that lead to run-on errors and incorrect-split errors. The traditional N-gram correction methods can handle single-word errors effectively. However, they show limitations when dealing with split and merge errors. In this paper, we develop an unsupervised method that can handle both errors. The method we develop leads to a sizable improvement in the correction rates. This tutorial paper addresses very difficult word correction problems - namely incorrect run-on and split errors - and illustrates what needs to be considered when addressing such problems. We outline a possible approach and assess its success on a limited study.

| Comments: | 11 pages, 1 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.12030](https://arxiv.org/abs/2106.12030) [cs.CL]** |
|           | (or **[arXiv:2106.12030v1](https://arxiv.org/abs/2106.12030v1) [cs.CL]** for this version) |





<h2 id="2021-06-24-2">2. End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages
</h2>

Title: [End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages](https://arxiv.org/abs/2106.12398)

Authors: [Josef Jon](https://arxiv.org/search/cs?searchtype=author&query=Jon%2C+J), [João Paulo Aires](https://arxiv.org/search/cs?searchtype=author&query=Aires%2C+J+P), [Dušan Variš](https://arxiv.org/search/cs?searchtype=author&query=Variš%2C+D), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

> Lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases. Although current approaches can enforce terms to appear in the translation, they often struggle to make the constraint word form agree with the rest of the generated output. Our manual analysis shows that 46% of the errors in the output of a baseline constrained model for English to Czech translation are related to agreement. We investigate mechanisms to allow neural machine translation to infer the correct word inflection given lemmatized constraints. In particular, we focus on methods based on training the model with constraints provided as part of the input sequence. Our experiments on the English-Czech language pair show that this approach improves the translation of constrained terms in both automatic and manual evaluation by reducing errors in agreement. Our approach thus eliminates inflection errors, without introducing new errors or decreasing the overall quality of the translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.12398](https://arxiv.org/abs/2106.12398) [cs.CL]** |
|           | (or **[arXiv:2106.12398v1](https://arxiv.org/abs/2106.12398v1) [cs.CL]** for this version) |





<h2 id="2021-06-24-3">3. Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations
</h2>

Title: [Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations](https://arxiv.org/abs/2106.12479)

Authors: [Charaf Eddine Benarab](https://arxiv.org/search/cs?searchtype=author&query=Benarab%2C+C+E)

> Knowledge is acquired by humans through experience, and no boundary is set between the kinds of knowledge or skill levels we can achieve on different tasks at the same time. When it comes to Neural Networks, that is not the case, the major breakthroughs in the field are extremely task and domain specific. Vision and language are dealt with in separate manners, using separate methods and different datasets. In this work, we propose to use knowledge acquired by benchmark Vision Models which are trained on ImageNet to help a much smaller architecture learn to classify text. After transforming the textual data contained in the IMDB dataset to gray scale images. An analysis of different domains and the Transfer Learning method is carried out. Despite the challenge posed by the very different datasets, promising results are achieved. The main contribution of this work is a novel approach which links large pretrained models on both language and vision to achieve state-of-the-art results in different sub-fields from the original task. Without needing high compute capacity resources. Specifically, Sentiment Analysis is achieved after transferring knowledge between vision and language models. BERT embeddings are transformed into grayscale images, these images are then used as training examples for pretrained vision models such as VGG16 and ResNet
> Index Terms: Natural language, Vision, BERT, Transfer Learning, CNN, Domain Adaptation.

| Comments: | Paper contains: 5 pages, 6 figures, 1 table                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.12479](https://arxiv.org/abs/2106.12479) [cs.CL]** |
|           | (or **[arXiv:2106.12479v1](https://arxiv.org/abs/2106.12479v1) [cs.CL]** for this version) |





# 2021-06-23

[Return to Index](#Index)



<h2 id="2021-06-23-1">1. On the Evaluation of Machine Translation for Terminology Consistency
</h2>

Title: [On the Evaluation of Machine Translation for Terminology Consistency](https://arxiv.org/abs/2106.11891)

Authors: [Md Mahfuz ibn Alam](https://arxiv.org/search/cs?searchtype=author&query=Alam%2C+M+M+i), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L), [James Cross](https://arxiv.org/search/cs?searchtype=author&query=Cross%2C+J), [Matthias Gallé](https://arxiv.org/search/cs?searchtype=author&query=Gallé%2C+M), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P), [Vassilina Nikoulina](https://arxiv.org/search/cs?searchtype=author&query=Nikoulina%2C+V)

> As neural machine translation (NMT) systems become an important part of professional translator pipelines, a growing body of work focuses on combining NMT with terminologies. In many scenarios and particularly in cases of domain adaptation, one expects the MT output to adhere to the constraints provided by a terminology. In this work, we propose metrics to measure the consistency of MT output with regards to a domain terminology. We perform studies on the COVID-19 domain over 5 languages, also performing terminology-targeted human evaluation. We open-source the code for computing all proposed metrics: [this https URL](https://github.com/mahfuzibnalam/terminology_evaluation)

| Comments: | preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.11891](https://arxiv.org/abs/2106.11891) [cs.CL]** |
|           | (or **[arXiv:2106.11891v1](https://arxiv.org/abs/2106.11891v1) [cs.CL]** for this version) |





<h2 id="2021-06-23-2">2. Dive into Deep Learning
</h2>

Title: [Dive into Deep Learning](https://arxiv.org/abs/2106.11342)

Authors: [Aston Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+A), [Zachary C. Lipton](https://arxiv.org/search/cs?searchtype=author&query=Lipton%2C+Z+C), [Mu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M), [Alexander J. Smola](https://arxiv.org/search/cs?searchtype=author&query=Smola%2C+A+J)

> This open-source book represents our attempt to make deep learning approachable, teaching readers the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code. Our goal is to offer a resource that could (i) be freely available for everyone; (ii) offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist; (iii) include runnable code, showing readers how to solve problems in practice; (iv) allow for rapid updates, both by us and also by the community at large; (v) be complemented by a forum for interactive discussion of technical details and to answer questions.

| Comments: | (HTML) [this https URL](https://d2l.ai/) (GitHub) [this https URL](https://github.com/d2l-ai/d2l-en/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2106.11342](https://arxiv.org/abs/2106.11342) [cs.LG]** |
|           | (or **[arXiv:2106.11342v1](https://arxiv.org/abs/2106.11342v1) [cs.LG]** for this version) |





<h2 id="2021-06-23-3">3. Incremental Deep Neural Network Learning using Classification Confidence Thresholding
</h2>

Title: [Incremental Deep Neural Network Learning using Classification Confidence Thresholding](https://arxiv.org/abs/2106.11437)

Authors: [Justin Leo](https://arxiv.org/search/cs?searchtype=author&query=Leo%2C+J), [Jugal Kalita](https://arxiv.org/search/cs?searchtype=author&query=Kalita%2C+J)

> Most modern neural networks for classification fail to take into account the concept of the unknown. Trained neural networks are usually tested in an unrealistic scenario with only examples from a closed set of known classes. In an attempt to develop a more realistic model, the concept of working in an open set environment has been introduced. This in turn leads to the concept of incremental learning where a model with its own architecture and initial trained set of data can identify unknown classes during the testing phase and autonomously update itself if evidence of a new class is detected. Some problems that arise in incremental learning are inefficient use of resources to retrain the classifier repeatedly and the decrease of classification accuracy as multiple classes are added over time. This process of instantiating new classes is repeated as many times as necessary, accruing errors. To address these problems, this paper proposes the Classification Confidence Threshold approach to prime neural networks for incremental learning to keep accuracies high by limiting forgetting. A lean method is also used to reduce resources used in the retraining of the neural network. The proposed method is based on the idea that a network is able to incrementally learn a new class even when exposed to a limited number samples associated with the new class. This method can be applied to most existing neural networks with minimal changes to network architecture.

| Comments: | Accepted to IEEE TNNLS                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| DOI:      | [10.1109/TNNLS.2021.3087104](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1109%2FTNNLS.2021.3087104&v=b5b24540) |
| Cite as:  | **[arXiv:2106.11437](https://arxiv.org/abs/2106.11437) [cs.LG]** |
|           | (or **[arXiv:2106.11437v1](https://arxiv.org/abs/2106.11437v1) [cs.LG]** for this version) |





<h2 id="2021-06-23-4">4. Phrase-level Active Learning for Neural Machine Translation
</h2>

Title: [Phrase-level Active Learning for Neural Machine Translation](https://arxiv.org/abs/2106.11375)

Authors: [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Neural machine translation (NMT) is sensitive to domain shift. In this paper, we address this problem in an active learning setting where we can spend a given budget on translating in-domain data, and gradually fine-tune a pre-trained out-of-domain NMT model on the newly translated data. Existing active learning methods for NMT usually select sentences based on uncertainty scores, but these methods require costly translation of full sentences even when only one or two key phrases within the sentence are informative. To address this limitation, we re-examine previous work from the phrase-based machine translation (PBMT) era that selected not full sentences, but rather individual phrases. However, while incorporating these phrases into PBMT systems was relatively simple, it is less trivial for NMT systems, which need to be trained on full sequences to capture larger structural properties of sentences unique to the new domain. To overcome these hurdles, we propose to select both full sentences and individual phrases from unlabelled data in the new domain for routing to human translators. In a German-English translation task, our active learning approach achieves consistent improvements over uncertainty-based sentence selection methods, improving up to 1.2 BLEU score over strong active learning baselines.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.11375](https://arxiv.org/abs/2106.11375) [cs.CL]** |
|           | (or **[arXiv:2106.11375v1](https://arxiv.org/abs/2106.11375v1) [cs.CL]** for this version) |





<h2 id="2021-06-23-5">5. BARTScore: Evaluating Generated Text as Text Generation
</h2>

Title: [BARTScore: Evaluating Generated Text as Text Generation](https://arxiv.org/abs/2106.11520)

Authors: [Weizhe Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+W), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Pengfei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P)

> A wide variety of NLP applications, such as machine translation, summarization, and dialog, involve text generation. One major challenge for these applications is how to evaluate whether such generated texts are actually fluent, accurate, or effective. In this work, we conceptualize the evaluation of generated text as a text generation problem, modeled using pre-trained sequence-to-sequence models. The general idea is that models trained to convert the generated text to/from a reference output or the source text will achieve higher scores when the generated text is better. We operationalize this idea using BART, an encoder-decoder based pre-trained model, and propose a metric BARTScore with a number of variants that can be flexibly applied in an unsupervised fashion to evaluation of text from different perspectives (e.g. informativeness, fluency, or factuality). BARTScore is conceptually simple and empirically effective. It can outperform existing top-scoring metrics in 16 of 22 test settings, covering evaluation of 16 datasets (e.g., machine translation, text summarization) and 7 different perspectives (e.g., informativeness, factuality). Code to calculate BARTScore is available at [this https URL](https://github.com/neulab/BARTScore), and we have released an interactive leaderboard for meta-evaluation at [this http URL](http://explainaboard.nlpedia.ai/leaderboard/task-meval/) on the ExplainaBoard platform, which allows us to interactively understand the strengths, weaknesses, and complementarity of each metric.

| Comments: | Demo at [this http URL](http://explainaboard.nlpedia.ai/leaderboard/task-meval/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.11520](https://arxiv.org/abs/2106.11520) [cs.CL]** |
|           | (or **[arXiv:2106.11520v1](https://arxiv.org/abs/2106.11520v1) [cs.CL]** for this version) |





<h2 id="2021-06-23-6">6. Do Language Models Perform Generalizable Commonsense Inference?
</h2>

Title: [Do Language Models Perform Generalizable Commonsense Inference?](https://arxiv.org/abs/2106.11533)

Authors: [Peifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Filip Ilievski](https://arxiv.org/search/cs?searchtype=author&query=Ilievski%2C+F), [Muhao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Xiang Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X)

> Inspired by evidence that pretrained language models (LMs) encode commonsense knowledge, recent work has applied LMs to automatically populate commonsense knowledge graphs (CKGs). However, there is a lack of understanding on their generalization to multiple CKGs, unseen relations, and novel entities. This paper analyzes the ability of LMs to perform generalizable commonsense inference, in terms of knowledge capacity, transferability, and induction. Our experiments with these three aspects show that: (1) LMs can adapt to different schemas defined by multiple CKGs but fail to reuse the knowledge to generalize to new relations. (2) Adapted LMs generalize well to unseen subjects, but less so on novel objects. Future work should investigate how to improve the transferability and induction of commonsense mining from LMs.

| Comments: | 8 pages, 4 figures. Accepted to ACL'21 Findings              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2106.11533](https://arxiv.org/abs/2106.11533) [cs.CL]** |
|           | (or **[arXiv:2106.11533v1](https://arxiv.org/abs/2106.11533v1) [cs.CL]** for this version) |





<h2 id="2021-06-23-7">7. LV-BERT: Exploiting Layer Variety for BERT
</h2>

Title: [LV-BERT: Exploiting Layer Variety for BERT](https://arxiv.org/abs/2106.11740)

Authors: [Weihao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+W), [Zihang Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z), [Fei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+F), [Qibin Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+Q), [Jiashi Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J)

> Modern pre-trained language models are mostly built upon backbones stacking self-attention and feed-forward layers in an interleaved order. In this paper, beyond this stereotyped layer pattern, we aim to improve pre-trained models by exploiting layer variety from two aspects: the layer type set and the layer order. Specifically, besides the original self-attention and feed-forward layers, we introduce convolution into the layer type set, which is experimentally found beneficial to pre-trained models. Furthermore, beyond the original interleaved order, we explore more layer orders to discover more powerful architectures. However, the introduced layer variety leads to a large architecture space of more than billions of candidates, while training a single candidate model from scratch already requires huge computation cost, making it not affordable to search such a space by directly training large amounts of candidate models. To solve this problem, we first pre-train a supernet from which the weights of all candidate models can be inherited, and then adopt an evolutionary algorithm guided by pre-training accuracy to find the optimal architecture. Extensive experiments show that LV-BERT model obtained by our method outperforms BERT and its variants on various downstream tasks. For example, LV-BERT-small achieves 78.8 on the GLUE testing set, 1.8 higher than the strong baseline ELECTRA-small.

| Comments: | Accepted to Findings of ACL 2021. The code and pre-trained models are available at [this https URL](https://github.com/yuweihao/LV-BERT) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.11740](https://arxiv.org/abs/2106.11740) [cs.CL]** |
|           | (or **[arXiv:2106.11740v1](https://arxiv.org/abs/2106.11740v1) [cs.CL]** for this version) |






# 2021-06-22

[Return to Index](#Index)



<h2 id="2021-06-22-1">1. TCIC: Theme Concepts Learning Cross Language and Vision for Image Captioning
</h2>

Title: [TCIC: Theme Concepts Learning Cross Language and Vision for Image Captioning](https://arxiv.org/abs/2106.10936)

Authors: [Zhihao Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+Z), [Zhongyu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Z), [Siyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Ruize Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Zejun Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Haijun Shan](https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+H), [Xuanjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X)

> Existing research for image captioning usually represents an image using a scene graph with low-level facts (objects and relations) and fails to capture the high-level semantics. In this paper, we propose a Theme Concepts extended Image Captioning (TCIC) framework that incorporates theme concepts to represent high-level cross-modality semantics. In practice, we model theme concepts as memory vectors and propose Transformer with Theme Nodes (TTN) to incorporate those vectors for image captioning. Considering that theme concepts can be learned from both images and captions, we propose two settings for their representations learning based on TTN. On the vision side, TTN is configured to take both scene graph based features and theme concepts as input for visual representation learning. On the language side, TTN is configured to take both captions and theme concepts as input for text representation re-construction. Both settings aim to generate target captions with the same transformer-based decoder. During the training, we further align representations of theme concepts learned from images and corresponding captions to enforce the cross-modality learning. Experimental results on MS COCO show the effectiveness of our approach compared to some state-of-the-art models.

| Comments: | IJCAI2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.10936](https://arxiv.org/abs/2106.10936) [cs.CV]** |
|           | (or **[arXiv:2106.10936v1](https://arxiv.org/abs/2106.10936v1) [cs.CV]** for this version) |





<h2 id="2021-06-22-2">2. Interventional Video Grounding with Dual Contrastive Learning
</h2>

Title: [Interventional Video Grounding with Dual Contrastive Learning](https://arxiv.org/abs/2106.11013)

Authors: [Guoshun Nan](https://arxiv.org/search/cs?searchtype=author&query=Nan%2C+G), [Rui Qiao](https://arxiv.org/search/cs?searchtype=author&query=Qiao%2C+R), [Yao Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+Y), [Jun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Sicong Leng](https://arxiv.org/search/cs?searchtype=author&query=Leng%2C+S), [Hao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Wei Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+W)

> Video grounding aims to localize a moment from an untrimmed video for a given textual query. Existing approaches focus more on the alignment of visual and language stimuli with various likelihood-based matching or regression strategies, i.e., P(Y|X). Consequently, these models may suffer from spurious correlations between the language and video features due to the selection bias of the dataset. 1) To uncover the causality behind the model and data, we first propose a novel paradigm from the perspective of the causal inference, i.e., interventional video grounding (IVG) that leverages backdoor adjustment to deconfound the selection bias based on structured causal model (SCM) and do-calculus P(Y|do(X)). Then, we present a simple yet effective method to approximate the unobserved confounder as it cannot be directly sampled from the dataset. 2) Meanwhile, we introduce a dual contrastive learning approach (DCL) to better align the text and video by maximizing the mutual information (MI) between query and video clips, and the MI between start/end frames of a target moment and the others within a video to learn more informative visual representations. Experiments on three standard benchmarks show the effectiveness of our approaches.

| Comments: | Accepted in CVPR 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.11013](https://arxiv.org/abs/2106.11013) [cs.CV]** |
|           | (or **[arXiv:2106.11013v1](https://arxiv.org/abs/2106.11013v1) [cs.CV]** for this version) |





<h2 id="2021-06-22-3">3. CPM-2: Large-scale Cost-effective Pre-trained Language Models
</h2>

Title: [CPM-2: Large-scale Cost-effective Pre-trained Language Models](https://arxiv.org/abs/2106.10715)

Authors: [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Yuxian Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+Y), [Xu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Shengqi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Chaojun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C), [Zhenbo Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Yuan Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y), [Fanchao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+F), [Jian Guan](https://arxiv.org/search/cs?searchtype=author&query=Guan%2C+J), [Pei Ke](https://arxiv.org/search/cs?searchtype=author&query=Ke%2C+P), [Yanzheng Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+Y), [Guoyang Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+G), [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Minlie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M), [Wentao Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+W), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Xiaoyan Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

> In recent years, the size of pre-trained language models (PLMs) has grown by leaps and bounds. However, efficiency issues of these large-scale PLMs limit their utilization in real-world scenarios. We present a suite of cost-effective techniques for the use of PLMs to deal with the efficiency issues of pre-training, fine-tuning, and inference. (1) We introduce knowledge inheritance to accelerate the pre-training process by exploiting existing PLMs instead of training models from scratch. (2) We explore the best practice of prompt tuning with large-scale PLMs. Compared with conventional fine-tuning, prompt tuning significantly reduces the number of task-specific parameters. (3) We implement a new inference toolkit, namely InfMoE, for using large-scale PLMs with limited computational resources. Based on our cost-effective pipeline, we pre-train two models: an encoder-decoder bilingual model with 11 billion parameters (CPM-2) and its corresponding MoE version with 198 billion parameters. In our experiments, we compare CPM-2 with mT5 on downstream tasks. Experimental results show that CPM-2 has excellent general language intelligence. Moreover, we validate the efficiency of InfMoE when conducting inference of large-scale models having tens of billions of parameters on a single GPU. All source code and model parameters are available at [this https URL](https://github.com/TsinghuaAI/CPM).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.10715](https://arxiv.org/abs/2106.10715) [cs.CL]** |
|           | (or **[arXiv:2106.10715v1](https://arxiv.org/abs/2106.10715v1) [cs.CL]** for this version) |





<h2 id="2021-06-22-4">4. Challenges in Translation of Emotions in Multilingual User-Generated Content: Twitter as a Case Study
</h2>

Title: [Challenges in Translation of Emotions in Multilingual User-Generated Content: Twitter as a Case Study](https://arxiv.org/abs/2106.10719)

Authors: [Hadeel Saadany](https://arxiv.org/search/cs?searchtype=author&query=Saadany%2C+H), [Constantin Orasan](https://arxiv.org/search/cs?searchtype=author&query=Orasan%2C+C), [Rocio Caro Quintana](https://arxiv.org/search/cs?searchtype=author&query=Quintana%2C+R+C), [Felix do Carmo](https://arxiv.org/search/cs?searchtype=author&query=Carmo%2C+F+d), [Leonardo Zilio](https://arxiv.org/search/cs?searchtype=author&query=Zilio%2C+L)

> Although emotions are universal concepts, transferring the different shades of emotion from one language to another may not always be straightforward for human translators, let alone for machine translation systems. Moreover, the cognitive states are established by verbal explanations of experience which is shaped by both the verbal and cultural contexts. There are a number of verbal contexts where expression of emotions constitutes the pivotal component of the message. This is particularly true for User-Generated Content (UGC) which can be in the form of a review of a product or a service, a tweet, or a social media post. Recently, it has become common practice for multilingual websites such as Twitter to provide an automatic translation of UGC to reach out to their linguistically diverse users. In such scenarios, the process of translating the user's emotion is entirely automatic with no human intervention, neither for post-editing nor for accuracy checking. In this research, we assess whether automatic translation tools can be a successful real-life utility in transferring emotion in user-generated multilingual data such as tweets. We show that there are linguistic phenomena specific of Twitter data that pose a challenge in translation of emotions in different languages. We summarise these challenges in a list of linguistic features and show how frequent these features are in different language pairs. We also assess the capacity of commonly used methods for evaluating the performance of an MT system with respect to the preservation of emotion in the source text.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.10719](https://arxiv.org/abs/2106.10719) [cs.CL]** |
|           | (or **[arXiv:2106.10719v1](https://arxiv.org/abs/2106.10719v1) [cs.CL]** for this version) |





<h2 id="2021-06-22-5">5. Pay Better Attention to Attention: Head Selection in Multilingual and Multi-Domain Sequence Modeling
</h2>

Title: [Pay Better Attention to Attention: Head Selection in Multilingual and Multi-Domain Sequence Modeling](https://arxiv.org/abs/2106.10840)

Authors: [Hongyu Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H), [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X)

> Multi-head attention has each of the attention heads collect salient information from different parts of an input sequence, making it a powerful mechanism for sequence modeling. Multilingual and multi-domain learning are common scenarios for sequence modeling, where the key challenge is to maximize positive transfer and mitigate negative transfer across languages and domains. In this paper, we find that non-selective attention sharing is sub-optimal for achieving good generalization across all languages and domains. We further propose attention sharing strategies to facilitate parameter sharing and specialization in multilingual and multi-domain sequence modeling. Our approach automatically learns shared and specialized attention heads for different languages and domains to mitigate their interference. Evaluated in various tasks including speech recognition, text-to-text and speech-to-text translation, the proposed attention sharing strategies consistently bring gains to sequence models built upon multi-head attention. For speech-to-text translation, our approach yields an average of +2.0 BLEU over 13 language directions in multilingual setting and +2.0 BLEU over 3 domains in multi-domain setting.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.10840](https://arxiv.org/abs/2106.10840) [cs.CL]** |
|           | (or **[arXiv:2106.10840v1](https://arxiv.org/abs/2106.10840v1) [cs.CL]** for this version) |





# 2021-06-21

[Return to Index](#Index)



<h2 id="2021-06-21-1">1. Multi-mode Transformer Transducer with Stochastic Future Context
</h2>

Title: [Multi-mode Transformer Transducer with Stochastic Future Context](https://arxiv.org/abs/2106.09760)

Authors: [Kwangyoun Kim](https://arxiv.org/search/eess?searchtype=author&query=Kim%2C+K), [Felix Wu](https://arxiv.org/search/eess?searchtype=author&query=Wu%2C+F), [Prashant Sridhar](https://arxiv.org/search/eess?searchtype=author&query=Sridhar%2C+P), [Kyu J. Han](https://arxiv.org/search/eess?searchtype=author&query=Han%2C+K+J), [Shinji Watanabe](https://arxiv.org/search/eess?searchtype=author&query=Watanabe%2C+S)

> Automatic speech recognition (ASR) models make fewer errors when more surrounding speech information is presented as context. Unfortunately, acquiring a larger future context leads to higher latency. There exists an inevitable trade-off between speed and accuracy. Naively, to fit different latency requirements, people have to store multiple models and pick the best one under the constraints. Instead, a more desirable approach is to have a single model that can dynamically adjust its latency based on different constraints, which we refer to as Multi-mode ASR. A Multi-mode ASR model can fulfill various latency requirements during inference -- when a larger latency becomes acceptable, the model can process longer future context to achieve higher accuracy and when a latency budget is not flexible, the model can be less dependent on future context but still achieve reliable accuracy. In pursuit of Multi-mode ASR, we propose Stochastic Future Context, a simple training procedure that samples one streaming configuration in each iteration. Through extensive experiments on AISHELL-1 and LibriSpeech datasets, we show that a Multi-mode ASR model rivals, if not surpasses, a set of competitive streaming baselines trained with different latency budgets.

| Comments: | Accepted to Interspeech 2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2106.09760](https://arxiv.org/abs/2106.09760) [eess.AS]** |
|           | (or **[arXiv:2106.09760v1](https://arxiv.org/abs/2106.09760v1) [eess.AS]** for this version) |





<h2 id="2021-06-21-2">2. Investigating the Role of Negatives in Contrastive Representation Learning
</h2>

Title: [Investigating the Role of Negatives in Contrastive Representation Learning](https://arxiv.org/abs/2106.09943)

Authors: [Jordan T. Ash](https://arxiv.org/search/cs?searchtype=author&query=Ash%2C+J+T), [Surbhi Goel](https://arxiv.org/search/cs?searchtype=author&query=Goel%2C+S), [Akshay Krishnamurthy](https://arxiv.org/search/cs?searchtype=author&query=Krishnamurthy%2C+A), [Dipendra Misra](https://arxiv.org/search/cs?searchtype=author&query=Misra%2C+D)

> Noise contrastive learning is a popular technique for unsupervised representation learning. In this approach, a representation is obtained via reduction to supervised learning, where given a notion of semantic similarity, the learner tries to distinguish a similar (positive) example from a collection of random (negative) examples. The success of modern contrastive learning pipelines relies on many parameters such as the choice of data augmentation, the number of negative examples, and the batch size; however, there is limited understanding as to how these parameters interact and affect downstream performance. We focus on disambiguating the role of one of these parameters: the number of negative examples. Theoretically, we show the existence of a collision-coverage trade-off suggesting that the optimal number of negative examples should scale with the number of underlying concepts in the data. Empirically, we scrutinize the role of the number of negatives in both NLP and vision tasks. In the NLP task, we find that the results broadly agree with our theory, while our vision experiments are murkier with performance sometimes even being insensitive to the number of negatives. We discuss plausible explanations for this behavior and suggest future directions to better align theory and practice.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.09943](https://arxiv.org/abs/2106.09943) [cs.LG]** |
|           | (or **[arXiv:2106.09943v1](https://arxiv.org/abs/2106.09943v1) [cs.LG]** for this version) |





<h2 id="2021-06-21-3">3. BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models
</h2>

Title: [BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199)

Authors: [Elad Ben Zaken](https://arxiv.org/search/cs?searchtype=author&query=Zaken%2C+E+B), [Shauli Ravfogel](https://arxiv.org/search/cs?searchtype=author&query=Ravfogel%2C+S), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y)

> We show that with small-to-medium training data, fine-tuning only the bias terms (or a subset of the bias terms) of pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, bias-only fine-tuning is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.10199](https://arxiv.org/abs/2106.10199) [cs.LG]** |
|           | (or **[arXiv:2106.10199v1](https://arxiv.org/abs/2106.10199v1) [cs.LG]** for this version) |





<h2 id="2021-06-21-4">4. GEM: A General Evaluation Benchmark for Multimodal Tasks
</h2>

Title: [GEM: A General Evaluation Benchmark for Multimodal Tasks](https://arxiv.org/abs/2106.09889)

Authors: [Lin Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+L), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Edward Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+E), [Lei Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+L), [Chenfei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Huaishao Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+H), [Yongfei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Ming Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+M), [Taroon Bharti](https://arxiv.org/search/cs?searchtype=author&query=Bharti%2C+T), [Arun Sacheti](https://arxiv.org/search/cs?searchtype=author&query=Sacheti%2C+A)

> In this paper, we present GEM as a General Evaluation benchmark for Multimodal tasks. Different from existing datasets such as GLUE, SuperGLUE, XGLUE and XTREME that mainly focus on natural language tasks, GEM is a large-scale vision-language benchmark, which consists of GEM-I for image-language tasks and GEM-V for video-language tasks. Comparing with existing multimodal datasets such as MSCOCO and Flicker30K for image-language tasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the largest vision-language dataset covering image-language tasks and video-language tasks at the same time, but also labeled in multiple languages. We also provide two baseline models for this benchmark. We will release the dataset, code and baseline models, aiming to advance the development of multilingual multimodal research.

| Comments: | Accepted by Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM) |
| Cite as:  | **[arXiv:2106.09889](https://arxiv.org/abs/2106.09889) [cs.CL]** |
|           | (or **[arXiv:2106.09889v1](https://arxiv.org/abs/2106.09889v1) [cs.CL]** for this version) |





<h2 id="2021-06-21-5">5. Bad Characters: Imperceptible NLP Attacks
</h2>

Title: [Bad Characters: Imperceptible NLP Attacks](https://arxiv.org/abs/2106.09898)

Authors: [Nicholas Boucher](https://arxiv.org/search/cs?searchtype=author&query=Boucher%2C+N), [Ilia Shumailov](https://arxiv.org/search/cs?searchtype=author&query=Shumailov%2C+I), [Ross Anderson](https://arxiv.org/search/cs?searchtype=author&query=Anderson%2C+R), [Nicolas Papernot](https://arxiv.org/search/cs?searchtype=author&query=Papernot%2C+N)

> Several years of research have shown that machine-learning systems are vulnerable to adversarial examples, both in theory and in practice. Until now, such attacks have primarily targeted visual models, exploiting the gap between human and machine perception. Although text-based models have also been attacked with adversarial examples, such attacks struggled to preserve semantic meaning and indistinguishability. In this paper, we explore a large class of adversarial examples that can be used to attack text-based models in a black-box setting without making any human-perceptible visual modification to inputs. We use encoding-specific perturbations that are imperceptible to the human eye to manipulate the outputs of a wide range of Natural Language Processing (NLP) systems from neural machine-translation pipelines to web search engines. We find that with a single imperceptible encoding injection -- representing one invisible character, homoglyph, reordering, or deletion -- an attacker can significantly reduce the performance of vulnerable models, and with three injections most models can be functionally broken. Our attacks work against currently-deployed commercial systems, including those produced by Microsoft and Google, in addition to open source models published by Facebook and IBM. This novel series of attacks presents a significant threat to many language processing systems: an attacker can affect systems in a targeted manner without any assumptions about the underlying model. We conclude that text-based NLP systems require careful input sanitization, just like conventional applications, and that given such systems are now being deployed rapidly at scale, the urgent attention of architects and operators is required.

| Subjects: | **Computation and Language (cs.CL)**; Cryptography and Security (cs.CR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.09898](https://arxiv.org/abs/2106.09898) [cs.CL]** |
|           | (or **[arXiv:2106.09898v1](https://arxiv.org/abs/2106.09898v1) [cs.CL]** for this version) |







<h2 id="2021-06-21-6">6. Recurrent Stacking of Layers in Neural Networks: An Application to Neural Machine Translation
</h2>

Title: [Recurrent Stacking of Layers in Neural Networks: An Application to Neural Machine Translation](https://arxiv.org/abs/2106.10002)

Authors: [Raj Dabre](https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R), [Atsushi Fujita](https://arxiv.org/search/cs?searchtype=author&query=Fujita%2C+A)

> In deep neural network modeling, the most common practice is to stack a number of recurrent, convolutional, or feed-forward layers in order to obtain high-quality continuous space representations which in turn improves the quality of the network's prediction. Conventionally, each layer in the stack has its own parameters which leads to a significant increase in the number of model parameters. In this paper, we propose to share parameters across all layers thereby leading to a recurrently stacked neural network model. We report on an extensive case study on neural machine translation (NMT), where we apply our proposed method to an encoder-decoder based neural network model, i.e., the Transformer model, and experiment with three Japanese--English translation datasets. We empirically demonstrate that the translation quality of a model that recurrently stacks a single layer 6 times, despite having significantly fewer parameters, approaches that of a model that stacks 6 layers where each layer has different parameters. We also explore the limits of recurrent stacking where we train extremely deep NMT models. This paper also examines the utility of our recurrently stacked model as a student model through transfer learning via leveraging pre-trained parameters and knowledge distillation, and shows that it compensates for the performance drops in translation quality that the direct training of recurrently stacked model brings. We also show how transfer learning helps in faster decoding on top of the already reduced number of parameters due to recurrent stacking. Finally, we analyze the effects of recurrently stacked layers by visualizing the attentions of models that use recurrently stacked layers and models that do not.

| Comments: | 22 pages. Under review. Work in progress. Extended version of [this https URL](https://ojs.aaai.org//index.php/AAAI/article/view/4590) which is an extension of [arXiv:1807.05353](https://arxiv.org/abs/1807.05353) . The focus is on analyzing the limitations of recurrently stacked layers and methods to overcome said limitations |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.10002](https://arxiv.org/abs/2106.10002) [cs.CL]** |
|           | (or **[arXiv:2106.10002v1](https://arxiv.org/abs/2106.10002v1) [cs.CL]** for this version) |





<h2 id="2021-06-21-7">7. Challenges and Limitations with the Metrics Measuring the Complexity of Code-Mixed Text
</h2>

Title: [Challenges and Limitations with the Metrics Measuring the Complexity of Code-Mixed Text](https://arxiv.org/abs/2106.10123)

Authors: [Vivek Srivastava](https://arxiv.org/search/cs?searchtype=author&query=Srivastava%2C+V), [Mayank Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M)

> Code-mixing is a frequent communication style among multilingual speakers where they mix words and phrases from two different languages in the same utterance of text or speech. Identifying and filtering code-mixed text is a challenging task due to its co-existence with monolingual and noisy text. Over the years, several code-mixing metrics have been extensively used to identify and validate code-mixed text quality. This paper demonstrates several inherent limitations of code-mixing metrics with examples from the already existing datasets that are popularly used across various experiments.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.10123](https://arxiv.org/abs/2106.10123) [cs.CL]** |
|           | (or **[arXiv:2106.10123v1](https://arxiv.org/abs/2106.10123v1) [cs.CL]** for this version) |








# 2021-06-18

[Return to Index](#Index)



<h2 id="2021-06-18-1">1. Specializing Multilingual Language Models: An Empirical Study
</h2>

Title: [Specializing Multilingual Language Models: An Empirical Study](https://arxiv.org/abs/2106.09063)

Authors: [Ethan C. Chau](https://arxiv.org/search/cs?searchtype=author&query=Chau%2C+E+C), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> Contextualized word representations from pretrained multilingual language models have become the de facto standard for addressing natural language tasks in many different languages, but the success of this approach is far from universal. For languages rarely or never seen by these models, directly using such models often results in suboptimal representation or use of data, motivating additional model adaptations to achieve reasonably strong performance. In this work, we study the performance, extensibility, and interaction of two such adaptations for this low-resource setting: vocabulary augmentation and script transliteration. Our evaluations on a set of three tasks in nine diverse low-resource languages yield a mixed result, upholding the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low-resource settings.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.09063](https://arxiv.org/abs/2106.09063) [cs.CL]** |
|           | (or **[arXiv:2106.09063v1](https://arxiv.org/abs/2106.09063v1) [cs.CL]** for this version) |



<h2 id="2021-06-18-2">2. Probing Image-Language Transformers for Verb Understanding
</h2>

Title: [Probing Image-Language Transformers for Verb Understanding](https://arxiv.org/abs/2106.09141)

Authors: [Lisa Anne Hendricks](https://arxiv.org/search/cs?searchtype=author&query=Hendricks%2C+L+A), [Aida Nematzadeh](https://arxiv.org/search/cs?searchtype=author&query=Nematzadeh%2C+A)

> Multimodal image-language transformers have achieved impressive results on a variety of tasks that rely on fine-tuning (e.g., visual question answering and image retrieval). We are interested in shedding light on the quality of their pretrained representations -- in particular, if these models can distinguish different types of verbs or if they rely solely on nouns in a given sentence. To do so, we collect a dataset of image-sentence pairs (in English) consisting of 421 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset). We use this dataset to evaluate pretrained image-language transformers and find that they fail more in situations that require verb understanding compared to other parts of speech. We also investigate what category of verbs are particularly challenging.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.09141](https://arxiv.org/abs/2106.09141) [cs.CL]** |
|           | (or **[arXiv:2106.09141v1](https://arxiv.org/abs/2106.09141v1) [cs.CL]** for this version) |





<h2 id="2021-06-18-3">3. An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models
</h2>

Title: [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://arxiv.org/abs/2106.09204)

Authors: [Xueqing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Chi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C)

> The performance of fine-tuning pre-trained language models largely depends on the hyperparameter configuration. In this paper, we investigate the performance of modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained language models. First, we study and report three HPO algorithms' performances on fine-tuning two state-of-the-art language models on the GLUE dataset. We find that using the same time budget, HPO often fails to outperform grid search due to two reasons: insufficient time budget and overfitting. We propose two general strategies and an experimental procedure to systematically troubleshoot HPO's failure cases. By applying the procedure, we observe that HPO can succeed with more appropriate settings in the search space and time budget; however, in certain cases overfitting remains. Finally, we make suggestions for future work. Our implementation can be found in [this https URL](https://github.com/microsoft/FLAML/tree/main/flaml/nlp/).

| Comments: | To appear in ACL-IJCNLP 2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.09204](https://arxiv.org/abs/2106.09204) [cs.CL]** |
|           | (or **[arXiv:2106.09204v1](https://arxiv.org/abs/2106.09204v1) [cs.CL]** for this version) |





<h2 id="2021-06-18-4">4. Lost in Interpreting: Speech Translation from Source or Interpreter?
</h2>

Title: [Lost in Interpreting: Speech Translation from Source or Interpreter?](https://arxiv.org/abs/2106.09343)

Authors: [Dominik Macháček](https://arxiv.org/search/cs?searchtype=author&query=Macháček%2C+D), [Matúš Žilinec](https://arxiv.org/search/cs?searchtype=author&query=Žilinec%2C+M), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

> Interpreters facilitate multi-lingual meetings but the affordable set of languages is often smaller than what is needed. Automatic simultaneous speech translation can extend the set of provided languages. We investigate if such an automatic system should rather follow the original speaker, or an interpreter to achieve better translation quality at the cost of increased delay.
> To answer the question, we release Europarl Simultaneous Interpreting Corpus (ESIC), 10 hours of recordings and transcripts of European Parliament speeches in English, with simultaneous interpreting into Czech and German. We evaluate quality and latency of speaker-based and interpreter-based spoken translation systems from English to Czech. We study the differences in implicit simplification and summarization of the human interpreter compared to a machine translation system trained to shorten the output to some extent. Finally, we perform human evaluation to measure information loss of each of these approaches.

| Comments: | to be published at INTERSPEECH 2021                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.09343](https://arxiv.org/abs/2106.09343) [cs.CL]** |
|           | (or **[arXiv:2106.09343v1](https://arxiv.org/abs/2106.09343v1) [cs.CL]** for this version) |





<h2 id="2021-06-18-5">5. Modeling Worlds in Text
</h2>

Title: [Modeling Worlds in Text](https://arxiv.org/abs/2106.09578)

Authors: [Prithviraj Ammanabrolu](https://arxiv.org/search/cs?searchtype=author&query=Ammanabrolu%2C+P), [Mark O. Riedl](https://arxiv.org/search/cs?searchtype=author&query=Riedl%2C+M+O)

> We provide a dataset that enables the creation of learning agents that can build knowledge graph-based world models of interactive narratives. Interactive narratives -- or text-adventure games -- are partially observable environments structured as long puzzles or quests in which an agent perceives and interacts with the world purely through textual natural language. Each individual game typically contains hundreds of locations, characters, and objects -- each with their own unique descriptions -- providing an opportunity to study the problem of giving language-based agents the structured memory necessary to operate in such worlds. Our dataset provides 24198 mappings between rich natural language observations and: (1) knowledge graphs that reflect the world state in the form of a map; (2) natural language actions that are guaranteed to cause a change in that particular world state. The training data is collected across 27 games in multiple genres and contains a further 7836 heldout instances over 9 additional games in the test set. We further provide baseline models using rules-based, question-answering, and sequence learning approaches in addition to an analysis of the data and corresponding learning tasks.

| Comments: | Preprint. Under review. Benchmark can be found at [this https URL](https://github.com/JerichoWorld/JerichoWorld) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2106.09578](https://arxiv.org/abs/2106.09578) [cs.CL]** |
|           | (or **[arXiv:2106.09578v1](https://arxiv.org/abs/2106.09578v1) [cs.CL]** for this version) |





<h2 id="2021-06-18-6">6. Multi-head or Single-head? An Empirical Comparison for Transformer Training
</h2>

Title: [Multi-head or Single-head? An Empirical Comparison for Transformer Training](https://arxiv.org/abs/2106.09650)

Authors: [Liyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Jialu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Jiawei Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J)

> Multi-head attention plays a crucial role in the recent success of Transformer models, which leads to consistent performance improvements over conventional attention in various applications. The popular belief is that this effectiveness stems from the ability of jointly attending multiple positions. In this paper, we first demonstrate that jointly attending multiple positions is not a unique feature of multi-head attention, as multi-layer single-head attention also attends multiple positions and is more effective. Then, we suggest the main advantage of the multi-head attention is the training stability, since it has less number of layers than the single-head attention, when attending the same number of positions. For example, 24-layer 16-head Transformer (BERT-large) and 384-layer single-head Transformer has the same total attention head number and roughly the same model size, while the multi-head one is significantly shallower. Meanwhile, we show that, with recent advances in deep learning, we can successfully stabilize the training of the 384-layer Transformer. As the training difficulty is no longer a bottleneck, substantially deeper single-head Transformer achieves consistent performance improvements without tuning hyper-parameters.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.09650](https://arxiv.org/abs/2106.09650) [cs.CL]** |
|           | (or **[arXiv:2106.09650v1](https://arxiv.org/abs/2106.09650v1) [cs.CL]** for this version) |








# 2021-06-17

[Return to Index](#Index)



<h2 id="2021-06-17-1">1. Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors
</h2>

Title: [Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors](https://arxiv.org/abs/2106.08415)

Authors: [Junayed Mahmud](https://arxiv.org/search/cs?searchtype=author&query=Mahmud%2C+J), [Fahim Faisal](https://arxiv.org/search/cs?searchtype=author&query=Faisal%2C+F), [Raihan Islam Arnob](https://arxiv.org/search/cs?searchtype=author&query=Arnob%2C+R+I), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Kevin Moran](https://arxiv.org/search/cs?searchtype=author&query=Moran%2C+K)

> Automated source code summarization is a popular software engineering research topic wherein machine translation models are employed to "translate" code snippets into relevant natural language descriptions. Most evaluations of such models are conducted using automatic reference-based metrics. However, given the relatively large semantic gap between programming languages and natural language, we argue that this line of research would benefit from a qualitative investigation into the various error modes of current state-of-the-art models. Therefore, in this work, we perform both a quantitative and qualitative comparison of three recently proposed source code summarization models. In our quantitative evaluation, we compare the models based on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics, and in our qualitative evaluation, we perform a manual open-coding of the most common errors committed by the models when compared to ground truth captions. Our investigation reveals new insights into the relationship between metric-based performance and model prediction errors grounded in an empirically derived error taxonomy that can be used to drive future research efforts

| Comments: | Accepted to the 2021 NLP4Prog Workshop co-located with The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Software Engineering (cs.SE)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.08415](https://arxiv.org/abs/2106.08415) [cs.SE]** |
|           | (or **[arXiv:2106.08415v1](https://arxiv.org/abs/2106.08415v1) [cs.SE]** for this version) |





<h2 id="2021-06-17-2">2. What Context Features Can Transformer Language Models Use?
</h2>

Title: [What Context Features Can Transformer Language Models Use?](https://arxiv.org/abs/2106.08367)

Authors: [Joe O'Connor](https://arxiv.org/search/cs?searchtype=author&query=O'Connor%2C+J), [Jacob Andreas](https://arxiv.org/search/cs?searchtype=author&query=Andreas%2C+J)

> Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations -- including shuffling word order within sentences and deleting all words other than nouns -- remove less than 15% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.

| Comments: | 14 pages, 7 figures, to be published at ACL 2021             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.08367](https://arxiv.org/abs/2106.08367) [cs.CL]** |
|           | (or **[arXiv:2106.08367v1](https://arxiv.org/abs/2106.08367v1) [cs.CL]** for this version) |





<h2 id="2021-06-17-3">3. Alternated Training with Synthetic and Authentic Data for Neural Machine Translation
</h2>

Title: [Alternated Training with Synthetic and Authentic Data for Neural Machine Translation](https://arxiv.org/abs/2106.08582)

Authors: [Rui Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+R), [Zonghan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> While synthetic bilingual corpora have demonstrated their effectiveness in low-resource neural machine translation (NMT), adding more synthetic data often deteriorates translation performance. In this work, we propose alternated training with synthetic and authentic data for NMT. The basic idea is to alternate synthetic and authentic corpora iteratively during training. Compared with previous work, we introduce authentic data as guidance to prevent the training of NMT models from being disturbed by noisy synthetic data. Experiments on Chinese-English and German-English translation tasks show that our approach improves the performance over several strong baselines. We visualize the BLEU landscape to further investigate the role of authentic and synthetic data during alternated training. From the visualization, we find that authentic data helps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement.

| Comments: | ACL 2021, Short Findings                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.08582](https://arxiv.org/abs/2106.08582) [cs.CL]** |
|           | (or **[arXiv:2106.08582v1](https://arxiv.org/abs/2106.08582v1) [cs.CL]** for this version) |





<h2 id="2021-06-17-4">4. Semantic sentence similarity: size does not always matter
</h2>

Title: [Semantic sentence similarity: size does not always matter](https://arxiv.org/abs/2106.08648)

Authors: [Danny Merkx](https://arxiv.org/search/cs?searchtype=author&query=Merkx%2C+D), [Stefan L. Frank](https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+S+L), [Mirjam Ernestus](https://arxiv.org/search/cs?searchtype=author&query=Ernestus%2C+M)

> This study addresses the question whether visually grounded speech recognition (VGS) models learn to capture sentence semantics without access to any prior linguistic knowledge. We produce synthetic and natural spoken versions of a well known semantic textual similarity database and show that our VGS model produces embeddings that correlate well with human semantic similarity judgements. Our results show that a model trained on a small image-caption database outperforms two models trained on much larger databases, indicating that database size is not all that matters. We also investigate the importance of having multiple captions per image and find that this is indeed helpful even if the total number of images is lower, suggesting that paraphrasing is a valuable learning signal. While the general trend in the field is to create ever larger datasets to train models on, our findings indicate other characteristics of the database can just as important important.

| Comments: | This paper has been accepted at Interspeech 2021 where it will be presented and appear in the conference proceedings in September 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.08648](https://arxiv.org/abs/2106.08648) [cs.CL]** |
|           | (or **[arXiv:2106.08648v1](https://arxiv.org/abs/2106.08648v1) [cs.CL]** for this version) |







<h2 id="2021-06-17-5">5. Evaluating Gender Bias in Hindi-English Machine Translation
</h2>

Title: [Evaluating Gender Bias in Hindi-English Machine Translation](https://arxiv.org/abs/2106.08680)

Authors: [Gauri Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+G), [Krithika Ramesh](https://arxiv.org/search/cs?searchtype=author&query=Ramesh%2C+K), [Sanjay Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+S)

> With language models being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these language models often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like Hindi, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for Indic languages. In our work, we attempt to evaluate and quantify the gender bias within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for Hindi. We also compare and contrast the resulting bias measurements across multiple metrics for pre-trained embeddings and the ones learned by our machine translation model.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.08680](https://arxiv.org/abs/2106.08680) [cs.CL]** |
|           | (or **[arXiv:2106.08680v1](https://arxiv.org/abs/2106.08680v1) [cs.CL]** for this version) |



<h2 id="2021-06-17-6">6. Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study
</h2>

Title: [Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study](https://arxiv.org/abs/2106.08686)

Authors: [Badr M. Abdullah](https://arxiv.org/search/cs?searchtype=author&query=Abdullah%2C+B+M), [Marius Mosbach](https://arxiv.org/search/cs?searchtype=author&query=Mosbach%2C+M), [Iuliia Zaitova](https://arxiv.org/search/cs?searchtype=author&query=Zaitova%2C+I), [Bernd Möbius](https://arxiv.org/search/cs?searchtype=author&query=Möbius%2C+B), [Dietrich Klakow](https://arxiv.org/search/cs?searchtype=author&query=Klakow%2C+D)

> Several variants of deep neural networks have been successfully employed for building parametric models that project variable-duration spoken word segments onto fixed-size vector representations, or acoustic word embeddings (AWEs). However, it remains unclear to what degree we can rely on the distance in the emerging AWE space as an estimate of word-form similarity. In this paper, we ask: does the distance in the acoustic embedding space correlate with phonological dissimilarity? To answer this question, we empirically investigate the performance of supervised approaches for AWEs with different neural architectures and learning objectives. We train AWE models in controlled settings for two languages (German and Czech) and evaluate the embeddings on two tasks: word discrimination and phonological similarity. Our experiments show that (1) the distance in the embedding space in the best cases only moderately correlates with phonological distance, and (2) improving the performance on the word discrimination task does not necessarily yield models that better reflect word phonological similarity. Our findings highlight the necessity to rethink the current intrinsic evaluations for AWEs.

| Comments: | Accepted in Interspeech 2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2106.08686](https://arxiv.org/abs/2106.08686) [cs.CL]** |
|           | (or **[arXiv:2106.08686v1](https://arxiv.org/abs/2106.08686v1) [cs.CL]** for this version) |







<h2 id="2021-06-17-7">7. RefBERT: Compressing BERT by Referencing to Pre-computed Representations
</h2>

Title: [RefBERT: Compressing BERT by Referencing to Pre-computed Representations](https://arxiv.org/abs/2106.08898)

Authors: [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Haiqin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H), [Liang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+L), [Yang Mo](https://arxiv.org/search/cs?searchtype=author&query=Mo%2C+Y), [Jianping Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J)

> Recently developed large pre-trained language models, e.g., BERT, have achieved remarkable performance in many downstream natural language processing applications. These pre-trained language models often contain hundreds of millions of parameters and suffer from high computation and latency in real-world applications. It is desirable to reduce the computation overhead of the models for fast training and inference while keeping the model performance in downstream applications. Several lines of work utilize knowledge distillation to compress the teacher model to a smaller student model. However, they usually discard the teacher's knowledge when in inference. Differently, in this paper, we propose RefBERT to leverage the knowledge learned from the teacher, i.e., facilitating the pre-computed BERT representation on the reference sample and compressing BERT into a smaller student model. To guarantee our proposal, we provide theoretical justification on the loss function and the usage of reference samples. Significantly, the theoretical result shows that including the pre-computed teacher's representations on the reference samples indeed increases the mutual information in learning the student model. Finally, we conduct the empirical evaluation and show that our RefBERT can beat the vanilla TinyBERT over 8.1\% and achieves more than 94\% of the performance of $\BERTBASE$ on the GLUE benchmark. Meanwhile, RefBERT is 7.4x smaller and 9.5x faster on inference than BERTBASE.

| Comments: | 8 pages, 1 figure, 3 tables, in IJCNN'21                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.08898](https://arxiv.org/abs/2106.08898) [cs.CL]** |
|           | (or **[arXiv:2106.08898v1](https://arxiv.org/abs/2106.08898v1) [cs.CL]** for this version) |





<h2 id="2021-06-17-8">8. Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation
</h2>

Title: [Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation](https://arxiv.org/abs/2106.08942)

Authors: [Samuel Kiegeland](https://arxiv.org/search/cs?searchtype=author&query=Kiegeland%2C+S), [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J)

> Policy gradient algorithms have found wide adoption in NLP, but have recently become subject to criticism, doubting their suitability for NMT. Choshen et al. (2020) identify multiple weaknesses and suspect that their success is determined by the shape of output distributions rather than the reward. In this paper, we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward scaling, and provide empirical counter-evidence to these claims.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | North American Chapter of the Association for Computational Linguistics, 2021, 1673-1681 |
| DOI:               | [10.18653/v1/2021.naacl-main.133](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18653%2Fv1%2F2021.naacl-main.133&v=5c72b5f8) |
| Cite as:           | **[arXiv:2106.08942](https://arxiv.org/abs/2106.08942) [cs.CL]** |
|                    | (or **[arXiv:2106.08942v1](https://arxiv.org/abs/2106.08942v1) [cs.CL]** for this version) |





<h2 id="2021-06-17-9">9. Collaborative Training of Acoustic Encoders for Speech Recognition
</h2>

Title: [Collaborative Training of Acoustic Encoders for Speech Recognition](https://arxiv.org/abs/2106.08960)

Authors: [Varun Nagaraja](https://arxiv.org/search/cs?searchtype=author&query=Nagaraja%2C+V), [Yangyang Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y), [Ganesh Venkatesh](https://arxiv.org/search/cs?searchtype=author&query=Venkatesh%2C+G), [Ozlem Kalinli](https://arxiv.org/search/cs?searchtype=author&query=Kalinli%2C+O), [Michael L. Seltzer](https://arxiv.org/search/cs?searchtype=author&query=Seltzer%2C+M+L), [Vikas Chandra](https://arxiv.org/search/cs?searchtype=author&query=Chandra%2C+V)

> On-device speech recognition requires training models of different sizes for deploying on devices with various computational budgets. When building such different models, we can benefit from training them jointly to take advantage of the knowledge shared between them. Joint training is also efficient since it reduces the redundancy in the training procedure's data handling operations. We propose a method for collaboratively training acoustic encoders of different sizes for speech recognition. We use a sequence transducer setup where different acoustic encoders share a common predictor and joiner modules. The acoustic encoders are also trained using co-distillation through an auxiliary task for frame level chenone prediction, along with the transducer loss. We perform experiments using the LibriSpeech corpus and demonstrate that the collaboratively trained acoustic encoders can provide up to a 11% relative improvement in the word error rate on both the test partitions.

| Comments: | INTERSPEECH 2021                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.08960](https://arxiv.org/abs/2106.08960) [cs.CL]** |
|           | (or **[arXiv:2106.08960v1](https://arxiv.org/abs/2106.08960v1) [cs.CL]** for this version) |






# 2021-06-16

[Return to Index](#Index)



<h2 id="2021-06-16-1">1. Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup
</h2>

Title: [Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup](https://arxiv.org/abs/2101.06983)

Authors: [Luyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+L), [Yunyi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Jiawei Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J), [Jamie Callan](https://arxiv.org/search/cs?searchtype=author&query=Callan%2C+J)

> Contrastive learning has been applied successfully to learn vector representations of text. Previous research demonstrated that learning high-quality representations benefits from batch-wise contrastive loss with a large number of negatives. In practice, the technique of in-batch negative is used, where for each example in a batch, other batch examples' positives will be taken as its negatives, avoiding encoding extra negatives. This, however, still conditions each example's loss on all batch examples and requires fitting the entire large batch into GPU memory. This paper introduces a gradient caching technique that decouples backpropagation between contrastive loss and the encoder, removing encoder backward pass data dependency along the batch dimension. As a result, gradients can be computed for one subset of the batch at a time, leading to almost constant memory usage.

| Comments: | RepL4NLP 2021                                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| Cite as:  | **[arXiv:2101.06983](https://arxiv.org/abs/2101.06983) [cs.LG]** |
|           | (or **[arXiv:2101.06983v2](https://arxiv.org/abs/2101.06983v2) [cs.LG]** for this version) |





<h2 id="2021-06-16-2">2. Targeted Data Acquisition for Evolving Negotiation Agents
</h2>

Title: [Targeted Data Acquisition for Evolving Negotiation Agents](https://arxiv.org/abs/2106.07728)

Authors: [Minae Kwon](https://arxiv.org/search/cs?searchtype=author&query=Kwon%2C+M), [Siddharth Karamcheti](https://arxiv.org/search/cs?searchtype=author&query=Karamcheti%2C+S), [Mariano-Florentino Cuellar](https://arxiv.org/search/cs?searchtype=author&query=Cuellar%2C+M), [Dorsa Sadigh](https://arxiv.org/search/cs?searchtype=author&query=Sadigh%2C+D)

> Successful negotiators must learn how to balance optimizing for self-interest and cooperation. Yet current artificial negotiation agents often heavily depend on the quality of the static datasets they were trained on, limiting their capacity to fashion an adaptive response balancing self-interest and cooperation. For this reason, we find that these agents can achieve either high utility or cooperation, but not both. To address this, we introduce a targeted data acquisition framework where we guide the exploration of a reinforcement learning agent using annotations from an expert oracle. The guided exploration incentivizes the learning agent to go beyond its static dataset and develop new negotiation strategies. We show that this enables our agents to obtain higher-reward and more Pareto-optimal solutions when negotiating with both simulated and human partners compared to standard supervised learning and reinforcement learning methods. This trend additionally holds when comparing agents using our targeted data acquisition framework to variants of agents trained with a mix of supervised learning and reinforcement learning, or to agents using tailored reward functions that explicitly optimize for utility and Pareto-optimality.

| Comments: | The Thirty-eighth International Conference on Machine Learning |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Multiagent Systems (cs.MA) |
| Cite as:  | **[arXiv:2106.07728](https://arxiv.org/abs/2106.07728) [cs.AI]** |
|           | (or **[arXiv:2106.07728v1](https://arxiv.org/abs/2106.07728v1) [cs.AI]** for this version) |





<h2 id="2021-06-16-3">3. Language Tags Matter for Zero-Shot Neural Machine Translation
</h2>

Title: [Language Tags Matter for Zero-Shot Neural Machine Translation](https://arxiv.org/abs/2106.07930)

Authors: [Liwei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Shanbo Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+S), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Multilingual Neural Machine Translation (MNMT) has aroused widespread interest due to its efficiency. An exciting advantage of MNMT models is that they could also translate between unsupervised (zero-shot) language directions. Language tag (LT) strategies are often adopted to indicate the translation directions in MNMT. In this paper, we demonstrate that the LTs are not only indicators for translation directions but also crucial to zero-shot translation qualities. Unfortunately, previous work tends to ignore the importance of LT strategies. We demonstrate that a proper LT strategy could enhance the consistency of semantic representations and alleviate the off-target issue in zero-shot directions. Experimental results show that by ignoring the source language tag (SLT) and adding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks.

| Comments: | 7 pages, 3 figures, Accepted by the Findings of ACL2021      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.07930](https://arxiv.org/abs/2106.07930) [cs.CL]** |
|           | (or **[arXiv:2106.07930v1](https://arxiv.org/abs/2106.07930v1) [cs.CL]** for this version) |





<h2 id="2021-06-16-4">4. Semantic Representation and Inference for NLP
</h2>

Title: [Semantic Representation and Inference for NLP](https://arxiv.org/abs/2106.08117)

Authors: [Dongsheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D)

> Semantic representation and inference is essential for Natural Language Processing (NLP). The state of the art for semantic representation and inference is deep learning, and particularly Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and transformer Self-Attention models. This thesis investigates the use of deep learning for novel semantic representation and inference, and makes contributions in the following three areas: creating training data, improving semantic representations and extending inference learning. In terms of creating training data, we contribute the largest publicly available dataset of real-life factual claims for the purpose of automatic claim verification (MultiFC), and we present a novel inference model composed of multi-scale CNNs with different kernel sizes that learn from external sources to infer fact checking labels. In terms of improving semantic representations, we contribute a novel model that captures non-compositional semantic indicators. By definition, the meaning of a non-compositional phrase cannot be inferred from the individual meanings of its composing words (e.g., hot dog). Motivated by this, we operationalize the compositionality of a phrase contextually by enriching the phrase representation with external word embeddings and knowledge graphs. Finally, in terms of inference learning, we propose a series of novel deep learning architectures that improve inference by using syntactic dependencies, by ensembling role guided attention heads, incorporating gating layers, and concatenating multiple heads in novel and effective ways. This thesis consists of seven publications (five published and two under review).

| Comments: | PhD thesis, the University of Copenhagen                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.08117](https://arxiv.org/abs/2106.08117) [cs.CL]** |
|           | (or **[arXiv:2106.08117v1](https://arxiv.org/abs/2106.08117v1) [cs.CL]** for this version) |





<h2 id="2021-06-16-5">5. Sequence-Level Training for Non-Autoregressive Neural Machine Translation
</h2>

Title: [Sequence-Level Training for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/2106.08122)

Authors: [Chenze Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+C), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Jinchao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> In recent years, Neural Machine Translation (NMT) has achieved notable results in various translation tasks. However, the word-by-word generation manner determined by the autoregressive mechanism leads to high translation latency of the NMT and restricts its low-latency applications. Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive mechanism and achieves significant decoding speedup through generating target words independently and simultaneously. Nevertheless, NAT still takes the word-level cross-entropy loss as the training objective, which is not optimal because the output of NAT cannot be properly evaluated due to the multimodality problem. In this paper, we propose using sequence-level training objectives to train NAT models, which evaluate the NAT outputs as a whole and correlates well with the real translation quality. Firstly, we propose training NAT models to optimize sequence-level evaluation metrics (e.g., BLEU) based on several novel reinforcement algorithms customized for NAT, which outperforms the conventional method by reducing the variance of gradient estimation. Secondly, we introduce a novel training objective for NAT models, which aims to minimize the Bag-of-Ngrams (BoN) difference between the model output and the reference sentence. The BoN training objective is differentiable and can be calculated efficiently without doing any approximations. Finally, we apply a three-stage training strategy to combine these two methods to train the NAT model. We validate our approach on four translation tasks (WMT14 En↔De, WMT16 En↔Ro), which shows that our approach largely outperforms NAT baselines and achieves remarkable performance on all translation tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.08122](https://arxiv.org/abs/2106.08122) [cs.CL]** |
|           | (or **[arXiv:2106.08122v1](https://arxiv.org/abs/2106.08122v1) [cs.CL]** for this version) |





<h2 id="2021-06-16-6">6. Consistency Regularization for Cross-Lingual Fine-Tuning
</h2>

Title: [Consistency Regularization for Cross-Lingual Fine-Tuning](https://arxiv.org/abs/2106.08226)

Authors: [Bo Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+B), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Wenhui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Zewen Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+Z), [Saksham Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal%2C+S), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Ting Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. In this work, we propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling.

| Comments: | ACL-2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.08226](https://arxiv.org/abs/2106.08226) [cs.CL]** |
|           | (or **[arXiv:2106.08226v1](https://arxiv.org/abs/2106.08226v1) [cs.CL]** for this version) |






# 2021-06-15

[Return to Index](#Index)



<h2 id="2021-06-15-1">1. Thinking Like Transformers
</h2>

Title: [Thinking Like Transformers](https://arxiv.org/abs/2106.06981)

Authors: [Gail Weiss](https://arxiv.org/search/cs?searchtype=author&query=Weiss%2C+G), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y), [Eran Yahav](https://arxiv.org/search/cs?searchtype=author&query=Yahav%2C+E)

> What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.

| Comments: | ICML 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.06981](https://arxiv.org/abs/2106.06981) [cs.LG]** |
|           | (or **[arXiv:2106.06981v1](https://arxiv.org/abs/2106.06981v1) [cs.LG]** for this version) |





<h2 id="2021-06-15-2">2. Pre-Trained Models: Past, Present and Future
</h2>

Title: [Pre-Trained Models: Past, Present and Future](https://arxiv.org/abs/2106.07139)

Authors: [Han Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Zhang Zhengyan](https://arxiv.org/search/cs?searchtype=author&query=Zhengyan%2C+Z), [Ding Ning](https://arxiv.org/search/cs?searchtype=author&query=Ning%2C+D), [Gu Yuxian](https://arxiv.org/search/cs?searchtype=author&query=Yuxian%2C+G), [Liu Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+L), [Huo Yuqi](https://arxiv.org/search/cs?searchtype=author&query=Yuqi%2C+H), [Qiu Jiezhong](https://arxiv.org/search/cs?searchtype=author&query=Jiezhong%2C+Q), [Zhang Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z), [Han Wentao](https://arxiv.org/search/cs?searchtype=author&query=Wentao%2C+H), [Huang Minlie](https://arxiv.org/search/cs?searchtype=author&query=Minlie%2C+H), [Jin Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+J), [Lan Yanyan](https://arxiv.org/search/cs?searchtype=author&query=Yanyan%2C+L), [Liu Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L), [Liu Zhiyuan](https://arxiv.org/search/cs?searchtype=author&query=Zhiyuan%2C+L), [Lu Zhiwu](https://arxiv.org/search/cs?searchtype=author&query=Zhiwu%2C+L), [Qiu Xipeng](https://arxiv.org/search/cs?searchtype=author&query=Xipeng%2C+Q), [Song Ruihua](https://arxiv.org/search/cs?searchtype=author&query=Ruihua%2C+S), [Tang Jie](https://arxiv.org/search/cs?searchtype=author&query=Jie%2C+T), [Wen Ji-Rong](https://arxiv.org/search/cs?searchtype=author&query=Ji-Rong%2C+W), [Yuan Jinhui](https://arxiv.org/search/cs?searchtype=author&query=Jinhui%2C+Y), [Zhao Wayne Xin](https://arxiv.org/search/cs?searchtype=author&query=Xin%2C+Z+W), [Zhu Jun](https://arxiv.org/search/cs?searchtype=author&query=Jun%2C+Z)

> Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.07139](https://arxiv.org/abs/2106.07139) [cs.AI]** |
|           | (or **[arXiv:2106.07139v1](https://arxiv.org/abs/2106.07139v1) [cs.AI]** for this version) |





<h2 id="2021-06-15-3">3. Model Explainability in Deep Learning Based Natural Language Processing
</h2>

Title: [Model Explainability in Deep Learning Based Natural Language Processing](https://arxiv.org/abs/2106.07410)

Authors: [Shafie Gholizadeh](https://arxiv.org/search/cs?searchtype=author&query=Gholizadeh%2C+S), [Nengfeng Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+N)

> Machine learning (ML) model explainability has received growing attention, especially in the area related to model risk and regulations. In this paper, we reviewed and compared some popular ML model explainability methodologies, especially those related to Natural Language Processing (NLP) models. We then applied one of the NLP explainability methods Layer-wise Relevance Propagation (LRP) to a NLP classification model. We used the LRP method to derive a relevance score for each word in an instance, which is a local explainability. The relevance scores are then aggregated together to achieve global variable importance of the model. Through the case study, we also demonstrated how to apply the local explainability method to false positive and false negative instances to discover the weakness of a NLP model. These analysis can help us to understand NLP models better and reduce the risk due to the black-box nature of NLP models. We also identified some common issues due to the special natures of NLP models and discussed how explainability analysis can act as a control to detect these issues after the model has been trained.

| Comments: | 12 pages, 8 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.07410](https://arxiv.org/abs/2106.07410) [cs.AI]** |
|           | (or **[arXiv:2106.07410v1](https://arxiv.org/abs/2106.07410v1) [cs.AI]** for this version) |





<h2 id="2021-06-15-4">4. Direct Simultaneous Speech-to-Text Translation Assisted by Synchronized Streaming ASR
</h2>

Title: [Direct Simultaneous Speech-to-Text Translation Assisted by Synchronized Streaming ASR](https://arxiv.org/abs/2106.06636)

Authors: [Junkun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Mingbo Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+M), [Renjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R), [Liang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

> Simultaneous speech-to-text translation is widely useful in many scenarios. The conventional cascaded approach uses a pipeline of streaming ASR followed by simultaneous MT, but suffers from error propagation and extra latency. To alleviate these issues, recent efforts attempt to directly translate the source speech into target text simultaneously, but this is much harder due to the combination of two separate tasks. We instead propose a new paradigm with the advantages of both cascaded and end-to-end approaches. The key idea is to use two separate, but synchronized, decoders on streaming ASR and direct speech-to-text translation (ST), respectively, and the intermediate results of ASR guide the decoding policy of (but is not fed as input to) ST. During training time, we use multitask learning to jointly learn these two tasks with a shared encoder. En-to-De and En-to-Es experiments on the MuSTC dataset demonstrate that our proposed technique achieves substantially better translation quality at similar levels of latency.

| Comments: | accepted by Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.06636](https://arxiv.org/abs/2106.06636) [cs.CL]** |
|           | (or **[arXiv:2106.06636v1](https://arxiv.org/abs/2106.06636v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-5">5. Assessing Multilingual Fairness in Pre-trained Multimodal Representations
</h2>

Title: [Assessing Multilingual Fairness in Pre-trained Multimodal Representations](https://arxiv.org/abs/2106.06683)

Authors: [Jialu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Xin Eric Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X+E)

> Recently pre-trained multimodal models, such as CLIP, have received a surge of attention for their exceptional capabilities towards connecting images and natural language. The textual representations in English can be desirably transferred to multilingualism and support promising downstream multimodal tasks for different languages. Nevertheless, previous fairness discourse in vision-and-language learning mainly focuses on monolingual representational biases, and rarely scrutinizes the principles of multilingual fairness in this multimodal setting, where one language is equated to a group of individuals and images provide the universal grounding for bridging different languages.
> In this paper, we provide a nuanced understanding of individual fairness and group fairness by viewing language as the recipient of fairness notions. We define new fairness notions within multilingual context and analytically articulate that, pre-trained vision-and-language representations are individually fair across languages but not guaranteed to group fairness. Furthermore, we conduct extensive experiments to explore the prevalent group disparity across languages and protected groups including race, gender and age.

| Comments: | 12 pages, 14 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.06683](https://arxiv.org/abs/2106.06683) [cs.CL]** |
|           | (or **[arXiv:2106.06683v1](https://arxiv.org/abs/2106.06683v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-6">6. Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation
</h2>

Title: [Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation](https://arxiv.org/abs/2106.06751)

Authors: [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Shuhao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+S), [Dengji Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+D), [Zhengxin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Chenze Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+C)

> Although teacher forcing has become the main training paradigm for neural machine translation, it usually makes predictions only conditioned on past information, and hence lacks global planning for the future. To address this problem, we introduce another decoder, called seer decoder, into the encoder-decoder framework during training, which involves future information in target predictions. Meanwhile, we force the conventional decoder to simulate the behaviors of the seer decoder via knowledge distillation. In this way, at test the conventional decoder can perform like the seer decoder without the attendance of it. Experiment results on the Chinese-English, English-German and English-Romanian translation tasks show our method can outperform competitive baselines significantly and achieves greater improvements on the bigger data sets. Besides, the experiments also prove knowledge distillation the best way to transfer knowledge from the seer decoder to the conventional decoder compared to adversarial learning and L2 regularization.

| Comments: | Accepted by ACL-IJCNLP 2021 main conference                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.06751](https://arxiv.org/abs/2106.06751) [cs.CL]** |
|           | (or **[arXiv:2106.06751v1](https://arxiv.org/abs/2106.06751v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-7">7. Machine Translation into Low-resource Language Varieties
</h2>

Title: [Machine Translation into Low-resource Language Varieties](https://arxiv.org/abs/2106.06797)

Authors: [Sachin Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Shuly Wintner](https://arxiv.org/search/cs?searchtype=author&query=Wintner%2C+S), [Yulia Tsvetkov](https://arxiv.org/search/cs?searchtype=author&query=Tsvetkov%2C+Y)

> State-of-the-art machine translation (MT) systems are typically trained to generate the "standard" target language; however, many languages have multiple varieties (regional varieties, dialects, sociolects, non-native varieties) that are different from the standard language. Such varieties are often low-resource, and hence do not benefit from contemporary NLP solutions, MT included. We propose a general framework to rapidly adapt MT systems to generate language varieties that are close to, but different from, the standard target language, using no parallel (source--variety) data. This also includes adaptation of MT systems to low-resource typologically-related target languages. We experiment with adapting an English--Russian MT system to generate Ukrainian and Belarusian, an English--Norwegian Bokmål system to generate Nynorsk, and an English--Arabic system to generate four Arabic dialects, obtaining significant improvements over competitive baselines.

| Comments: | The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.06797](https://arxiv.org/abs/2106.06797) [cs.CL]** |
|           | (or **[arXiv:2106.06797v1](https://arxiv.org/abs/2106.06797v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-8">8. Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data
</h2>

Title: [Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data](https://arxiv.org/abs/2106.06875)

Authors: [Rajat Bhatnagar](https://arxiv.org/search/cs?searchtype=author&query=Bhatnagar%2C+R), [Ananya Ganesh](https://arxiv.org/search/cs?searchtype=author&query=Ganesh%2C+A), [Katharina Kann](https://arxiv.org/search/cs?searchtype=author&query=Kann%2C+K)

> High-performing machine translation (MT) systems can help overcome language barriers while making it possible for everyone to communicate and use language technologies in the language of their choice. However, such systems require large amounts of parallel sentences for training, and translators can be difficult to find and expensive. Here, we present a data collection strategy for MT which, in contrast, is cheap and simple, as it does not require bilingual speakers. Based on the insight that humans pay specific attention to movements, we use graphics interchange formats (GIFs) as a pivot to collect parallel sentences from monolingual annotators. We use our strategy to collect data in Hindi, Tamil and English. As a baseline, we also collect data using images as a pivot. We perform an intrinsic evaluation by manually evaluating a subset of the sentence pairs and an extrinsic evaluation by finetuning mBART on the collected data. We find that sentences collected via GIFs are indeed of higher quality.

| Comments:    | 5 pages, 1 figure, ACL-IJCNLP 2021 submission, Natural Language Processing, Data Collection, Monolingual Speakers, Machine Translation, GIFs, Images |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2106.06875](https://arxiv.org/abs/2106.06875) [cs.CL]** |
|              | (or **[arXiv:2106.06875v1](https://arxiv.org/abs/2106.06875v1) [cs.CL]** for this version) |







<h2 id="2021-06-15-9">9. Memory-efficient Transformers via Top-k Attention
</h2>

Title: [Memory-efficient Transformers via Top-k Attention](https://arxiv.org/abs/2106.06899)

Authors: [Ankit Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Guy Dar](https://arxiv.org/search/cs?searchtype=author&query=Dar%2C+G), [Shaya Goodman](https://arxiv.org/search/cs?searchtype=author&query=Goodman%2C+S), [David Ciprut](https://arxiv.org/search/cs?searchtype=author&query=Ciprut%2C+D), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J)

> Following the success of dot-product attention in Transformers, numerous approximations have been recently proposed to address its quadratic complexity with respect to the input length. While these variants are memory and compute efficient, it is not possible to directly use them with popular pre-trained language models trained using vanilla attention, without an expensive corrective pre-training stage. In this work, we propose a simple yet highly accurate approximation for vanilla attention. We process the queries in chunks, and for each query, compute the top-k scores with respect to the keys. Our approach offers several advantages: (a) its memory usage is linear in the input size, similar to linear attention variants, such as Performer and RFA (b) it is a drop-in replacement for vanilla attention that does not require any corrective pre-training, and (c) it can also lead to significant memory savings in the feed-forward layers after casting them into the familiar query-key-value framework. We evaluate the quality of top-k approximation for multi-head attention layers on the Long Range Arena Benchmark, and for feed-forward layers of T5 and UnifiedQA on multiple QA datasets. We show our approach leads to accuracy that is nearly-identical to vanilla attention in multiple setups including training from scratch, fine-tuning, and zero-shot inference.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.06899](https://arxiv.org/abs/2106.06899) [cs.CL]** |
|           | (or **[arXiv:2106.06899v1](https://arxiv.org/abs/2106.06899v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-10">10. SASICM A Multi-Task Benchmark For Subtext Recognition
</h2>

Title: [SASICM A Multi-Task Benchmark For Subtext Recognition](https://arxiv.org/abs/2106.06944)

Authors: [Hua Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+H), [Weikang Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+W), [Feng Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+F), [Jian Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J), [Furao Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+F)

> Subtext is a kind of deep semantics which can be acquired after one or more rounds of expression transformation. As a popular way of expressing one's intentions, it is well worth studying. In this paper, we try to make computers understand whether there is a subtext by means of machine learning. We build a Chinese dataset whose source data comes from the popular social media (e.g. Weibo, Netease Music, Zhihu, and Bilibili). In addition, we also build a baseline model called SASICM to deal with subtext recognition. The F1 score of SASICMg, whose pretrained model is GloVe, is as high as 64.37%, which is 3.97% higher than that of BERT based model, 12.7% higher than that of traditional methods on average, including support vector machine, logistic regression classifier, maximum entropy classifier, naive bayes classifier and decision tree and 2.39% higher than that of the state-of-the-art, including MARIN and BTM. The F1 score of SASICMBERT, whose pretrained model is BERT, is 65.12%, which is 0.75% higher than that of SASICMg. The accuracy rates of SASICMg and SASICMBERT are 71.16% and 70.76%, respectively, which can compete with those of other methods which are mentioned before.

| Comments:    | 34 pages, 6 figures, 6 tables. Submitted to the journal of artificial intelligence |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| MSC classes: | 68T50 (Primary), 03B65, 91F20 (Secondary)                    |
| ACM classes: | I.2.2                                                        |
| Cite as:     | **[arXiv:2106.06944](https://arxiv.org/abs/2106.06944) [cs.CL]** |
|              | (or **[arXiv:2106.06944v1](https://arxiv.org/abs/2106.06944v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-11">11. Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces
</h2>

Title: [Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces](https://arxiv.org/abs/2106.06964)

Authors: [Alexey Tikhonov](https://arxiv.org/search/cs?searchtype=author&query=Tikhonov%2C+A)

> Pre-trained word representations became a key component in many NLP tasks. However, the global geometry of the word embeddings remains poorly understood. In this paper, we demonstrate that a typical word embeddings cloud is shaped as a high-dimensional simplex with interpretable vertices and propose a simple yet effective method for enumeration of these vertices. We show that the proposed method can detect and describe vertices of the simplex for GloVe and fasttext spaces.

| Comments:    | 3 pages, 2 figures, EEML-2021 poster                         |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2106.06964](https://arxiv.org/abs/2106.06964) [cs.CL]** |
|              | (or **[arXiv:2106.06964v1](https://arxiv.org/abs/2106.06964v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-12">12. SAS: Self-Augmented Strategy for Language Model Pre-training
</h2>

Title: [SAS: Self-Augmented Strategy for Language Model Pre-training](https://arxiv.org/abs/2106.07176)

Authors: [Yifei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Jingqiao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Ru He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+R), [Liangzhu Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+L), [Chao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+C), [Cheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+C), [Ying Nian Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y+N)

> The core of a self-supervised learning method for pre-training language models includes the design of appropriate data augmentation and corresponding pre-training task(s). Most data augmentations in language model pre-training are context-independent. The seminal contextualized augmentation recently proposed by the ELECTRA requires a separate generator, which leads to extra computation cost as well as the challenge in adjusting the capability of its generator relative to that of the other model component(s). We propose a self-augmented strategy (SAS) that uses a single forward pass through the model to augment the input data for model training in the next epoch. Essentially our strategy eliminates a separate generator network and uses only one network to generate the data augmentation and undertake two pre-training tasks (the MLM task and the RTD task) jointly, which naturally avoids the challenge in adjusting the generator's capability as well as reduces the computation cost. Additionally, our SAS is a general strategy such that it can seamlessly incorporate many new techniques emerging recently or in the future, such as the disentangled attention mechanism recently proposed by the DeBERTa model. Our experiments show that our SAS is able to outperform the ELECTRA and other state-of-the-art models in the GLUE tasks with the same or less computation cost.

| Comments: | 13 pages, 3 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2106.07176](https://arxiv.org/abs/2106.07176) [cs.CL]** |
|           | (or **[arXiv:2106.07176v1](https://arxiv.org/abs/2106.07176v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-13">13. Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation
</h2>

Title: [Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation](https://arxiv.org/abs/2106.07207)

Authors: [Xiang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+X), [Simeng Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S)

> Advanced large-scale neural language models have led to significant success in many language generation tasks. However, the most commonly used training objective, Maximum Likelihood Estimation (MLE), has been shown problematic, where the trained model prefers using dull and repetitive phrases. In this work, we introduce ScaleGrad, a modification straight to the gradient of the loss function, to remedy the degeneration issue of the standard MLE objective. By directly maneuvering the gradient information, ScaleGrad makes the model learn to use novel tokens. Empirical results show the effectiveness of our method not only in open-ended generation, but also in directed generation tasks. With the simplicity in architecture, our method can serve as a general training objective that is applicable to most of the neural text generation tasks.

| Comments: | ICML 2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.07207](https://arxiv.org/abs/2106.07207) [cs.CL]** |
|           | (or **[arXiv:2106.07207v1](https://arxiv.org/abs/2106.07207v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-14">14. English to Bangla Machine Translation Using Recurrent Neural Network
</h2>

Title: [English to Bangla Machine Translation Using Recurrent Neural Network](https://arxiv.org/abs/2106.07225)

Authors: [Shaykh Siddique](https://arxiv.org/search/cs?searchtype=author&query=Siddique%2C+S), [Tahmid Ahmed](https://arxiv.org/search/cs?searchtype=author&query=Ahmed%2C+T), [Md. Rifayet Azam Talukder](https://arxiv.org/search/cs?searchtype=author&query=Talukder%2C+M+R+A), [Md. Mohsin Uddin](https://arxiv.org/search/cs?searchtype=author&query=Uddin%2C+M+M)

> The applications of recurrent neural networks in machine translation are increasing in natural language processing. Besides other languages, Bangla language contains a large amount of vocabulary. Improvement of English to Bangla machine translation would be a significant contribution to Bangla Language processing. This paper describes an architecture of English to Bangla machine translation system. The system has been implemented with the encoder-decoder recurrent neural network. The model uses a knowledge-based context vector for the mapping of English and Bangla words. Performances of the model based on activation functions are measured here. The best performance is achieved for the linear activation function in encoder layer and the tanh activation function in decoder layer. From the execution of GRU and LSTM layer, GRU performed better than LSTM. The attention layers are enacted with softmax and sigmoid activation function. The approach of the model outperforms the previous state-of-the-art systems in terms of cross-entropy loss metrics. The reader can easily find out the structure of the machine translation of English to Bangla and the efficient activation functions from the paper.

| Comments:          | 6 pages, 7 figures, Published with International Journal of Future Computer and Communication (IJFCC) |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Journal reference: | Siddique, Shaykh, et al. "English to Bangla Machine Translation Using Recurrent Neural Network." International Journal of Future Computer and Communication 9.2 (2020) |
| DOI:               | [10.18178/ijfcc.2020.9.2.564](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.18178%2Fijfcc.2020.9.2.564&v=dd7687eb) |
| Cite as:           | **[arXiv:2106.07225](https://arxiv.org/abs/2106.07225) [cs.CL]** |
|                    | (or **[arXiv:2106.07225v1](https://arxiv.org/abs/2106.07225v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-15">15. MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education
</h2>

Title: [MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education](https://arxiv.org/abs/2106.07340)

Authors: [Jia Tracy Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J+T), [Michiharu Yamashita](https://arxiv.org/search/cs?searchtype=author&query=Yamashita%2C+M), [Ethan Prihar](https://arxiv.org/search/cs?searchtype=author&query=Prihar%2C+E), [Neil Heffernan](https://arxiv.org/search/cs?searchtype=author&query=Heffernan%2C+N), [Xintao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Dongwon Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+D)

> Due to the transfer learning nature of BERT model, researchers have achieved better performance than base BERT by further pre-training the original BERT on a huge domain-specific corpus. Due to the special nature of mathematical texts which often contain math equations and symbols, the original BERT model pre-trained on general English context will not fit Natural Language Processing (NLP) tasks in mathematical education well. Therefore, we propose MathBERT, a BERT pre-trained on large mathematical corpus including pre-k to graduate level mathematical content to tackle math-specific tasks. In addition, We generate a customized mathematical vocabulary to pre-train with MathBERT and compare the performance to the MathBERT pre-trained with the original BERT vocabulary. We select three important tasks in mathematical education such as knowledge component, auto-grading, and knowledge tracing prediction to evaluate the performance of MathBERT. Our experiments show that MathBERT outperforms the base BERT by 2-9\% margin. In some cases, MathBERT pre-trained with mathematical vocabulary is better than MathBERT trained with original [this http URL](http://vocabulary.to/) our best knowledge, MathBERT is the first pre-trained model for general purpose mathematics education tasks.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.07340](https://arxiv.org/abs/2106.07340) [cs.CL]** |
|           | (or **[arXiv:2106.07340v1](https://arxiv.org/abs/2106.07340v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-16">16. Self-Guided Contrastive Learning for BERT Sentence Representations
</h2>

Title: [Self-Guided Contrastive Learning for BERT Sentence Representations](https://arxiv.org/abs/2106.07345)

Authors: [Taeuk Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+T), [Kang Min Yoo](https://arxiv.org/search/cs?searchtype=author&query=Yoo%2C+K+M), [Sang-goo Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S)

> Although BERT and its variants have reshaped the NLP landscape, it still remains unclear how best to derive sentence embeddings from such pre-trained Transformers. In this work, we propose a contrastive learning method that utilizes self-guidance for improving the quality of BERT sentence representations. Our method fine-tunes BERT in a self-supervised fashion, does not rely on data augmentation, and enables the usual [CLS] token embeddings to function as sentence vectors. Moreover, we redesign the contrastive learning objective (NT-Xent) and apply it to sentence representation learning. We demonstrate with extensive experiments that our approach is more effective than competitive baselines on diverse sentence-related tasks. We also show it is efficient at inference and robust to domain shifts.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2106.07345](https://arxiv.org/abs/2106.07345) [cs.CL]** |
|           | (or **[arXiv:2106.07345v1](https://arxiv.org/abs/2106.07345v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-17">17. Is it a click bait? Let's predict using Machine Learning
</h2>

Title: [Is it a click bait? Let's predict using Machine Learning](https://arxiv.org/abs/2106.07348)

Authors: [Sohom Ghosh](https://arxiv.org/search/cs?searchtype=author&query=Ghosh%2C+S)

> In this era of digitisation, news reader tend to read news online. This is because, online media instantly provides access to a wide variety of content. Thus, people don't have to wait for tomorrow's newspaper to know what's happening today. Along with these virtues, online news have some vices as well. One such vice is presence of social media posts (tweets) relating to news articles whose sole purpose is to draw attention of the users rather than directing them to read the actual content. Such posts are referred to as clickbaits. The objective of this project is to develop a system which would be capable of predicting how likely are the social media posts (tweets) relating to new articles tend to be clickbait.

| Comments: | M.Tech Thesis defended at BITS, Pilani                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| Cite as:  | **[arXiv:2106.07348](https://arxiv.org/abs/2106.07348) [cs.CL]** |
|           | (or **[arXiv:2106.07348v1](https://arxiv.org/abs/2106.07348v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-18">18. Using Integrated Gradients to explain Linguistic Acceptability learnt by BERT
</h2>

Title: [Using Integrated Gradients to explain Linguistic Acceptability learnt by BERT](https://arxiv.org/abs/2106.07349)

Authors: [Anmol Nayak](https://arxiv.org/search/cs?searchtype=author&query=Nayak%2C+A), [Hari Prasad Timmapathini](https://arxiv.org/search/cs?searchtype=author&query=Timmapathini%2C+H+P)

> BERT has been a breakthrough in language understanding by leveraging the multi-head self-attention mechanism in its architecture. To the best of our knowledge this work is the first to leverage Layer Integrated Gradients Attribution Scores (LIGAS) to explain the Linguistic Acceptability criteria that are learnt by BERT on the Corpus of Linguistic Acceptability (CoLA) benchmark dataset. Our experiments on 5 different categories of sentences lead to the following interesting findings: 1) LIGAS for Linguistically Acceptable (LA) sentences are significantly smaller in comparison to Linguistically Unacceptable (LUA) sentences, 2) There are specific subtrees of the Constituency Parse Tree (CPT) for LA and LUA sentences which contribute larger LIGAS, 3) Across the different categories of sentences we observed around 88% to 100% of the Correctly classified sentences had positive LIGAS, indicating a strong positive relationship to the prediction confidence of the model, and 4) Around 57% of the Misclassified sentences had positive LIGAS, which we believe can become correctly classified sentences if the LIGAS are parameterized in the loss function of the model.

| Comments: | 3 pages, 1 figure                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.07349](https://arxiv.org/abs/2106.07349) [cs.CL]** |
|           | (or **[arXiv:2106.07349v1](https://arxiv.org/abs/2106.07349v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-19">19. Determinantal Beam Search
</h2>

Title: [Determinantal Beam Search](https://arxiv.org/abs/2106.07400)

Authors: [Clara Meister](https://arxiv.org/search/cs?searchtype=author&query=Meister%2C+C), [Martina Forster](https://arxiv.org/search/cs?searchtype=author&query=Forster%2C+M), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R)

> Beam search is a go-to strategy for decoding neural sequence models. The algorithm can naturally be viewed as a subset optimization problem, albeit one where the corresponding set function does not reflect interactions between candidates. Empirically, this leads to sets often exhibiting high overlap, e.g., strings may differ by only a single word. Yet in use-cases that call for multiple solutions, a diverse or representative set is often desired. To address this issue, we propose a reformulation of beam search, which we call determinantal beam search. Determinantal beam search has a natural relationship to determinantal point processes (DPPs), models over sets that inherently encode intra-set interactions. By posing iterations in beam search as a series of subdeterminant maximization problems, we can turn the algorithm into a diverse subset selection process. In a case study, we use the string subsequence kernel to explicitly encourage n-gram coverage in text generated from a sequence model. We observe that our algorithm offers competitive performance against other diverse set generation strategies in the context of language generation, while providing a more general approach to optimizing for diversity.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.07400](https://arxiv.org/abs/2106.07400) [cs.CL]** |
|           | (or **[arXiv:2106.07400v1](https://arxiv.org/abs/2106.07400v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-20">20. Grammar Equations
</h2>

Title: [Grammar Equations](https://arxiv.org/abs/2106.07485)

Authors: [Bob Coecke](https://arxiv.org/search/cs?searchtype=author&query=Coecke%2C+B), [Vincent Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+V)

> Diagrammatically speaking, grammatical calculi such as pregroups provide wires between words in order to elucidate their interactions, and this enables one to verify grammatical correctness of phrases and sentences. In this paper we also provide wirings within words. This will enable us to identify grammatical constructs that we expect to be either equal or closely related. Hence, our work paves the way for a new theory of grammar, that provides novel `grammatical truths'. We give a nogo-theorem for the fact that our wirings for words make no sense for preordered monoids, the form which grammatical calculi usually take. Instead, they require diagrams -- or equivalently, (free) monoidal categories.

| Comments: | 10 pages, many pictures                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Category Theory (math.CT) |
| Cite as:  | **[arXiv:2106.07485](https://arxiv.org/abs/2106.07485) [cs.CL]** |
|           | (or **[arXiv:2106.07485v1](https://arxiv.org/abs/2106.07485v1) [cs.CL]** for this version) |





<h2 id="2021-06-15-21">21. An Empirical Survey of Data Augmentation for Limited Data Learning in NLP
</h2>

Title: [An Empirical Survey of Data Augmentation for Limited Data Learning in NLP](https://arxiv.org/abs/2106.07499)

Authors: [Jiaao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Derek Tam](https://arxiv.org/search/cs?searchtype=author&query=Tam%2C+D), [Colin Raffel](https://arxiv.org/search/cs?searchtype=author&query=Raffel%2C+C), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M), [Diyi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+D)

> NLP has achieved great progress in the past decade through the use of neural models and large labeled datasets. The dependence on abundant data prevents NLP models from being applied to low-resource settings or novel tasks where significant time, money, or expertise is required to label massive amounts of textual data. Recently, data augmentation methods have been explored as a means of improving data efficiency in NLP. To date, there has been no systematic empirical overview of data augmentation for NLP in the limited labeled data setting, making it difficult to understand which methods work in which settings. In this paper, we provide an empirical survey of recent progress on data augmentation for NLP in the limited labeled data setting, summarizing the landscape of methods (including token-level augmentations, sentence-level augmentations, adversarial augmentations, and hidden-space augmentations) and carrying out experiments on 11 datasets covering topics/news classification, inference tasks, paraphrasing tasks, and single-sentence tasks. Based on the results, we draw several conclusions to help practitioners choose appropriate augmentations in different settings and discuss the current challenges and future directions for limited data learning in NLP.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.07499](https://arxiv.org/abs/2106.07499) [cs.CL]** |
|           | (or **[arXiv:2106.07499v1](https://arxiv.org/abs/2106.07499v1) [cs.CL]** for this version) |






# 2021-06-14

[Return to Index](#Index)



<h2 id="2021-06-14-1">1. One Sense Per Translation
</h2>

Title: [One Sense Per Translation](https://arxiv.org/abs/2106.06082)

Authors: [Bradley Hauer](https://arxiv.org/search/cs?searchtype=author&query=Hauer%2C+B), [Grzegorz Kondrak](https://arxiv.org/search/cs?searchtype=author&query=Kondrak%2C+G)

> The idea of using lexical translations to define sense inventories has a long history in lexical semantics. We propose a theoretical framework which allows us to answer the question of why this apparently reasonable idea failed to produce useful results. We formally prove several propositions on how the translations of a word relate to its senses, as well as on the relationship between synonymy and polysemy. We empirically validate our theoretical findings on BabelNet, and demonstrate how they could be used to perform unsupervised word sense disambiguation of a substantial fraction of the lexicon.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.06082](https://arxiv.org/abs/2106.06082) [cs.CL]** |
|           | (or **[arXiv:2106.06082v1](https://arxiv.org/abs/2106.06082v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-2">2. Graph Neural Networks for Natural Language Processing: A Survey
</h2>

Title: [Graph Neural Networks for Natural Language Processing: A Survey](https://arxiv.org/abs/2106.06090)

Authors: [Lingfei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Yu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Kai Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+K), [Xiaojie Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+X), [Hanning Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+H), [Shucheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Jian Pei](https://arxiv.org/search/cs?searchtype=author&query=Pei%2C+J), [Bo Long](https://arxiv.org/search/cs?searchtype=author&query=Long%2C+B)

> Deep learning has become the dominant approach in coping with various tasks in Natural LanguageProcessing (NLP). Although text inputs are typically represented as a sequence of tokens, there isa rich variety of NLP problems that can be best expressed with a graph structure. As a result, thereis a surge of interests in developing new deep learning techniques on graphs for a large numberof NLP tasks. In this survey, we present a comprehensive overview onGraph Neural Networks(GNNs) for Natural Language Processing. We propose a new taxonomy of GNNs for NLP, whichsystematically organizes existing research of GNNs for NLP along three axes: graph construction,graph representation learning, and graph based encoder-decoder models. We further introducea large number of NLP applications that are exploiting the power of GNNs and summarize thecorresponding benchmark datasets, evaluation metrics, and open-source codes. Finally, we discussvarious outstanding challenges for making the full use of GNNs for NLP as well as future researchdirections. To the best of our knowledge, this is the first comprehensive overview of Graph NeuralNetworks for Natural Language Processing.

| Comments: | 127 pages                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.06090](https://arxiv.org/abs/2106.06090) [cs.CL]** |
|           | (or **[arXiv:2106.06090v1](https://arxiv.org/abs/2106.06090v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-3">3. Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation
</h2>

Title: [Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation](https://arxiv.org/abs/2106.06125)

Authors: [Xin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Baosong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B), [Dayiheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Haibo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Haiying Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J)

> A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary. This potentially weakens the effect when applying pretrained models into natural language generation (NLG) tasks, especially for the subword distributions between upstream and downstream tasks with significant discrepancy. Towards approaching this problem, we extend the vanilla pretrain-finetune pipeline with an extra embedding transfer step. Specifically, a plug-and-play embedding generator is introduced to produce the representation of any input token, according to pre-trained embeddings of its morphologically similar ones. Thus, embeddings of mismatch tokens in downstream tasks can also be efficiently initialized. We conduct experiments on a variety of NLG tasks under the pretrain-finetune fashion. Experimental results and extensive analyses show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models.

| Comments: | Accepted by ACL2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.06125](https://arxiv.org/abs/2106.06125) [cs.CL]** |
|           | (or **[arXiv:2106.06125v1](https://arxiv.org/abs/2106.06125v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-4">4. Towards User-Driven Neural Machine Translation
</h2>

Title: [Towards User-Driven Neural Machine Translation](https://arxiv.org/abs/2106.06200)

Authors: [Huan Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H), [Liang Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+L), [Baosong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B), [Dayiheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Haibo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W), [Degen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+D), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J)

> A good translation should not only translate the original content semantically, but also incarnate personal traits of the original text. For a real-world neural machine translation (NMT) system, these user traits (e.g., topic preference, stylistic characteristics and expression habits) can be preserved in user behavior (e.g., historical inputs). However, current NMT systems marginally consider the user behavior due to: 1) the difficulty of modeling user portraits in zero-shot scenarios, and 2) the lack of user-behavior annotated parallel dataset. To fill this gap, we introduce a novel framework called user-driven NMT. Specifically, a cache-based module and a user-driven contrastive learning method are proposed to offer NMT the ability to capture potential user traits from their historical inputs under a zero-shot learning fashion. Furthermore, we contribute the first Chinese-English parallel corpus annotated with user behavior called UDT-Corpus. Experimental results confirm that the proposed user-driven NMT can generate user-specific translations.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.06200](https://arxiv.org/abs/2106.06200) [cs.CL]** |
|           | (or **[arXiv:2106.06200v1](https://arxiv.org/abs/2106.06200v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-5">5. A Discussion on Building Practical NLP Leaderboards: The Case of Machine Translation
</h2>

Title: [A Discussion on Building Practical NLP Leaderboards: The Case of Machine Translation](https://arxiv.org/abs/2106.06292)

Authors: [Sebastin Santy](https://arxiv.org/search/cs?searchtype=author&query=Santy%2C+S), [Prasanta Bhattacharya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharya%2C+P)

> Recent advances in AI and ML applications have benefited from rapid progress in NLP research. Leaderboards have emerged as a popular mechanism to track and accelerate progress in NLP through competitive model development. While this has increased interest and participation, the over-reliance on single, and accuracy-based metrics have shifted focus from other important metrics that might be equally pertinent to consider in real-world contexts. In this paper, we offer a preliminary discussion of the risks associated with focusing exclusively on accuracy metrics and draw on recent discussions to highlight prescriptive suggestions on how to develop more practical and effective leaderboards that can better reflect the real-world utility of models.

| Comments: | pre-print: comments and suggestions welcome                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.06292](https://arxiv.org/abs/2106.06292) [cs.CL]** |
|           | (or **[arXiv:2106.06292v1](https://arxiv.org/abs/2106.06292v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-6">6. Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment
</h2>

Title: [Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment](https://arxiv.org/abs/2106.06381)

Authors: [Zewen Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+Z), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Bo Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+B), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Xian-Ling Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+X), [Heyan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. Specifically, the model first self-labels word alignments for parallel sentences. Then we randomly mask tokens in a bitext pair. Given a masked token, the model uses a pointer network to predict the aligned token in the other language. We alternately perform the above two steps in an expectation-maximization manner. Experimental results show that our method improves cross-lingual transferability on various datasets, especially on the token-level tasks, such as question answering, and structured prediction. Moreover, the model can serve as a pretrained word aligner, which achieves reasonably low error rates on the alignment benchmarks. The code and pretrained parameters are available at [this https URL](https://github.com/CZWin32768/XLM-Align).

| Comments: | ACL-2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.06381](https://arxiv.org/abs/2106.06381) [cs.CL]** |
|           | (or **[arXiv:2106.06381v1](https://arxiv.org/abs/2106.06381v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-7">7. Zero-Shot Controlled Generation with Encoder-Decoder Transformers
</h2>

Title: [Zero-Shot Controlled Generation with Encoder-Decoder Transformers](https://arxiv.org/abs/2106.06411)

Authors: [Devamanyu Hazarika](https://arxiv.org/search/cs?searchtype=author&query=Hazarika%2C+D), [Mahdi Namazifar](https://arxiv.org/search/cs?searchtype=author&query=Namazifar%2C+M), [Dilek Hakkani-Tür](https://arxiv.org/search/cs?searchtype=author&query=Hakkani-Tür%2C+D)

> Controlling neural network-based models for natural language generation (NLG) has broad applications in numerous areas such as machine translation, document summarization, and dialog systems. Approaches that enable such control in a zero-shot manner would be of great importance as, among other reasons, they remove the need for additional annotated data and training. In this work, we propose novel approaches for controlling encoder-decoder transformer-based NLG models in a zero-shot manner. This is done by introducing three control knobs; namely, attention biasing, decoder mixing, and context augmentation, that are applied to these models at generation time. These knobs control the generation process by directly manipulating trained NLG models (e.g., biasing cross-attention layers) to realize the desired attributes in the generated outputs. We show that not only are these NLG models robust to such manipulations, but also their behavior could be controlled without an impact on their generation performance. These results, to the best of our knowledge, are the first of their kind. Through these control knobs, we also investigate the role of transformer decoder's self-attention module and show strong evidence that its primary role is maintaining fluency of sentences generated by these models. Based on this hypothesis, we show that alternative architectures for transformer decoders could be viable options. We also study how this hypothesis could lead to more efficient ways for training encoder-decoder transformer models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.06411](https://arxiv.org/abs/2106.06411) [cs.CL]** |
|           | (or **[arXiv:2106.06411v1](https://arxiv.org/abs/2106.06411v1) [cs.CL]** for this version) |





<h2 id="2021-06-14-8">8. Semi-Supervised and Unsupervised Sense Annotation via Translations
</h2>

Title: [Semi-Supervised and Unsupervised Sense Annotation via Translations](https://arxiv.org/abs/2106.06462)

Authors: [Bradley Hauer](https://arxiv.org/search/cs?searchtype=author&query=Hauer%2C+B), [Grzegorz Kondrak](https://arxiv.org/search/cs?searchtype=author&query=Kondrak%2C+G), [Yixing Luan](https://arxiv.org/search/cs?searchtype=author&query=Luan%2C+Y), [Arnob Mallik](https://arxiv.org/search/cs?searchtype=author&query=Mallik%2C+A), [Lili Mou](https://arxiv.org/search/cs?searchtype=author&query=Mou%2C+L)

> Acquisition of multilingual training data continues to be a challenge in word sense disambiguation (WSD). To address this problem, unsupervised approaches have been developed in recent years that automatically generate sense annotations suitable for training supervised WSD systems. We present three new methods to creating sense-annotated corpora, which leverage translations, parallel corpora, lexical resources, and contextual and synset embeddings. Our semi-supervised method applies machine translation to transfer existing sense annotations to other languages. Our two unsupervised methods use a knowledge-based WSD system to annotate a parallel corpus, and refine the resulting sense annotations by identifying lexical translations. We obtain state-of-the-art results on standard WSD benchmarks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.06462](https://arxiv.org/abs/2106.06462) [cs.CL]** |
|           | (or **[arXiv:2106.06462v1](https://arxiv.org/abs/2106.06462v1) [cs.CL]** for this version) |






# 2021-06-11

[Return to Index](#Index)



<h2 id="2021-06-11-1">1. Data augmentation to improve robustness of image captioning solutions
</h2>

Title: [Data augmentation to improve robustness of image captioning solutions](https://arxiv.org/abs/2106.05437)

Authors: [Shashank Bujimalla](https://arxiv.org/search/cs?searchtype=author&query=Bujimalla%2C+S), [Mahesh Subedar](https://arxiv.org/search/cs?searchtype=author&query=Subedar%2C+M), [Omesh Tickoo](https://arxiv.org/search/cs?searchtype=author&query=Tickoo%2C+O)

> In this paper, we study the impact of motion blur, a common quality flaw in real world images, on a state-of-the-art two-stage image captioning solution, and notice a degradation in solution performance as blur intensity increases. We investigate techniques to improve the robustness of the solution to motion blur using training data augmentation at each or both stages of the solution, i.e., object detection and captioning, and observe improved results. In particular, augmenting both the stages reduces the CIDEr-D degradation for high motion blur intensity from 68.7 to 11.7 on MS COCO dataset, and from 22.4 to 6.8 on Vizwiz dataset.

| Comments: | CVPR VizWiz 2021 workshop                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2106.05437](https://arxiv.org/abs/2106.05437) [cs.CL]** |
|           | (or **[arXiv:2106.05437v1](https://arxiv.org/abs/2106.05437v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-2">2. Input Augmentation Improves Constrained Beam Search for Neural Machine Translation: NTT at WAT 2021
</h2>

Title: [Input Augmentation Improves Constrained Beam Search for Neural Machine Translation: NTT at WAT 2021](https://arxiv.org/abs/2106.05450)

Authors: [Katsuki Chousa](https://arxiv.org/search/cs?searchtype=author&query=Chousa%2C+K), [Makoto Morishita](https://arxiv.org/search/cs?searchtype=author&query=Morishita%2C+M)

> This paper describes our systems that were submitted to the restricted translation task at WAT 2021. In this task, the systems are required to output translated sentences that contain all given word constraints. Our system combined input augmentation and constrained beam search algorithms. Through experiments, we found that this combination significantly improves translation accuracy and can save inference time while containing all the constraints in the output. For both En->Ja and Ja->En, our systems obtained the best evaluation performances in automatic evaluation.

| Comments: | 9 pages, 4 figures, WAT 2021 Restricted Translation Task     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.05450](https://arxiv.org/abs/2106.05450) [cs.CL]** |
|           | (or **[arXiv:2106.05450v1](https://arxiv.org/abs/2106.05450v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-3">3. Progressive Multi-Granularity Training for Non-Autoregressive Translation
</h2>

Title: [Progressive Multi-Granularity Training for Non-Autoregressive Translation](https://arxiv.org/abs/2106.05546)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Non-autoregressive translation (NAT) significantly accelerates the inference process via predicting the entire target sequence. However, recent studies show that NAT is weak at learning high-mode of knowledge such as one-to-many translations. We argue that modes can be divided into various granularities which can be learned from easy to hard. In this study, we empirically show that NAT models are prone to learn fine-grained lower-mode knowledge, such as words and phrases, compared with sentences. Based on this observation, we propose progressive multi-granularity training for NAT. More specifically, to make the most of the training data, we break down the sentence-level examples into three types, i.e. words, phrases, sentences, and with the training goes, we progressively increase the granularities. Experiments on Romanian-English, English-German, Chinese-English, and Japanese-English demonstrate that our approach improves the phrase translation accuracy and model reordering ability, therefore resulting in better translation quality against strong NAT baselines. Also, we show that more deterministic fine-grained knowledge can further enhance performance.

| Comments: | findings of ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.05546](https://arxiv.org/abs/2106.05546) [cs.CL]** |
|           | (or **[arXiv:2106.05546v1](https://arxiv.org/abs/2106.05546v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-4">4. AUGNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation
</h2>

Title: [AUGNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation](https://arxiv.org/abs/2106.05589)

Authors: [Xinnuo Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+X), [Guoyin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G), [Young-Bum Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y), [Sungjin Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S)

> Natural Language Generation (NLG) is a key component in a task-oriented dialogue system, which converts the structured meaning representation (MR) to the natural language. For large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots, neither template-based approaches nor model-based approaches are scalable. Recently, neural NLGs started leveraging transfer learning and showed promising results in few-shot settings. This paper proposes AUGNLG, a novel data augmentation approach that combines a self-trained neural retrieval model with a few-shot learned NLU model, to automatically create MR-to-Text data from open-domain texts. The proposed system mostly outperforms the state-of-the-art methods on the FewShotWOZ data in both BLEU and Slot Error Rate. We further confirm improved results on the FewShotSGD data and provide comprehensive analysis results on key components of our system. Our code and data are available at [this https URL](https://github.com/XinnuoXu/AugNLG).

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL2021) |
| Cite as:           | **[arXiv:2106.05589](https://arxiv.org/abs/2106.05589) [cs.CL]** |
|                    | (or **[arXiv:2106.05589v1](https://arxiv.org/abs/2106.05589v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-5">5. Exploring Unsupervised Pretraining Objectives for Machine Translation
</h2>

Title: [Exploring Unsupervised Pretraining Objectives for Machine Translation](https://arxiv.org/abs/2106.05634)

Authors: [Christos Baziotis](https://arxiv.org/search/cs?searchtype=author&query=Baziotis%2C+C), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B)

> Unsupervised cross-lingual pretraining has achieved strong results in neural machine translation (NMT), by drastically reducing the need for large parallel data. Most approaches adapt masked-language modeling (MLM) to sequence-to-sequence architectures, by masking parts of the input and reconstructing them in the decoder. In this work, we systematically compare masking with alternative objectives that produce inputs resembling real (full) sentences, by reordering and replacing words based on their context. We pretrain models with different methods on English↔German, English↔Nepali and English↔Sinhala monolingual data, and evaluate them on NMT. In (semi-) supervised NMT, varying the pretraining objective leads to surprisingly small differences in the finetuned performance, whereas unsupervised NMT is much more sensitive to it. To understand these results, we thoroughly study the pretrained models using a series of probes and verify that they encode and use information in different ways. We conclude that finetuning on parallel data is mostly sensitive to few properties that are shared by most models, such as a strong decoder, in contrast to unsupervised NMT that also requires models with strong cross-lingual abilities.

| Comments: | Findings of ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.05634](https://arxiv.org/abs/2106.05634) [cs.CL]** |
|           | (or **[arXiv:2106.05634v1](https://arxiv.org/abs/2106.05634v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-6">6. Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation
</h2>

Title: [Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation](https://arxiv.org/abs/2106.05691)

Authors: [Yuanxin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Zheng Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Weiping Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Recently, knowledge distillation (KD) has shown great success in BERT compression. Instead of only learning from the teacher's soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student's performance. To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher's hidden states of all the tokens in a layer-wise manner. In this paper, however, we observe that although distilling the teacher's hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled. To understand this effect, we conduct a series of analysis. Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width. We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. In this way, we show that 1) the student's performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7x ~ 3.4x.

| Comments: | Accepted by ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.05691](https://arxiv.org/abs/2106.05691) [cs.CL]** |
|           | (or **[arXiv:2106.05691v1](https://arxiv.org/abs/2106.05691v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-7">7. GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures
</h2>

Title: [GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures](https://arxiv.org/abs/2106.05822)

Authors: [Ivan Chelombiev](https://arxiv.org/search/cs?searchtype=author&query=Chelombiev%2C+I), [Daniel Justus](https://arxiv.org/search/cs?searchtype=author&query=Justus%2C+D), [Douglas Orr](https://arxiv.org/search/cs?searchtype=author&query=Orr%2C+D), [Anastasia Dietrich](https://arxiv.org/search/cs?searchtype=author&query=Dietrich%2C+A), [Frithjof Gressmann](https://arxiv.org/search/cs?searchtype=author&query=Gressmann%2C+F), [Alexandros Koliousis](https://arxiv.org/search/cs?searchtype=author&query=Koliousis%2C+A), [Carlo Luschi](https://arxiv.org/search/cs?searchtype=author&query=Luschi%2C+C)

> Attention based language models have become a critical component in state-of-the-art natural language processing systems. However, these models have significant computational requirements, due to long training times, dense operations and large parameter count. In this work we demonstrate a set of modifications to the structure of a Transformer layer, producing a more efficient architecture. First, we add a convolutional module to complement the self-attention module, decoupling the learning of local and global interactions. Secondly, we rely on grouped transformations to reduce the computational cost of dense feed-forward layers and convolutions, while preserving the expressivity of the model. We apply the resulting architecture to language representation learning and demonstrate its superior performance compared to BERT models of different scales. We further highlight its improved efficiency, both in terms of floating-point operations (FLOPs) and time-to-train.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.05822](https://arxiv.org/abs/2106.05822) [cs.CL]** |
|           | (or **[arXiv:2106.05822v1](https://arxiv.org/abs/2106.05822v1) [cs.CL]** for this version) |





<h2 id="2021-06-11-8">8. ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation
</h2>

Title: [ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation](https://arxiv.org/abs/2106.05970)

Authors: [Wanrong Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Xin Eric Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X+E), [An Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+A), [Miguel Eckstein](https://arxiv.org/search/cs?searchtype=author&query=Eckstein%2C+M), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y)

> Automatic evaluations for natural language generation (NLG) conventionally rely on token-level or embedding-level comparisons with the text references. This is different from human language processing, for which visual imaginations often improve comprehension. In this work, we propose ImaginE, an imagination-based automatic evaluation metric for natural language generation. With the help of CLIP and DALL-E, two cross-modal models pre-trained on large-scale image-text pairs, we automatically generate an image as the embodied imagination for the text snippet and compute the imagination similarity using contextual embeddings. Experiments spanning several text generation tasks demonstrate that adding imagination with our ImaginE displays great potential in introducing multi-modal information into NLG evaluation, and improves existing automatic metrics' correlations with human similarity judgments in many circumstances.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.05970](https://arxiv.org/abs/2106.05970) [cs.CL]** |
|           | (or **[arXiv:2106.05970v1](https://arxiv.org/abs/2106.05970v1) [cs.CL]** for this version) |





# 2021-06-10

[Return to Index](#Index)



<h2 id="2021-06-10-1">1. PAM: Understanding Product Images in Cross Product Category Attribute Extraction
</h2>

Title: [PAM: Understanding Product Images in Cross Product Category Attribute Extraction](https://arxiv.org/abs/2106.04630)

Authors: [Rongmei Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+R), [Xiang He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X), [Jie Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J), [Nasser Zalmout](https://arxiv.org/search/cs?searchtype=author&query=Zalmout%2C+N), [Yan Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Li Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+L), [Xin Luna Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+X+L)

> Understanding product attributes plays an important role in improving online shopping experience for customers and serves as an integral part for constructing a product knowledge graph. Most existing methods focus on attribute extraction from text description or utilize visual information from product images such as shape and color. Compared to the inputs considered in prior works, a product image in fact contains more information, represented by a rich mixture of words and visual clues with a layout carefully designed to impress customers. This work proposes a more inclusive framework that fully utilizes these different modalities for attribute extraction. Inspired by recent works in visual question answering, we use a transformer based sequence to sequence model to fuse representations of product text, Optical Character Recognition (OCR) tokens and visual objects detected in the product image. The framework is further extended with the capability to extract attribute value across multiple product categories with a single model, by training the decoder to predict both product category and attribute value and conditioning its output on product category. The model provides a unified attribute extraction solution desirable at an e-commerce platform that offers numerous product categories with a diverse body of product attributes. We evaluated the model on two product attributes, one with many possible values and one with a small set of possible values, over 14 product categories and found the model could achieve 15% gain on the Recall and 10% gain on the F1 score compared to existing methods using text-only features.

| Comments: | KDD 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| DOI:      | [10.1145/3447548.3467164](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3447548.3467164&v=ffa91bdd) |
| Cite as:  | **[arXiv:2106.04630](https://arxiv.org/abs/2106.04630) [cs.CV]** |
|           | (or **[arXiv:2106.04630v1](https://arxiv.org/abs/2106.04630v1) [cs.CV]** for this version) |





<h2 id="2021-06-10-2">2. VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation
</h2>

Title: [VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation](https://arxiv.org/abs/2106.04632)

Authors: [Linjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Jie Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+J), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Licheng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+L), [Yen-Chun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Rohit Pillai](https://arxiv.org/search/cs?searchtype=author&query=Pillai%2C+R), [Yu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Luowei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Xin Eric Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X+E), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y), [Tamara Lee Berg](https://arxiv.org/search/cs?searchtype=author&query=Berg%2C+T+L), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Zicheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z)

> Most existing video-and-language (VidL) research focuses on a single dataset, or multiple datasets of a single task. In reality, a truly useful VidL system is expected to be easily generalizable to diverse tasks, domains, and datasets. To facilitate the evaluation of such systems, we introduce Video-And-Language Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets over 3 popular tasks: (i) text-to-video retrieval; (ii) video question answering; and (iii) video captioning. VALUE benchmark aims to cover a broad range of video genres, video lengths, data volumes, and task difficulty levels. Rather than focusing on single-channel videos with visual information only, VALUE promotes models that leverage information from both video frames and their associated subtitles, as well as models that share knowledge across multiple tasks. We evaluate various baseline methods with and without large-scale VidL pre-training, and systematically investigate the impact of video input channels, fusion methods, and different video representations. We also study the transferability between tasks, and conduct multi-task learning under different settings. The significant gap between our best model and human performance calls for future study for advanced VidL models. VALUE is available at [this https URL](https://value-leaderboard.github.io/).

| Comments: | VALUE is available at [this https URL](https://value-leaderboard.github.io/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.04632](https://arxiv.org/abs/2106.04632) [cs.CV]** |
|           | (or **[arXiv:2106.04632v1](https://arxiv.org/abs/2106.04632v1) [cs.CV]** for this version) |





<h2 id="2021-06-10-3">3. FastSeq: Make Sequence Generation Faster
</h2>

Title: [FastSeq: Make Sequence Generation Faster](https://arxiv.org/abs/2106.04718)

Authors: [Yu Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y), [Fei Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+F), [Jiusheng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Nikhil Bhendawade](https://arxiv.org/search/cs?searchtype=author&query=Bhendawade%2C+N), [Ting Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+T), [Yeyun Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N), [Desheng Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+D), [Bingyu Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+B), [Ruifei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R)

> Transformer-based models have made tremendous impacts in natural language generation. However the inference speed is a bottleneck due to large model size and intensive computing involved in auto-regressive decoding process. We develop FastSeq framework to accelerate sequence generation without accuracy loss. The proposed optimization techniques include an attention cache optimization, an efficient algorithm for detecting repeated n-grams, and an asynchronous generation pipeline with parallel I/O. These optimizations are general enough to be applicable to Transformer-based models (e.g., T5, GPT2, and UniLM). Our benchmark results on a set of widely used and diverse models demonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use with a simple one-line code change. The source code is available at [this https URL](https://github.com/microsoft/fastseq).

| Comments: | ACL 2021 Demo Track                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.04718](https://arxiv.org/abs/2106.04718) [cs.CL]** |
|           | (or **[arXiv:2106.04718v1](https://arxiv.org/abs/2106.04718v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-4">4. Sentence Embeddings using Supervised Contrastive Learning
</h2>

Title: [Sentence Embeddings using Supervised Contrastive Learning](https://arxiv.org/abs/2106.04791)

Authors: [Danqi Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+D)

> Sentence embeddings encode sentences in fixed dense vectors and have played an important role in various NLP tasks and systems. Methods for building sentence embeddings include unsupervised learning such as Quick-Thoughts and supervised learning such as InferSent. With the success of pretrained NLP models, recent research shows that fine-tuning pretrained BERT on SNLI and Multi-NLI data creates state-of-the-art sentence embeddings, outperforming previous sentence embeddings methods on various evaluation benchmarks. In this paper, we propose a new method to build sentence embeddings by doing supervised contrastive learning. Specifically our method fine-tunes pretrained BERT on SNLI data, incorporating both supervised crossentropy loss and supervised contrastive loss. Compared with baseline where fine-tuning is only done with supervised cross-entropy loss similar to current state-of-the-art method SBERT, our supervised contrastive method improves 2.8% in average on Semantic Textual Similarity (STS) benchmarks and 1.05% in average on various sentence transfer tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.04791](https://arxiv.org/abs/2106.04791) [cs.CL]** |
|           | (or **[arXiv:2106.04791v1](https://arxiv.org/abs/2106.04791v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-5">5. Probing Multilingual Language Models for Discourse
</h2>

Title: [Probing Multilingual Language Models for Discourse](https://arxiv.org/abs/2106.04832)

Authors: [Murathan Kurfalı](https://arxiv.org/search/cs?searchtype=author&query=Kurfalı%2C+M), [Robert Östling](https://arxiv.org/search/cs?searchtype=author&query=Östling%2C+R)

> Pre-trained multilingual language models have become an important building block in multilingual natural language processing. In the present paper, we investigate a range of such models to find out how well they transfer discourse-level knowledge across languages. This is done with a systematic evaluation on a broader set of discourse-level tasks than has been previously been assembled. We find that the XLM-RoBERTa family of models consistently show the best performance, by simultaneously being good monolingual models and degrading relatively little in a zero-shot setting. Our results also indicate that model distillation may hurt the ability of cross-lingual transfer of sentence representations, while language dissimilarity at most has a modest effect. We hope that our test suite, covering 5 tasks with a total of 22 languages in 10 distinct families, will serve as a useful evaluation platform for multilingual performance at and beyond the sentence level.

| Comments: | To be presented at RepL4NLP 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.04832](https://arxiv.org/abs/2106.04832) [cs.CL]** |
|           | (or **[arXiv:2106.04832v1](https://arxiv.org/abs/2106.04832v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-6">6. RealTranS: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer
</h2>

Title: [RealTranS: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer](https://arxiv.org/abs/2106.04833)

Authors: [Xingshan Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> End-to-end simultaneous speech translation (SST), which directly translates speech in one language into text in another language in real-time, is useful in many scenarios but has not been fully investigated. In this work, we propose RealTranS, an end-to-end model for SST. To bridge the modality gap between speech and text, RealTranS gradually downsamples the input speech with interleaved convolution and unidirectional Transformer layers for acoustic modeling, and then maps speech features into text space with a weighted-shrinking operation and a semantic encoder. Besides, to improve the model performance in simultaneous scenarios, we propose a blank penalty to enhance the shrinking quality and a Wait-K-Stride-N strategy to allow local reranking during decoding. Experiments on public and widely-used datasets show that RealTranS with the Wait-K-Stride-N strategy outperforms prior end-to-end models as well as cascaded models in diverse latency settings.

| Comments: | Accepted by ACL2021 Findings                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2106.04833](https://arxiv.org/abs/2106.04833) [cs.CL]** |
|           | (or **[arXiv:2106.04833v1](https://arxiv.org/abs/2106.04833v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-7">7. Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding
</h2>

Title: [Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding](https://arxiv.org/abs/2106.04970)

Authors: [Xin Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Tao Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Houfeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC). SAD optimizes the online inference efficiency for GEC by two innovations: 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism; 2) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during inference. Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield the same predictions as greedy decoding but with a significant speedup for online inference. Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss. Not only does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14 and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at [this https URL](https://github.com/AutoTemp/Shallow-Aggressive-Decoding).

| Comments: | Accepted by ACL2021 (main conference)                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.04970](https://arxiv.org/abs/2106.04970) [cs.CL]** |
|           | (or **[arXiv:2106.04970v1](https://arxiv.org/abs/2106.04970v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-8">8. Crosslingual Embeddings are Essential in UNMT for Distant Languages: An English to IndoAryan Case Study
</h2>

Title: [Crosslingual Embeddings are Essential in UNMT for Distant Languages: An English to IndoAryan Case Study](https://arxiv.org/abs/2106.04995)

Authors: [Tamali Banerjee](https://arxiv.org/search/cs?searchtype=author&query=Banerjee%2C+T), [Rudra Murthy V](https://arxiv.org/search/cs?searchtype=author&query=V%2C+R+M), [Pushpak Bhattacharyya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharyya%2C+P)

> Recent advances in Unsupervised Neural Machine Translation (UNMT) have minimized the gap between supervised and unsupervised machine translation performance for closely related language pairs. However, the situation is very different for distant language pairs. Lack of lexical overlap and low syntactic similarities such as between English and Indo-Aryan languages leads to poor translation quality in existing UNMT systems. In this paper, we show that initializing the embedding layer of UNMT models with cross-lingual embeddings shows significant improvements in BLEU score over existing approaches with embeddings randomly initialized. Further, static embeddings (freezing the embedding layer weights) lead to better gains compared to updating the embedding layer weights during training (non-static). We experimented using Masked Sequence to Sequence (MASS) and Denoising Autoencoder (DAE) UNMT approaches for three distant language pairs. The proposed cross-lingual embedding initialization yields BLEU score improvement of as much as ten times over the baseline for English-Hindi, English-Bengali, and English-Gujarati. Our analysis shows the importance of cross-lingual embedding, comparisons between approaches, and the scope of improvements in these systems.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.04995](https://arxiv.org/abs/2106.04995) [cs.CL]** |
|           | (or **[arXiv:2106.04995v1](https://arxiv.org/abs/2106.04995v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-9">9. Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation
</h2>

Title: [Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation](https://arxiv.org/abs/2106.05093)

Authors: [Cunxiao Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+C), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Jing Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+J)

> We propose a new training objective named order-agnostic cross entropy (OaXE) for fully non-autoregressive translation (NAT) models. OaXE improves the standard cross-entropy loss to ameliorate the effect of word reordering, which is a common source of the critical multimodality problem in NAT. Concretely, OaXE removes the penalty for word order errors, and computes the cross entropy loss based on the best possible alignment between model predictions and target tokens. Since the log loss is very sensitive to invalid references, we leverage cross entropy initialization and loss truncation to ensure the model focuses on a good part of the search space. Extensive experiments on major WMT benchmarks show that OaXE substantially improves translation performance, setting new state of the art for fully NAT models. Further analyses show that OaXE alleviates the multimodality problem by reducing token repetitions and increasing prediction confidence. Our code, data, and trained models are available at [this https URL](https://github.com/tencent-ailab/ICML21_OAXE).

| Comments: | ICML 2021 (Oral), Code at [this https URL](https://github.com/tencent-ailab/ICML21_OAXE) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.05093](https://arxiv.org/abs/2106.05093) [cs.CL]** |
|           | (or **[arXiv:2106.05093v1](https://arxiv.org/abs/2106.05093v1) [cs.CL]** for this version) |





<h2 id="2021-06-10-10">10. AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT
</h2>

Title: [AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT](https://arxiv.org/abs/2106.05141)

Authors: [Tasnim Mohiuddin](https://arxiv.org/search/cs?searchtype=author&query=Mohiuddin%2C+T), [M Saiful Bari](https://arxiv.org/search/cs?searchtype=author&query=Bari%2C+M+S), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S)

> The success of Neural Machine Translation (NMT) largely depends on the availability of large bitext training corpora. Due to the lack of such large corpora in low-resource language pairs, NMT systems often exhibit poor performance. Extra relevant monolingual data often helps, but acquiring it could be quite expensive, especially for low-resource languages. Moreover, domain mismatch between bitext (train/test) and monolingual data might degrade the performance. To alleviate such issues, we propose AUGVIC, a novel data augmentation framework for low-resource NMT which exploits the vicinal samples of the given bitext without using any extra monolingual data explicitly. It can diversify the in-domain bitext data with finer level control. Through extensive experiments on four low-resource language pairs comprising data from different domains, we have shown that our method is comparable to the traditional back-translation that uses extra in-domain monolingual data. When we combine the synthetic parallel data generated from AUGVIC with the ones from the extra monolingual data, we achieve further improvements. We show that AUGVIC helps to attenuate the discrepancies between relevant and distant-domain monolingual data in traditional back-translation. To understand the contributions of different components of AUGVIC, we perform an in-depth framework analysis.

| Comments: | ACL-2021 accepted paper                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.05141](https://arxiv.org/abs/2106.05141) [cs.CL]** |
|           | (or **[arXiv:2106.05141v1](https://arxiv.org/abs/2106.05141v1) [cs.CL]** for this version) |





# 2021-06-09

[Return to Index](#Index)



<h2 id="2021-06-09-1">1. A Survey of Transformers
</h2>

Title: [A Survey of Transformers](https://arxiv.org/abs/2106.04554)

Authors: [Tianyang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+T), [Yuxin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Xiangyang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X)

> Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.04554](https://arxiv.org/abs/2106.04554) [cs.LG]** |
|           | (or **[arXiv:2106.04554v1](https://arxiv.org/abs/2106.04554v1) [cs.LG]** for this version) |





<h2 id="2021-06-09-2">2. Meta Learning for Knowledge Distillation
</h2>

Title: [Meta Learning for Knowledge Distillation](https://arxiv.org/abs/2106.04570)

Authors: [Wangchunshu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Canwen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Julian McAuley](https://arxiv.org/search/cs?searchtype=author&query=McAuley%2C+J)

> We present Meta Learning for Knowledge Distillation (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models. The code is available at [this https URL](https://github.com/JetRunner/MetaDistil)

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.04570](https://arxiv.org/abs/2106.04570) [cs.LG]** |
|           | (or **[arXiv:2106.04570v1](https://arxiv.org/abs/2106.04570v1) [cs.LG]** for this version) |





<h2 id="2021-06-09-3">3. Lexicon Learning for Few-Shot Neural Sequence Modeling
</h2>

Title: [Lexicon Learning for Few-Shot Neural Sequence Modeling](https://arxiv.org/abs/2106.03993)

Authors: [Ekin Akyürek](https://arxiv.org/search/cs?searchtype=author&query=Akyürek%2C+E), [Jacob Andreas](https://arxiv.org/search/cs?searchtype=author&query=Andreas%2C+J)

> Sequence-to-sequence transduction is the core problem in language processing applications as diverse as semantic parsing, machine translation, and instruction following. The neural network models that provide the dominant solution to these problems are brittle, especially in low-resource settings: they fail to generalize correctly or systematically from small datasets. Past work has shown that many failures of systematic generalization arise from neural models' inability to disentangle lexical phenomena from syntactic ones. To address this, we augment neural decoders with a lexical translation mechanism that generalizes existing copy mechanisms to incorporate learned, decontextualized, token-level translation rules. We describe how to initialize this mechanism using a variety of lexicon learning algorithms, and show that it improves systematic generalization on a diverse set of sequence modeling tasks drawn from cognitive science, formal semantics, and machine translation.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.03993](https://arxiv.org/abs/2106.03993) [cs.CL]** |
|           | (or **[arXiv:2106.03993v1](https://arxiv.org/abs/2106.03993v1) [cs.CL]** for this version) |





<h2 id="2021-06-09-4">4. Self-supervised and Supervised Joint Training for Resource-rich Machine Translation
</h2>

Title: [Self-supervised and Supervised Joint Training for Resource-rich Machine Translation](https://arxiv.org/abs/2106.04060)

Authors: [Yong Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Lu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+L), [Wolfgang Macherey](https://arxiv.org/search/cs?searchtype=author&query=Macherey%2C+W)

> Self-supervised pre-training of text representations has been successfully applied to low-resource Neural Machine Translation (NMT). However, it usually fails to achieve notable gains on resource-rich NMT. In this paper, we propose a joint training approach, F2-XEnDec, to combine self-supervised and supervised learning to optimize NMT models. To exploit complementary self-supervised signals for supervised learning, NMT models are trained on examples that are interbred from monolingual and parallel sentences through a new process called crossover encoder-decoder. Experiments on two resource-rich translation benchmarks, WMT'14 English-German and WMT'14 English-French, demonstrate that our approach achieves substantial improvements over several strong baseline methods and obtains a new state of the art of 46.19 BLEU on English-French when incorporating back translation. Results also show that our approach is capable of improving model robustness to input perturbations such as code-switching noise which frequently appears on social media.

| Comments: | Accepted by ICML 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.04060](https://arxiv.org/abs/2106.04060) [cs.CL]** |
|           | (or **[arXiv:2106.04060v1](https://arxiv.org/abs/2106.04060v1) [cs.CL]** for this version) |





<h2 id="2021-06-09-5">5. Obtaining Better Static Word Embeddings Using Contextual Embedding Models
</h2>

Title: [Obtaining Better Static Word Embeddings Using Contextual Embedding Models](https://arxiv.org/abs/2106.04302)

Authors: [Prakhar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+P), [Martin Jaggi](https://arxiv.org/search/cs?searchtype=author&query=Jaggi%2C+M)

> The advent of contextual word embeddings -- representations of words which incorporate semantic and syntactic information from their context -- has led to tremendous improvements on a wide variety of NLP tasks. However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret. In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve computational efficiency of NLP applications, while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods. As a side-effect, our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks.

| Comments: | ACL 2021 accept                                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.04302](https://arxiv.org/abs/2106.04302) [cs.CL]** |
|           | (or **[arXiv:2106.04302v1](https://arxiv.org/abs/2106.04302v1) [cs.CL]** for this version) |





<h2 id="2021-06-09-6">6. CLTR: An End-to-End, Transformer-Based System for Cell Level TableRetrieval and Table Question Answering
</h2>

Title: [CLTR: An End-to-End, Transformer-Based System for Cell Level TableRetrieval and Table Question Answering](https://arxiv.org/abs/2106.04441)

Authors: [Feifei Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+F), [Mustafa Canim](https://arxiv.org/search/cs?searchtype=author&query=Canim%2C+M), [Michael Glass](https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+M), [Alfio Gliozzo](https://arxiv.org/search/cs?searchtype=author&query=Gliozzo%2C+A), [Peter Fox](https://arxiv.org/search/cs?searchtype=author&query=Fox%2C+P)

> We present the first end-to-end, transformer-based table question answering (QA) system that takes natural language questions and massive table corpus as inputs to retrieve the most relevant tables and locate the correct table cells to answer the question. Our system, CLTR, extends the current state-of-the-art QA over tables model to build an end-to-end table QA architecture. This system has successfully tackled many real-world table QA problems with a simple, unified pipeline. Our proposed system can also generate a heatmap of candidate columns and rows over complex tables and allow users to quickly identify the correct cells to answer questions. In addition, we introduce two new open-domain benchmarks, E2E_WTQ and E2E_GNQ, consisting of 2,005 natural language questions over 76,242 tables. The benchmarks are designed to validate CLTR as well as accommodate future table retrieval and end-to-end table QA research and experiments. Our experiments demonstrate that our system is the current state-of-the-art model on the table retrieval task and produces promising results for end-to-end table QA.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.04441](https://arxiv.org/abs/2106.04441) [cs.CL]** |
|           | (or **[arXiv:2106.04441v1](https://arxiv.org/abs/2106.04441v1) [cs.CL]** for this version) |





<h2 id="2021-06-09-7">7. Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks
</h2>

Title: [Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks](https://arxiv.org/abs/2106.04489)

Authors: [Rabeeh Karimi Mahabadi](https://arxiv.org/search/cs?searchtype=author&query=Mahabadi%2C+R+K), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Mostafa Dehghani](https://arxiv.org/search/cs?searchtype=author&query=Dehghani%2C+M), [James Henderson](https://arxiv.org/search/cs?searchtype=author&query=Henderson%2C+J)

> State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in [this https URL](https://github.com/rabeehk/hyperformer).

| Comments: | accepted in ACL, 2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.04489](https://arxiv.org/abs/2106.04489) [cs.CL]** |
|           | (or **[arXiv:2106.04489v1](https://arxiv.org/abs/2106.04489v1) [cs.CL]** for this version) |



# 2021-06-08

[Return to Index](#Index)



<h2 id="2021-06-08-1">1. CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings
</h2>

Title: [CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings](https://arxiv.org/abs/2106.03143)

Authors: [Tatiana Likhomanenko](https://arxiv.org/search/cs?searchtype=author&query=Likhomanenko%2C+T), [Qiantong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Q), [Ronan Collobert](https://arxiv.org/search/cs?searchtype=author&query=Collobert%2C+R), [Gabriel Synnaeve](https://arxiv.org/search/cs?searchtype=author&query=Synnaeve%2C+G), [Alex Rogozhnikov](https://arxiv.org/search/cs?searchtype=author&query=Rogozhnikov%2C+A)

> Without positional information, attention-based transformer neural networks are permutation-invariant. Absolute or relative positional embeddings are the most popular ways to feed transformer models positional information. Absolute positional embeddings are simple to implement, but suffer from generalization issues when evaluating on sequences of different length than those seen at training time. Relative positions are more robust to length change, but are more complex to implement and yield inferior model throughput. In this paper, we propose an augmentation-based approach (CAPE) for absolute positional embeddings, which keeps the advantages of both absolute (simplicity and speed) and relative position embeddings (better generalization). In addition, our empirical evaluation on state-of-the-art models in machine translation, image and speech recognition demonstrates that CAPE leads to better generalization performance as well as increased stability with respect to training hyper-parameters.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.03143](https://arxiv.org/abs/2106.03143) [cs.LG]** |
|           | (or **[arXiv:2106.03143v1](https://arxiv.org/abs/2106.03143v1) [cs.LG]** for this version) |





<h2 id="2021-06-08-2">2. SelfDoc: Self-Supervised Document Representation Learning
</h2>

Title: [SelfDoc: Self-Supervised Document Representation Learning](https://arxiv.org/abs/2106.03331)

Authors: [Peizhao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Jiuxiang Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Jason Kuen](https://arxiv.org/search/cs?searchtype=author&query=Kuen%2C+J), [Vlad I. Morariu](https://arxiv.org/search/cs?searchtype=author&query=Morariu%2C+V+I), [Handong Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Rajiv Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+R), [Varun Manjunatha](https://arxiv.org/search/cs?searchtype=author&query=Manjunatha%2C+V), [Hongfu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H)

> We propose SelfDoc, a task-agnostic pre-training framework for document image understanding. Because documents are multimodal and are intended for sequential reading, our framework exploits the positional, textual, and visual information of every semantically meaningful component in a document, and it models the contextualization between each block of content. Unlike existing document pre-training models, our model is coarse-grained instead of treating individual words as input, therefore avoiding an overly fine-grained with excessive contextualization. Beyond that, we introduce cross-modal learning in the model pre-training phase to fully leverage multimodal information from unlabeled documents. For downstream usage, we propose a novel modality-adaptive attention mechanism for multimodal feature fusion by adaptively emphasizing language and vision signals. Our framework benefits from self-supervised pre-training on documents without requiring annotations by a feature masking training strategy. It achieves superior performance on multiple downstream tasks with significantly fewer document images used in the pre-training stage compared to previous works.

| Comments: | To appear in CVPR'2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.03331](https://arxiv.org/abs/2106.03331) [cs.CV]** |
|           | (or **[arXiv:2106.03331v1](https://arxiv.org/abs/2106.03331v1) [cs.CV]** for this version) |





<h2 id="2021-06-08-3">3. Do Grammatical Error Correction Models Realize Grammatical Generalization?
</h2>

Title: [Do Grammatical Error Correction Models Realize Grammatical Generalization?](https://arxiv.org/abs/2106.03031)

Authors: [Masato Mita](https://arxiv.org/search/cs?searchtype=author&query=Mita%2C+M), [Hitomi Yanaka](https://arxiv.org/search/cs?searchtype=author&query=Yanaka%2C+H)

> There has been an increased interest in data generation approaches to grammatical error correction (GEC) using pseudo data. However, these approaches suffer from several issues that make them inconvenient for real-world deployment including a demand for large amounts of training data. On the other hand, some errors based on grammatical rules may not necessarily require a large amount of data if GEC models can realize grammatical generalization. This study explores to what extent GEC models generalize grammatical knowledge required for correcting errors. We introduce an analysis method using synthetic and real GEC datasets with controlled vocabularies to evaluate whether models can generalize to unseen errors. We found that a current standard Transformer-based GEC model fails to realize grammatical generalization even in simple settings with limited vocabulary and syntax, suggesting that it lacks the generalization ability required to correct errors from provided training examples.

| Comments: | ACL 2021 (Findings)                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.03031](https://arxiv.org/abs/2106.03031) [cs.CL]** |
|           | (or **[arXiv:2106.03031v1](https://arxiv.org/abs/2106.03031v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-4">4. The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation
</h2>

Title: [The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation](https://arxiv.org/abs/2106.03193)

Authors: [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Cynthia Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Guillaume Wenzek](https://arxiv.org/search/cs?searchtype=author&query=Wenzek%2C+G), [Da Ju](https://arxiv.org/search/cs?searchtype=author&query=Ju%2C+D), [Sanjana Krishnan](https://arxiv.org/search/cs?searchtype=author&query=Krishnan%2C+S), [Marc'Aurelio Ranzato](https://arxiv.org/search/cs?searchtype=author&query=Ranzato%2C+M), [Francisco Guzman](https://arxiv.org/search/cs?searchtype=author&query=Guzman%2C+F), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A)

> One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the FLORES-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.03193](https://arxiv.org/abs/2106.03193) [cs.CL]** |
|           | (or **[arXiv:2106.03193v1](https://arxiv.org/abs/2106.03193v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-5">5. Itihasa: A large-scale corpus for Sanskrit to English translation
</h2>

Title: [Itihasa: A large-scale corpus for Sanskrit to English translation](https://arxiv.org/abs/2106.03269)

Authors: [Rahul Aralikatte](https://arxiv.org/search/cs?searchtype=author&query=Aralikatte%2C+R), [Miryam de Lhoneux](https://arxiv.org/search/cs?searchtype=author&query=de+Lhoneux%2C+M), [Anoop Kunchukuttan](https://arxiv.org/search/cs?searchtype=author&query=Kunchukuttan%2C+A), [Anders Søgaard](https://arxiv.org/search/cs?searchtype=author&query=Søgaard%2C+A)

> This work introduces Itihasa, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The shlokas are extracted from two Indian epics viz., The Ramayana and The Mahabharata. We first describe the motivation behind the curation of such a dataset and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this corpus and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.

| Comments: | WAT 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.03269](https://arxiv.org/abs/2106.03269) [cs.CL]** |
|           | (or **[arXiv:2106.03269v1](https://arxiv.org/abs/2106.03269v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-6">6. On the Language Coverage Bias for Neural Machine Translation
</h2>

Title: [On the Language Coverage Bias for Neural Machine Translation](https://arxiv.org/abs/2106.03297)

Authors: [Shuo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Language coverage bias, which indicates the content-dependent differences between sentence pairs originating from the source and target languages, is important for neural machine translation (NMT) because the target-original training data is not well exploited in current practice. By carefully designing experiments, we provide comprehensive analyses of the language coverage bias in the training data, and find that using only the source-original data achieves comparable performance with using full training data. Based on these observations, we further propose two simple and effective approaches to alleviate the language coverage bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation. We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants.

| Comments: | ACL 2021, Long Findings                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.03297](https://arxiv.org/abs/2106.03297) [cs.CL]** |
|           | (or **[arXiv:2106.03297v1](https://arxiv.org/abs/2106.03297v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-7">7. BERTGEN: Multi-task Generation through BERT
</h2>

Title: [BERTGEN: Multi-task Generation through BERT](https://arxiv.org/abs/2106.03484)

Authors: [Faidon Mitzalis](https://arxiv.org/search/cs?searchtype=author&query=Mitzalis%2C+F), [Ozan Caglayan](https://arxiv.org/search/cs?searchtype=author&query=Caglayan%2C+O), [Pranava Madhyastha](https://arxiv.org/search/cs?searchtype=author&query=Madhyastha%2C+P), [Lucia Specia](https://arxiv.org/search/cs?searchtype=author&query=Specia%2C+L)

> We present BERTGEN, a novel generative, decoder-only model which extends BERT by fusing multimodal and multilingual pretrained models VL-BERT and M-BERT, respectively. BERTGEN is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multitask setting. With a comprehensive set of evaluations, we show that BERTGEN outperforms many strong baselines across the tasks explored. We also show BERTGEN's ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that BERTGEN substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models.

| Comments: | Accepted to ACL 2021 Main Conference                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.03484](https://arxiv.org/abs/2106.03484) [cs.CL]** |
|           | (or **[arXiv:2106.03484v1](https://arxiv.org/abs/2106.03484v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-8">8. RoSearch: Search for Robust Student Architectures When Distilling Pre-trained Language Models
</h2>

Title: [RoSearch: Search for Robust Student Architectures When Distilling Pre-trained Language Models](https://arxiv.org/abs/2106.03613)

Authors: [Xin Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+X), [Jianlei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Haoyi Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Xucheng Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+X), [Jianxin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Pre-trained language models achieve outstanding performance in NLP tasks. Various knowledge distillation methods have been proposed to reduce the heavy computation and storage requirements of pre-trained language models. However, from our observations, student models acquired by knowledge distillation suffer from adversarial attacks, which limits their usage in security sensitive scenarios. In order to overcome these security problems, RoSearch is proposed as a comprehensive framework to search the student models with better adversarial robustness when performing knowledge distillation. A directed acyclic graph based search space is built and an evolutionary search strategy is utilized to guide the searching approach. Each searched architecture is trained by knowledge distillation on pre-trained language model and then evaluated under a robustness-, accuracy- and efficiency-aware metric as environmental fitness. Experimental results show that RoSearch can improve robustness of student models from 7%~18% up to 45.8%~47.8% on different datasets with comparable weight compression ratio to existing distillation methods (4.6×~6.5× improvement from teacher model BERT_BASE) and low accuracy drop. In addition, we summarize the relationship between student architecture and robustness through statistics of searched models.

| Comments: | 10 pages, 9 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.03613](https://arxiv.org/abs/2106.03613) [cs.CL]** |
|           | (or **[arXiv:2106.03613v1](https://arxiv.org/abs/2106.03613v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-9">9. Diverse Pretrained Context Encodings Improve Document Translation
</h2>

Title: [Diverse Pretrained Context Encodings Improve Document Translation](https://arxiv.org/abs/2106.03717)

Authors: [Domenic Donato](https://arxiv.org/search/cs?searchtype=author&query=Donato%2C+D), [Lei Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+L), [Chris Dyer](https://arxiv.org/search/cs?searchtype=author&query=Dyer%2C+C)

> We propose a new architecture for adapting a sentence-level sequence-to-sequence transformer by incorporating multiple pretrained document context signals and assess the impact on translation performance of (1) different pretraining approaches for generating these signals, (2) the quantity of parallel data for which document context is available, and (3) conditioning on source, target, or source and target contexts. Experiments on the NIST Chinese-English, and IWSLT and WMT English-German tasks support four general conclusions: that using pretrained context representations markedly improves sample efficiency, that adequate parallel data resources are crucial for learning to use document context, that jointly conditioning on multiple context representations outperforms any single representation, and that source context is more valuable for translation performance than target side context. Our best multi-context model consistently outperforms the best existing context-aware transformers.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.03717](https://arxiv.org/abs/2106.03717) [cs.CL]** |
|           | (or **[arXiv:2106.03717v1](https://arxiv.org/abs/2106.03717v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-10">10. Encouraging Neural Machine Translation to Satisfy Terminology Constraints
</h2>

Title: [Encouraging Neural Machine Translation to Satisfy Terminology Constraints](https://arxiv.org/abs/2106.03730)

Authors: [Melissa Ailem](https://arxiv.org/search/cs?searchtype=author&query=Ailem%2C+M), [Jinghsu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Raheel Qader](https://arxiv.org/search/cs?searchtype=author&query=Qader%2C+R)

> We present a new approach to encourage neural machine translation to satisfy lexical constraints. Our method acts at the training step and thereby avoiding the introduction of any extra computational overhead at inference step. The proposed method combines three main ingredients. The first one consists in augmenting the training data to specify the constraints. Intuitively, this encourages the model to learn a copy behavior when it encounters constraint terms. Compared to previous work, we use a simplified augmentation strategy without source factors. The second ingredient is constraint token masking, which makes it even easier for the model to learn the copy behavior and generalize better. The third one, is a modification of the standard cross entropy loss to bias the model towards assigning high probabilities to constraint words. Empirical results show that our method improves upon related baselines in terms of both BLEU score and the percentage of generated constraint terms.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.03730](https://arxiv.org/abs/2106.03730) [cs.CL]** |
|           | (or **[arXiv:2106.03730v1](https://arxiv.org/abs/2106.03730v1) [cs.CL]** for this version) |





<h2 id="2021-06-08-11">11. A Simple Recipe for Multilingual Grammatical Error Correction
</h2>

Title: [A Simple Recipe for Multilingual Grammatical Error Correction](https://arxiv.org/abs/2106.03830)

Authors: [Sascha Rothe](https://arxiv.org/search/cs?searchtype=author&query=Rothe%2C+S), [Jonathan Mallinson](https://arxiv.org/search/cs?searchtype=author&query=Mallinson%2C+J), [Eric Malmi](https://arxiv.org/search/cs?searchtype=author&query=Malmi%2C+E), [Sebastian Krause](https://arxiv.org/search/cs?searchtype=author&query=Krause%2C+S), [Aliaksei Severyn](https://arxiv.org/search/cs?searchtype=author&query=Severyn%2C+A)

> This paper presents a simple recipe to train state-of-the-art multilingual Grammatical Error Correction (GEC) models. We achieve this by first proposing a language-agnostic method to generate a large number of synthetic examples. The second ingredient is to use large-scale multilingual language models (up to 11B parameters). Once fine-tuned on language-specific supervised sets we surpass the previous state-of-the-art results on GEC benchmarks in four languages: English, Czech, German and Russian. Having established a new set of baselines for GEC, we make our results easily reproducible and accessible by releasing a cLang-8 dataset. It is produced by using our best model, which we call gT5, to clean the targets of a widely used yet noisy lang-8 dataset. cLang-8 greatly simplifies typical GEC training pipelines composed of multiple fine-tuning stages -- we demonstrate that performing a single fine-tuning step on cLang-8 with the off-the-shelf language models yields further accuracy improvements over an already top-performing gT5 model for English.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.03830](https://arxiv.org/abs/2106.03830) [cs.CL]** |
|           | (or **[arXiv:2106.03830v1](https://arxiv.org/abs/2106.03830v1) [cs.CL]** for this version) |






# 2021-06-07

[Return to Index](#Index)



<h2 id="2021-06-07-1">1. Neural semi-Markov CRF for Monolingual Word Alignment
</h2>

Title: [Neural semi-Markov CRF for Monolingual Word Alignment](https://arxiv.org/abs/2106.02569)

Authors: [Wuwei Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+W), [Chao Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+C), [Wei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W)

> Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data. Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks.

| Comments: | Accepted to ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02569](https://arxiv.org/abs/2106.02569) [cs.CL]** |
|           | (or **[arXiv:2106.02569v1](https://arxiv.org/abs/2106.02569v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-2">2. Human-Adversarial Visual Question Answering
</h2>

Title: [Human-Adversarial Visual Question Answering](https://arxiv.org/abs/2106.02280)

Authors: [Sasha Sheng](https://arxiv.org/search/cs?searchtype=author&query=Sheng%2C+S), [Amanpreet Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+A), [Vedanuj Goswami](https://arxiv.org/search/cs?searchtype=author&query=Goswami%2C+V), [Jose Alberto Lopez Magana](https://arxiv.org/search/cs?searchtype=author&query=Magana%2C+J+A+L), [Wojciech Galuba](https://arxiv.org/search/cs?searchtype=author&query=Galuba%2C+W), [Devi Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+D), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D)

> Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model's predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art.

| Comments: | 22 pages, 13 figures. First two authors contributed equally  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2106.02280](https://arxiv.org/abs/2106.02280) [cs.CV]** |
|           | (or **[arXiv:2106.02280v1](https://arxiv.org/abs/2106.02280v1) [cs.CV]** for this version) |





<h2 id="2021-06-07-3">3. How to Adapt Your Pretrained Multilingual Model to 1600 Languages
</h2>

Title: [How to Adapt Your Pretrained Multilingual Model to 1600 Languages](https://arxiv.org/abs/2106.02124)

Authors: [Abteen Ebrahimi](https://arxiv.org/search/cs?searchtype=author&query=Ebrahimi%2C+A), [Katharina Kann](https://arxiv.org/search/cs?searchtype=author&query=Kann%2C+K)

> Pretrained multilingual models (PMMs) enable zero-shot learning via cross-lingual transfer, performing best for languages seen during pretraining. While methods exist to improve performance for unseen languages, they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the world's languages. In this paper, we evaluate the performance of existing methods to adapt PMMs to new languages using a resource available for over 1600 languages: the New Testament. This is challenging for two reasons: (1) the small corpus size, and (2) the narrow domain. While performance drops for all approaches, we surprisingly still see gains of up to 17.69% accuracy for part-of-speech tagging and 6.29 F1 for NER on average over all languages as compared to XLM-R. Another unexpected finding is that continued pretraining, the simplest approach, performs best. Finally, we perform a case study to disentangle the effects of domain and size and to shed light on the influence of the finetuning source language.

| Comments: | Accepted to ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02124](https://arxiv.org/abs/2106.02124) [cs.CL]** |
|           | (or **[arXiv:2106.02124v1](https://arxiv.org/abs/2106.02124v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-4">4. Syntax-augmented Multilingual BERT for Cross-lingual Transfer
</h2>

Title: [Syntax-augmented Multilingual BERT for Cross-lingual Transfer](https://arxiv.org/abs/2106.02134)

Authors: [Wasi Uddin Ahmad](https://arxiv.org/search/cs?searchtype=author&query=Ahmad%2C+W+U), [Haoran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K), [Yashar Mehdad](https://arxiv.org/search/cs?searchtype=author&query=Mehdad%2C+Y)

> In recent years, we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning. However, due to typological differences across languages, the cross-lingual transfer is challenging. Nevertheless, language syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous works have shown that pre-trained multilingual encoders, such as mBERT \cite{devlin-etal-2019-bert}, capture language syntax, helping cross-lingual transfer. This work shows that explicitly providing language syntax and training mBERT using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer. We perform rigorous experiments on four NLP tasks, including text classification, question answering, named entity recognition, and task-oriented semantic parsing. The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across all languages. In the \emph{generalized} transfer setting, the performance boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.

| Comments: | ACL 2021 (camera ready)                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02134](https://arxiv.org/abs/2106.02134) [cs.CL]** |
|           | (or **[arXiv:2106.02134v1](https://arxiv.org/abs/2106.02134v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-5">5. Grounding 'Grounding' in NLP
</h2>

Title: [Grounding 'Grounding' in NLP](https://arxiv.org/abs/2106.02192)

Authors: [Khyathi Raghavi Chandu](https://arxiv.org/search/cs?searchtype=author&query=Chandu%2C+K+R), [Yonatan Bisk](https://arxiv.org/search/cs?searchtype=author&query=Bisk%2C+Y), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W)

> The NLP community has seen substantial recent interest in grounding to facilitate interaction between language technologies and the world. However, as a community, we use the term broadly to reference any linking of text to data or non-textual modality. In contrast, Cognitive Science more formally defines "grounding" as the process of establishing what mutual information is required for successful communication between two interlocutors -- a definition which might implicitly capture the NLP usage but differs in intent and scope. We investigate the gap between these definitions and seek answers to the following questions: (1) What aspects of grounding are missing from NLP tasks? Here we present the dimensions of coordination, purviews and constraints. (2) How is the term "grounding" used in the current research? We study the trends in datasets, domains, and tasks introduced in recent NLP conferences. And finally, (3) How to advance our current definition to bridge the gap with Cognitive Science? We present ways to both create new tasks or repurpose existing ones to make advancements towards achieving a more complete sense of grounding.

| Comments: | 24 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02192](https://arxiv.org/abs/2106.02192) [cs.CL]** |
|           | (or **[arXiv:2106.02192v1](https://arxiv.org/abs/2106.02192v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-6">6. BERTTune: Fine-Tuning Neural Machine Translation with BERTScore
</h2>

Title: [BERTTune: Fine-Tuning Neural Machine Translation with BERTScore](https://arxiv.org/abs/2106.02208)

Authors: [Inigo Jauregi Unanue](https://arxiv.org/search/cs?searchtype=author&query=Unanue%2C+I+J), [Jacob Parnell](https://arxiv.org/search/cs?searchtype=author&query=Parnell%2C+J), [Massimo Piccardi](https://arxiv.org/search/cs?searchtype=author&query=Piccardi%2C+M)

> Neural machine translation models are often biased toward the limited translation references seen during training. To amend this form of overfitting, in this paper we propose fine-tuning the models with a novel training objective based on the recently-proposed BERTScore evaluation metric. BERTScore is a scoring function based on contextual embeddings that overcomes the typical limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing translations that are different from the references, yet close in the contextual embedding space, to be treated as substantially correct. To be able to use BERTScore as a training objective, we propose three approaches for generating soft predictions, allowing the network to remain completely differentiable end-to-end. Experiments carried out over four, diverse language pairs have achieved improvements of up to 0.58 pp (3.28%) in BLEU score and up to 0.76 pp (0.98%) in BERTScore (F_BERT) when fine-tuning a strong baseline.

| Comments: | Accepted at ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.02208](https://arxiv.org/abs/2106.02208) [cs.CL]** |
|           | (or **[arXiv:2106.02208v1](https://arxiv.org/abs/2106.02208v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-7">7. Scalable Transformers for Neural Machine Translation
</h2>

Title: [Scalable Transformers for Neural Machine Translation](https://arxiv.org/abs/2106.02242)

Authors: [Peng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+P), [Shijie Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+S), [Xiaogang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Jifeng Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+J), [Hongsheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H)

> Transformer has been widely adopted in Neural Machine Translation (NMT) because of its large capacity and parallel training of sequence generation. However, the deployment of Transformer is challenging because different scenarios require models of different complexities and scales. Naively training multiple Transformers is redundant in terms of both computation and memory. In this paper, we propose a novel scalable Transformers, which naturally contains sub-Transformers of different scales and have shared parameters. Each sub-Transformer can be easily obtained by cropping the parameters of the largest Transformer. A three-stage training scheme is proposed to tackle the difficulty of training the scalable Transformers, which introduces additional supervisions from word-level and sequence-level self-distillation. Extensive experiments were conducted on WMT EN-De and En-Fr to validate our proposed scalable Transformers.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.02242](https://arxiv.org/abs/2106.02242) [cs.CL]** |
|           | (or **[arXiv:2106.02242v1](https://arxiv.org/abs/2106.02242v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-8">8. Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene
</h2>

Title: [Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene](https://arxiv.org/abs/2106.02327)

Authors: [Ruikun Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R), [Guanhuan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Xiaojun Quan](https://arxiv.org/search/cs?searchtype=author&query=Quan%2C+X)

> The major paradigm of applying a pre-trained language model to downstream tasks is to fine-tune it on labeled task data, which often suffers instability and low performance when the labeled examples are scarce.~One way to alleviate this problem is to apply post-training on unlabeled task data before fine-tuning, adapting the pre-trained model to target domains by contrastive learning that considers either token-level or sequence-level similarity. Inspired by the success of sequence masking, we argue that both token-level and sequence-level similarities can be captured with a pair of masked sequences.~Therefore, we propose complementary random masking (CRM) to generate a pair of masked sequences from an input sequence for sequence-level contrastive learning and then develop contrastive masked language modeling (CMLM) for post-training to integrate both token-level and sequence-level contrastive learnings.~Empirical results show that CMLM surpasses several recent post-training methods in few-shot settings without the need for data augmentation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.02327](https://arxiv.org/abs/2106.02327) [cs.CL]** |
|           | (or **[arXiv:2106.02327v1](https://arxiv.org/abs/2106.02327v1) [cs.CL]** for this version) |





<h2 id="2021-06-07-9">9. Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings
</h2>

Title: [Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings](https://arxiv.org/abs/2106.02490)

Authors: [Thomas Conley](https://arxiv.org/search/cs?searchtype=author&query=Conley%2C+T), [Jugal Kalita](https://arxiv.org/search/cs?searchtype=author&query=Kalita%2C+J)

> Artificial Neural networks are mathematical models at their core. This truismpresents some fundamental difficulty when networks are tasked with Natural Language Processing. A key problem lies in measuring the similarity or distance among vectors in NLP embedding space, since the mathematical concept of distance does not always agree with the linguistic concept. We suggest that the best way to measure linguistic distance among vectors is by employing the Language Model (LM) that created them. We introduce Language Model Distance (LMD) for measuring accuracy of vector transformations based on the Distributional Hypothesis ( LMD Accuracy ). We show the efficacy of this metric by applying it to a simple neural network learning the Procrustes algorithm for bilingual word mapping.

| Subjects:          | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the 17th International Conference on Natural Language Processing, pages 170-174, Patna, India, December 18-21, 2020 |
| Cite as:           | **[arXiv:2106.02490](https://arxiv.org/abs/2106.02490) [cs.CL]** |
|                    | (or **[arXiv:2106.02490v1](https://arxiv.org/abs/2106.02490v1) [cs.CL]** for this version) |








# 2021-06-04

[Return to Index](#Index)



<h2 id="2021-06-04-1">1. TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data
</h2>

Title: [TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data](https://arxiv.org/abs/2106.01797)

Authors: [Pengda Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+P), [Yuhong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y)

> Among ubiquitous multimodal data in the real world, text is the modality generated by human, while image reflects the physical world honestly. In a visual understanding application, machines are expected to understand images like human. Inspired by this, we propose a novel self-supervised learning method, named Text-enhanced Visual Deep InfoMax (TVDIM), to learn better visual representations by fully utilizing the naturally-existing multimodal data. Our core idea of self-supervised learning is to maximize the mutual information between features extracted from multiple views of a shared context to a rational degree. Different from previous methods which only consider multiple views from a single modality, our work produces multiple views from different modalities, and jointly optimizes the mutual information for features pairs of intra-modality and inter-modality. Considering the information gap between inter-modality features pairs from data noise, we adopt a \emph{ranking-based} contrastive learning to optimize the mutual information. During evaluation, we directly use the pre-trained visual representations to complete various image classification tasks. Experimental results show that, TVDIM significantly outperforms previous visual self-supervised methods when processing the same set of images.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.01797](https://arxiv.org/abs/2106.01797) [cs.CL]** |
|           | (or **[arXiv:2106.01797v1](https://arxiv.org/abs/2106.01797v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-2">2. Representing Syntax and Composition with Geometric Transformations
</h2>

Title: [Representing Syntax and Composition with Geometric Transformations](https://arxiv.org/abs/2106.01904)

Authors: [Lorenzo Bertolini](https://arxiv.org/search/cs?searchtype=author&query=Bertolini%2C+L), [Julie Weeds](https://arxiv.org/search/cs?searchtype=author&query=Weeds%2C+J), [David Weir](https://arxiv.org/search/cs?searchtype=author&query=Weir%2C+D), [Qiwei Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+Q)

> The exploitation of syntactic graphs (SyGs) as a word's context has been shown to be beneficial for distributional semantic models (DSMs), both at the level of individual word representations and in deriving phrasal representations via composition. However, notwithstanding the potential performance benefit, the syntactically-aware DSMs proposed to date have huge numbers of parameters (compared to conventional DSMs) and suffer from data sparsity. Furthermore, the encoding of the SyG links (i.e., the syntactic relations) has been largely limited to linear maps. The knowledge graphs' literature, on the other hand, has proposed light-weight models employing different geometric transformations (GTs) to encode edges in a knowledge graph (KG). Our work explores the possibility of adopting this family of models to encode SyGs. Furthermore, we investigate which GT better encodes syntactic relations, so that these representations can be used to enhance phrase-level composition via syntactic contextualisation.

| Comments: | to appear in Findings of ACL 2021                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01904](https://arxiv.org/abs/2106.01904) [cs.CL]** |
|           | (or **[arXiv:2106.01904v1](https://arxiv.org/abs/2106.01904v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-3">3. The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models
</h2>

Title: [The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models](https://arxiv.org/abs/2106.01950)

Authors: [Ulme Wennberg](https://arxiv.org/search/cs?searchtype=author&query=Wennberg%2C+U), [Gustav Eje Henter](https://arxiv.org/search/cs?searchtype=author&query=Henter%2C+G+E)

> Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. Our proposal has several theoretical advantages over existing position-representation approaches. Experiments show that it improves on regular ALBERT on GLUE tasks, while only adding orders of magnitude less positional parameters.

| Comments: | 11 pages, 8 figures, Accepted to ACL 2021                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.01950](https://arxiv.org/abs/2106.01950) [cs.CL]** |
|           | (or **[arXiv:2106.01950v1](https://arxiv.org/abs/2106.01950v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-4">4. A Dataset and Baselines for Multilingual Reply Suggestion
</h2>

Title: [A Dataset and Baselines for Multilingual Reply Suggestion](https://arxiv.org/abs/2106.02017)

Authors: [Mozhi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Wei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Budhaditya Deb](https://arxiv.org/search/cs?searchtype=author&query=Deb%2C+B), [Guoqing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+G), [Milad Shokouhi](https://arxiv.org/search/cs?searchtype=author&query=Shokouhi%2C+M), [Ahmed Hassan Awadallah](https://arxiv.org/search/cs?searchtype=author&query=Awadallah%2C+A+H)

> Reply suggestion models help users process emails and chats faster. Previous work only studies English reply suggestion. Instead, we present MRS, a multilingual reply suggestion dataset with ten languages. MRS can be used to compare two families of models: 1) retrieval models that select the reply from a fixed set and 2) generation models that produce the reply from scratch. Therefore, MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence labeling tasks. We build a generation model and a retrieval model as baselines for MRS. The two models have different strengths in the monolingual setting, and they require different strategies to generalize across languages. MRS is publicly available at [this https URL](https://github.com/zhangmozhi/mrs).

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.02017](https://arxiv.org/abs/2106.02017) [cs.CL]** |
|           | (or **[arXiv:2106.02017v1](https://arxiv.org/abs/2106.02017v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-5">5. E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning
</h2>

Title: [E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning](https://arxiv.org/abs/2106.01804)

Authors: [Haiyang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Ming Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+M), [Chenliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Bin Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+B), [Songfang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Wenming Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+W), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F)

> Vision-language pre-training (VLP) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks. The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train. However, these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding, and the computation inefficiency of two-stage pipeline. In this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text. We incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel VLP paradigm.

| Subjects:          | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | ACL2021 main conference                                      |
| Cite as:           | **[arXiv:2106.01804](https://arxiv.org/abs/2106.01804) [cs.CV]** |
|                    | (or **[arXiv:2106.01804v1](https://arxiv.org/abs/2106.01804v1) [cs.CV]** for this version) |





<h2 id="2021-06-04-6">6. Lightweight Adapter Tuning for Multilingual Speech Translation
</h2>

Title: [Lightweight Adapter Tuning for Multilingual Speech Translation](https://arxiv.org/abs/2106.01463)

Authors: [Hang Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+H), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Didier Schwab](https://arxiv.org/search/cs?searchtype=author&query=Schwab%2C+D), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L)

> Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of task-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from different pre-trained models (a multilingual ST trained on parallel data or a multilingual BART (mBART) trained on non-parallel multilingual data), we show that adapters can be used to: (a) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters, and (b) transfer from an automatic speech recognition (ASR) task and an mBART pre-trained model to a multilingual ST task. Experiments show that adapter tuning offer competitive results to full fine-tuning, while being much more parameter-efficient.

| Comments: | Accepted at ACL-IJCNLP 2021                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01463](https://arxiv.org/abs/2106.01463) [cs.CL]** |
|           | (or **[arXiv:2106.01463v1](https://arxiv.org/abs/2106.01463v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-7">7. Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?
</h2>

Title: [Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?](https://arxiv.org/abs/2106.01561)

Authors: [Cunxiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Pai Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Recent work has investigated the interesting question using pre-trained language models (PLMs) as knowledge bases for answering open questions. However, existing work is limited in using small benchmarks with high test-train overlaps. We construct a new dataset of closed-book QA using SQuAD, and investigate the performance of BART. Experiments show that it is challenging for BART to remember training facts in high precision, and also challenging to answer closed-book questions even if relevant knowledge is retained. Some promising directions are found, including decoupling the knowledge memorizing process and the QA finetune process, forcing the model to recall relevant knowledge when question answering.

| Comments: | Accepted By ACL-IJCNLP 2021                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01561](https://arxiv.org/abs/2106.01561) [cs.CL]** |
|           | (or **[arXiv:2106.01561v1](https://arxiv.org/abs/2106.01561v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-8">8. Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction
</h2>

Title: [Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction](https://arxiv.org/abs/2106.01609)

Authors: [Piji Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> We investigate the problem of Chinese Grammatical Error Correction (CGEC) and present a new framework named Tail-to-Tail (\textbf{TtT}) non-autoregressive sequence prediction to address the deep issues hidden in CGEC. Considering that most tokens are correct and can be conveyed directly from source to target, and the error positions can be estimated and corrected based on the bidirectional context information, thus we employ a BERT-initialized Transformer Encoder as the backbone model to conduct information modeling and conveying. Considering that only relying on the same position substitution cannot handle the variable-length correction cases, various operations such substitution, deletion, insertion, and local paraphrasing are required jointly. Therefore, a Conditional Random Fields (CRF) layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modeling the token dependencies. Since most tokens are correct and easily to be predicted/conveyed to the target, then the models may suffer from a severe class imbalance issue. To alleviate this problem, focal loss penalty strategies are integrated into the loss functions. Moreover, besides the typical fix-length error correction datasets, we also construct a variable-length corpus to conduct experiments. Experimental results on standard datasets, especially on the variable-length datasets, demonstrate the effectiveness of TtT in terms of sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and Correction.

| Comments: | Accepted in the main conference of ACL 2021. Code: [this https URL](https://github.com/lipiji/TtT) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2106.01609](https://arxiv.org/abs/2106.01609) [cs.CL]** |
|           | (or **[arXiv:2106.01609v1](https://arxiv.org/abs/2106.01609v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-9">9. Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models
</h2>

Title: [Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models](https://arxiv.org/abs/2106.01623)

Authors: [Junyi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Tianyi Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+T), [Wayne Xin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W+X), [Zhicheng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Z), [Nicholas Jing Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+N+J), [Ji-Rong Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+J)

> This paper studies how to automatically generate a natural language text that describes the facts in knowledge graph (KG). Considering the few-shot setting, we leverage the excellent capacities of pretrained language models (PLMs) in language understanding and generation. We make three major technical contributions, namely representation alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for deriving better input representations, and multi-task learning for learning the correspondence between KG and text. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our model on KG-to-text generation task. In particular, our model outperforms all comparison methods on both fully-supervised and few-shot settings. Our code and datasets are available at [this https URL](https://github.com/RUCAIBox/Few-Shot-KG2Text).

| Comments: | Accepted to ACL 2021 Findings                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01623](https://arxiv.org/abs/2106.01623) [cs.CL]** |
|           | (or **[arXiv:2106.01623v1](https://arxiv.org/abs/2106.01623v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-10">10. Fingerprinting Fine-tuned Language Models in the Wild
</h2>

Title: [Fingerprinting Fine-tuned Language Models in the Wild](https://arxiv.org/abs/2106.01703)

Authors: [Nirav Diwan](https://arxiv.org/search/cs?searchtype=author&query=Diwan%2C+N), [Tanmoy Chakravorty](https://arxiv.org/search/cs?searchtype=author&query=Chakravorty%2C+T), [Zubair Shafiq](https://arxiv.org/search/cs?searchtype=author&query=Shafiq%2C+Z)

> There are concerns that the ability of language models (LMs) to generate high quality synthetic text can be misused to launch spam, disinformation, or propaganda. Therefore, the research community is actively working on developing approaches to detect whether a given text is organic or synthetic. While this is a useful first step, it is important to be able to further fingerprint the author LM to attribute its origin. Prior work on fingerprinting LMs is limited to attributing synthetic text generated by a handful (usually < 10) of pre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad of ways (e.g., on a domain-specific text corpus) before being used to generate synthetic text. It is challenging to fingerprinting fine-tuned LMs because the universe of fine-tuned LMs is much larger in realistic scenarios. To address this challenge, we study the problem of large-scale fingerprinting of fine-tuned LMs in the wild. Using a real-world dataset of synthetic text generated by 108 different fine-tuned LMs, we conduct comprehensive experiments to demonstrate the limitations of existing fingerprinting approaches. Our results show that fine-tuning itself is the most effective in attributing the synthetic text generated by fine-tuned LMs.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.01703](https://arxiv.org/abs/2106.01703) [cs.CL]** |
|           | (or **[arXiv:2106.01703v1](https://arxiv.org/abs/2106.01703v1) [cs.CL]** for this version) |





<h2 id="2021-06-04-11">11. Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer
</h2>

Title: [Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer](https://arxiv.org/abs/2106.01732)

Authors: [Ziqing Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Wentao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+W), [Yiming Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y), [Jiani Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+J), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W), [Shijin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S)

> Multilingual pre-trained models have achieved remarkable transfer performance by pre-trained on rich kinds of languages. Most of the models such as mBERT are pre-trained on unlabeled corpora. The static and contextual embeddings from the models could not be aligned very well. In this paper, we aim to improve the zero-shot cross-lingual transfer performance by aligning the embeddings better. We propose a pre-training task named Alignment Language Model (AlignLM), which uses the statistical alignment information as the prior knowledge to guide bilingual word prediction. We evaluate our method on multilingual machine reading comprehension and natural language interface tasks. The results show AlignLM can improve the zero-shot performance significantly on MLQA and XNLI datasets.

| Comments: | 4 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01732](https://arxiv.org/abs/2106.01732) [cs.CL]** |
|           | (or **[arXiv:2106.01732v1](https://arxiv.org/abs/2106.01732v1) [cs.CL]** for this version) |









# 2021-06-03

[Return to Index](#Index)



<h2 id="2021-06-03-1">1. Part of Speech and Universal Dependency effects on English Arabic Machine Translation
</h2>

Title: [Part of Speech and Universal Dependency effects on English Arabic Machine Translation](https://arxiv.org/abs/2106.00745)

Authors: [Omri Abend](https://arxiv.org/search/cs?searchtype=author&query=Abend%2C+O), [Leshem Choshen](https://arxiv.org/search/cs?searchtype=author&query=Choshen%2C+L), [Dmitry Nikolaev](https://arxiv.org/search/cs?searchtype=author&query=Nikolaev%2C+D), [Ofek Rafaeli](https://arxiv.org/search/cs?searchtype=author&query=Rafaeli%2C+O)

> In this research paper, I will elaborate on a method to evaluate machine translation models based on their performance on underlying syntactical phenomena between English and Arabic languages. This method is especially important as such "neural" and "machine learning" are hard to fine-tune and change. Thus, finding a way to evaluate them easily and diversely would greatly help the task of bettering them.

| Comments: | 19 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.00745](https://arxiv.org/abs/2106.00745) [cs.CL]** |
|           | (or **[arXiv:2106.00745v1](https://arxiv.org/abs/2106.00745v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-2">2. Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation
</h2>

Title: [Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation](https://arxiv.org/abs/2106.00903)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Knowledge distillation (KD) is commonly used to construct synthetic data for training non-autoregressive translation (NAT) models. However, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on predicting low-frequency words. To alleviate the problem, we directly expose the raw data into NAT by leveraging pretraining. By analyzing directed alignments, we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source. Accordingly, we propose reverse KD to rejuvenate more alignments for low-frequency target words. To make the most of authentic and synthetic data, we combine these complementary approaches as a new training strategy for further boosting NAT performance. We conduct experiments on five translation benchmarks over two advanced architectures. Results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on low-frequency words. Encouragingly, our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, respectively. Our code, data, and trained models are available at \url{[this https URL](https://github.com/longyuewangdcu/RLFW-NAT)}.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2106.00903](https://arxiv.org/abs/2106.00903) [cs.CL]** |
|           | (or **[arXiv:2106.00903v1](https://arxiv.org/abs/2106.00903v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-3">3. Discrete Cosine Transform as Universal Sentence Encoder
</h2>

Title: [Discrete Cosine Transform as Universal Sentence Encoder](https://arxiv.org/abs/2106.00934)

Authors: [Nada Almarwani](https://arxiv.org/search/cs?searchtype=author&query=Almarwani%2C+N), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M)

> Modern sentence encoders are used to generate dense vector representations that capture the underlying linguistic characteristics for a sequence of words, including phrases, sentences, or paragraphs. These kinds of representations are ideal for training a classifier for an end task such as sentiment analysis, question answering and text classification. Different models have been proposed to efficiently generate general purpose sentence representations to be used in pretraining protocols. While averaging is the most commonly used efficient sentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an alternative that captures the underlying syntactic characteristics of a given text without compromising practical efficiency compared to averaging. However, as with most other sentence encoders, the DCT sentence encoder was only evaluated in English. To this end, we utilize DCT encoder to generate universal sentence representation for different languages such as German, French, Spanish and Russian. The experimental results clearly show the superior effectiveness of DCT encoding in which consistent performance improvements are achieved over strong baselines on multiple standardized datasets.

| Comments: | to be published in ACL-IJCNLP 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00934](https://arxiv.org/abs/2106.00934) [cs.CL]** |
|           | (or **[arXiv:2106.00934v1](https://arxiv.org/abs/2106.00934v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-4">4. Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation
</h2>

Title: [Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation](https://arxiv.org/abs/2106.00941)

Authors: [Wenxiang Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+W), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Michael R. Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+M+R), [Irwin King](https://arxiv.org/search/cs?searchtype=author&query=King%2C+I)

> Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.

| Comments: | ACL 2021 main conference, long paper, 11 pages               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Theory (cs.IT) |
| Cite as:  | **[arXiv:2106.00941](https://arxiv.org/abs/2106.00941) [cs.CL]** |
|           | (or **[arXiv:2106.00941v1](https://arxiv.org/abs/2106.00941v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-5">5. One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers
</h2>

Title: [One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers](https://arxiv.org/abs/2106.01023)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Pre-trained language models (PLMs) achieve great success in NLP. However, their huge model sizes hinder their applications in many practical systems. Knowledge distillation is a popular technique to compress PLMs, which learns a small student model from a large teacher PLM. However, the knowledge learned from a single teacher may be limited and even biased, resulting in low-quality student model. In this paper, we propose a multi-teacher knowledge distillation framework named MT-BERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs. In MT-BERT we design a multi-teacher co-finetuning method to jointly finetune multiple teacher PLMs in downstream tasks with shared pooling and prediction layers to align their output space for better collaborative teaching. In addition, we propose a multi-teacher hidden loss and a multi-teacher distillation loss to transfer the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MT-BERT in compressing PLMs.

| Comments: | Findings of ACL-IJCNLP 2021                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01023](https://arxiv.org/abs/2106.01023) [cs.CL]** |
|           | (or **[arXiv:2106.01023v1](https://arxiv.org/abs/2106.01023v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-6">6. Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling
</h2>

Title: [Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling](https://arxiv.org/abs/2106.01040)

Authors: [Chuhan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Fangzhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Tao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+T), [Yongfeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y)

> Transformer is important for text modeling. However, it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem, we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way, i.e., first learns sentence representations and then learns document representations. It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. More specifically, we first use a sentence Transformer to learn the representations of each sentence. Then we use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling.

| Comments: | ACL-IJCNLP 2021                                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01040](https://arxiv.org/abs/2106.01040) [cs.CL]** |
|           | (or **[arXiv:2106.01040v1](https://arxiv.org/abs/2106.01040v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-7">7. Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?
</h2>

Title: [Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?](https://arxiv.org/abs/2106.01045)

Authors: [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Mauro Cettolo](https://arxiv.org/search/cs?searchtype=author&query=Cettolo%2C+M), [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Alberto Martinelli](https://arxiv.org/search/cs?searchtype=author&query=Martinelli%2C+A), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Five years after the first published proofs of concept, direct approaches to speech translation (ST) are now competing with traditional cascade solutions. In light of this steady progress, can we claim that the performance gap between the two is closed? Starting from this question, we present a systematic comparison between state-of-the-art systems representative of the two paradigms. Focusing on three language directions (English-German/Italian/Spanish), we conduct automatic and manual evaluations, exploiting high-quality professional post-edits and annotations. Our multi-faceted analysis on one of the few publicly available ST benchmarks attests for the first time that: i) the gap between the two paradigms is now closed, and ii) the subtle differences observed in their behavior are not sufficient for humans neither to distinguish them nor to prefer one over the other.

| Comments: | Accepted at ACL2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01045](https://arxiv.org/abs/2106.01045) [cs.CL]** |
|           | (or **[arXiv:2106.01045v1](https://arxiv.org/abs/2106.01045v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-8">8. Evidence-based Factual Error Correction
</h2>

Title: [Evidence-based Factual Error Correction](https://arxiv.org/abs/2106.01072)

Authors: [James Thorne](https://arxiv.org/search/cs?searchtype=author&query=Thorne%2C+J), [Andreas Vlachos](https://arxiv.org/search/cs?searchtype=author&query=Vlachos%2C+A)

> This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score. The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task.

| Comments: | To appear at ACL2021. arXiv admin note: text overlap with [arXiv:2012.15788](https://arxiv.org/abs/2012.15788) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.01072](https://arxiv.org/abs/2106.01072) [cs.CL]** |
|           | (or **[arXiv:2106.01072v1](https://arxiv.org/abs/2106.01072v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-9">9. Is Sparse Attention more Interpretable?
</h2>

Title: [Is Sparse Attention more Interpretable?](https://arxiv.org/abs/2106.01087)

Authors: [Clara Meister](https://arxiv.org/search/cs?searchtype=author&query=Meister%2C+C), [Stefan Lazov](https://arxiv.org/search/cs?searchtype=author&query=Lazov%2C+S), [Isabelle Augenstein](https://arxiv.org/search/cs?searchtype=author&query=Augenstein%2C+I), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R)

> Sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs. Yet the attention distribution is typically over representations internal to the model rather than the inputs themselves, suggesting this assumption may not have merit. We build on the recent work exploring the interpretability of attention; we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability tool. On three text classification tasks, we verify that only a weak relationship between inputs and co-indexed intermediate representations exists -- under sparse attention and otherwise. Further, we do not find any plausible mappings from sparse attention distributions to a sparse set of influential inputs through other avenues. Rather, we observe in this setting that inducing sparsity may make it less plausible that attention can be used as a tool for understanding model behavior.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of ACL-IJCNLP 2021                               |
| Cite as:           | **[arXiv:2106.01087](https://arxiv.org/abs/2106.01087) [cs.CL]** |
|                    | (or **[arXiv:2106.01087v1](https://arxiv.org/abs/2106.01087v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-10">10. End-to-End NLP Knowledge Graph Construction
</h2>

Title: [End-to-End NLP Knowledge Graph Construction](https://arxiv.org/abs/2106.01167)

Authors: [Ishani Mondal](https://arxiv.org/search/cs?searchtype=author&query=Mondal%2C+I), [Yufang Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+Y), [Charles Jochim](https://arxiv.org/search/cs?searchtype=author&query=Jochim%2C+C)

> This paper studies the end-to-end construction of an NLP Knowledge Graph (KG) from scientific papers. We focus on extracting four types of relations: evaluatedOn between tasks and datasets, evaluatedBy between tasks and evaluation metrics, as well as coreferent and related relations between the same type of entities. For instance, F1-score is coreferent with F-measure. We introduce novel methods for each of these relation types and apply our final framework (SciNLP-KG) to 30,000 NLP papers from ACL Anthology to build a large-scale KG, which can facilitate automatically constructing scientific leaderboards for the NLP community. The results of our experiments indicate that the resulting KG contains high-quality information.

| Comments: | Accepted in ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01167](https://arxiv.org/abs/2106.01167) [cs.CL]** |
|           | (or **[arXiv:2106.01167v1](https://arxiv.org/abs/2106.01167v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-11">11. IrEne: Interpretable Energy Prediction for Transformers
</h2>

Title: [IrEne: Interpretable Energy Prediction for Transformers](https://arxiv.org/abs/2106.01199)

Authors: [Qingqing Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Q), [Yash Kumar Lal](https://arxiv.org/search/cs?searchtype=author&query=Lal%2C+Y+K), [Harsh Trivedi](https://arxiv.org/search/cs?searchtype=author&query=Trivedi%2C+H), [Aruna Balasubramanian](https://arxiv.org/search/cs?searchtype=author&query=Balasubramanian%2C+A), [Niranjan Balasubramanian](https://arxiv.org/search/cs?searchtype=author&query=Balasubramanian%2C+N)

> Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution. We present IrEne, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models. IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning (ML) primitives. IrEne predicts the inference energy consumption of the ML primitives as a function of generalizable features and fine-grained runtime resource usage. IrEne then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model. Experiments across multiple Transformer models show IrEne predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth. In contrast, existing energy models see an error of over 50%. We also show how IrEne can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices. We release the code and data at [this https URL](https://github.com/StonyBrookNLP/irene).

| Comments: | ACL 2021 camera ready                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01199](https://arxiv.org/abs/2106.01199) [cs.CL]** |
|           | (or **[arXiv:2106.01199v1](https://arxiv.org/abs/2106.01199v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-12">12. Lower Perplexity is Not Always Human-Like
</h2>

Title: [Lower Perplexity is Not Always Human-Like](https://arxiv.org/abs/2106.01229)

Authors: [Tatsuki Kuribayashi](https://arxiv.org/search/cs?searchtype=author&query=Kuribayashi%2C+T), [Yohei Oseki](https://arxiv.org/search/cs?searchtype=author&query=Oseki%2C+Y), [Takumi Ito](https://arxiv.org/search/cs?searchtype=author&query=Ito%2C+T), [Ryo Yoshida](https://arxiv.org/search/cs?searchtype=author&query=Yoshida%2C+R), [Masayuki Asahara](https://arxiv.org/search/cs?searchtype=author&query=Asahara%2C+M), [Kentaro Inui](https://arxiv.org/search/cs?searchtype=author&query=Inui%2C+K)

> In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization -- the lower perplexity a language model has, the more human-like the language model is -- in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.

| Comments: | Accepted by ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.01229](https://arxiv.org/abs/2106.01229) [cs.CL]** |
|           | (or **[arXiv:2106.01229v1](https://arxiv.org/abs/2106.01229v1) [cs.CL]** for this version) |





<h2 id="2021-06-03-13">13. On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers
</h2>

Title: [On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers](https://arxiv.org/abs/2106.01335)

Authors: [Tianchu Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+T), [Shraddhan Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+S), [Michael Ferdman](https://arxiv.org/search/cs?searchtype=author&query=Ferdman%2C+M), [Peter Milder](https://arxiv.org/search/cs?searchtype=author&query=Milder%2C+P), [H. Andrew Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+H+A), [Niranjan Balasubramanian](https://arxiv.org/search/cs?searchtype=author&query=Balasubramanian%2C+N)

> How much information do NLP tasks really need from a transformer's attention mechanism at application-time (inference)? From recent work, we know that there is sparsity in transformers and that the floating-points within its computation can be discretized to fewer values with minimal loss to task accuracies. However, this requires retraining or even creating entirely new models, both of which can be expensive and carbon-emitting. Focused on optimizations that do not require training, we systematically study the full range of typical attention values necessary. This informs the design of an inference-time quantization technique using both pruning and log-scaled mapping which produces only a few (e.g. 23) unique values. Over the tasks of question answering and sentiment analysis, we find nearly 80% of attention values can be pruned to zeros with minimal (<1.0%) relative loss in accuracy. We use this pruning technique in conjunction with quantizing the attention values to only a 3-bit format, without retraining, resulting in only a 0.8% accuracy reduction on question answering with fine-tuned RoBERTa.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.01335](https://arxiv.org/abs/2106.01335) [cs.CL]** |
|           | (or **[arXiv:2106.01335v1](https://arxiv.org/abs/2106.01335v1) [cs.CL]** for this version) |









# 2021-06-02

[Return to Index](#Index)



<h2 id="2021-06-02-1">1. Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models
</h2>

Title: [Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models](https://arxiv.org/abs/2106.00245)

Authors: [Linjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Jie Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+J), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> With large-scale pre-training, the past two years have witnessed significant performance boost on the Visual Question Answering (VQA) task. Though rapid progresses have been made, it remains unclear whether these state-of-the-art (SOTA) VQA models are robust when encountering test examples in the wild. To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adversarial human-and-model-in-the-loop procedure. Through this new benchmark, we present several interesting findings. (i) Surprisingly, during dataset collection, we find that non-expert annotators can successfully attack SOTA VQA models with relative ease. (ii) We test a variety of SOTA VQA models on our new dataset to highlight their fragility, and find that both large-scale pre-trained models and adversarial training methods can only achieve far lower performance than what they can achieve on the standard VQA v2 dataset. (iii) When considered as data augmentation, our dataset can be used to improve the performance on other robust VQA benchmarks. (iv) We present a detailed analysis of the dataset, providing valuable insights on the challenges it brings to the community. We hope Adversarial VQA can serve as a valuable benchmark that will be used by future work to test the robustness of its developed VQA models. Our dataset is publicly available at https://adversarialvqa. [this http URL](http://github.io/).

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.00245](https://arxiv.org/abs/2106.00245) [cs.CV]** |
|           | (or **[arXiv:2106.00245v1](https://arxiv.org/abs/2106.00245v1) [cs.CV]** for this version) |





<h2 id="2021-06-02-2">2. Language Model Evaluation Beyond Perplexity
</h2>

Title: [Language Model Evaluation Beyond Perplexity](https://arxiv.org/abs/2106.00085)

Authors: [Clara Meister](https://arxiv.org/search/cs?searchtype=author&query=Meister%2C+C), [Ryan Cotterell](https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R)

> We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a framework--paired with significance tests--for evaluating the fit of language models to certain statistical tendencies of natural language. We find that neural language models appear to learn only a subset of the statistical tendencies considered, but align much more closely with empirical trends than theoretical laws (when present). Further, the fit to different distributions is dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type--token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols suprisingly well.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00085](https://arxiv.org/abs/2106.00085) [cs.CL]** |
|           | (or **[arXiv:2106.00085v1](https://arxiv.org/abs/2106.00085v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-3">3. An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers
</h2>

Title: [An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers](https://arxiv.org/abs/2106.00143)

Authors: [Tharindu Ranasinghe](https://arxiv.org/search/cs?searchtype=author&query=Ranasinghe%2C+T), [Constantin Orasan](https://arxiv.org/search/cs?searchtype=author&query=Orasan%2C+C), [Ruslan Mitkov](https://arxiv.org/search/cs?searchtype=author&query=Mitkov%2C+R)

> Most studies on word-level Quality Estimation (QE) of machine translation focus on language-specific models. The obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language-specific models. To overcome these problems, we explore different approaches to multilingual, word-level QE. We show that these QE models perform on par with the current language-specific models. In the cases of zero-shot and few-shot QE, we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios.

| Comments: | Accepted to appear at the ACL-IJCNLP 2021 Main conference    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.00143](https://arxiv.org/abs/2106.00143) [cs.CL]** |
|           | (or **[arXiv:2106.00143v1](https://arxiv.org/abs/2106.00143v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-4">4. Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation
</h2>

Title: [Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation](https://arxiv.org/abs/2106.00169)

Authors: [Adithya Renduchintala](https://arxiv.org/search/cs?searchtype=author&query=Renduchintala%2C+A), [Denise Diaz](https://arxiv.org/search/cs?searchtype=author&query=Diaz%2C+D), [Kenneth Heafield](https://arxiv.org/search/cs?searchtype=author&query=Heafield%2C+K), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M)

> Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.

| Comments: | Accepted at ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00169](https://arxiv.org/abs/2106.00169) [cs.CL]** |
|           | (or **[arXiv:2106.00169v1](https://arxiv.org/abs/2106.00169v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-5">5. Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives
</h2>

Title: [Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives](https://arxiv.org/abs/2106.00181)

Authors: [Meichun Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+M), [Ziyang Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Z)

> Gender bias in word embeddings gradually becomes a vivid research field in recent years. Most studies in this field aim at measurement and debiasing methods with English as the target language. This paper investigates gender bias in static word embeddings from a unique perspective, Chinese adjectives. By training word representations with different models, the gender bias behind the vectors of adjectives is assessed. Through a comparison between the produced results and a human-scored data set, we demonstrate how gender bias encoded in word embeddings differentiates from people's attitudes.

| Comments: | Accepted at the 3rd Workshop on Gender Bias in Natural Language Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00181](https://arxiv.org/abs/2106.00181) [cs.CL]** |
|           | (or **[arXiv:2106.00181v1](https://arxiv.org/abs/2106.00181v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-6">6. Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021
</h2>

Title: [Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021](https://arxiv.org/abs/2106.00197)

Authors: [Xingshan Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> This paper describes the system submitted to the IWSLT 2021 Multilingual Speech Translation (MultiST) task from Huawei Noah's Ark Lab. We use a unified transformer architecture for our MultiST model, so that the data from different modalities (i.e., speech and text) and different tasks (i.e., Speech Recognition, Machine Translation, and Speech Translation) can be exploited to enhance the model's ability. Specifically, speech and text inputs are firstly fed to different feature extractors to extract acoustic and textual features, respectively. Then, these features are processed by a shared encoder--decoder architecture. We apply several training techniques to improve the performance, including multi-task learning, task-level curriculum learning, data augmentation, etc. Our final system achieves significantly better results than bilingual baselines on supervised language pairs and yields reasonable results on zero-shot language pairs.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2106.00197](https://arxiv.org/abs/2106.00197) [cs.CL]** |
|           | (or **[arXiv:2106.00197v1](https://arxiv.org/abs/2106.00197v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-7">7. ViTA: Visual-Linguistic Translation by Aligning Object Tags
</h2>

Title: [ViTA: Visual-Linguistic Translation by Aligning Object Tags](https://arxiv.org/abs/2106.00250)

Authors: [Kshitij Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+K), [Devansh Gautam](https://arxiv.org/search/cs?searchtype=author&query=Gautam%2C+D), [Radhika Mamidi](https://arxiv.org/search/cs?searchtype=author&query=Mamidi%2C+R)

> Multimodal Machine Translation (MMT) enriches the source text with visual information for translation. It has gained popularity in recent years, and several pipelines have been proposed in the same direction. Yet, the task lacks quality datasets to illustrate the contribution of visual modality in the translation systems. In this paper, we propose our system for the Multimodal Translation Task of WAT 2021 from English to Hindi. We propose to use mBART, a pretrained multilingual sequence-to-sequence model, for the textual-only translations. Further, we bring the visual information to a textual domain by extracting object tags from the image and enhance the input for the multimodal task. We also explore the robustness of our system by systematically degrading the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test set and challenge set of the task.

| Comments: | 7 pages, accepted at WAT-2021 co-located with ACL-IJCNLP 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2106.00250](https://arxiv.org/abs/2106.00250) [cs.CL]** |
|           | (or **[arXiv:2106.00250v1](https://arxiv.org/abs/2106.00250v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-8">8. An In-depth Study on Internal Structure of Chinese Words
</h2>

Title: [An In-depth Study on Internal Structure of Chinese Words](https://arxiv.org/abs/2106.00334)

Authors: [Chen Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+C), [Saihao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Houquan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Zhenghua Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Zhefeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Baoxing Huai](https://arxiv.org/search/cs?searchtype=author&query=Huai%2C+B), [Nicholas Jing Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+N+J)

> Unlike English letters, Chinese characters have rich and specific meanings. Usually, the meaning of a word can be derived from its constituent characters in some way. Several previous works on syntactic parsing propose to annotate shallow word-internal structures for better utilizing character-level information. This work proposes to model the deep internal structures of Chinese words as dependency trees with 11 labels for distinguishing syntactic relationships. First, based on newly compiled annotation guidelines, we manually annotate a word-internal structure treebank (WIST) consisting of over 30K multi-char words from Chinese Penn Treebank. To guarantee quality, each word is independently annotated by two annotators and inconsistencies are handled by a third senior annotator. Second, we present detailed and interesting analysis on WIST to reveal insights on Chinese word formation. Third, we propose word-internal structure parsing as a new task, and conduct benchmark experiments using a competitive dependency parser. Finally, we present two simple ways to encode word-internal structures, leading to promising gains on the sentence-level syntactic parsing task.

| Comments: | Accepted by ACL-IJCNLP 2021 (long paper)                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00334](https://arxiv.org/abs/2106.00334) [cs.CL]** |
|           | (or **[arXiv:2106.00334v1](https://arxiv.org/abs/2106.00334v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-9">9. SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining
</h2>

Title: [SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining](https://arxiv.org/abs/2106.00400)

Authors: [Chenglei Si](https://arxiv.org/search/cs?searchtype=author&query=Si%2C+C), [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Yingfa Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Fanchao Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+F), [Xiaozhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

> Conventional tokenization methods for Chinese pretrained language models (PLMs) treat each character as an indivisible token (Devlin et al., 2019), which ignores the characteristics of the Chinese writing system. In this work, we comprehensively study the influences of three main factors on the Chinese tokenization for PLM: pronunciation, glyph (i.e., shape), and word boundary. Correspondingly, we propose three kinds of tokenizers: 1) SHUOWEN (meaning Talk Word), the pronunciation-based tokenizers; 2) JIEZI (meaning Solve Character), the glyph-based tokenizers; 3) Word segmented tokenizers, the tokenizers with Chinese word segmentation. To empirically compare the effectiveness of studied tokenizers, we pretrain BERT-style language models with them and evaluate the models on various downstream NLU tasks. We find that SHUOWEN and JIEZI tokenizers can generally outperform conventional single-character tokenizers, while Chinese word segmentation shows no benefit as a preprocessing step. Moreover, the proposed SHUOWEN and JIEZI tokenizers exhibit significantly better robustness in handling noisy texts. The code and pretrained models will be publicly released to facilitate linguistically informed Chinese NLP.

| Comments: | Work in progress. Feedback is welcome                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.00400](https://arxiv.org/abs/2106.00400) [cs.CL]** |
|           | (or **[arXiv:2106.00400v1](https://arxiv.org/abs/2106.00400v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-10">10. DoT: An efficient Double Transformer for NLP tasks with tables
</h2>

Title: [DoT: An efficient Double Transformer for NLP tasks with tables](https://arxiv.org/abs/2106.00479)

Authors: [Syrine Krichene](https://arxiv.org/search/cs?searchtype=author&query=Krichene%2C+S), [Thomas Müller](https://arxiv.org/search/cs?searchtype=author&query=Müller%2C+T), [Julian Martin Eisenschlos](https://arxiv.org/search/cs?searchtype=author&query=Eisenschlos%2C+J+M)

> Transformer-based approaches have been successfully used to obtain state-of-the-art accuracy on natural language processing (NLP) tasks with semi-structured tables. These model architectures are typically deep, resulting in slow training and inference, especially for long inputs. To improve efficiency while maintaining a high accuracy, we propose a new architecture, DoT, a double transformer model, that decomposes the problem into two sub-tasks: A shallow pruning transformer that selects the top-K tokens, followed by a deep task-specific transformer that takes as input those K tokens. Additionally, we modify the task-specific attention to incorporate the pruning scores. The two transformers are jointly trained by optimizing the task-specific loss. We run experiments on three benchmarks, including entailment and question-answering. We show that for a small drop of accuracy, DoT improves training and inference time by at least 50%. We also show that the pruning transformer effectively selects relevant tokens enabling the end-to-end model to maintain similar accuracy as slower baseline models. Finally, we analyse the pruning and give some insight into its impact on the task model.

| Comments:    | 11 pages, 4 figures, to be published in Findings of ACL-IJCNLP 2021 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| MSC classes: | 68-06                                                        |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2106.00479](https://arxiv.org/abs/2106.00479) [cs.CL]** |
|              | (or **[arXiv:2106.00479v1](https://arxiv.org/abs/2106.00479v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-11">11. NewsEmbed: Modeling News through Pre-trained DocumentRepresentations
</h2>

Title: [NewsEmbed: Modeling News through Pre-trained DocumentRepresentations](https://arxiv.org/abs/2106.00590)

Authors: [Jialu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Tianqi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Cong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+C)

> Effectively modeling text-rich fresh content such as news articles at document-level is a challenging problem. To ensure a content-based model generalize well to a broad range of applications, it is critical to have a training dataset that is large beyond the scale of human labels while achieving desired quality. In this work, we address those two challenges by proposing a novel approach to mine semantically-relevant fresh documents, and their topic labels, with little human supervision. Meanwhile, we design a multitask model called NewsEmbed that alternatively trains a contrastive learning with a multi-label classification to derive a universal document encoder. We show that the proposed approach can provide billions of high quality organic training examples and can be naturally extended to multilingual setting where texts in different languages are encoded in the same semantic space. We experimentally demonstrate NewsEmbed's competitive performance across multiple natural language understanding tasks, both supervised and unsupervised.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| DOI:      | [10.1145/3447548.3467392](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3447548.3467392&v=694504da) |
| Cite as:  | **[arXiv:2106.00590](https://arxiv.org/abs/2106.00590) [cs.CL]** |
|           | (or **[arXiv:2106.00590v1](https://arxiv.org/abs/2106.00590v1) [cs.CL]** for this version) |







<h2 id="2021-06-02-12">12. Incorporating Visual Layout Structures for Scientific Text Classification
</h2>

Title: [Incorporating Visual Layout Structures for Scientific Text Classification](https://arxiv.org/abs/2106.00676)

Authors: [Zejiang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Z), [Kyle Lo](https://arxiv.org/search/cs?searchtype=author&query=Lo%2C+K), [Lucy Lu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L+L), [Bailey Kuehl](https://arxiv.org/search/cs?searchtype=author&query=Kuehl%2C+B), [Daniel S. Weld](https://arxiv.org/search/cs?searchtype=author&query=Weld%2C+D+S), [Doug Downey](https://arxiv.org/search/cs?searchtype=author&query=Downey%2C+D)

> Classifying the core textual components of a scientific paper-title, author, body text, etc.-is a critical first step in automated scientific document understanding. Previous work has shown how using elementary layout information, i.e., each token's 2D position on the page, leads to more accurate classification. We introduce new methods for incorporating VIsual LAyout structures (VILA), e.g., the grouping of page texts into text lines or text blocks, into language models to further improve performance. We show that the I-VILA approach, which simply adds special tokens denoting boundaries between layout structures into model inputs, can lead to +1~4.5 F1 Score improvements in token classification tasks. Moreover, we design a hierarchical model H-VILA that encodes these layout structures and record a up-to 70% efficiency boost without hurting prediction accuracy. The experiments are conducted on a newly curated evaluation suite, S2-VLUE, with a novel metric measuring VILA awareness and a new dataset covering 19 scientific disciplines with gold annotations. Pre-trained weights, benchmark datasets, and source code will be available at [this https URL](https://github.com/allenai/VILA)}{[this https URL](https://github.com/allenai/VILA).

| Comments: | 13 pages, 5 figures, 6 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2106.00676](https://arxiv.org/abs/2106.00676) [cs.CL]** |
|           | (or **[arXiv:2106.00676v1](https://arxiv.org/abs/2106.00676v1) [cs.CL]** for this version) |








# 2021-06-01

[Return to Index](#Index)



<h2 id="2021-06-01-1">1. An Attention Free Transformer
</h2>

Title: [An Attention Free Transformer](https://arxiv.org/abs/2105.14103)

Authors: [Shuangfei Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+S), [Walter Talbott](https://arxiv.org/search/cs?searchtype=author&query=Talbott%2C+W), [Nitish Srivastava](https://arxiv.org/search/cs?searchtype=author&query=Srivastava%2C+N), [Chen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Hanlin Goh](https://arxiv.org/search/cs?searchtype=author&query=Goh%2C+H), [Ruixiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Josh Susskind](https://arxiv.org/search/cs?searchtype=author&query=Susskind%2C+J)

> We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14103](https://arxiv.org/abs/2105.14103) [cs.LG]** |
|           | (or **[arXiv:2105.14103v1](https://arxiv.org/abs/2105.14103v1) [cs.LG]** for this version) |



<h2 id="2021-06-01-2">2. LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering
</h2>

Title: [LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering](https://arxiv.org/abs/2105.14300)

Authors: [Zujie Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z), [Haifeng Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+H), [Jiaying Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> Most existing Visual Question Answering (VQA) systems tend to overly rely on language bias and hence fail to reason from the visual clue. To address this issue, we propose a novel Language-Prior Feedback (LPF) objective function, to re-balance the proportion of each answer's loss value in the total VQA loss. The LPF firstly calculates a modulating factor to determine the language bias using a question-only branch. Then, the LPF assigns a self-adaptive weight to each training sample in the training process. With this reweighting mechanism, the LPF ensures that the total VQA loss can be reshaped to a more balanced form. By this means, the samples that require certain visual information to predict will be efficiently used during training. Our method is simple to implement, model-agnostic, and end-to-end trainable. We conduct extensive experiments and the results show that the LPF (1) brings a significant improvement over various VQA models, (2) achieves competitive performance on the bias-sensitive VQA-CP v2 benchmark.

| Comments: | Accepted by ACM SIGIR 2021                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| DOI:      | [10.1145/3404835.3462981](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3404835.3462981&v=a1843a78) |
| Cite as:  | **[arXiv:2105.14300](https://arxiv.org/abs/2105.14300) [cs.CV]** |
|           | (or **[arXiv:2105.14300v1](https://arxiv.org/abs/2105.14300v1) [cs.CV]** for this version) |





<h2 id="2021-06-01-3">3. Re-evaluating Word Mover's Distance
</h2>

Title: [Re-evaluating Word Mover's Distance](https://arxiv.org/abs/2105.14403)

Authors: [Ryoma Sato](https://arxiv.org/search/cs?searchtype=author&query=Sato%2C+R), [Makoto Yamada](https://arxiv.org/search/cs?searchtype=author&query=Yamada%2C+M), [Hisashi Kashima](https://arxiv.org/search/cs?searchtype=author&query=Kashima%2C+H)

> The word mover's distance (WMD) is a fundamental technique for measuring the similarity of two documents. As the crux of WMD, it can take advantage of the underlying geometry of the word space by employing an optimal transport formulation. The original study on WMD reported that WMD outperforms classical baselines such as bag-of-words (BOW) and TF-IDF by significant margins in various datasets. In this paper, we point out that the evaluation in the original study could be misleading. We re-evaluate the performances of WMD and the classical baselines and find that the classical baselines are competitive with WMD if we employ an appropriate preprocessing, i.e., L1 normalization. However, this result is not intuitive. WMD should be superior to BOW because WMD can take the underlying geometry into account, whereas BOW cannot. Our analysis shows that this is due to the high-dimensional nature of the underlying metric. We find that WMD in high-dimensional spaces behaves more similarly to BOW than in low-dimensional spaces due to the curse of dimensionality.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14403](https://arxiv.org/abs/2105.14403) [cs.LG]** |
|           | (or **[arXiv:2105.14403v1](https://arxiv.org/abs/2105.14403v1) [cs.LG]** for this version) |





<h2 id="2021-06-01-4">4. Memory-Efficient Differentiable Transformer Architecture Search
</h2>

Title: [Memory-Efficient Differentiable Transformer Architecture Search](https://arxiv.org/abs/2105.14669)

Authors: [Yuekai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Yelong Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Zhihua Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W)

> Differentiable architecture search (DARTS) is successfully applied in many vision tasks. However, directly using DARTS for Transformers is memory-intensive, which renders the search process infeasible. To this end, we propose a multi-split reversible network and combine it with DARTS. Specifically, we devise a backpropagation-with-reconstruction algorithm so that we only need to store the last layer's outputs. By relieving the memory burden for DARTS, it allows us to search with larger hidden size and more candidate operations. We evaluate the searched architecture on three sequence-to-sequence datasets, i.e., WMT'14 English-German, WMT'14 English-French, and WMT'14 English-Czech. Experimental results show that our network consistently outperforms standard Transformers across the tasks. Moreover, our method compares favorably with big-size Evolved Transformers, reducing search computation by an order of magnitude.

| Comments: | Accepted by Findings of ACL 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2105.14669](https://arxiv.org/abs/2105.14669) [cs.LG]** |
|           | (or **[arXiv:2105.14669v1](https://arxiv.org/abs/2105.14669v1) [cs.LG]** for this version) |





<h2 id="2021-06-01-5">5. Why does CTC result in peaky behavior?
</h2>

Title: [Why does CTC result in peaky behavior?](https://arxiv.org/abs/2105.14849)

Authors: [Albert Zeyer](https://arxiv.org/search/cs?searchtype=author&query=Zeyer%2C+A), [Ralf Schlüter](https://arxiv.org/search/cs?searchtype=author&query=Schlüter%2C+R), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

> The peaky behavior of CTC models is well known experimentally. However, an understanding about why peaky behavior occurs is missing, and whether this is a good property. We provide a formal analysis of the peaky behavior and gradient descent convergence properties of the CTC loss and related training criteria. Our analysis provides a deep understanding why peaky behavior occurs and when it is suboptimal. On a simple example which should be trivial to learn for any model, we prove that a feed-forward neural network trained with CTC from uniform initialization converges towards peaky behavior with a 100% error rate. Our analysis further explains why CTC only works well together with the blank label. We further demonstrate that peaky behavior does not occur on other related losses including a label prior model, and that this improves convergence.

| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE); Sound (cs.SD); Audio and Speech Processing (eess.AS); Statistics Theory (math.ST) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14849](https://arxiv.org/abs/2105.14849) [cs.LG]** |
|           | (or **[arXiv:2105.14849v1](https://arxiv.org/abs/2105.14849v1) [cs.LG]** for this version) |





<h2 id="2021-06-01-6">6. Grammatical Error Correction as GAN-like Sequence Labeling
</h2>

Title: [Grammatical Error Correction as GAN-like Sequence Labeling](https://arxiv.org/abs/2105.14209)

Authors: [Kevin Parnow](https://arxiv.org/search/cs?searchtype=author&query=Parnow%2C+K), [Zuchao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> In Grammatical Error Correction (GEC), sequence labeling models enjoy fast inference compared to sequence-to-sequence models; however, inference in sequence labeling GEC models is an iterative process, as sentences are passed to the model for multiple rounds of correction, which exposes the model to sentences with progressively fewer errors at each round. Traditional GEC models learn from sentences with fixed error rates. Coupling this with the iterative correction process causes a mismatch between training and inference that affects final performance. In order to address this mismatch, we propose a GAN-like sequence labeling model, which consists of a grammatical error detector as a discriminator and a grammatical error labeler with Gumbel-Softmax sampling as a generator. By sampling from real error distributions, our errors are more genuine compared to traditional synthesized GEC errors, thus alleviating the aforementioned mismatch and allowing for better training. Our results on several evaluation benchmarks demonstrate that our proposed approach is effective and improves the previous state-of-the-art baseline.

| Comments: | Accepted by ACL21, Findings                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14209](https://arxiv.org/abs/2105.14209) [cs.CL]** |
|           | (or **[arXiv:2105.14209v1](https://arxiv.org/abs/2105.14209v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-7">7. Predictive Representation Learning for Language Modeling
</h2>

Title: [Predictive Representation Learning for Language Modeling](https://arxiv.org/abs/2105.14214)

Authors: [Qingfeng Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Q), [Luke Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+L), [Martha White](https://arxiv.org/search/cs?searchtype=author&query=White%2C+M), [Alona Fyshe](https://arxiv.org/search/cs?searchtype=author&query=Fyshe%2C+A)

> To effectively perform the task of next-word prediction, long short-term memory networks (LSTMs) must keep track of many types of information. Some information is directly related to the next word's identity, but some is more secondary (e.g. discourse-level features or features of downstream words). Correlates of secondary information appear in LSTM representations even though they are not part of an \emph{explicitly} supervised prediction task. In contrast, in reinforcement learning (RL), techniques that explicitly supervise representations to predict secondary information have been shown to be beneficial. Inspired by that success, we propose Predictive Representation Learning (PRL), which explicitly constrains LSTMs to encode specific predictions, like those that might need to be learned implicitly. We show that PRL 1) significantly improves two strong language modeling methods, 2) converges more quickly, and 3) performs better when data is limited. Our work shows that explicitly encoding a simple predictive task facilitates the search for a more effective language model.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14214](https://arxiv.org/abs/2105.14214) [cs.CL]** |
|           | (or **[arXiv:2105.14214v1](https://arxiv.org/abs/2105.14214v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-8">8. Korean-English Machine Translation with Multiple Tokenization Strategy
</h2>

Title: [Korean-English Machine Translation with Multiple Tokenization Strategy](https://arxiv.org/abs/2105.14274)

Authors: [Dojun Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+D), [Youngjin Jang](https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+Y), [Harksoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H)

> This study was conducted to find out how tokenization methods affect the training results of machine translation models. In this work, character tokenization, morpheme tokenization, and BPE tokenization were applied to Korean as the source language and English as the target language respectively, and the comparison experiment was conducted by repeating 50,000 epochs of each 9 models using the Transformer neural network. As a result of measuring the BLEU scores of the experimental models, the model that applied BPE tokenization to Korean and morpheme tokenization to English recorded 35.73, showing the best performance.

| Comments: | KCC2021 Undergraduate/Junior Thesis Competition              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14274](https://arxiv.org/abs/2105.14274) [cs.CL]** |
|           | (or **[arXiv:2105.14274v1](https://arxiv.org/abs/2105.14274v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-9">9. Grammar Accuracy Evaluation (GAE): Quantifiable Intrinsic Evaluation of Machine Translation Models
</h2>

Title: [Grammar Accuracy Evaluation (GAE): Quantifiable Intrinsic Evaluation of Machine Translation Models](https://arxiv.org/abs/2105.14277)

Authors: [Dojun Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+D), [Youngjin Jang](https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+Y), [Harksoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H)

> Intrinsic evaluation by humans for the performance of natural language generation models is conducted to overcome the fact that the quality of generated sentences cannot be fully represented by only extrinsic evaluation. Nevertheless, existing intrinsic evaluations have a large score deviation according to the evaluator's criteria. In this paper, we propose Grammar Accuracy Evaluation (GAE) that can provide specific evaluating criteria. As a result of analyzing the quality of machine translation by BLEU and GAE, it was confirmed that the BLEU score does not represent the absolute performance of machine translation models and that GAE compensates for the shortcomings of BLEU with a flexible evaluation on alternative synonyms and changes in sentence structure.

| Comments: | Journal of KIISE                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14277](https://arxiv.org/abs/2105.14277) [cs.CL]** |
|           | (or **[arXiv:2105.14277v1](https://arxiv.org/abs/2105.14277v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-10">10. NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search
</h2>

Title: [NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search](https://arxiv.org/abs/2105.14444)

Authors: [Jin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Renqian Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R), [Kaitao Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+K), [Jian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> While pre-trained language models (e.g., BERT) have achieved impressive results on different natural language processing tasks, they have large numbers of parameters and suffer from big computational and memory costs, which make them difficult for real-world deployment. Therefore, model compression is necessary to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, in order to support devices with different memory and latency limitations; (2) The algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks. We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a search space containing a variety of architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the compressed models can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE and SQuAD benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.

| Comments: | Accepted by KDD 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| DOI:      | [10.1145/3447548.3467262](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3447548.3467262&v=2e2b005f) |
| Cite as:  | **[arXiv:2105.14444](https://arxiv.org/abs/2105.14444) [cs.CL]** |
|           | (or **[arXiv:2105.14444v1](https://arxiv.org/abs/2105.14444v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-11">11. Pre-training Universal Language Representation
</h2>

Title: [Pre-training Universal Language Representation](https://arxiv.org/abs/2105.14478)

Authors: [Yian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H)

> Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models. Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset.

| Comments: | Accepted by ACL-IJCNLP 2021 main conference                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.14478](https://arxiv.org/abs/2105.14478) [cs.CL]** |
|           | (or **[arXiv:2105.14478v1](https://arxiv.org/abs/2105.14478v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-12">12. Fast Nearest Neighbor Machine Translation
</h2>

Title: [Fast Nearest Neighbor Machine Translation](https://arxiv.org/abs/2105.14528)

Authors: [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Xiayu Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Tianwei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Though nearest neighbor Machine Translation (kNN-MT) \cite{khandelwal2020nearest} has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference corpus. kNN-MT is thus two-order slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services. In this work, we propose Fast kNN-MT to address this issue. Fast kNN-MT constructs a significantly smaller datastore for the nearest neighbor search: for each word in a source sentence, Fast kNN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token. Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens. This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency. Without loss of performance, Fast kNN-MT is two-order faster than kNN-MT, and is only two times slower than the standard NMT model. Fast kNN-MT enables the practical use of kNN-MT systems in real-world MT applications.\footnote{Code is available at \url{[this https URL](https://github.com/ShannonAI/fast-knn-nmt).}}

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14528](https://arxiv.org/abs/2105.14528) [cs.CL]** |
|           | (or **[arXiv:2105.14528v1](https://arxiv.org/abs/2105.14528v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-13">13. HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation
</h2>


Title: [HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation](https://arxiv.org/abs/2105.14600)

Authors: [Ayan Sengupta](https://arxiv.org/search/cs?searchtype=author&query=Sengupta%2C+A), [Sourabh Kumar Bhattacharjee](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharjee%2C+S+K), [Tanmoy Chakraborty](https://arxiv.org/search/cs?searchtype=author&query=Chakraborty%2C+T), [Md Shad Akhtar](https://arxiv.org/search/cs?searchtype=author&query=Akhtar%2C+M+S)

> Understanding linguistics and morphology of resource-scarce code-mixed texts remains a key challenge in text processing. Although word embedding comes in handy to support downstream tasks for low-resource languages, there are plenty of scopes in improving the quality of language representation particularly for code-mixed languages. In this paper, we propose HIT, a robust representation learning method for code-mixed texts. HIT is a hierarchical transformer-based framework that captures the semantic relationship among words and hierarchically learns the sentence-level semantics using a fused attention mechanism. HIT incorporates two attention modules, a multi-headed self-attention and an outer product attention module, and computes their weighted sum to obtain the attention weights. Our evaluation of HIT on one European (Spanish) and five Indic (Hindi, Bengali, Tamil, Telugu, and Malayalam) languages across four NLP tasks on eleven datasets suggests significant performance improvement against various state-of-the-art systems. We further show the adaptability of learned representation across tasks in a transfer learning setup (with and without fine-tuning).

| Comments: | 15 pages, 13 tables, 6 Figures. Accepted at ACL-IJCNLP-2021 (Findings) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14600](https://arxiv.org/abs/2105.14600) [cs.CL]** |
|           | (or **[arXiv:2105.14600v1](https://arxiv.org/abs/2105.14600v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-14">14. Attention Flows are Shapley Value Explanations
</h2>


Title: [Attention Flows are Shapley Value Explanations](https://arxiv.org/abs/2105.14652)

Authors: [Kawin Ethayarajh](https://arxiv.org/search/cs?searchtype=author&query=Ethayarajh%2C+K), [Dan Jurafsky](https://arxiv.org/search/cs?searchtype=author&query=Jurafsky%2C+D)

> Shapley Values, a solution to the credit assignment problem in cooperative game theory, are a popular type of explanation in machine learning, having been used to explain the importance of features, embeddings, and even neurons. In NLP, however, leave-one-out and attention-based explanations still predominate. Can we draw a connection between these different methods? We formally prove that -- save for the degenerate case -- attention weights and leave-one-out values cannot be Shapley Values. Attention flow is a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph. Perhaps surprisingly, we prove that attention flows are indeed Shapley Values, at least at the layerwise level. Given the many desirable theoretical qualities of Shapley Values -- which has driven their adoption among the ML community -- we argue that NLP practitioners should, when possible, adopt attention flow explanations alongside more traditional ones.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14652](https://arxiv.org/abs/2105.14652) [cs.CL]** |
|           | (or **[arXiv:2105.14652v1](https://arxiv.org/abs/2105.14652v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-15">15. G-Transformer for Document-level Machine Translation
</h2>


Title: [G-Transformer for Document-level Machine Translation](https://arxiv.org/abs/2105.14761)

Authors: [Guangsheng Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+G), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Zhiyang Teng](https://arxiv.org/search/cs?searchtype=author&query=Teng%2C+Z), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both non-pretraining and pre-training settings on three benchmark datasets.

| Comments: | Accepted by ACL2021 main track                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.14761](https://arxiv.org/abs/2105.14761) [cs.CL]** |
|           | (or **[arXiv:2105.14761v1](https://arxiv.org/abs/2105.14761v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-16">16. On Compositional Generalization of Neural Machine Translation
</h2>


Title: [On Compositional Generalization of Neural Machine Translation](https://arxiv.org/abs/2105.14802)

Authors: [Yafu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yongjing Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Yulong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Modern neural machine translation (NMT) models have achieved competitive performance in standard benchmarks such as WMT. However, there still exist significant issues such as robustness, domain generalization, etc. In this paper, we study NMT models from the perspective of compositional generalization by building a benchmark dataset, CoGnition, consisting of 216k clean and consistent sentence pairs. We quantitatively analyze effects of various factors using compound translation error rate, then demonstrate that the NMT model fails badly on compositional generalization, although it performs remarkably well under traditional metrics.

| Comments: | To appear at the ACL 2021 main conference                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.14802](https://arxiv.org/abs/2105.14802) [cs.CL]** |
|           | (or **[arXiv:2105.14802v1](https://arxiv.org/abs/2105.14802v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-17">17. Transfer Learning for Sequence Generation: from Single-source to Multi-source
</h2>


Title: [Transfer Learning for Sequence Generation: from Single-source to Multi-source](https://arxiv.org/abs/2105.14809)

Authors: [Xuancheng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Jingfang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Multi-source sequence generation (MSG) is an important kind of sequence generation tasks that takes multiple sources, including automatic post-editing, multi-source translation, multi-document summarization, etc. As MSG tasks suffer from the data scarcity problem and recent pretrained models have been proven to be effective for low-resource downstream tasks, transferring pretrained sequence-to-sequence models to MSG tasks is essential. Although directly finetuning pretrained models on MSG tasks and concatenating multiple sources into a single long sequence is regarded as a simple method to transfer pretrained models to MSG tasks, we conjecture that the direct finetuning method leads to catastrophic forgetting and solely relying on pretrained self-attention layers to capture cross-source information is not sufficient. Therefore, we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks. Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set. When adapted to document-level translation, our framework outperforms strong baselines significantly.

| Comments: | ACL2021 main track long paper                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.14809](https://arxiv.org/abs/2105.14809) [cs.CL]** |
|           | (or **[arXiv:2105.14809v1](https://arxiv.org/abs/2105.14809v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-18">18. Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models
</h2>


Title: [Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models](https://arxiv.org/abs/2105.14813)

Authors: [Chong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Cenyuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Xiaoqing Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X), [Xuanjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X)

> A sequence-to-sequence learning with neural networks has empirically proven to be an effective framework for Chinese Spelling Correction (CSC), which takes a sentence with some spelling errors as input and outputs the corrected one. However, CSC models may fail to correct spelling errors covered by the confusion sets, and also will encounter unseen ones. We propose a method, which continually identifies the weak spots of a model to generate more valuable training instances, and apply a task-specific pre-training strategy to enhance the model. The generated adversarial examples are gradually added to the training set. Experimental results show that such an adversarial training method combined with the pretraining strategy can improve both the generalization and robustness of multiple CSC models across three different datasets, achieving stateof-the-art performance for CSC task.

| Comments: | Accepted by ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14813](https://arxiv.org/abs/2105.14813) [cs.CL]** |
|           | (or **[arXiv:2105.14813v1](https://arxiv.org/abs/2105.14813v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-19">19. Effective Batching for Recurrent Neural Network Grammars
</h2>


Title: [Effective Batching for Recurrent Neural Network Grammars](https://arxiv.org/abs/2105.14822)

Authors: [Hiroshi Noji](https://arxiv.org/search/cs?searchtype=author&query=Noji%2C+H), [Yohei Oseki](https://arxiv.org/search/cs?searchtype=author&query=Oseki%2C+Y)

> As a language model that integrates traditional symbolic operations and flexible neural representations, recurrent neural network grammars (RNNGs) have attracted great attention from both scientific and engineering perspectives. However, RNNGs are known to be harder to scale due to the difficulty of batched training. In this paper, we propose effective batching for RNNGs, where every operation is computed in parallel with tensors across multiple sentences. Our PyTorch implementation effectively employs a GPU and achieves x6 speedup compared to the existing C++ DyNet implementation with model-independent auto-batching. Moreover, our batched RNNG also accelerates inference and achieves x20-150 speedup for beam search depending on beam sizes. Finally, we evaluate syntactic generalization performance of the scaled RNNG against the LSTM baseline, based on the large training data of 100M tokens from English Wikipedia and the broad-coverage targeted syntactic evaluation benchmark. Our RNNG implementation is available at [this https URL](https://github.com/aistairc/rnng-pytorch/).

| Comments: | Findings of ACL: ACL-IJCNLP 2021                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14822](https://arxiv.org/abs/2105.14822) [cs.CL]** |
|           | (or **[arXiv:2105.14822v1](https://arxiv.org/abs/2105.14822v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-20">20. Greedy Layer Pruning: Decreasing Inference Time of Transformer Models
</h2>


Title: [Greedy Layer Pruning: Decreasing Inference Time of Transformer Models](https://arxiv.org/abs/2105.14839)

Authors: [David Peer](https://arxiv.org/search/cs?searchtype=author&query=Peer%2C+D), [Sebastian Stabinger](https://arxiv.org/search/cs?searchtype=author&query=Stabinger%2C+S), [Stefan Engl](https://arxiv.org/search/cs?searchtype=author&query=Engl%2C+S), [Antonio Rodriguez-Sanchez](https://arxiv.org/search/cs?searchtype=author&query=Rodriguez-Sanchez%2C+A)

> Fine-tuning transformer models after unsupervised pre-training reaches a very high performance on many different NLP tasks. Unfortunately, transformers suffer from long inference times which greatly increases costs in production and is a limiting factor for the deployment into embedded devices. One possible solution is to use knowledge distillation, which solves this problem by transferring information from large teacher models to smaller student models, but as it needs an additional expensive pre-training phase, this solution is computationally expensive and can be financially prohibitive for smaller academic research groups. Another solution is to use layer-wise pruning methods, which reach high compression rates for transformer models and avoids the computational load of the pre-training distillation stage. The price to pay is that the performance of layer-wise pruning algorithms is not on par with state-of-the-art knowledge distillation methods. In this paper, greedy layer pruning (GLP) is introduced to (1) outperform current state-of-the-art for layer-wise pruning (2) close the performance gap when compared to knowledge distillation, while (3) using only a modest budget. More precisely, with the methodology presented it is possible to prune and evaluate competitive models on the whole GLUE benchmark with a budget of just $300. Our source code is available on [this https URL](https://github.com/deepopinion/greedy-layer-pruning).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2105.14839](https://arxiv.org/abs/2105.14839) [cs.CL]** |
|           | (or **[arXiv:2105.14839v1](https://arxiv.org/abs/2105.14839v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-21">21. Verdi: Quality Estimation and Error Detection for Bilingual
</h2>


Title: [Verdi: Quality Estimation and Error Detection for Bilingual](https://arxiv.org/abs/2105.14878)

Authors: [Mingjun Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+M), [Haijiang Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Di Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+D), [Zixuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Xiaoli Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X)

> Translation Quality Estimation is critical to reducing post-editing efforts in machine translation and to cross-lingual corpus cleaning. As a research problem, quality estimation (QE) aims to directly estimate the quality of translation in a given pair of source and target sentences, and highlight the words that need corrections, without referencing to golden translations. In this paper, we propose Verdi, a novel framework for word-level and sentence-level post-editing effort estimation for bilingual corpora. Verdi adopts two word predictors to enable diverse features to be extracted from a pair of sentences for subsequent quality estimation, including a transformer-based neural machine translation (NMT) model and a pre-trained cross-lingual language model (XLM). We exploit the symmetric nature of bilingual corpora and apply model-level dual learning in the NMT predictor, which handles a primal task and a dual task simultaneously with weight sharing, leading to stronger context prediction ability than single-direction NMT models. By taking advantage of the dual learning scheme, we further design a novel feature to directly encode the translated target information without relying on the source context. Extensive experiments conducted on WMT20 QE tasks demonstrate that our method beats the winner of the competition and outperforms other baseline methods by a great margin. We further use the sentence-level scores provided by Verdi to clean a parallel corpus and observe benefits on both model performance and training efficiency.

| Comments: | Accepted by The Web Conference 2021                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2105.14878](https://arxiv.org/abs/2105.14878) [cs.CL]** |
|           | (or **[arXiv:2105.14878v1](https://arxiv.org/abs/2105.14878v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-22">22. GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation
</h2>


Title: [GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation](https://arxiv.org/abs/2105.14913)

Authors: [Huayang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Lemao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Guoping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> Computer-aided translation (CAT), the use of software to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators. Autocompletion, which suggests translation results according to the text pieces provided by human translators, is a core function of CAT. There are two limitations in previous research in this line. First, most research works on this topic focus on sentence-level autocompletion (i.e., generating the whole translation as a sentence based on human input), but word-level autocompletion is under-explored so far. Second, almost no public benchmarks are available for the autocompletion task of CAT. This might be among the reasons why research progress in CAT is much slower compared to automatic MT. In this paper, we propose the task of general word-level autocompletion (GWLAN) from a real-world CAT scenario, and construct the first public benchmark to facilitate research in this topic. In addition, we propose an effective method for GWLAN and compare it with several strong baselines. Experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets.

| Comments: | Accepted into the main conference of ACL 2021. arXiv admin note: text overlap with [arXiv:2105.13072](https://arxiv.org/abs/2105.13072) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.14913](https://arxiv.org/abs/2105.14913) [cs.CL]** |
|           | (or **[arXiv:2105.14913v1](https://arxiv.org/abs/2105.14913v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-23">23. Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?
</h2>


Title: [Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?](https://arxiv.org/abs/2105.14940)

Authors: [Zae Myung Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Z+M), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L), [Vassilina Nikoulina](https://arxiv.org/search/cs?searchtype=author&query=Nikoulina%2C+V), [Didier Schwab](https://arxiv.org/search/cs?searchtype=author&query=Schwab%2C+D)

> Recent studies on the analysis of the multilingual representations focus on identifying whether there is an emergence of language-independent representations, or whether a multilingual model partitions its weights among different languages. While most of such work has been conducted in a "black-box" manner, this paper aims to analyze individual components of a multilingual neural translation (NMT) model. In particular, we look at the encoder self-attention and encoder-decoder attention heads (in a many-to-one NMT model) that are more specific to the translation of a certain language pair than others by (1) employing metrics that quantify some aspects of the attention weights such as "variance" or "confidence", and (2) systematically ranking the importance of attention heads with respect to translation quality. Experimental results show that surprisingly, the set of most important attention heads are very similar across the language pairs and that it is possible to remove nearly one-third of the less important heads without hurting the translation quality greatly.

| Comments: | 10 pages, accepted at Findings of ACL 2021 (short)           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2105.14940](https://arxiv.org/abs/2105.14940) [cs.CL]** |
|           | (or **[arXiv:2105.14940v1](https://arxiv.org/abs/2105.14940v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-24">24. Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data
</h2>


Title: [Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data](https://arxiv.org/abs/2105.15071)

Authors: [Wei-Jen Ko](https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+W), [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Adithya Renduchintala](https://arxiv.org/search/cs?searchtype=author&query=Renduchintala%2C+A), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzmán%2C+F), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M)

> The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.15071](https://arxiv.org/abs/2105.15071) [cs.CL]** |
|           | (or **[arXiv:2105.15071v1](https://arxiv.org/abs/2105.15071v1) [cs.CL]** for this version) |





<h2 id="2021-06-01-25">25. Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation
</h2>


Title: [Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation](https://arxiv.org/abs/2105.15087)

Authors: [Eleftheria Briakou](https://arxiv.org/search/cs?searchtype=author&query=Briakou%2C+E), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

> While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN-FR tasks.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2105.15087](https://arxiv.org/abs/2105.15087) [cs.CL]** |
|           | (or **[arXiv:2105.15087v1](https://arxiv.org/abs/2105.15087v1) [cs.CL]** for this version) |


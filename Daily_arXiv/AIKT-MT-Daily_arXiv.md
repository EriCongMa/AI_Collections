# Daily arXiv: Machine Translation - September, 2021

# Index


- [2021-09-10](#2021-09-10)

  - [1. Non-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring](#2021-09-10-1)
  - [2. TxT: Crossmodal End-to-End Learning with Transformers](#2021-09-10-2)
  - [3. Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation](#2021-09-10-3)
  - [4. Ensemble Fine-tuned mBERT for Translation Quality Estimation](#2021-09-10-4)
  - [5. Competence-based Curriculum Learning for Multilingual Machine Translation](#2021-09-10-5)
  - [6. Distributionally Robust Multilingual Machine Translation](#2021-09-10-6)
  - [7. Efficient Nearest Neighbor Language Models](#2021-09-10-7)
  - [8. Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection](#2021-09-10-8)
  - [9. MATE: Multi-view Attention for Table Transformer Efficiency](#2021-09-10-9)
  - [10. Smoothed Contrastive Learning for Unsupervised Sentence Embedding](#2021-09-10-10)
  - [11. PPT: Pre-trained Prompt Tuning for Few-shot Learning](#2021-09-10-11)
  - [12. ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding](#2021-09-10-12)
  - [13. HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints](#2021-09-10-13)
  - [14. Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers](#2021-09-10-14)
- [2021-09-09](#2021-09-09)

  - [1. Mixup Decoding for Diverse Machine Translation](#2021-09-09-1)
  - [2. Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models](#2021-09-09-2)
  - [3. Sequence Level Contrastive Learning for Text Summarization](#2021-09-09-3)
  - [4. RefineCap: Concept-Aware Refinement for Image Captioning](#2021-09-09-4)
  - [5. Discrete and Soft Prompting for Multilingual Models](#2021-09-09-5)
  - [6. Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach](#2021-09-09-6)
  - [7. Active Learning by Acquiring Contrastive Examples](#2021-09-09-7)
- [2021-09-08](#2021-09-08)

  - [1. Paraphrase Generation as Unsupervised Machine Translation](#2021-09-08-1)
  - [2. Don't Go Far Off: An Empirical Study on Neural Poetry Translation](#2021-09-08-2)
  - [3. Revisiting Context Choices for Context-aware Machine Translation](#2021-09-08-3)
  - [4. Generate & Rank: A Multi-task Framework for Math Word Problems](#2021-09-08-4)
  - [5. NumGPT: Improving Numeracy Ability of Generative Pre-trained Models](#2021-09-08-5)
- [2021-09-07](#2021-09-07)

  - [1. Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models](#2021-09-07-1)
  - [2. On the ability of monolingual models to learn language-agnostic representations](#2021-09-07-2)
  - [3. Counterfactual Evaluation for Explainable AI](#2021-09-07-3)
  - [4. Data Efficient Masked Language Modeling for Vision and Language](#2021-09-07-4)
  - [5. Teaching Autoregressive Language Models Complex Tasks By Demonstration](#2021-09-07-5)
  - [6. Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack](#2021-09-07-6)
  - [7. Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training](#2021-09-07-7)
- [2021-09-06](#2021-09-06)
  - [1. Ranking Scientific Papers Using Preference Learning](#2021-09-06-1)
  - [2. Establishing Interlingua in Multilingual Language Models](#2021-09-06-2)
  - [3. Quantifying Reproducibility in NLP and ML](#2021-09-06-3)
  - [4. Multimodal Conditionality for Natural Language Generation](#2021-09-06-4)
  - [5. Do Prompt-Based Models Really Understand the Meaning of their Prompts?](#2021-09-06-5)
  - [6. Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT](#2021-09-06-6)
  - [7. Finetuned Language Models Are Zero-Shot Learners](#2021-09-06-7)
- [2021-09-03](#2021-09-03)
  - [1. Skim-Attention: Learning to Focus via Document Layout](#2021-09-03-1)
  - [2. How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?](#2021-09-03-2)
  - [3. Sequence-to-Sequence Learning with Latent Neural Grammars](#2021-09-03-3)
  - [4. Knowledge Perceived Multi-modal Pretraining in E-commerce](#2021-09-03-4)
  - [5. Improving Multimodal fusion via Mutual Dependency Maximisation](#2021-09-03-5)
  - [6. Towards Improving Adversarial Training of NLP Models](#2021-09-03-6)
  - [7. Point-of-Interest Type Prediction using Text and Images](#2021-09-03-7)
  - [8. Towards Making the Most of Dialogue Characteristics for Neural Chat Translation](#2021-09-03-8)
- [2021-09-02](#2021-09-02)

  - [1. Sentence Bottleneck Autoencoders from Transformer Language Models](#2021-09-02-1)
  - [2. It's not Rocket Science : Interpreting Figurative Language in Narratives](#2021-09-02-2)
  - [3. Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast](#2021-09-02-3)
  - [4. Discovering Representation Sprachbund For Multilingual Pre-Training](#2021-09-02-4)
  - [5. ∞-former: Infinite Memory Transformer](#2021-09-02-5)
  - [6. Masked Adversarial Generation for Neural Machine Translation](#2021-09-02-6)
  - [7. Position Masking for Improved Layout-Aware Document Understanding](#2021-09-02-7)
  - [8. Survey of Low-Resource Machine Translation](#2021-09-02-8)
- [2021-09-01](#2021-09-01)
  - [1. SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory](#2021-09-01-1)
  - [2. Want To Reduce Labeling Cost? GPT-3 Can Help](#2021-09-01-2)
  - [3. T3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP](#2021-09-01-3)
  - [4. Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience](#2021-09-01-4)
  - [5. Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools](#2021-09-01-5)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-09-10

[Return to Index](#Index)



<h2 id="2021-09-10-1">1. Non-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring
</h2>

Title: [Non-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring](https://arxiv.org/abs/2109.04411)

Authors: [Hirofumi Inaguma](https://arxiv.org/search/eess?searchtype=author&query=Inaguma%2C+H), [Yosuke Higuchi](https://arxiv.org/search/eess?searchtype=author&query=Higuchi%2C+Y), [Kevin Duh](https://arxiv.org/search/eess?searchtype=author&query=Duh%2C+K), [Tatsuya Kawahara](https://arxiv.org/search/eess?searchtype=author&query=Kawahara%2C+T), [Shinji Watanabe](https://arxiv.org/search/eess?searchtype=author&query=Watanabe%2C+S)

> This article describes an efficient end-to-end speech translation (E2E-ST) framework based on non-autoregressive (NAR) models. End-to-end speech translation models have several advantages over traditional cascade systems such as inference latency reduction. However, conventional AR decoding methods are not fast enough because each token is generated incrementally. NAR models, however, can accelerate the decoding speed by generating multiple tokens in parallel on the basis of the token-wise conditional independence assumption. We propose a unified NAR E2E-ST framework called Orthros, which has an NAR decoder and an auxiliary shallow AR decoder on top of the shared encoder. The auxiliary shallow AR decoder selects the best hypothesis by rescoring multiple candidates generated from the NAR decoder in parallel (parallel AR rescoring). We adopt conditional masked language model (CMLM) and a connectionist temporal classification (CTC)-based model as NAR decoders for Orthros, referred to as Orthros-CMLM and Orthros-CTC, respectively. We also propose two training methods to enhance the CMLM decoder. Experimental evaluations on three benchmark datasets with six language directions demonstrated that Orthros achieved large improvements in translation quality with a very small overhead compared with the baseline NAR model. Moreover, the Conformer encoder architecture enabled large quality improvements, especially for CTC-based models. Orthros-CTC with the Conformer encoder increased decoding speed by 3.63x on CPU with translation quality comparable to that of an AR model.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.04411](https://arxiv.org/abs/2109.04411) [eess.AS]** |
|           | (or **[arXiv:2109.04411v1](https://arxiv.org/abs/2109.04411v1) [eess.AS]** for this version) |





<h2 id="2021-09-10-2">2. TxT: Crossmodal End-to-End Learning with Transformers
</h2>

Title: [TxT: Crossmodal End-to-End Learning with Transformers](https://arxiv.org/abs/2109.04422)

Authors: [Jan-Martin O. Steitz](https://arxiv.org/search/cs?searchtype=author&query=Steitz%2C+J+O), [Jonas Pfeiffer](https://arxiv.org/search/cs?searchtype=author&query=Pfeiffer%2C+J), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I), [Stefan Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+S)

> Reasoning over multiple modalities, e.g. in Visual Question Answering (VQA), requires an alignment of semantic concepts across domains. Despite the widespread success of end-to-end learning, today's multimodal pipelines by and large leverage pre-extracted, fixed features from object detectors, typically Faster R-CNN, as representations of the visual world. The obvious downside is that the visual representation is not specifically tuned to the multimodal task at hand. At the same time, while transformer-based object detectors have gained popularity, they have not been employed in today's multimodal pipelines. We address both shortcomings with TxT, a transformer-based crossmodal pipeline that enables fine-tuning both language and visual components on the downstream task in a fully end-to-end manner. We overcome existing limitations of transformer-based detectors for multimodal reasoning regarding the integration of global context and their scalability. Our transformer-based multimodal model achieves considerable gains from end-to-end learning for multimodal question answering.

| Comments: | To appear at the 43rd DAGM German Conference on Pattern Recognition (GCPR) 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2109.04422](https://arxiv.org/abs/2109.04422) [cs.CV]** |
|           | (or **[arXiv:2109.04422v1](https://arxiv.org/abs/2109.04422v1) [cs.CV]** for this version) |





<h2 id="2021-09-10-3">3. Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation
</h2>

Title: [Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation](https://arxiv.org/abs/2109.03858)

Authors: [Shahar Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+S), [Koren Lazar](https://arxiv.org/search/cs?searchtype=author&query=Lazar%2C+K), [abriel Stanovsky](https://arxiv.org/search/cs?searchtype=author&query=Stanovsky%2C+a)

> Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains, resulting in a first large-scale gender bias dataset of 108K diverse real-world English sentences. We manually verify the quality of our corpus and use it to evaluate gender bias in various coreference resolution and machine translation models. We find that all tested models tend to over-rely on gender stereotypes when presented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at [this http URL](http://www.github.com/SLAB-NLP/BUG). We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings.

| Comments: | Accepted to Findings of EMNLP 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03858](https://arxiv.org/abs/2109.03858) [cs.CL]** |
|           | (or **[arXiv:2109.03858v1](https://arxiv.org/abs/2109.03858v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-4">4. Ensemble Fine-tuned mBERT for Translation Quality Estimation
</h2>

Title: [Ensemble Fine-tuned mBERT for Translation Quality Estimation](https://arxiv.org/abs/2109.03914)

Authors: [Shaika Chowdhury](https://arxiv.org/search/cs?searchtype=author&query=Chowdhury%2C+S), [Naouel Baili](https://arxiv.org/search/cs?searchtype=author&query=Baili%2C+N), [Brian Vannah](https://arxiv.org/search/cs?searchtype=author&query=Vannah%2C+B)

> Quality Estimation (QE) is an important component of the machine translation workflow as it assesses the quality of the translated output without consulting reference translations. In this paper, we discuss our submission to the WMT 2021 QE Shared Task. We participate in Task 2 sentence-level sub-task that challenge participants to predict the HTER score for sentence-level post-editing effort. Our proposed system is an ensemble of multilingual BERT (mBERT)-based regression models, which are generated by fine-tuning on different input settings. It demonstrates comparable performance with respect to the Pearson's correlation and beats the baseline system in MAE/ RMSE for several language pairs. In addition, we adapt our system for the zero-shot setting by exploiting target language-relevant language pairs and pseudo-reference translations.

| Comments: | The Sixth Conference on Machine Translation, WMT 2021        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03914](https://arxiv.org/abs/2109.03914) [cs.CL]** |
|           | (or **[arXiv:2109.03914v1](https://arxiv.org/abs/2109.03914v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-5">5. Competence-based Curriculum Learning for Multilingual Machine Translation
</h2>

Title: [Competence-based Curriculum Learning for Multilingual Machine Translation](https://arxiv.org/abs/2109.04002)

Authors: [Mingliang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yunhai Tong](https://arxiv.org/search/cs?searchtype=author&query=Tong%2C+Y), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Currently, multilingual machine translation is receiving more and more attention since it brings better performance for low resource languages (LRLs) and saves more space. However, existing multilingual machine translation models face a severe challenge: imbalance. As a result, the translation performance of different languages in multilingual translation models are quite different. We argue that this imbalance problem stems from the different learning competencies of different languages. Therefore, we focus on balancing the learning competencies of different languages and propose Competence-based Curriculum Learning for Multilingual Machine Translation, named CCL-M. Specifically, we firstly define two competencies to help schedule the high resource languages (HRLs) and the low resource languages: 1) Self-evaluated Competence, evaluating how well the language itself has been learned; and 2) HRLs-evaluated Competence, evaluating whether an LRL is ready to be learned according to HRLs' Self-evaluated Competence. Based on the above competencies, we utilize the proposed CCL-M algorithm to gradually add new languages into the training set in a curriculum learning manner. Furthermore, we propose a novel competenceaware dynamic balancing sampling strategy for better selecting training samples in multilingual training. Experimental results show that our approach has achieved a steady and significant performance gain compared to the previous state-of-the-art approach on the TED talks dataset.

| Comments: | Accepted by Findings of EMNLP 2021. We release the codes at [this https URL](https://github.com/zml24/ccl-m) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04002](https://arxiv.org/abs/2109.04002) [cs.CL]** |
|           | (or **[arXiv:2109.04002v1](https://arxiv.org/abs/2109.04002v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-6">6. Distributionally Robust Multilingual Machine Translation
</h2>

Title: [Distributionally Robust Multilingual Machine Translation](https://arxiv.org/abs/2109.04020)

Authors: [Chunting Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Daniel Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+D), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Multilingual neural machine translation (MNMT) learns to translate multiple language pairs with a single model, potentially improving both the accuracy and the memory-efficiency of deployed models. However, the heavy data imbalance between languages hinders the model from performing uniformly across language pairs. In this paper, we propose a new learning objective for MNMT based on distributionally robust optimization, which minimizes the worst-case expected loss over the set of language pairs. We further show how to practically optimize this objective for large translation corpora using an iterated best response scheme, which is both effective and incurs negligible additional computational cost compared to standard empirical risk minimization. We perform extensive experiments on three sets of languages from two datasets and show that our method consistently outperforms strong baseline methods in terms of average and per-language performance under both many-to-one and one-to-many translation settings.

| Comments: | Long paper accepted by EMNLP2021 main conference             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.04020](https://arxiv.org/abs/2109.04020) [cs.CL]** |
|           | (or **[arXiv:2109.04020v1](https://arxiv.org/abs/2109.04020v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-7">7. Efficient Nearest Neighbor Language Models
</h2>

Title: [Efficient Nearest Neighbor Language Models](https://arxiv.org/abs/2109.04212)

Authors: [Junxian He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+J), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Taylor Berg-Kirkpatrick](https://arxiv.org/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T)

> Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model (Khandelwal et al., 2019) as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04212](https://arxiv.org/abs/2109.04212) [cs.CL]** |
|           | (or **[arXiv:2109.04212v1](https://arxiv.org/abs/2109.04212v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-8">8. Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection
</h2>

Title: [Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection](https://arxiv.org/abs/2109.04292)

Authors: [Thuy-Trang Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+T), [Xuanli He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X), [Dinh Phung](https://arxiv.org/search/cs?searchtype=author&query=Phung%2C+D), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G)

> This paper considers the unsupervised domain adaptation problem for neural machine translation (NMT), where we assume the access to only monolingual text in either the source or target language in the new domain. We propose a cross-lingual data selection method to extract in-domain sentences in the missing language side from a large generic monolingual corpus. Our proposed method trains an adaptive layer on top of multilingual BERT by contrastive learning to align the representation between the source and target language. This then enables the transferability of the domain classifier between the languages in a zero-shot manner. Once the in-domain data is detected by the classifier, the NMT model is then adapted to the new domain by jointly learning translation and domain discrimination tasks. We evaluate our cross-lingual data selection method on NMT across five diverse domains in three language pairs, as well as a real-world scenario of translation for COVID-19. The results show that our proposed method outperforms other selection baselines up to +1.5 BLEU score.

| Comments: | EMNLP2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04292](https://arxiv.org/abs/2109.04292) [cs.CL]** |
|           | (or **[arXiv:2109.04292v1](https://arxiv.org/abs/2109.04292v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-9">9. MATE: Multi-view Attention for Table Transformer Efficiency
</h2>

Title: [MATE: Multi-view Attention for Table Transformer Efficiency](https://arxiv.org/abs/2109.04312)

Authors: [Julian Martin Eisenschlos](https://arxiv.org/search/cs?searchtype=author&query=Eisenschlos%2C+J+M), [Maharshi Gor](https://arxiv.org/search/cs?searchtype=author&query=Gor%2C+M), [Thomas Müller](https://arxiv.org/search/cs?searchtype=author&query=Müller%2C+T), [William W. Cohen](https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+W+W)

> This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20% of relational tables on the web have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose MATE, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This architecture scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate inductive bias for tabular data, and sets a new state-of-the-art for three table reasoning datasets. For HybridQA (Chen et al., 2020b), a dataset that involves large documents containing tables, we improve the best prior result by 19 points.

| Comments: | Accepted to EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.04312](https://arxiv.org/abs/2109.04312) [cs.CL]** |
|           | (or **[arXiv:2109.04312v1](https://arxiv.org/abs/2109.04312v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-10">10. Smoothed Contrastive Learning for Unsupervised Sentence Embedding
</h2>

Title: [Smoothed Contrastive Learning for Unsupervised Sentence Embedding](https://arxiv.org/abs/2109.04321)

Authors: [Xing Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Chaochen Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C), [Liangjun Zang](https://arxiv.org/search/cs?searchtype=author&query=Zang%2C+L), [Jizhong Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J), [Zhongyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Songlin Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S)

> Contrastive learning has been gradually applied to learn high-quality unsupervised sentence embedding. Among the previous un-supervised methods, the latest state-of-the-art method, as far as we know, is unsupervised SimCSE (unsup-SimCSE). Unsup-SimCSE uses the InfoNCE1loss function in the training stage by pulling semantically similar sentences together and pushing apart dis-similar ones.Theoretically, we expect to use larger batches in unsup-SimCSE to get more adequate comparisons among samples and avoid overfitting. However, increasing the batch size does not always lead to improvements, but instead even lead to performance degradation when the batch size exceeds a threshold. Through statistical observation, we find that this is probably due to the introduction of low-confidence negative pairs after in-creasing the batch size. To alleviate this problem, we introduce a simple smoothing strategy upon the InfoNCE loss function, termedGaussian Smoothing InfoNCE (GS-InfoNCE).Specifically, we add random Gaussian noise vectors as negative samples, which act asa smoothing of the negative sample space.Though being simple, the proposed smooth-ing strategy brings substantial improvements to unsup-SimCSE. We evaluate GS-InfoNCEon the standard semantic text similarity (STS)task. GS-InfoNCE outperforms the state-of-the-art unsup-SimCSE by an average Spear-man correlation of 1.38%, 0.72%, 1.17% and0.28% on the base of BERT-base, BERT-large,RoBERTa-base and RoBERTa-large, respectively.

| Comments: | 6 pages, 2 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.04321](https://arxiv.org/abs/2109.04321) [cs.CL]** |
|           | (or **[arXiv:2109.04321v1](https://arxiv.org/abs/2109.04321v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-11">11. PPT: Pre-trained Prompt Tuning for Few-shot Learning
</h2>

Title: [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://arxiv.org/abs/2109.04332)

Authors: [Yuxian Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+Y), [Xu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Minlie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M)

> Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model fine-tuning when downstream data are sufficient, whereas it performs much worse under few-shot learning settings, which may hinder the application of prompt tuning in practice. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework "PPT". To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice.

| Comments: | 10 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04332](https://arxiv.org/abs/2109.04332) [cs.CL]** |
|           | (or **[arXiv:2109.04332v1](https://arxiv.org/abs/2109.04332v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-12">12. ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding
</h2>

Title: [ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding](https://arxiv.org/abs/2109.04380)

Authors: [Xing Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Chaochen Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C), [Liangjun Zang](https://arxiv.org/search/cs?searchtype=author&query=Zang%2C+L), [Jizhong Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J), [Zhongyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Songlin Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S)

> Contrastive learning has been attracting much attention for learning unsupervised sentence embeddings. The current state-of-the-art unsupervised method is the unsupervised SimCSE (unsup-SimCSE). Unsup-SimCSE takes dropout as a minimal data augmentation method, and passes the same input sentence to a pre-trained Transformer encoder (with dropout turned on) twice to obtain the two corresponding embeddings to build a positive pair. As the length information of a sentence will generally be encoded into the sentence embeddings due to the usage of position embedding in Transformer, each positive pair in unsup-SimCSE actually contains the same length information. And thus unsup-SimCSE trained with these positive pairs is probably biased, which would tend to consider that sentences of the same or similar length are more similar in semantics. Through statistical observations, we find that unsup-SimCSE does have such a problem. To alleviate it, we apply a simple repetition operation to modify the input sentence, and then pass the input sentence and its modified counterpart to the pre-trained Transformer encoder, respectively, to get the positive pair. Additionally, we draw inspiration from the community of computer vision and introduce a momentum contrast, enlarging the number of negative pairs without additional calculations. The proposed two modifications are applied on positive and negative pairs separately, and build a new sentence embedding method, termed Enhanced Unsup-SimCSE (ESimCSE). We evaluate the proposed ESimCSE on several benchmark datasets w.r.t the semantic text similarity (STS) task. Experimental results show that ESimCSE outperforms the state-of-the-art unsup-SimCSE by an average Spearman correlation of 2.02% on BERT-base.

| Comments: | 9 pages, 2 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.04380](https://arxiv.org/abs/2109.04380) [cs.CL]** |
|           | (or **[arXiv:2109.04380v1](https://arxiv.org/abs/2109.04380v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-13">13. HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints
</h2>

Title: [HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints](https://arxiv.org/abs/2109.04443)

Authors: [Sahana Ramnath](https://arxiv.org/search/cs?searchtype=author&query=Ramnath%2C+S), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M), [Abhirut Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Aravindan Raghuveer](https://arxiv.org/search/cs?searchtype=author&query=Raghuveer%2C+A)

> Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT -- a family of techniques which provides hints (through tags) to the encoder and decoder. First, we propose a novel method of using both high and low quality BT data by providing hints (as source tags on the encoder) to the model about the quality of each source-target pair. We don't filter out low quality data but instead show that these hints enable the model to learn effectively from noisy data. Second, we address the problem of predicting whether a source token needs to be translated or transliterated to the target language, which is common in cross-script translation tasks (i.e., where source and target do not share the written script). For such cases, we propose training the model with additional hints (as target tags on the decoder) that provide information about the operation required on the source (translation or both translation and transliteration). We conduct experiments and detailed analyses on standard WMT benchmarks for three cross-script low/medium-resource language pairs: {Hindi,Gujarati,Tamil}-to-English. Our methods compare favorably with five strong and well established baselines. We show that using these hints, both separately and together, significantly improves translation quality and leads to state-of-the-art performance in all three language pairs in corresponding bilingual settings.

| Comments: | 17 pages including references and appendix. Accepted at EMNLP 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.04443](https://arxiv.org/abs/2109.04443) [cs.CL]** |
|           | (or **[arXiv:2109.04443v1](https://arxiv.org/abs/2109.04443v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-14">14. Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers
</h2>

Title: [Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers](https://arxiv.org/abs/2109.04448)

Authors: [Stella Frank](https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+S), [Emanuele Bugliarello](https://arxiv.org/search/cs?searchtype=author&query=Bugliarello%2C+E), [Desmond Elliott](https://arxiv.org/search/cs?searchtype=author&query=Elliott%2C+D)

> Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed models have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these models are not symmetrically cross-modal.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2109.04448](https://arxiv.org/abs/2109.04448) [cs.CL]** |
|           | (or **[arXiv:2109.04448v1](https://arxiv.org/abs/2109.04448v1) [cs.CL]** for this version) |





# 2021-09-09

[Return to Index](#Index)



<h2 id="2021-09-09-1">1. Mixup Decoding for Diverse Machine Translation
</h2>

Title: [Mixup Decoding for Diverse Machine Translation](https://arxiv.org/abs/2109.03402)

Authors: [Jicheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Pengzhi Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+P), [Xuanfu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Zhongjun He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Z), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Diverse machine translation aims at generating various target language translations for a given source language sentence. Leveraging the linear relationship in the sentence latent space introduced by the mixup training, we propose a novel method, MixDiversity, to generate different translations for the input sentence by linearly interpolating it with different sentence pairs sampled from the training corpus when decoding. To further improve the faithfulness and diversity of the translations, we propose two simple but effective approaches to select diverse sentence pairs in the training corpus and adjust the interpolation weight for each pair correspondingly. Moreover, by controlling the interpolation weight, our method can achieve the trade-off between faithfulness and diversity without any additional training, which is required in most of the previous methods. Experiments on WMT'16 en-ro, WMT'14 en-de, and WMT'17 zh-en are conducted to show that our method substantially outperforms all previous diverse machine translation methods.

| Comments: | Findings of EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03402](https://arxiv.org/abs/2109.03402) [cs.CL]** |
|           | (or **[arXiv:2109.03402v1](https://arxiv.org/abs/2109.03402v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-2">2. Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models
</h2>

Title: [Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models](https://arxiv.org/abs/2109.03415)

Authors: [Jiaoda Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Duygu Ataman](https://arxiv.org/search/cs?searchtype=author&query=Ataman%2C+D), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Multimodal machine translation (MMT) systems have been shown to outperform their text-only neural machine translation (NMT) counterparts when visual context is available. However, recent studies have also shown that the performance of MMT models is only marginally impacted when the associated image is replaced with an unrelated image or noise, which suggests that the visual context might not be exploited by the model at all. We hypothesize that this might be caused by the nature of the commonly used evaluation benchmark, also known as Multi30K, where the translations of image captions were prepared without actually showing the images to human translators. In this paper, we present a qualitative study that examines the role of datasets in stimulating the leverage of visual modality and we propose methods to highlight the importance of visual signals in the datasets which demonstrate improvements in reliance of models on the source images. Our findings suggest the research on effective MMT architectures is currently impaired by the lack of suitable datasets and careful consideration must be taken in creation of future MMT datasets, for which we also provide useful insights.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03415](https://arxiv.org/abs/2109.03415) [cs.CL]** |
|           | (or **[arXiv:2109.03415v1](https://arxiv.org/abs/2109.03415v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-3">3. Sequence Level Contrastive Learning for Text Summarization
</h2>

Title: [Sequence Level Contrastive Learning for Text Summarization](https://arxiv.org/abs/2109.03481)

Authors: [Shusheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S), [Xingxing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Yi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Contrastive learning models have achieved great success in unsupervised visual representation learning, which maximize the similarities between feature representations of different views of the same image, while minimize the similarities between feature representations of views of different images. In text summarization, the output summary is a shorter form of the input document and they have similar meanings. In this paper, we propose a contrastive learning model for supervised abstractive text summarization, where we view a document, its gold summary and its model generated summaries as different views of the same mean representation and maximize the similarities between them during training. We improve over a strong sequence-to-sequence text generation model (i.e., BART) on three different summarization datasets. Human evaluation also shows that our model achieves better faithfulness ratings compared to its counterpart without contrastive objectives.

| Comments: | 2 figures, 12 tables                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03481](https://arxiv.org/abs/2109.03481) [cs.CL]** |
|           | (or **[arXiv:2109.03481v1](https://arxiv.org/abs/2109.03481v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-4">4. RefineCap: Concept-Aware Refinement for Image Captioning
</h2>

Title: [RefineCap: Concept-Aware Refinement for Image Captioning](https://arxiv.org/abs/2109.03529)

Authors: [Yekun Chai](https://arxiv.org/search/cs?searchtype=author&query=Chai%2C+Y), [Shuo Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+S), [Junliang Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+J)

> Automatically translating images to texts involves image scene understanding and language modeling. In this paper, we propose a novel model, termed RefineCap, that refines the output vocabulary of the language decoder using decoder-guided visual semantics, and implicitly learns the mapping between visual tag words and images. The proposed Visual-Concept Refinement method can allow the generator to attend to semantic details in the image, thereby generating more semantically descriptive captions. Our model achieves superior performance on the MS-COCO dataset in comparison with previous visual-concept based models.

| Comments: | Accepted at ViGIL @NAACL 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03529](https://arxiv.org/abs/2109.03529) [cs.CL]** |
|           | (or **[arXiv:2109.03529v1](https://arxiv.org/abs/2109.03529v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-5">5. Discrete and Soft Prompting for Multilingual Models
</h2>

Title: [Discrete and Soft Prompting for Multilingual Models](https://arxiv.org/abs/2109.03630)

Authors: [Mengjie Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+M), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> It has been shown for English that discrete and soft prompting perform strongly in few-shot learning with pretrained language models (PLMs). In this paper, we show that discrete and soft prompting perform better than finetuning in multilingual cases: Crosslingual transfer and in-language training of multilingual natural language inference. For example, with 48 English training examples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely surpassing the majority baseline (33.33%). In contrast, discrete and soft prompting outperform finetuning, achieving 36.43% and 38.79%. We also demonstrate good performance of prompting with training data in multiple languages other than English.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03630](https://arxiv.org/abs/2109.03630) [cs.CL]** |
|           | (or **[arXiv:2109.03630v1](https://arxiv.org/abs/2109.03630v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-6">6. Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach
</h2>

Title: [Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach](https://arxiv.org/abs/2109.03645)

Authors: [Víctor M. Sánchez-Cartagena](https://arxiv.org/search/cs?searchtype=author&query=Sánchez-Cartagena%2C+V+M), [Miquel Esplà-Gomis](https://arxiv.org/search/cs?searchtype=author&query=Esplà-Gomis%2C+M), [Juan Antonio Pérez-Ortiz](https://arxiv.org/search/cs?searchtype=author&query=Pérez-Ortiz%2C+J+A), [Felipe Sánchez-Martínez](https://arxiv.org/search/cs?searchtype=author&query=Sánchez-Martínez%2C+F)

> In the context of neural machine translation, data augmentation (DA) techniques may be used for generating additional training samples when the available parallel data are scarce. Many DA approaches aim at expanding the support of the empirical data distribution by generating new sentence pairs that contain infrequent words, thus making it closer to the true data distribution of parallel sentences. In this paper, we propose to follow a completely different approach and present a multi-task DA approach in which we generate new sentence pairs with transformations, such as reversing the order of the target sentence, which produce unfluent target sentences. During training, these augmented sentences are used as auxiliary tasks in a multi-task framework with the aim of providing new contexts where the target prefix is not informative enough to predict the next word. This strengthens the encoder and forces the decoder to pay more attention to the source representations of the encoder. Experiments carried out on six low-resource translation tasks show consistent improvements over the baseline and over DA methods aiming at extending the support of the empirical data distribution. The systems trained with our approach rely more on the source tokens, are more robust against domain shift and suffer less hallucinations.

| Comments: | To be published as long paper in EMNLP 2021                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03645](https://arxiv.org/abs/2109.03645) [cs.CL]** |
|           | (or **[arXiv:2109.03645v1](https://arxiv.org/abs/2109.03645v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-7">7. Active Learning by Acquiring Contrastive Examples
</h2>

Title: [Active Learning by Acquiring Contrastive Examples](https://arxiv.org/abs/2109.03764)

Authors: [Katerina Margatina](https://arxiv.org/search/cs?searchtype=author&query=Margatina%2C+K), [Giorgos Vernikos](https://arxiv.org/search/cs?searchtype=author&query=Vernikos%2C+G), [Loïc Barrault](https://arxiv.org/search/cs?searchtype=author&query=Barrault%2C+L), [Nikolaos Aletras](https://arxiv.org/search/cs?searchtype=author&query=Aletras%2C+N)

> Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting \textit{contrastive examples}, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.

| Comments: | Accepted at EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.03764](https://arxiv.org/abs/2109.03764) [cs.CL]** |
|           | (or **[arXiv:2109.03764v1](https://arxiv.org/abs/2109.03764v1) [cs.CL]** for this version) |





# 2021-09-08

[Return to Index](#Index)



<h2 id="2021-09-08-1">1. Paraphrase Generation as Unsupervised Machine Translation
</h2>

Title: [Paraphrase Generation as Unsupervised Machine Translation](https://arxiv.org/abs/2109.02950)

Authors: [Chun Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+C), [Yufei Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Y), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Nanyun Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+N), [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> In this paper, we propose a new paradigm for paraphrase generation by treating the task as unsupervised machine translation (UMT) based on the assumption that there must be pairs of sentences expressing the same meaning in a large-scale unlabeled monolingual corpus. The proposed paradigm first splits a large unlabeled corpus into multiple clusters, and trains multiple UMT models using pairs of these clusters. Then based on the paraphrase pairs produced by these UMT models, a unified surrogate model can be trained to serve as the final Seq2Seq model to generate paraphrases, which can be directly used for test in the unsupervised setup, or be finetuned on labeled datasets in the supervised setup. The proposed method offers merits over machine-translation-based paraphrase generation methods, as it avoids reliance on bilingual sentence pairs. It also allows human intervene with the model so that more diverse paraphrases can be generated using different filtering criteria. Extensive experiments on existing paraphrase dataset for both the supervised and unsupervised setups demonstrate the effectiveness the proposed paradigm.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.02950](https://arxiv.org/abs/2109.02950) [cs.CL]** |
|           | (or **[arXiv:2109.02950v1](https://arxiv.org/abs/2109.02950v1) [cs.CL]** for this version) |





<h2 id="2021-09-08-2">2. Don't Go Far Off: An Empirical Study on Neural Poetry Translation
</h2>

Title: [Don't Go Far Off: An Empirical Study on Neural Poetry Translation](https://arxiv.org/abs/2109.02972)

Authors: [Tuhin Chakrabarty](https://arxiv.org/search/cs?searchtype=author&query=Chakrabarty%2C+T), [Arkadiy Saakyan](https://arxiv.org/search/cs?searchtype=author&query=Saakyan%2C+A), [Smaranda Muresan](https://arxiv.org/search/cs?searchtype=author&query=Muresan%2C+S)

> Despite constant improvements in machine translation quality, automatic poetry translation remains a challenging problem due to the lack of open-sourced parallel poetic corpora, and to the intrinsic complexities involved in preserving the semantics, style, and figurative nature of poetry. We present an empirical investigation for poetry translation along several dimensions: 1) size and style of training data (poetic vs. non-poetic), including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3) language-family-specific models vs. mixed-multilingual models. To accomplish this, we contribute a parallel dataset of poetry translations for several language pairs. Our results show that multilingual fine-tuning on poetic text significantly outperforms multilingual fine-tuning on non-poetic text that is 35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and human evaluation metrics such as faithfulness (meaning and poetic style). Moreover, multilingual fine-tuning on poetic data outperforms \emph{bilingual} fine-tuning on poetic data.

| Comments: | EMNLP 2021 Camera ready                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.02972](https://arxiv.org/abs/2109.02972) [cs.CL]** |
|           | (or **[arXiv:2109.02972v1](https://arxiv.org/abs/2109.02972v1) [cs.CL]** for this version) |





<h2 id="2021-09-08-3">3. Revisiting Context Choices for Context-aware Machine Translation
</h2>

Title: [Revisiting Context Choices for Context-aware Machine Translation](https://arxiv.org/abs/2109.02995)

Authors: [Matīss Rikters](https://arxiv.org/search/cs?searchtype=author&query=Rikters%2C+M), [Toshiaki Nakazawa](https://arxiv.org/search/cs?searchtype=author&query=Nakazawa%2C+T)

> One of the most popular methods for context-aware machine translation (MT) is to use separate encoders for the source sentence and context as multiple sources for one target sentence. Recent work has cast doubt on whether these models actually learn useful signals from the context or are improvements in automatic evaluation metrics just a side-effect. We show that multi-source transformer models improve MT over standard transformer-base models even with empty lines provided as context, but the translation quality improves significantly (1.51 - 2.65 BLEU) when a sufficient amount of correct context is provided. We also show that even though randomly shuffling in-domain context can also improve over baselines, the correct context further improves translation quality and random out-of-domain context further degrades it.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.02995](https://arxiv.org/abs/2109.02995) [cs.CL]** |
|           | (or **[arXiv:2109.02995v1](https://arxiv.org/abs/2109.02995v1) [cs.CL]** for this version) |





<h2 id="2021-09-08-4">4. Generate & Rank: A Multi-task Framework for Math Word Problems
</h2>

Title: [Generate & Rank: A Multi-task Framework for Math Word Problems](https://arxiv.org/abs/2109.03034)

Authors: [Jianhao Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J), [Yichun Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Lin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Lifeng Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Ming Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate & Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7% (78.4% → 85.4%) higher than the state-of-the-art.

| Comments: | Findings of EMNLP2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.03034](https://arxiv.org/abs/2109.03034) [cs.CL]** |
|           | (or **[arXiv:2109.03034v1](https://arxiv.org/abs/2109.03034v1) [cs.CL]** for this version) |





<h2 id="2021-09-08-5">5. NumGPT: Improving Numeracy Ability of Generative Pre-trained Models
</h2>

Title: [NumGPT: Improving Numeracy Ability of Generative Pre-trained Models](https://arxiv.org/abs/2109.03137)

Authors: [Zhihua Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Z), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Xingbo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Yong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Xiaozhe Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Huamin Qu](https://arxiv.org/search/cs?searchtype=author&query=Qu%2C+H)

> Existing generative pre-trained language models (e.g., GPT) focus on modeling the language structure and semantics of general texts. However, those models do not consider the numerical properties of numbers and cannot perform robustly on numerical reasoning tasks (e.g., math word problems and measurement estimation). In this paper, we propose NumGPT, a generative pre-trained model that explicitly models the numerical properties of numbers in texts. Specifically, it leverages a prototype-based numeral embedding to encode the mantissa of the number and an individual embedding to encode the exponent of the number. A numeral-aware loss function is designed to integrate numerals into the pre-training objective of NumGPT. We conduct extensive experiments on four different datasets to evaluate the numeracy ability of NumGPT. The experiment results show that NumGPT outperforms baseline models (e.g., GPT and GPT with DICE) on a range of numerical reasoning tasks such as measurement estimation, number comparison, math word problems, and magnitude classification. Ablation studies are also conducted to evaluate the impact of pre-training and model hyperparameters on the performance.

| Comments: | 8 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.03137](https://arxiv.org/abs/2109.03137) [cs.CL]** |
|           | (or **[arXiv:2109.03137v1](https://arxiv.org/abs/2109.03137v1) [cs.CL]** for this version) |





# 2021-09-07

[Return to Index](#Index)



<h2 id="2021-09-07-1">1. Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models
</h2>

Title: [Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models](https://arxiv.org/abs/2109.01754)

Authors: [Rakesh Chada](https://arxiv.org/search/cs?searchtype=author&query=Chada%2C+R), [Pradeep Natarajan](https://arxiv.org/search/cs?searchtype=author&query=Natarajan%2C+P), [Darshan Fofadiya](https://arxiv.org/search/cs?searchtype=author&query=Fofadiya%2C+D), [Prathap Ramachandra](https://arxiv.org/search/cs?searchtype=author&query=Ramachandra%2C+P)

> Large-scale conversational assistants like Alexa, Siri, Cortana and Google Assistant process every utterance using multiple models for domain, intent and named entity recognition. Given the decoupled nature of model development and large traffic volumes, it is extremely difficult to identify utterances processed erroneously by such systems. We address this challenge to detect domain classification errors using offline Transformer models. We combine utterance encodings from a RoBERTa model with the Nbest hypothesis produced by the production system. We then fine-tune end-to-end in a multitask setting using a small dataset of humanannotated utterances with domain classification errors. We tested our approach for detecting misclassifications from one domain that accounts for <0.5% of the traffic in a large-scale conversational AI system. Our approach achieves an F1 score of 30% outperforming a bi- LSTM baseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this further by 2.2% to 32.2% by ensembling multiple models.

| Comments: | Accepted to ACL Findings 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.01754](https://arxiv.org/abs/2109.01754) [cs.CL]** |
|           | (or **[arXiv:2109.01754v1](https://arxiv.org/abs/2109.01754v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-2">2. On the ability of monolingual models to learn language-agnostic representations
</h2>

Title: [On the ability of monolingual models to learn language-agnostic representations](https://arxiv.org/abs/2109.01942)

Authors: [Leandro Rodrigues de Souza](https://arxiv.org/search/cs?searchtype=author&query=de+Souza%2C+L+R), [Rodrigo Nogueira](https://arxiv.org/search/cs?searchtype=author&query=Nogueira%2C+R), [Roberto Lotufo](https://arxiv.org/search/cs?searchtype=author&query=Lotufo%2C+R)

> Pretrained multilingual models have become a de facto default approach for zero-shot cross-lingual transfer. Previous work has shown that these models are able to achieve cross-lingual representations when pretrained on two or more languages with shared parameters. In this work, we provide evidence that a model can achieve language-agnostic representations even when pretrained on a single language. That is, we find that monolingual models pretrained and finetuned on different languages achieve competitive performance compared to the ones that use the same target language. Surprisingly, the models show a similar performance on a same task regardless of the pretraining language. For example, models pretrained on distant languages such as German and Portuguese perform similarly on English tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01942](https://arxiv.org/abs/2109.01942) [cs.CL]** |
|           | (or **[arXiv:2109.01942v1](https://arxiv.org/abs/2109.01942v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-3">3. Counterfactual Evaluation for Explainable AI
</h2>

Title: [Counterfactual Evaluation for Explainable AI](https://arxiv.org/abs/2109.01962)

Authors: [Yingqiang Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+Y), [Shuchang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Zelong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Shuyuan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S), [Shijie Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+S), [Yunqi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Juntao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+J), [Fei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+F), [Yongfeng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> While recent years have witnessed the emergence of various explainable methods in machine learning, to what degree the explanations really represent the reasoning process behind the model prediction -- namely, the faithfulness of explanation -- is still an open problem. One commonly used way to measure faithfulness is \textit{erasure-based} criteria. Though conceptually simple, erasure-based criterion could inevitably introduce biases and artifacts. We propose a new methodology to evaluate the faithfulness of explanations from the \textit{counterfactual reasoning} perspective: the model should produce substantially different outputs for the original input and its corresponding counterfactual edited on a faithful feature. Specially, we introduce two algorithms to find the proper counterfactuals in both discrete and continuous scenarios and then use the acquired counterfactuals to measure faithfulness. Empirical results on several datasets show that compared with existing metrics, our proposed counterfactual evaluation method can achieve top correlation with the ground truth under diffe

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01962](https://arxiv.org/abs/2109.01962) [cs.CL]** |
|           | (or **[arXiv:2109.01962v1](https://arxiv.org/abs/2109.01962v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-4">4. Data Efficient Masked Language Modeling for Vision and Language
</h2>

Title: [Data Efficient Masked Language Modeling for Vision and Language](https://arxiv.org/abs/2109.02040)

Authors: [Yonatan Bitton](https://arxiv.org/search/cs?searchtype=author&query=Bitton%2C+Y), [Gabriel Stanovsky](https://arxiv.org/search/cs?searchtype=author&query=Stanovsky%2C+G), [Michael Elhadad](https://arxiv.org/search/cs?searchtype=author&query=Elhadad%2C+M), [Roy Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+R)

> Masked language modeling (MLM) is one of the key sub-tasks in vision-language pretraining. In the cross-modal setting, tokens in the sentence are masked at random, and the model predicts the masked tokens given the image and the text. In this paper, we observe several key disadvantages of MLM in this setting. First, as captions tend to be short, in a third of the sentences no token is sampled. Second, the majority of masked tokens are stop-words and punctuation, leading to under-utilization of the image. We investigate a range of alternative masking strategies specific to the cross-modal setting that address these shortcomings, aiming for better fusion of text and image in the learned representation. When pre-training the LXMERT model, our alternative masking strategies consistently improve over the original masking strategy on three downstream tasks, especially in low resource settings. Further, our pre-training approach substantially outperforms the baseline model on a prompt-based probing task designed to elicit image objects. These results and our analysis indicate that our method allows for better utilization of the training data.

| Comments: | Accepted to Findings of EMNLP 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.02040](https://arxiv.org/abs/2109.02040) [cs.CL]** |
|           | (or **[arXiv:2109.02040v1](https://arxiv.org/abs/2109.02040v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-5">5. Teaching Autoregressive Language Models Complex Tasks By Demonstration
</h2>

Title: [Teaching Autoregressive Language Models Complex Tasks By Demonstration](https://arxiv.org/abs/2109.02102)

Authors: [Gabriel Recchia](https://arxiv.org/search/cs?searchtype=author&query=Recchia%2C+G)

> This paper demonstrates that by fine-tuning an autoregressive language model (GPT-Neo) on appropriately structured step-by-step demonstrations, it is possible to teach it to execute a mathematical task that has previously proved difficult for Transformers - longhand modulo operations - with a relatively small number of examples. Specifically, we fine-tune GPT-Neo to solve the numbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et al. ([arXiv:1904.01557](https://arxiv.org/abs/1904.01557)) reported below 40% accuracy on this task with 2 million training examples. We show that after fine-tuning on 200 appropriately structured demonstrations of solving long division problems and reporting the remainders, the smallest available GPT-Neo model achieves over 80% accuracy. This is achieved by constructing an appropriate dataset for fine-tuning, with no changes to the learning algorithm. These results suggest that fine-tuning autoregressive language models on small sets of well-crafted demonstrations may be a useful paradigm for enabling individuals without training in machine learning to coax such models to perform some kinds of complex multi-step tasks.

| Comments:    | 15 pages, 2 tables, 4 figures. Associated code and data available at [this https URL](https://github.com/mesotron/teaching_transformers) |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ACM classes: | I.2.0; I.2.6                                                 |
| Cite as:     | **[arXiv:2109.02102](https://arxiv.org/abs/2109.02102) [cs.CL]** |
|              | (or **[arXiv:2109.02102v1](https://arxiv.org/abs/2109.02102v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-6">6. Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack
</h2>

Title: [Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack](https://arxiv.org/abs/2109.02229)

Authors: [Shengcai Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Ning Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+N), [Cheng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Ke Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+K)

> Over the past few years, various word-level textual attack approaches have been proposed to reveal the vulnerability of deep neural networks used in natural language processing. Typically, these approaches involve an important optimization step to determine which substitute to be used for each word in the original input. However, current research on this step is still rather limited, from the perspectives of both problem-understanding and problem-solving. In this paper, we address these issues by uncovering the theoretical properties of the problem and proposing an efficient local search algorithm (LS) to solve it. We establish the first provable approximation guarantee on solving the problem in general cases. Notably, for adversarial textual attack, it is even better than the previous bound which only holds in special case. Extensive experiments involving five NLP tasks, six datasets and eleven NLP models show that LS can largely reduce the number of queries usually by an order of magnitude to achieve high attack success rates. Further experiments show that the adversarial examples crafted by LS usually have higher quality, exhibit better transferability, and can bring more robustness improvement to victim models by adversarial training.

| Subjects: | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.02229](https://arxiv.org/abs/2109.02229) [cs.CL]** |
|           | (or **[arXiv:2109.02229v1](https://arxiv.org/abs/2109.02229v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-7">7. Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training
</h2>

Title: [Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training](https://arxiv.org/abs/2109.02284)

Authors: [Minghao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+M), [Yitong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Meng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the model converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the high resource ones. However, automatic balancing methods usually depend on the intra- and inter-dataset characteristics, which is usually agnostic or requires human priors. In this work, we propose an approach, MultiUAT, that dynamically adjusts the training data usage based on the model's uncertainty on a small set of trusted clean data for multi-corpus machine translation. We experiments with two classes of uncertainty measures on multilingual (16 languages with 4 settings) and multi-domain settings (4 for in-domain and 2 for out-of-domain on English-German translation) and demonstrate our approach MultiUAT substantially outperforms its baselines, including both static and dynamic strategies. We analyze the cross-domain transfer and show the deficiency of static and similarity based methods.

| Comments: | 15 pages, 4 figures, to appear at EMNLP 2021 main conference |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.02284](https://arxiv.org/abs/2109.02284) [cs.CL]** |
|           | (or **[arXiv:2109.02284v1](https://arxiv.org/abs/2109.02284v1) [cs.CL]** for this version) |








# 2021-09-06

[Return to Index](#Index)



<h2 id="2021-09-06-1">1. Ranking Scientific Papers Using Preference Learning
</h2>

Title: [Ranking Scientific Papers Using Preference Learning](https://arxiv.org/abs/2109.01190)

Authors: [Nils Dycke](https://arxiv.org/search/cs?searchtype=author&query=Dycke%2C+N), [Edwin Simpson](https://arxiv.org/search/cs?searchtype=author&query=Simpson%2C+E), [Ilia Kuznetsov](https://arxiv.org/search/cs?searchtype=author&query=Kuznetsov%2C+I), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> Peer review is the main quality control mechanism in academia. Quality of scientific work has many dimensions; coupled with the subjective nature of the reviewing task, this makes final decision making based on the reviews and scores therein very difficult and time-consuming. To assist with this important task, we cast it as a paper ranking problem based on peer review texts and reviewer scores. We introduce a novel, multi-faceted generic evaluation framework for making final decisions based on peer reviews that takes into account effectiveness, efficiency and fairness of the evaluated system. We propose a novel approach to paper ranking based on Gaussian Process Preference Learning (GPPL) and evaluate it on peer review data from the ACL-2018 conference. Our experiments demonstrate the superiority of our GPPL-based approach over prior work, while highlighting the importance of using both texts and review scores for paper ranking during peer review aggregation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01190](https://arxiv.org/abs/2109.01190) [cs.CL]** |
|           | (or **[arXiv:2109.01190v1](https://arxiv.org/abs/2109.01190v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-2">2. Establishing Interlingua in Multilingual Language Models
</h2>

Title: [Establishing Interlingua in Multilingual Language Models](https://arxiv.org/abs/2109.01207)

Authors: [Maksym Del](https://arxiv.org/search/cs?searchtype=author&query=Del%2C+M), [Mark Fishel](https://arxiv.org/search/cs?searchtype=author&query=Fishel%2C+M)

> Large multilingual language models show remarkable zero-shot cross-lingual transfer performance on a range of tasks. Follow-up works hypothesized that these models internally project representations of different languages into a shared interlingual space. However, they produced contradictory results. In this paper, we correct %one of the previous works the famous prior work claiming that "BERT is not an Interlingua" and show that with the proper choice of sentence representation different languages actually do converge to a shared space in such language models. Furthermore, we demonstrate that this convergence pattern is robust across four measures of correlation similarity and six mBERT-like models. We then extend our analysis to 28 diverse languages and find that the interlingual space exhibits a particular structure similar to the linguistic relatedness of languages. We also highlight a few outlier languages that seem to fail to converge to the shared space. The code for replicating our results is available at the following URL: [this https URL](https://github.com/maksym-del/interlingua).

| Comments:    | 8 pages, 10 figures                                          |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7; I.2.6                                                 |
| Cite as:     | **[arXiv:2109.01207](https://arxiv.org/abs/2109.01207) [cs.CL]** |
|              | (or **[arXiv:2109.01207v1](https://arxiv.org/abs/2109.01207v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-3">3. Quantifying Reproducibility in NLP and ML
</h2>

Title: [Quantifying Reproducibility in NLP and ML](https://arxiv.org/abs/2109.01211)

Authors: [Anya Belz](https://arxiv.org/search/cs?searchtype=author&query=Belz%2C+A)

> Reproducibility has become an intensely debated topic in NLP and ML over recent years, but no commonly accepted way of assessing reproducibility, let alone quantifying it, has so far emerged. The assumption has been that wider scientific reproducibility terminology and definitions are not applicable to NLP/ML, with the result that many different terms and definitions have been proposed, some diametrically opposed. In this paper, we test this assumption, by taking the standard terminology and definitions from metrology and applying them directly to NLP/ML. We find that we are able to straightforwardly derive a practical framework for assessing reproducibility which has the desirable property of yielding a quantified degree of reproducibility that is comparable across different reproduction studies.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01211](https://arxiv.org/abs/2109.01211) [cs.CL]** |
|           | (or **[arXiv:2109.01211v1](https://arxiv.org/abs/2109.01211v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-4">4. Multimodal Conditionality for Natural Language Generation
</h2>

Title: [Multimodal Conditionality for Natural Language Generation](https://arxiv.org/abs/2109.01229)

Authors: [Michael Sollami](https://arxiv.org/search/cs?searchtype=author&query=Sollami%2C+M), [Aashish Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+A)

> Large scale pretrained language models have demonstrated state-of-the-art performance in language understanding tasks. Their application has recently expanded into multimodality learning, leading to improved representations combining vision and language. However, progress in adapting language models towards conditional Natural Language Generation (NLG) has been limited to a single modality, generally text. We propose MAnTiS, Multimodal Adaptation for Text Synthesis, a general approach for multimodal conditionality in transformer-based NLG models. In this method, we pass inputs from each modality through modality-specific encoders, project to textual token space, and finally join to form a conditionality prefix. We fine-tune the pretrained language model and encoders with the conditionality prefix guiding the generation. We apply MAnTiS to the task of product description generation, conditioning a network on both product images and titles to generate descriptive text. We demonstrate that MAnTiS outperforms strong baseline approaches on standard NLG scoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS can generate human quality descriptions consistent with given multimodal inputs.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01229](https://arxiv.org/abs/2109.01229) [cs.CL]** |
|           | (or **[arXiv:2109.01229v1](https://arxiv.org/abs/2109.01229v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-5">5. Do Prompt-Based Models Really Understand the Meaning of their Prompts?
</h2>

Title: [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/abs/2109.01247)

Authors: [Albert Webson](https://arxiv.org/search/cs?searchtype=author&query=Webson%2C+A), [Ellie Pavlick](https://arxiv.org/search/cs?searchtype=author&query=Pavlick%2C+E)

> Recently, a boom of papers have shown extraordinary progress in few-shot learning with various prompt-based models. Such success can give the impression that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompts manually written for natural language inference (NLI). We find that models learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively "good" prompts. Additionally, we find that model performance is more dependent on the choice of the LM target words (a.k.a. the "verbalizer" that converts LM vocabulary prediction to class labels) than on the text of the prompt itself. In sum, we find little evidence that suggests existing prompt-based models truly understand the meaning of their given prompts.

| Comments: | Code available at [this https URL](https://github.com/awebson/prompt_semantics) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.01247](https://arxiv.org/abs/2109.01247) [cs.CL]** |
|           | (or **[arXiv:2109.01247v1](https://arxiv.org/abs/2109.01247v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-6">6. Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT
</h2>

Title: [Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT](https://arxiv.org/abs/2109.01396)

Authors: [Elena Voita](https://arxiv.org/search/cs?searchtype=author&query=Voita%2C+E), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I)

> Differently from the traditional statistical MT that decomposes the translation task into distinct separately learned components, neural machine translation uses a single neural network to model the entire translation process. Despite neural machine translation being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different models in traditional SMT. In this work, we look at the competences related to three core SMT components and find that during training, NMT first focuses on learning target-side language modeling, then improves translation quality approaching word-by-word translation, and finally learns more complicated reordering patterns. We show that this behavior holds for several models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla non-autoregressive neural machine translation by guiding teacher model selection.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.01396](https://arxiv.org/abs/2109.01396) [cs.CL]** |
|           | (or **[arXiv:2109.01396v1](https://arxiv.org/abs/2109.01396v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-7">7. Finetuned Language Models Are Zero-Shot Learners
</h2>

Title: [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)

Authors: [Jason Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+J), [Maarten Bosma](https://arxiv.org/search/cs?searchtype=author&query=Bosma%2C+M), [Vincent Y. Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+V+Y), [Kelvin Guu](https://arxiv.org/search/cs?searchtype=author&query=Guu%2C+K), [Adams Wei Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+A+W), [Brian Lester](https://arxiv.org/search/cs?searchtype=author&query=Lester%2C+B), [Nan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+N), [Andrew M. Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+A+M), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V)

> This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially boosts zero-shot performance on unseen tasks.
> We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 19 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of tasks and model scale are key components to the success of instruction tuning.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01652](https://arxiv.org/abs/2109.01652) [cs.CL]** |
|           | (or **[arXiv:2109.01652v1](https://arxiv.org/abs/2109.01652v1) [cs.CL]** for this version) |








# 2021-09-03

[Return to Index](#Index)



<h2 id="2021-09-03-1">1. Skim-Attention: Learning to Focus via Document Layout
</h2>

Title: [Skim-Attention: Learning to Focus via Document Layout](https://arxiv.org/abs/2109.01078)

Authors: [Laura Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+L), [Thomas Scialom](https://arxiv.org/search/cs?searchtype=author&query=Scialom%2C+T), [Jacopo Staiano](https://arxiv.org/search/cs?searchtype=author&query=Staiano%2C+J), [Benjamin Piwowarski](https://arxiv.org/search/cs?searchtype=author&query=Piwowarski%2C+B)

> Transformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success, multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies, this paper presents Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout. Skim-Attention only attends to the 2-dimensional position of the words in a document. Our experiments show that Skim-Attention obtains a lower perplexity than prior works, while being more computationally efficient. Skim-Attention can be further combined with long-range Transformers to efficiently process long documents. We also show how Skim-Attention can be used off-the-shelf as a mask for any Pre-trained Language Model, allowing to improve their performance while restricting attention. Finally, we show the emergence of a document structure representation in Skim-Attention.

| Comments: | 15 pages, 6 figures, to be published in EMNLP 2021 Findings  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.01078](https://arxiv.org/abs/2109.01078) [cs.CL]** |
|           | (or **[arXiv:2109.01078v1](https://arxiv.org/abs/2109.01078v1) [cs.CL]** for this version) |





<h2 id="2021-09-03-2">2. How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?
</h2>

Title: [How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?](https://arxiv.org/abs/2109.01100)

Authors: [Chantal Amrhein](https://arxiv.org/search/cs?searchtype=author&query=Amrhein%2C+C), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Data-driven subword segmentation has become the default strategy for open-vocabulary machine translation and other NLP tasks, but may not be sufficiently generic for optimal learning of non-concatenative morphology. We design a test suite to evaluate segmentation strategies on different types of morphological phenomena in a controlled, semi-synthetic setting. In our experiments, we compare how well machine translation models trained on subword- and character-level can translate these morphological phenomena. We find that learning to analyse and generate morphologically complex surface representations is still challenging, especially for non-concatenative morphological phenomena like reduplication or vowel harmony and for rare word stems. Based on our results, we recommend that novel text representation strategies be tested on a range of typologically diverse languages to minimise the risk of adopting a strategy that inadvertently disadvantages certain languages.

| Comments:    | Findings of EMNLP 2021                                       |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2109.01100](https://arxiv.org/abs/2109.01100) [cs.CL]** |
|              | (or **[arXiv:2109.01100v1](https://arxiv.org/abs/2109.01100v1) [cs.CL]** for this version) |





<h2 id="2021-09-03-3">3. Sequence-to-Sequence Learning with Latent Neural Grammars
</h2>

Title: [Sequence-to-Sequence Learning with Latent Neural Grammars](https://arxiv.org/abs/2109.01135)

Authors: [Yoon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y)

> Sequence-to-sequence learning with neural networks has become the de facto standard for sequence prediction tasks. This approach typically models the local distribution over the next word with a powerful neural network that can condition on arbitrary context. While flexible and performant, these models often require large datasets for training and can fail spectacularly on benchmarks designed to test for compositional generalization. This work explores an alternative, hierarchical approach to sequence-to-sequence learning with quasi-synchronous grammars, where each node in the target tree is transduced by a node in the source tree. Both the source and target trees are treated as latent and induced during training. We develop a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. We apply this latent neural grammar to various domains -- a diagnostic language navigation task designed to test for compositional generalization (SCAN), style transfer, and small-scale machine translation -- and find that it performs respectably compared to standard baselines.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.01135](https://arxiv.org/abs/2109.01135) [cs.CL]** |
|           | (or **[arXiv:2109.01135v1](https://arxiv.org/abs/2109.01135v1) [cs.CL]** for this version) |





<h2 id="2021-09-03-4">4. Knowledge Perceived Multi-modal Pretraining in E-commerce
</h2>

Title: [Knowledge Perceived Multi-modal Pretraining in E-commerce](https://arxiv.org/abs/2109.00895)

Authors: [Yushan Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Y), [Huaixiao Tou](https://arxiv.org/search/cs?searchtype=author&query=Tou%2C+H), [Wen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Ganqiang Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+G), [Hui Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H), [Ningyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+N), [Huajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H)

> In this paper, we address multi-modal pretraining of product data in the field of E-commerce. Current multi-modal pretraining methods proposed for image and text modalities lack robustness in the face of modality-missing and modality-noise, which are two pervasive problems of multi-modal product data in real E-commerce scenarios. To this end, we propose a novel method, K3M, which introduces knowledge modality in multi-modal pretraining to correct the noise and supplement the missing of image and text modalities. The modal-encoding layer extracts the features of each modality. The modal-interaction layer is capable of effectively modeling the interaction of multiple modalities, where an initial-interactive feature fusion model is designed to maintain the independence of image modality and text modality, and a structure aggregation module is designed to fuse the information of image, text, and knowledge modalities. We pretrain K3M with three pretraining tasks, including masked object modeling (MOM), masked language modeling (MLM), and link prediction modeling (LPM). Experimental results on a real-world E-commerce dataset and a series of product-based downstream tasks demonstrate that K3M achieves significant improvements in performances than the baseline and state-of-the-art methods when modality-noise or modality-missing exists.

| Comments: | Accepted to ACM MM 2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| DOI:      | [10.1145/3474085.3475648](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3474085.3475648&v=072100ff) |
| Cite as:  | **[arXiv:2109.00895](https://arxiv.org/abs/2109.00895) [cs.CV]** |
|           | (or **[arXiv:2109.00895v1](https://arxiv.org/abs/2109.00895v1) [cs.CV]** for this version) |





<h2 id="2021-09-03-5">5. Improving Multimodal fusion via Mutual Dependency Maximisation
</h2>

Title: [Improving Multimodal fusion via Mutual Dependency Maximisation](https://arxiv.org/abs/2109.00922)

Authors: [Pierre Colombo](https://arxiv.org/search/cs?searchtype=author&query=Colombo%2C+P), [Emile Chapuis](https://arxiv.org/search/cs?searchtype=author&query=Chapuis%2C+E), [Matthieu Labeau](https://arxiv.org/search/cs?searchtype=author&query=Labeau%2C+M), [Chloe Clavel](https://arxiv.org/search/cs?searchtype=author&query=Clavel%2C+C)

> Multimodal sentiment analysis is a trending area of research, and the multimodal fusion is one of its most active topic. Acknowledging humans communicate through a variety of channels (i.e visual, acoustic, linguistic), multimodal systems aim at integrating different unimodal representations into a synthetic one. So far, a consequent effort has been made on developing complex architectures allowing the fusion of these modalities. However, such systems are mainly trained by minimising simple losses such as L1 or cross-entropy. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to 4.3 on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets: \texttt{CMU-MOSI} and \texttt{CMU-MOSEI}. Our method not only achieves a new SOTA on both datasets but also produces representations that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the model.

| Subjects:          | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | EMNLP 2021                                                   |
| Cite as:           | **[arXiv:2109.00922](https://arxiv.org/abs/2109.00922) [cs.LG]** |
|                    | (or **[arXiv:2109.00922v1](https://arxiv.org/abs/2109.00922v1) [cs.LG]** for this version) |





<h2 id="2021-09-03-6">6. Towards Improving Adversarial Training of NLP Models
</h2>

Title: [Towards Improving Adversarial Training of NLP Models](https://arxiv.org/abs/2109.00544)

Authors: [Jin Yong Yoo](https://arxiv.org/search/cs?searchtype=author&query=Yoo%2C+J+Y), [Yanjun Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+Y)

> Adversarial training, a method for learning robust deep neural networks, constructs adversarial examples during training. However, recent methods for generating NLP adversarial examples involve combinatorial search and expensive sentence encoders for constraining the generated instances. As a result, it remains challenging to use vanilla adversarial training to improve NLP models' performance, and the benefits are mainly uninvestigated. This paper proposes a simple and improved vanilla adversarial training process for NLP, which we name Attacking to Training (𝙰𝟸𝚃). The core part of 𝙰𝟸𝚃 is a new and cheaper word substitution attack optimized for vanilla adversarial training. We use 𝙰𝟸𝚃 to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI datasets. Our results show that it is possible to train empirically robust NLP models using a much cheaper adversary. We demonstrate that vanilla adversarial training with 𝙰𝟸𝚃 can improve an NLP model's robustness to the attack it was originally trained with and also defend the model against other types of attacks. Furthermore, we show that 𝙰𝟸𝚃 can improve NLP models' standard accuracy, cross-domain generalization, and interpretability. Code is available at [this http URL](http://github.com/jinyongyoo/A2T) .

| Comments: | EMNLP Findings 2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.00544](https://arxiv.org/abs/2109.00544) [cs.CL]** |
|           | (or **[arXiv:2109.00544v1](https://arxiv.org/abs/2109.00544v1) [cs.CL]** for this version) |





<h2 id="2021-09-03-7">7. Point-of-Interest Type Prediction using Text and Images
</h2>

Title: [Point-of-Interest Type Prediction using Text and Images](https://arxiv.org/abs/2109.00602)

Authors: [Danae Sánchez Villegas](https://arxiv.org/search/cs?searchtype=author&query=Villegas%2C+D+S), [Nikolaos Aletras](https://arxiv.org/search/cs?searchtype=author&query=Aletras%2C+N)

> Point-of-interest (POI) type prediction is the task of inferring the type of a place from where a social media post was shared. Inferring a POI's type is useful for studies in computational social science including sociolinguistics, geosemiotics, and cultural geography, and has applications in geosocial networking technologies such as recommendation and visualization systems. Prior efforts in POI type prediction focus solely on text, without taking visual information into account. However in reality, the variety of modalities, as well as their semiotic relationships with one another, shape communication and interactions in social media. This paper presents a study on POI type prediction using multimodal information from text and images available at posting time. For that purpose, we enrich a currently available data set for POI type prediction with the images that accompany the text messages. Our proposed method extracts relevant information from each modality to effectively capture interactions between text and image achieving a macro F1 of 47.21 across eight categories significantly outperforming the state-of-the-art method for POI type prediction based on text-only methods. Finally, we provide a detailed analysis to shed light on cross-modal interactions and the limitations of our best performing model.

| Comments: | Accepted at EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.00602](https://arxiv.org/abs/2109.00602) [cs.CL]** |
|           | (or **[arXiv:2109.00602v1](https://arxiv.org/abs/2109.00602v1) [cs.CL]** for this version) |





<h2 id="2021-09-03-8">8. Towards Making the Most of Dialogue Characteristics for Neural Chat Translation
</h2>

Title: [Towards Making the Most of Dialogue Characteristics for Neural Chat Translation](https://arxiv.org/abs/2109.00668)

Authors: [Yunlong Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Chulun Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jinan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Yufeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Neural Chat Translation (NCT) aims to translate conversational text between speakers of different languages. Despite the promising performance of sentence-level and context-aware neural machine translation models, there still remain limitations in current NCT models because the inherent dialogue characteristics of chat, such as dialogue coherence and speaker personality, are neglected. In this paper, we propose to promote the chat translation by introducing the modeling of dialogue characteristics into the NCT model. To this end, we design four auxiliary tasks including monolingual response generation, cross-lingual response generation, next utterance discrimination, and speaker identification. Together with the main chat translation task, we optimize the NCT model through the training objectives of all these tasks. By this means, the NCT model can be enhanced by capturing the inherent dialogue characteristics, thus generating more coherent and speaker-relevant translations. Comprehensive experiments on four language directions (English-German and English-Chinese) verify the effectiveness and superiority of the proposed approach.

| Comments: | Accepted as a long paper at EMNLP 2021 main conference. The first two authors contributed equally. Code: [this https URL](https://github.com/XL2248/CSA-NCT) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.00668](https://arxiv.org/abs/2109.00668) [cs.CL]** |
|           | (or **[arXiv:2109.00668v1](https://arxiv.org/abs/2109.00668v1) [cs.CL]** for this version) |





# 2021-09-02

[Return to Index](#Index)



<h2 id="2021-09-02-1">1. Sentence Bottleneck Autoencoders from Transformer Language Models
</h2>

Title: [Sentence Bottleneck Autoencoders from Transformer Language Models](https://arxiv.org/abs/2109.00055)

Authors: [Ivan Montero](https://arxiv.org/search/cs?searchtype=author&query=Montero%2C+I), [Nikolaos Pappas](https://arxiv.org/search/cs?searchtype=author&query=Pappas%2C+N), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> Representation learning for text via pretraining a language model on a large corpus has become a standard starting point for building NLP systems. This approach stands in contrast to autoencoders, also trained on raw text, but with the objective of learning to encode each input as a vector that allows full reconstruction. Autoencoders are attractive because of their latent space structure and generative properties. We therefore explore the construction of a sentence-level autoencoder from a pretrained, frozen transformer language model. We adapt the masked language modeling objective as a generative, denoising one, while only training a sentence bottleneck and a single-layer modified transformer decoder. We demonstrate that the sentence representations discovered by our model achieve better quality than previous methods that extract representations from pretrained transformers on text similarity tasks, style transfer (an example of controlled generation), and single-sentence classification tasks in the GLUE benchmark, while using fewer parameters than large pretrained models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.00055](https://arxiv.org/abs/2109.00055) [cs.CL]** |
|           | (or **[arXiv:2109.00055v1](https://arxiv.org/abs/2109.00055v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-2">2. It's not Rocket Science : Interpreting Figurative Language in Narratives
</h2>

Title: [It's not Rocket Science : Interpreting Figurative Language in Narratives](https://arxiv.org/abs/2109.00087)

Authors: [Tuhin Chakrabarty](https://arxiv.org/search/cs?searchtype=author&query=Chakrabarty%2C+T), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Vered Shwartz](https://arxiv.org/search/cs?searchtype=author&query=Shwartz%2C+V)

> Figurative language is ubiquitous in English. Yet, the vast majority of NLP research focuses on literal language. Existing text representations by design rely on compositionality, while figurative language is often non-compositional. In this paper, we study the interpretation of two non-compositional figurative languages (idioms and similes). We collected datasets of fictional narratives containing a figurative expression along with crowd-sourced plausible and implausible continuations relying on the correct interpretation of the expression. We then trained models to choose or generate the plausible continuation. Our experiments show that models based solely on pre-trained language models perform substantially worse than humans on these tasks. We additionally propose knowledge-enhanced models, adopting human strategies for interpreting figurative language: inferring meaning from the context and relying on the constituent word's literal meanings. The knowledge-enhanced models improve the performance on both the discriminative and generative tasks, further bridging the gap from human performance.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.00087](https://arxiv.org/abs/2109.00087) [cs.CL]** |
|           | (or **[arXiv:2109.00087v1](https://arxiv.org/abs/2109.00087v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-3">3. Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast
</h2>

Title: [Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast](https://arxiv.org/abs/2109.00253)

Authors: [Liang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Wei Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Jingming Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> In this paper, we propose to align sentence representations from different languages into a unified embedding space, where semantic similarities (both cross-lingual and monolingual) can be computed with a simple dot product. Pre-trained language models are fine-tuned with the translation ranking task. Existing work (Feng et al., 2020) uses sentences within the same batch as negatives, which can suffer from the issue of easy negatives. We adapt MoCo (He et al., 2020) to further improve the quality of alignment. As the experimental results show, the sentence representations produced by our model achieve the new state-of-the-art on several tasks, including Tatoeba en-zh similarity search (Artetxe and Schwenk, 2019b), BUCC en-zh bitext mining, and semantic textual similarity on 7 datasets.

| Comments: | Accepted to EMNLP 2021 main conference                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| Cite as:  | **[arXiv:2109.00253](https://arxiv.org/abs/2109.00253) [cs.CL]** |
|           | (or **[arXiv:2109.00253v1](https://arxiv.org/abs/2109.00253v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-4">4. Discovering Representation Sprachbund For Multilingual Pre-Training
</h2>

Title: [Discovering Representation Sprachbund For Multilingual Pre-Training](https://arxiv.org/abs/2109.00271)

Authors: [Yimin Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+Y), [Yaobo Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Alexandre Muzio](https://arxiv.org/search/cs?searchtype=author&query=Muzio%2C+A), [Hany Hassan](https://arxiv.org/search/cs?searchtype=author&query=Hassan%2C+H), [Houqiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N)

> Multilingual pre-trained models have demonstrated their effectiveness in many multilingual NLP tasks and enabled zero-shot or few-shot transfer from high-resource languages to low resource ones. However, due to significant typological differences and contradictions between some languages, such models usually perform poorly on many languages and cross-lingual settings, which shows the difficulty of learning a single model to handle massive diverse languages well at the same time. To alleviate this issue, we present a new multilingual pre-training pipeline. We propose to generate language representation from multilingual pre-trained models and conduct linguistic analysis to show that language representation similarity reflect linguistic similarity from multiple perspectives, including language family, geographical sprachbund, lexicostatistics and syntax. Then we cluster all the target languages into multiple groups and name each group as a representation sprachbund. Thus, languages in the same representation sprachbund are supposed to boost each other in both pre-training and fine-tuning as they share rich linguistic similarity. We pre-train one multilingual model for each representation sprachbund. Experiments are conducted on cross-lingual benchmarks and significant improvements are achieved compared to strong baselines.

| Comments: | To Appear at the Findings of EMNLP2021                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.00271](https://arxiv.org/abs/2109.00271) [cs.CL]** |
|           | (or **[arXiv:2109.00271v1](https://arxiv.org/abs/2109.00271v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-5">5. ∞-former: Infinite Memory Transformer
</h2>

Title: [∞-former: Infinite Memory Transformer](https://arxiv.org/abs/2109.00301)

Authors: [Pedro Henrique Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+P+H), [Zita Marinho](https://arxiv.org/search/cs?searchtype=author&query=Marinho%2C+Z), [André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T)

> Transformers struggle when attending to long contexts, since the amount of computation grows with the context length, and therefore they cannot model long-term memories effectively. Several variations have been proposed to alleviate this problem, but they all have a finite memory capacity, being forced to drop old information. In this paper, we propose the ∞-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the ∞-former's attention complexity becomes independent of the context length. Thus, it is able to model arbitrarily long contexts and maintain "sticky memories" while keeping a fixed computation budget. Experiments on a synthetic sorting task demonstrate the ability of the ∞-former to retain information from long sequences. We also perform experiments on language modeling, by training a model from scratch and by fine-tuning a pre-trained language model, which show benefits of unbounded long-term memories.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.00301](https://arxiv.org/abs/2109.00301) [cs.CL]** |
|           | (or **[arXiv:2109.00301v1](https://arxiv.org/abs/2109.00301v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-6">6. Masked Adversarial Generation for Neural Machine Translation
</h2>

Title: [Masked Adversarial Generation for Neural Machine Translation](https://arxiv.org/abs/2109.00417)

Authors: [Badr Youbi Idrissi](https://arxiv.org/search/cs?searchtype=author&query=Idrissi%2C+B+Y), [Stéphane Clinchant](https://arxiv.org/search/cs?searchtype=author&query=Clinchant%2C+S)

> Attacking Neural Machine Translation models is an inherently combinatorial task on discrete sequences, solved with approximate heuristics. Most methods use the gradient to attack the model on each sample independently. Instead of mechanically applying the gradient, could we learn to produce meaningful adversarial attacks ? In contrast to existing approaches, we learn to attack a model by training an adversarial generator based on a language model. We propose the Masked Adversarial Generation (MAG) model, that learns to perturb the translation model throughout the training process. The experiments show that it improves the robustness of machine translation models, while being faster than competing methods.

| Comments: | 5 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.00417](https://arxiv.org/abs/2109.00417) [cs.CL]** |
|           | (or **[arXiv:2109.00417v1](https://arxiv.org/abs/2109.00417v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-7">7. Position Masking for Improved Layout-Aware Document Understanding
</h2>

Title: [Position Masking for Improved Layout-Aware Document Understanding](https://arxiv.org/abs/2109.00442)

Authors: [Anik Saha](https://arxiv.org/search/cs?searchtype=author&query=Saha%2C+A), [Catherine Finegan-Dollak](https://arxiv.org/search/cs?searchtype=author&query=Finegan-Dollak%2C+C), [Ashish Verma](https://arxiv.org/search/cs?searchtype=author&query=Verma%2C+A)

> Natural language processing for document scans and PDFs has the potential to enormously improve the efficiency of business processes. Layout-aware word embeddings such as LayoutLM have shown promise for classification of and information extraction from such documents. This paper proposes a new pre-training task called that can improve performance of layout-aware word embeddings that incorporate 2-D position embeddings. We compare models pre-trained with only language masking against models pre-trained with both language masking and position masking, and we find that position masking improves performance by over 5% on a form understanding task.

| Comments: | Document Intelligence Workshop at KDD, 2021                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.00442](https://arxiv.org/abs/2109.00442) [cs.CL]** |
|           | (or **[arXiv:2109.00442v1](https://arxiv.org/abs/2109.00442v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-8">8. Survey of Low-Resource Machine Translation
</h2>

Title: [Survey of Low-Resource Machine Translation](https://arxiv.org/abs/2109.00486)

Authors: [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B), [Rachel Bawden](https://arxiv.org/search/cs?searchtype=author&query=Bawden%2C+R), [Antonio Valerio Miceli Barone](https://arxiv.org/search/cs?searchtype=author&query=Barone%2C+A+V+M), [Jindřich Helcl](https://arxiv.org/search/cs?searchtype=author&query=Helcl%2C+J), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

> We present a survey covering the state of the art in low-resource machine translation. There are currently around 7000 languages spoken in the world and almost all language pairs lack significant resources for training machine translation models. There has been increasing interest in research addressing the challenge of producing useful translation models when very little translated training data is available. We present a high level summary of this topical field and provide an overview of best practices.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.00486](https://arxiv.org/abs/2109.00486) [cs.CL]** |
|           | (or **[arXiv:2109.00486v1](https://arxiv.org/abs/2109.00486v1) [cs.CL]** for this version) |








# 2021-09-01

[Return to Index](#Index)



<h2 id="2021-09-01-1">1. SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory
</h2>


Title: [SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory](https://arxiv.org/abs/2108.13630)

Authors: [Zhijie Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Zhou Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Haoyuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Jinglin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Meng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Xingshan Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Xiaofei He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X)

> Lip reading, aiming to recognize spoken sentences according to the given video of lip movements without relying on the audio stream, has attracted great interest due to its application in many scenarios. Although prior works that explore lip reading have obtained salient achievements, they are all trained in a non-simultaneous manner where the predictions are generated requiring access to the full video. To breakthrough this constraint, we study the task of simultaneous lip reading and devise SimulLR, a simultaneous lip Reading transducer with attention-guided adaptive memory from three aspects: (1) To address the challenge of monotonic alignments while considering the syntactic structure of the generated sentences under simultaneous setting, we build a transducer-based model and design several effective training strategies including CTC pre-training, model warm-up and curriculum learning to promote the training of the lip reading transducer. (2) To learn better spatio-temporal representations for simultaneous encoder, we construct a truncated 3D convolution and time-restricted self-attention layer to perform the frame-to-frame interaction within a video segment containing fixed number of frames. (3) The history information is always limited due to the storage in real-time scenarios, especially for massive video data. Therefore, we devise a novel attention-guided adaptive memory to organize semantic information of history segments and enhance the visual representations with acceptable computation-aware latency. The experiments show that the SimulLR achieves the translation speedup 9.10× compared with the state-of-the-art non-simultaneous methods, and also obtains competitive results, which indicates the effectiveness of our proposed methods.

| Comments: | ACMMM 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.13630](https://arxiv.org/abs/2108.13630) [cs.CV]** |
|           | (or **[arXiv:2108.13630v1](https://arxiv.org/abs/2108.13630v1) [cs.CV]** for this version) |





<h2 id="2021-09-01-2">2. Want To Reduce Labeling Cost? GPT-3 Can Help
</h2>


Title: [Want To Reduce Labeling Cost? GPT-3 Can Help](https://arxiv.org/abs/2108.13487)

Authors: [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Yichong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M)

> Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 175 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that, to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance with limited labeling budget. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.

| Comments: | Findings of EMNLP 2021, 11 pages                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2108.13487](https://arxiv.org/abs/2108.13487) [cs.CL]** |
|           | (or **[arXiv:2108.13487v1](https://arxiv.org/abs/2108.13487v1) [cs.CL]** for this version) |





<h2 id="2021-09-01-3">3. T3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP
</h2>


Title: [T3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP](https://arxiv.org/abs/2108.13587)

Authors: [Raymond Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+R) (1), [Wen Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+W) (1), [Lanjun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L) (2), [Hyeju Jang](https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+H) (1), [Giuseppe Carenini](https://arxiv.org/search/cs?searchtype=author&query=Carenini%2C+G) (1) ((1) University of British Columbia, (2) Huawei Cananda Technologies Co. Ltd.)

> Transformers are the dominant architecture in NLP, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the model's intrinsic properties and behaviours. Our framework offers an intuitive overview that allows the user to explore different facets of the model (e.g., hidden states, attention) through interactive visualization, and allows a suite of built-in algorithms that compute the importance of model components and different parts of the input sequence. Case studies and feedback from a user focus group indicate that the framework is useful, and suggest several improvements.

| Comments: | 10 pages, 4 figures, accepted to EMNLP 2021 System Demonstration |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2108.13587](https://arxiv.org/abs/2108.13587) [cs.CL]** |
|           | (or **[arXiv:2108.13587v1](https://arxiv.org/abs/2108.13587v1) [cs.CL]** for this version) |





<h2 id="2021-09-01-4">4. Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience
</h2>


Title: [Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience](https://arxiv.org/abs/2108.13759)

Authors: [George Chrysostomou](https://arxiv.org/search/cs?searchtype=author&query=Chrysostomou%2C+G), [Nikolaos Aletras](https://arxiv.org/search/cs?searchtype=author&query=Aletras%2C+N)

> Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this paper, we hypothesize that salient information extracted a priori from the training data can complement the task-specific information learned by the model during fine-tuning on a downstream task. In this way, we aim to help BERT not to forget assigning importance to informative input tokens when making predictions by proposing SaLoss; an auxiliary loss function for guiding the multi-head attention mechanism during training to be close to salient information extracted a priori using TextRank. Experiments for explanation faithfulness across five datasets, show that models trained with SaLoss consistently provide more faithful explanations across four different feature attribution methods compared to vanilla BERT. Using the rationales extracted from vanilla BERT and SaLoss models to train inherently faithful classifiers, we further show that the latter result in higher predictive performance in downstream tasks.

| Comments: | EMNLP 2021 Pre-print                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.13759](https://arxiv.org/abs/2108.13759) [cs.CL]** |
|           | (or **[arXiv:2108.13759v1](https://arxiv.org/abs/2108.13759v1) [cs.CL]** for this version) |





<h2 id="2021-09-01-5">5. Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools
</h2>


Title: [Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools](https://arxiv.org/abs/2108.13961)

Authors: [Nils Feldhus](https://arxiv.org/search/cs?searchtype=author&query=Feldhus%2C+N), [Robert Schwarzenberg](https://arxiv.org/search/cs?searchtype=author&query=Schwarzenberg%2C+R), [Sebastian Möller](https://arxiv.org/search/cs?searchtype=author&query=Möller%2C+S)

> In the language domain, as in other domains, neural explainability takes an ever more important role, with feature attribution methods on the forefront. Many such methods require considerable computational resources and expert knowledge about implementation details and parameter choices. To facilitate research, we present Thermostat which consists of a large collection of model explanations and accompanying analysis tools. Thermostat allows easy access to over 200k explanations for the decisions of prominent state-of-the-art models spanning across different NLP tasks, generated with multiple explainers. The dataset took over 10k GPU hours (> one year) to compile; compute time that the community now saves. The accompanying software tools allow to analyse explanations instance-wise but also accumulatively on corpus level. Users can investigate and compare models, datasets and explainers without the need to orchestrate implementation details. Thermostat is fully open source, democratizes explainability research in the language domain, circumvents redundant computations and increases comparability and replicability.

| Comments: | Accepted to EMNLP 2021 System Demonstrations                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.13961](https://arxiv.org/abs/2108.13961) [cs.CL]** |
|           | (or **[arXiv:2108.13961v1](https://arxiv.org/abs/2108.13961v1) [cs.CL]** for this version) |




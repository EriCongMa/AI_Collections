# Daily arXiv: Machine Translation - July, 2021

# Index


- [2021-07-15](#2021-07-15)

  - [1. How Much Can CLIP Benefit Vision-and-Language Tasks?](#2021-07-15-1)
  - [2. From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text](#2021-07-15-2)
  - [3. Deduplicating Training Data Makes Language Models Better](#2021-07-15-3)
  - [4. Importance-based Neuron Allocation for Multilingual Neural Machine Translation](#2021-07-15-4)
  - [5. ParCourE: A Parallel Corpus Explorer fora Massively Multilingual Corpus](#2021-07-15-5)
- [2021-07-14](#2021-07-14)
  - [1. A Configurable Multilingual Model is All You Need to Recognize All Languages](#2021-07-14-1)
  - [2. Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task](#2021-07-14-2)
  - [3. Zero-shot Speech Translation](#2021-07-14-3)
  - [4. The IWSLT 2021 BUT Speech Translation Systems](#2021-07-14-4)
  - [5. Between Flexibility and Consistency: Joint Generation of Captions and Subtitles](#2021-07-14-5)
- [2021-07-13](#2021-07-13)

  - [1. Oriental Language Recognition (OLR) 2020: Summary and Analysis](#2021-07-13-1)
  - [2. Parameter Selection: Why We Should Pay More Attention to It](#2021-07-13-2)
  - [3. Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning](#2021-07-13-3)
  - [4. Direct speech-to-speech translation with discrete units](#2021-07-13-4)
- [2021-07-12](#2021-07-21)
  - [1. Improved Language Identification Through Cross-Lingual Self-Supervised Learning](#2021-07-12-1)
  - [2. A Systematic Survey of Text Worlds as Embodied Natural Language Environments](#2021-07-12-2)
  - [3. A Survey on Low-Resource Neural Machine Translation](#2021-07-12-3)
  - [4. Using Machine Translation to Localize Task Oriented NLG Output](#2021-07-12-4)
- [2021-07-09](#2021-07-09)
  - [1. Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text](#2021-07-09-1)
  - [2. Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation](#2021-07-09-2)
- [2021-07-08](#2021-07-08)
  - [1. Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking](#2021-07-08-1)
  - [2. Kosp2e: Korean Speech to English Translation Corpus](#2021-07-08-2)
  - [3. Efficient Transformer for Direct Speech Translation](#2021-07-08-3)
  - [4. On Training Instance Selection for Few-Shot Neural Text Generation](#2021-07-08-4)
  - [5. Time-Aware Ancient Chinese Text Translation and Inference](#2021-07-08-5)
- [2021-07-07](#2021-07-07)

  - [1. Long-Short Transformer: Efficient Transformers for Language and Vision](#2021-07-07-1)
  - [2. Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering](#2021-07-07-2)
  - [3. An NLG pipeline for a legal expert system: a work in progress](#2021-07-07-3)
  - [4. The NiuTrans End-to-End Speech Translation System \\for IWSLT 2021 Offline Task](#2021-07-07-4)
  - [5. VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer](#2021-07-07-5)
- [2021-07-06](#2021-07-06)
  - [1. Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition](#2021-07-06-1)
  - [2. IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task](#2021-07-06-2)
  - [3. Packing: Towards 2x NLP BERT Acceleration](#2021-07-06-3)
  - [4. Power Law Graph Transformer for Machine Translation and Representation Learning](#2021-07-06-4)
  - [5. ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](#2021-07-06-5)


- [2021-07-05](#2021-07-05)
  - [1. Transformer-F: A Transformer network with effective methods for learning universal sentence representation](#2021-07-05-1)
  - [2. A Primer on Pretrained Multilingual Language Models](#2021-07-05-2)
  - [3. Interactive decoding of words from visual speech recognition models](#2021-07-05-3)
  - [4. Data Centric Domain Adaptation for Historical Text with OCR Errors](#2021-07-05-4)
- [2021-07-02](#2021-07-02)

  - [1. GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph](#2021-07-02-1)
  - [2. ESPnet-ST IWSLT 2021 Offline Speech Translation System](#2021-07-02-2)
  - [3. Word-Free Spoken Language Understanding for Mandarin-Chinese](#2021-07-02-3)
  - [4. The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021](#2021-07-02-4)
  - [5. Zero-pronoun Data Augmentation for Japanese-to-English Translation](#2021-07-02-5)
  - [6. Modeling Target-side Inflection in Placeholder Translation](#2021-07-02-6)
  - [7. CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding](#2021-07-02-7 )
- [2021-07-01](#2021-07-01)
  - [1. What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?](#2021-07-01-1)
  - [2. Mixed Cross Entropy Loss for Neural Machine Translation](#2021-07-01-2)
  - [3. Cross-lingual alignments of ELMo contextual embeddings](#2021-07-01-3)
  - [4. ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information](#2021-07-01-4)
  - [5. IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task](#2021-07-01-5)
  - [6. XLM-E: Cross-lingual Language Model Pre-training via ELECTRA](#2021-07-01-6)
  - [7. On the Power of Saturated Transformers: A View from Circuit Complexity](#2021-07-01-7)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-07-15

[Return to Index](#Index)



<h2 id="2021-07-15-1">1. How Much Can CLIP Benefit Vision-and-Language Tasks?
</h2>

Title: [How Much Can CLIP Benefit Vision-and-Language Tasks?](https://arxiv.org/abs/2107.06383)

Authors: [Sheng Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+S), [Liunian Harold Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L+H), [Hao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M), [Anna Rohrbach](https://arxiv.org/search/cs?searchtype=author&query=Rohrbach%2C+A), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K), [Zhewei Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Z), [Kurt Keutzer](https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K)

> Most existing Vision-and-Language (V&L) models rely on pre-trained visual encoders, using a relatively small set of manually-annotated data (as compared to web-crawled data), to perceive the visual world. However, it has been observed that large-scale pretraining usually can result in better generalization performance, e.g., CLIP (Contrastive Language-Image Pre-training), trained on a massive amount of image-caption pairs, has shown a strong zero-shot capability on various vision tasks. To further study the advantage brought by CLIP, we propose to use CLIP as the visual encoder in various V&L models in two typical scenarios: 1) plugging CLIP into task-specific fine-tuning; 2) combining CLIP with V&L pre-training and transferring to downstream tasks. We show that CLIP significantly outperforms widely-used visual encoders trained with in-domain annotated data, such as BottomUp-TopDown. We achieve competitive or better results on diverse V&L tasks, while establishing new state-of-the-art results on Visual Question Answering, Visual Entailment, and V&L Navigation tasks. We release our code at [this https URL](https://github.com/clip-vil/CLIP-ViL).

| Comments: | 14 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.06383](https://arxiv.org/abs/2107.06383) [cs.CV]** |
|           | (or **[arXiv:2107.06383v1](https://arxiv.org/abs/2107.06383v1) [cs.CV]** for this version) |





<h2 id="2021-07-15-2">2. From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text
</h2>

Title: [From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text](https://arxiv.org/abs/2107.06483)

Authors: [Ishan Tarunesh](https://arxiv.org/search/cs?searchtype=author&query=Tarunesh%2C+I), [Syamantak Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S), [Preethi Jyothi](https://arxiv.org/search/cs?searchtype=author&query=Jyothi%2C+P)

> Generating code-switched text is a problem of growing interest, especially given the scarcity of corpora containing large volumes of real code-switched text. In this work, we adapt a state-of-the-art neural machine translation model to generate Hindi-English code-switched sentences starting from monolingual Hindi sentences. We outline a carefully designed curriculum of pretraining steps, including the use of synthetic code-switched text, that enable the model to generate high-quality code-switched text. Using text generated from our model as data augmentation, we show significant reductions in perplexity on a language modeling task, compared to using text from other generative models of CS text. We also show improvements using our text for a downstream code-switched natural language inference task. Our generated text is further subjected to a rigorous evaluation using a human evaluation study and a range of objective metrics, where we show performance comparable (and sometimes even superior) to code-switched text obtained via crowd workers who are native Hindi speakers.

| Comments: | In Proceedings of The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.06483](https://arxiv.org/abs/2107.06483) [cs.CL]** |
|           | (or **[arXiv:2107.06483v1](https://arxiv.org/abs/2107.06483v1) [cs.CL]** for this version) |





<h2 id="2021-07-15-3">3. Deduplicating Training Data Makes Language Models Better
</h2>

Title: [Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499)

Authors: [Katherine Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K), [Daphne Ippolito](https://arxiv.org/search/cs?searchtype=author&query=Ippolito%2C+D), [Andrew Nystrom](https://arxiv.org/search/cs?searchtype=author&query=Nystrom%2C+A), [Chiyuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Douglas Eck](https://arxiv.org/search/cs?searchtype=author&query=Eck%2C+D), [Chris Callison-Burch](https://arxiv.org/search/cs?searchtype=author&query=Callison-Burch%2C+C), [Nicholas Carlini](https://arxiv.org/search/cs?searchtype=author&query=Carlini%2C+N)

> We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at [this https URL](https://github.com/google-research/deduplicate-text-datasets).

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.06499](https://arxiv.org/abs/2107.06499) [cs.CL]** |
|           | (or **[arXiv:2107.06499v1](https://arxiv.org/abs/2107.06499v1) [cs.CL]** for this version) |





<h2 id="2021-07-15-4">4. Importance-based Neuron Allocation for Multilingual Neural Machine Translation
</h2>

Title: [Importance-based Neuron Allocation for Multilingual Neural Machine Translation](https://arxiv.org/abs/2107.06569)

Authors: [Wanying Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+W), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Shuhao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+S), [Dong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D)

> Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages. However, the current multilingual translation paradigm often makes the model tend to preserve the general knowledge, but ignore the language-specific knowledge. Some previous works try to solve this problem by adding various kinds of language-specific modules to the model, but they suffer from the parameter explosion problem and require specialized manual design. To solve these problems, we propose to divide the model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the general knowledge and participating in the translation of all the languages, while the language-specific part is responsible for preserving the language-specific knowledge and participating in the translation of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method.

| Comments: | ACL 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.06569](https://arxiv.org/abs/2107.06569) [cs.CL]** |
|           | (or **[arXiv:2107.06569v1](https://arxiv.org/abs/2107.06569v1) [cs.CL]** for this version) |





<h2 id="2021-07-15-5">5. ParCourE: A Parallel Corpus Explorer fora Massively Multilingual Corpus
</h2>

Title: [ParCourE: A Parallel Corpus Explorer fora Massively Multilingual Corpus](https://arxiv.org/abs/2107.06632)

Authors: [Ayyoob Imani](https://arxiv.org/search/cs?searchtype=author&query=Imani%2C+A), [Masoud Jalili Sabet](https://arxiv.org/search/cs?searchtype=author&query=Sabet%2C+M+J), [Philipp Dufter](https://arxiv.org/search/cs?searchtype=author&query=Dufter%2C+P), [Michael Cysouw](https://arxiv.org/search/cs?searchtype=author&query=Cysouw%2C+M), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> With more than 7000 languages worldwide, multilingual natural language processing (NLP) is essential both from an academic and commercial perspective. Researching typological properties of languages is fundamental for progress in multilingual NLP. Examples include assessing language similarity for effective transfer learning, injecting inductive biases into machine learning models or creating resources such as dictionaries and inflection tables. We provide ParCourE, an online tool that allows to browse a word-aligned parallel corpus, covering 1334 languages. We give evidence that this is useful for typological research. ParCourE can be set up for any parallel corpus and can thus be used for typological research on other corpora as well as for exploring their quality and properties.

| Comments: | The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.06632](https://arxiv.org/abs/2107.06632) [cs.CL]** |
|           | (or **[arXiv:2107.06632v1](https://arxiv.org/abs/2107.06632v1) [cs.CL]** for this version) |








# 2021-07-14

[Return to Index](#Index)



<h2 id="2021-07-14-1">1. A Configurable Multilingual Model is All You Need to Recognize All Languages
</h2>

Title: [A Configurable Multilingual Model is All You Need to Recognize All Languages](https://arxiv.org/abs/2107.05876)

Authors: [Long Zhou](https://arxiv.org/search/eess?searchtype=author&query=Zhou%2C+L), [Jinyu Li](https://arxiv.org/search/eess?searchtype=author&query=Li%2C+J), [Eric Sun](https://arxiv.org/search/eess?searchtype=author&query=Sun%2C+E), [Shujie Liu](https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+S)

> Multilingual automatic speech recognition (ASR) models have shown great promise in recent years because of the simplified model training and deployment process. Conventional methods either train a universal multilingual model without taking any language information or with a 1-hot language ID (LID) vector to guide the recognition of the target language. In practice, the user can be prompted to pre-select several languages he/she can speak. The multilingual model without LID cannot well utilize the language information set by the user while the multilingual model with LID can only handle one pre-selected language. In this paper, we propose a novel configurable multilingual model (CMM) which is trained only once but can be configured as different models based on users' choices by extracting language-specific modules together with a universal model from the trained CMM. Particularly, a single CMM can be deployed to any user scenario where the users can pre-select any combination of languages. Trained with 75K hours of transcribed anonymized Microsoft multilingual data and evaluated with 10-language test sets, the proposed CMM improves from the universal multilingual model by 26.0%, 16.9%, and 10.4% relative word error reduction when the user selects 1, 2, or 3 languages, respectively. CMM also performs significantly better on code-switching test sets.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.05876](https://arxiv.org/abs/2107.05876) [eess.AS]** |
|           | (or **[arXiv:2107.05876v1](https://arxiv.org/abs/2107.05876v1) [eess.AS]** for this version) |





<h2 id="2021-07-14-2">2. Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task
</h2>

Title: [Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task](https://arxiv.org/abs/2107.05782)

Authors: [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Dmitriy Genzel](https://arxiv.org/search/cs?searchtype=author&query=Genzel%2C+D)

> Pretraining and multitask learning are widely used to improve the speech to text translation performance. In this study, we are interested in training a speech to text translation model along with an auxiliary text to text translation task. We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework. Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more information from the pretrained text translation modules. We observe minimal negative transfer effect between the two tasks and sharing more parameters is helpful to transfer knowledge from the text task to the speech task. The analysis also reveals that the modality representation difference at the top decoder layers is still not negligible, and those layers are critical for the translation quality. Inspired by these findings, we propose three methods to improve translation quality. First, a parameter sharing and initialization strategy is proposed to enhance information sharing between the tasks. Second, a novel attention-based regularization is proposed for the encoders and pulls the representations from different modalities closer. Third, an online knowledge distillation is proposed to enhance the knowledge transfer from the text to the speech task. Our experiments show that the proposed approach improves translation performance by more than 2 BLEU over a strong baseline and achieves state-of-the-art results on the \textsc{MuST-C} English-German, English-French and English-Spanish language pairs.

| Comments: | Accepted by ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2107.05782](https://arxiv.org/abs/2107.05782) [cs.CL]** |
|           | (or **[arXiv:2107.05782v1](https://arxiv.org/abs/2107.05782v1) [cs.CL]** for this version) |





<h2 id="2021-07-14-3">3. Zero-shot Speech Translation
</h2>

Title: [Zero-shot Speech Translation](https://arxiv.org/abs/2107.06010)

Authors: [Tu Anh Dinh](https://arxiv.org/search/cs?searchtype=author&query=Dinh%2C+T+A)

> Speech Translation (ST) is the task of translating speech in one language into text in another language. Traditional cascaded approaches for ST, using Automatic Speech Recognition (ASR) and Machine Translation (MT) systems, are prone to error propagation. End-to-end approaches use only one system to avoid propagating error, yet are difficult to employ due to data scarcity. We explore zero-shot translation, which enables translating a pair of languages that is unseen during training, thus avoid the use of end-to-end ST data. Zero-shot translation has been shown to work for multilingual machine translation, yet has not been studied for speech translation. We attempt to build zero-shot ST models that are trained only on ASR and MT tasks but can do ST task during inference. The challenge is that the representation of text and audio is significantly different, thus the models learn ASR and MT tasks in different ways, making it non-trivial to perform zero-shot. These models tend to output the wrong language when performing zero-shot ST. We tackle the issues by including additional training data and an auxiliary loss function that minimizes the text-audio difference. Our experiment results and analysis show that the methods are promising for zero-shot ST. Moreover, our methods are particularly useful in the few-shot settings where a limited amount of ST data is available, with improvements of up to +11.8 BLEU points compared to direct end-to-end ST models and +3.9 BLEU points compared to ST models fine-tuned from pre-trained ASR model.

| Subjects:    | **Computation and Language (cs.CL)**                         |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2107.06010](https://arxiv.org/abs/2107.06010) [cs.CL]** |
|              | (or **[arXiv:2107.06010v1](https://arxiv.org/abs/2107.06010v1) [cs.CL]** for this version) |





<h2 id="2021-07-14-4">4. The IWSLT 2021 BUT Speech Translation Systems
</h2>

Title: [The IWSLT 2021 BUT Speech Translation Systems](https://arxiv.org/abs/2107.06155)

Authors: [Hari Krishna Vydana](https://arxiv.org/search/cs?searchtype=author&query=Vydana%2C+H+K), [Martin Karafi'at](https://arxiv.org/search/cs?searchtype=author&query=Karafi'at%2C+M), [Luk'as Burget](https://arxiv.org/search/cs?searchtype=author&query=Burget%2C+L), ["Honza" Cernock'y](https://arxiv.org/search/cs?searchtype=author&query=Cernock'y%2C+")

> The paper describes BUT's English to German offline speech translation(ST) systems developed for IWSLT2021. They are based on jointly trained Automatic Speech Recognition-Machine Translation models. Their performances is evaluated on MustC-Common test set. In this work, we study their efficiency from the perspective of having a large amount of separate ASR training data and MT training data, and a smaller amount of speech-translation training data. Large amounts of ASR and MT training data are utilized for pre-training the ASR and MT models. Speech-translation data is used to jointly optimize ASR-MT models by defining an end-to-end differentiable path from speech to translations. For this purpose, we use the internal continuous representations from the ASR-decoder as the input to MT module. We show that speech translation can be further improved by training the ASR-decoder jointly with the MT-module using large amount of text-only MT training data. We also show significant improvements by training an ASR module capable of generating punctuated text, rather than leaving the punctuation task to the MT module.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.06155](https://arxiv.org/abs/2107.06155) [cs.CL]** |
|           | (or **[arXiv:2107.06155v1](https://arxiv.org/abs/2107.06155v1) [cs.CL]** for this version) |





<h2 id="2021-07-14-5">5. Between Flexibility and Consistency: Joint Generation of Captions and Subtitles
</h2>

Title: [Between Flexibility and Consistency: Joint Generation of Captions and Subtitles](https://arxiv.org/abs/2107.06246)

Authors: [Alina Karakanta](https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A), [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Speech translation (ST) has lately received growing interest for the generation of subtitles without the need for an intermediate source language transcription and timing (i.e. captions). However, the joint generation of source captions and target subtitles does not only bring potential output quality advantages when the two decoding processes inform each other, but it is also often required in multilingual scenarios. In this work, we focus on ST models which generate consistent captions-subtitles in terms of structure and lexical content. We further introduce new metrics for evaluating subtitling consistency. Our findings show that joint decoding leads to increased performance and consistency between the generated captions and subtitles while still allowing for sufficient flexibility to produce subtitles conforming to language-specific needs and norms.

| Comments: | Accepted at IWSLT 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.06246](https://arxiv.org/abs/2107.06246) [cs.CL]** |
|           | (or **[arXiv:2107.06246v1](https://arxiv.org/abs/2107.06246v1) [cs.CL]** for this version) |







# 2021-07-13

[Return to Index](#Index)



<h2 id="2021-07-13-1">1. Oriental Language Recognition (OLR) 2020: Summary and Analysis
</h2>

Title: [Oriental Language Recognition (OLR) 2020: Summary and Analysis](https://arxiv.org/abs/2107.05365)

Authors: [Jing Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Binling Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Yiming Zhi](https://arxiv.org/search/cs?searchtype=author&query=Zhi%2C+Y), [Zheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Lin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Qingyang Hong](https://arxiv.org/search/cs?searchtype=author&query=Hong%2C+Q), [Dong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D)

> The fifth Oriental Language Recognition (OLR) Challenge focuses on language recognition in a variety of complex environments to promote its development. The OLR 2020 Challenge includes three tasks: (1) cross-channel language identification, (2) dialect identification, and (3) noisy language identification. We choose Cavg as the principle evaluation metric, and the Equal Error Rate (EER) as the secondary metric. There were 58 teams participating in this challenge and one third of the teams submitted valid results. Compared with the best baseline, the Cavg values of Top 1 system for the three tasks were relatively reduced by 82%, 62% and 48%, respectively. This paper describes the three tasks, the database profile, and the final results. We also outline the novel approaches that improve the performance of language recognition systems most significantly, such as the utilization of auxiliary information.

| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL)          |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.05365](https://arxiv.org/abs/2107.05365) [cs.SD]** |
|           | (or **[arXiv:2107.05365v1](https://arxiv.org/abs/2107.05365v1) [cs.SD]** for this version) |





<h2 id="2021-07-13-2">2. Parameter Selection: Why We Should Pay More Attention to It
</h2>

Title: [Parameter Selection: Why We Should Pay More Attention to It](https://arxiv.org/abs/2107.05393)

Authors: [Jie-Jyun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Tsung-Han Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+T), [Si-An Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Chih-Jen Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+C)

> The importance of parameter selection in supervised learning is well known. However, due to the many parameter combinations, an incomplete or an insufficient procedure is often applied. This situation may cause misleading or confusing conclusions. In this opinion paper, through an intriguing example we point out that the seriousness goes beyond what is generally recognized. In the topic of multi-label classification for medical code prediction, one influential paper conducted a proper parameter selection on a set, but when moving to a subset of frequently occurring labels, the authors used the same parameters without a separate tuning. The set of frequent labels became a popular benchmark in subsequent studies, which kept pushing the state of the art. However, we discovered that most of the results in these studies cannot surpass the approach in the original paper if a parameter tuning had been conducted at the time. Thus it is unclear how much progress the subsequent developments have actually brought. The lesson clearly indicates that without enough attention on parameter selection, the research progress in our field can be uncertain or even illusive.

| Comments: | Accepted by ACL-IJCNLP 2021                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2107.05393](https://arxiv.org/abs/2107.05393) [cs.LG]** |
|           | (or **[arXiv:2107.05393v1](https://arxiv.org/abs/2107.05393v1) [cs.LG]** for this version) |





<h2 id="2021-07-13-3">3. Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning
</h2>

Title: [Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning](https://arxiv.org/abs/2107.05243)

Authors: [Jun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Chang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Francisco Guzman](https://arxiv.org/search/cs?searchtype=author&query=Guzman%2C+F), [Ahmed El-Kishky](https://arxiv.org/search/cs?searchtype=author&query=El-Kishky%2C+A), [Yuqing Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Benjamin I. P. Rubinstein](https://arxiv.org/search/cs?searchtype=author&query=Rubinstein%2C+B+I+P), [Trevor Cohn](https://arxiv.org/search/cs?searchtype=author&query=Cohn%2C+T)

> Neural machine translation systems are known to be vulnerable to adversarial test inputs, however, as we show in this paper, these systems are also vulnerable to training attacks. Specifically, we propose a poisoning attack in which a malicious adversary inserts a small poisoned sample of monolingual text into the training set of a system trained using back-translation. This sample is designed to induce a specific, targeted translation behaviour, such as peddling misinformation. We present two methods for crafting poisoned examples, and show that only a tiny handful of instances, amounting to only 0.02% of the training set, is sufficient to enact a successful attack. We outline a defence method against said attacks, which partly ameliorates the problem. However, we stress that this is a blind-spot in modern NMT, demanding immediate attention.

| Comments: | Findings of ACL, to appear                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Cryptography and Security (cs.CR) |
| Cite as:  | **[arXiv:2107.05243](https://arxiv.org/abs/2107.05243) [cs.CL]** |
|           | (or **[arXiv:2107.05243v1](https://arxiv.org/abs/2107.05243v1) [cs.CL]** for this version) |





<h2 id="2021-07-13-4">4. Direct speech-to-speech translation with discrete units
</h2>

Title: [Direct speech-to-speech translation with discrete units](https://arxiv.org/abs/2107.05604)

Authors: [Ann Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+A), [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Adam Polyak](https://arxiv.org/search/cs?searchtype=author&query=Polyak%2C+A), [Yossi Adi](https://arxiv.org/search/cs?searchtype=author&query=Adi%2C+Y), [Qing He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Q), [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Wei-Ning Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+W)

> We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation. Previous work addresses the problem by training an attention-based sequence-to-sequence model that maps source speech spectrograms into target spectrograms. To tackle the challenge of modeling continuous spectrogram features of the target speech, we propose to predict the self-supervised discrete representations learned from an unlabeled speech corpus instead. When target text transcripts are available, we design a multitask learning framework with joint speech and text training that enables the model to generate dual mode output (speech and text) simultaneously in the same inference pass. Experiments on the Fisher Spanish-English dataset show that predicting discrete units and joint speech and text training improve model performance by 11 BLEU compared with a baseline that predicts spectrograms and bridges 83% of the performance gap towards a cascaded system. When trained without any text transcripts, our model achieves similar performance as a baseline that predicts spectrograms and is trained with text data.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.05604](https://arxiv.org/abs/2107.05604) [cs.CL]** |
|           | (or **[arXiv:2107.05604v1](https://arxiv.org/abs/2107.05604v1) [cs.CL]** for this version) |








# 2021-07-12

[Return to Index](#Index)



<h2 id="2021-07-12-1">1. Improved Language Identification Through Cross-Lingual Self-Supervised Learning
</h2>

Title: [Improved Language Identification Through Cross-Lingual Self-Supervised Learning](https://arxiv.org/abs/2107.04082)

Authors: [Andros Tjandra](https://arxiv.org/search/cs?searchtype=author&query=Tjandra%2C+A), [Diptanu Gon Choudhury](https://arxiv.org/search/cs?searchtype=author&query=Choudhury%2C+D+G), [Frank Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+F), [Kritika Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+K), [Alexei Baevski](https://arxiv.org/search/cs?searchtype=author&query=Baevski%2C+A), [Assaf Sela](https://arxiv.org/search/cs?searchtype=author&query=Sela%2C+A), [Yatharth Saraf](https://arxiv.org/search/cs?searchtype=author&query=Saraf%2C+Y), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M)

> Language identification greatly impacts the success of downstream tasks such as automatic speech recognition. Recently, self-supervised speech representations learned by wav2vec 2.0 have been shown to be very effective for a range of speech tasks. We extend previous self-supervised work on language identification by experimenting with pre-trained models which were learned on real-world unconstrained speech in multiple languages and not just on English. We show that models pre-trained on many languages perform better and enable language identification systems that require very little labeled data to perform well. Results on a 25 languages setup show that with only 10 minutes of labeled data per language, a cross-lingually pre-trained model can achieve over 93% accuracy.

| Comments: | Submitted to ASRU 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2107.04082](https://arxiv.org/abs/2107.04082) [cs.CL]** |
|           | (or **[arXiv:2107.04082v1](https://arxiv.org/abs/2107.04082v1) [cs.CL]** for this version) |





<h2 id="2021-07-12-2">2. A Systematic Survey of Text Worlds as Embodied Natural Language Environments
</h2>

Title: [A Systematic Survey of Text Worlds as Embodied Natural Language Environments](https://arxiv.org/abs/2107.04132)

Authors: [Peter A Jansen](https://arxiv.org/search/cs?searchtype=author&query=Jansen%2C+P+A)

> Text Worlds are virtual environments for embodied agents that, unlike 2D or 3D environments, are rendered exclusively using textual descriptions. These environments offer an alternative to higher-fidelity 3D environments due to their low barrier to entry, providing the ability to study semantics, compositional inference, and other high-level tasks with rich high-level action spaces while controlling for perceptual input. This systematic survey outlines recent developments in tooling, environments, and agent modeling for Text Worlds, while examining recent trends in knowledge graphs, common sense reasoning, transfer learning of Text World performance to higher-fidelity environments, as well as near-term development targets that, once achieved, make Text Worlds an attractive general research paradigm for natural language processing.

| Comments: | 18 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2107.04132](https://arxiv.org/abs/2107.04132) [cs.CL]** |
|           | (or **[arXiv:2107.04132v1](https://arxiv.org/abs/2107.04132v1) [cs.CL]** for this version) |





<h2 id="2021-07-12-3">3. A Survey on Low-Resource Neural Machine Translation
</h2>

Title: [A Survey on Low-Resource Neural Machine Translation](https://arxiv.org/abs/2107.04239)

Authors: [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Xu Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X), [Renqian Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R), [Tao Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T), [Tie-Yan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T)

> Neural approaches have achieved state-of-the-art accuracy on machine translation but suffer from the high cost of collecting large scale parallel data. Thus, a lot of research has been conducted for neural machine translation (NMT) with very limited parallel data, i.e., the low-resource setting. In this paper, we provide a survey for low-resource NMT and classify related works into three categories according to the auxiliary data they used: (1) exploiting monolingual data of source and/or target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.

| Comments: | A short version has been submitted to IJCAI2021 Survey Track on Feb. 26th, 2021, accepted on Apr. 16th, 2021. 14 pages, 4 figures |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.04239](https://arxiv.org/abs/2107.04239) [cs.CL]** |
|           | (or **[arXiv:2107.04239v1](https://arxiv.org/abs/2107.04239v1) [cs.CL]** for this version) |





<h2 id="2021-07-12-4">4. Using Machine Translation to Localize Task Oriented NLG Output
</h2>

Title: [Using Machine Translation to Localize Task Oriented NLG Output](https://arxiv.org/abs/2107.04512)

Authors: [Scott Roy](https://arxiv.org/search/cs?searchtype=author&query=Roy%2C+S), [Cliff Brunk](https://arxiv.org/search/cs?searchtype=author&query=Brunk%2C+C), [Kyu-Young Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+K), [Justin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [Mihir Kale](https://arxiv.org/search/cs?searchtype=author&query=Kale%2C+M), [Gagan Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+G), [Sidharth Mudgal](https://arxiv.org/search/cs?searchtype=author&query=Mudgal%2C+S), [Chris Varano](https://arxiv.org/search/cs?searchtype=author&query=Varano%2C+C)

> One of the challenges in a task oriented natural language application like the Google Assistant, Siri, or Alexa is to localize the output to many languages. This paper explores doing this by applying machine translation to the English output. Using machine translation is very scalable, as it can work with any English output and can handle dynamic text, but otherwise the problem is a poor fit. The required quality bar is close to perfection, the range of sentences is extremely narrow, and the sentences are often very different than the ones in the machine translation training data. This combination of requirements is novel in the field of domain adaptation for machine translation. We are able to reach the required quality bar by building on existing ideas and adding new ones: finetuning on in-domain translations, adding sentences from the Web, adding semantic annotations, and using automatic error detection. The paper shares our approach and results, together with a distillation model to serve the translation models at scale.

| Comments: | 12 pages, 10 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.04512](https://arxiv.org/abs/2107.04512) [cs.CL]** |
|           | (or **[arXiv:2107.04512v1](https://arxiv.org/abs/2107.04512v1) [cs.CL]** for this version) |






# 2021-07-09

[Return to Index](#Index)



<h2 id="2021-07-09-1">1. Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text
</h2>

Title: [Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text](https://arxiv.org/abs/2107.03444)

Authors: [Philippe Laban](https://arxiv.org/search/cs?searchtype=author&query=Laban%2C+P), [Tobias Schnabel](https://arxiv.org/search/cs?searchtype=author&query=Schnabel%2C+T), [Paul Bennett](https://arxiv.org/search/cs?searchtype=author&query=Bennett%2C+P), [Marti A. Hearst](https://arxiv.org/search/cs?searchtype=author&query=Hearst%2C+M+A)

> This work presents Keep it Simple (KiS), a new approach to unsupervised text simplification which learns to balance a reward across three properties: fluency, salience and simplicity. We train the model with a novel algorithm to optimize the reward (k-SCST), in which the model proposes several candidate simplifications, computes each candidate's reward, and encourages candidates that outperform the mean reward. Finally, we propose a realistic text comprehension task as an evaluation method for text simplification. When tested on the English news domain, the KiS model outperforms strong supervised baselines by more than 4 SARI points, and can help people complete a comprehension task an average of 18% faster while retaining accuracy, when compared to the original text. Code available: [this https URL](https://github.com/tingofurro/keep_it_simple)

| Comments:          | Accepted at ACL-IJCNLP 2021, 14 pages, 7 figures             |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Association for Computational Linguistics (2021)             |
| Cite as:           | **[arXiv:2107.03444](https://arxiv.org/abs/2107.03444) [cs.CL]** |
|                    | (or **[arXiv:2107.03444v1](https://arxiv.org/abs/2107.03444v1) [cs.CL]** for this version) |





<h2 id="2021-07-09-2">2. Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation
</h2>

Title: [Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation](https://arxiv.org/abs/2107.03625)

Authors: [Yves Bestgen](https://arxiv.org/search/cs?searchtype=author&query=Bestgen%2C+Y)

> A comparison of formulaic sequences in human and neural machine translation of quality newspaper articles shows that neural machine translations contain less lower-frequency, but strongly-associated formulaic sequences, and more high-frequency formulaic sequences. These differences were statistically significant and the effect sizes were almost always medium or large. These observations can be related to the differences between second language learners of various levels and between translated and untranslated texts. The comparison between the neural machine translation systems indicates that some systems produce more formulaic sequences of both types than other systems.

| Comments: | Accepted at Translation and Interpreting Technology Online - TRITON 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.03625](https://arxiv.org/abs/2107.03625) [cs.CL]** |
|           | (or **[arXiv:2107.03625v1](https://arxiv.org/abs/2107.03625v1) [cs.CL]** for this version) |






# 2021-07-08

[Return to Index](#Index)



<h2 id="2021-07-08-1">1. Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking
</h2>

Title: [Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking](https://arxiv.org/abs/2107.02865)

Authors: [Daniel Diomedi](https://arxiv.org/search/cs?searchtype=author&query=Diomedi%2C+D), [Aidan Hogan](https://arxiv.org/search/cs?searchtype=author&query=Hogan%2C+A)

> The goal of Question Answering over Knowledge Graphs (KGQA) is to find answers for natural language questions over a knowledge graph. Recent KGQA approaches adopt a neural machine translation (NMT) approach, where the natural language question is translated into a structured query language. However, NMT suffers from the out-of-vocabulary problem, where terms in a question may not have been seen during training, impeding their translation. This issue is particularly problematic for the millions of entities that large knowledge graphs describe. We rather propose a KGQA approach that delegates the processing of entities to entity linking (EL) systems. NMT is then used to create a query template with placeholders that are filled by entities identified in an EL phase. Slot filling is used to decide which entity fills which placeholder. Experiments for QA over Wikidata show that our approach outperforms pure NMT: while there remains a strong dependence on having seen similar query templates during training, errors relating to entities are greatly reduced.

| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02865](https://arxiv.org/abs/2107.02865) [cs.AI]** |
|           | (or **[arXiv:2107.02865v1](https://arxiv.org/abs/2107.02865v1) [cs.AI]** for this version) |





<h2 id="2021-07-08-2">2. Kosp2e: Korean Speech to English Translation Corpus
</h2>

Title: [Kosp2e: Korean Speech to English Translation Corpus](https://arxiv.org/abs/2107.02875)

Authors: [Won Ik Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+W+I), [Seok Min Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S+M), [Hyunchang Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+H), [Nam Soo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+N+S)

> Most speech-to-text (S2T) translation studies use English speech as a source, which makes it difficult for non-English speakers to take advantage of the S2T technologies. For some languages, this problem was tackled through corpus construction, but the farther linguistically from English or the more under-resourced, this deficiency and underrepresentedness becomes more significant. In this paper, we introduce kosp2e (read as `kospi'), a corpus that allows Korean speech to be translated into English text in an end-to-end manner. We adopt open license speech recognition corpus, translation corpus, and spoken language corpora to make our dataset freely available to the public, and check the performance through the pipeline and training-based approaches. Using pipeline and various end-to-end schemes, we obtain the highest BLEU of 21.3 and 18.0 for each based on the English hypothesis, validating the feasibility of our data. We plan to supplement annotations for other target languages through community contributions in the future.

| Comments: | Interspeech 2021 Camera-ready                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.02875](https://arxiv.org/abs/2107.02875) [cs.CL]** |
|           | (or **[arXiv:2107.02875v1](https://arxiv.org/abs/2107.02875v1) [cs.CL]** for this version) |





<h2 id="2021-07-08-3">3. Efficient Transformer for Direct Speech Translation
</h2>

Title: [Efficient Transformer for Direct Speech Translation](https://arxiv.org/abs/2107.03069)

Authors: [Belen Alastruey](https://arxiv.org/search/cs?searchtype=author&query=Alastruey%2C+B), [Gerard I. Gállego](https://arxiv.org/search/cs?searchtype=author&query=Gállego%2C+G+I), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

> The advent of Transformer-based models has surpassed the barriers of text. When working with speech, we must face a problem: the sequence length of an audio input is not suitable for the Transformer. To bypass this problem, a usual approach is adding strided convolutional layers, to reduce the sequence length before using the Transformer. In this paper, we propose a new approach for direct Speech Translation, where thanks to an efficient Transformer we can work with a spectrogram without having to use convolutional layers before the Transformer. This allows the encoder to learn directly from the spectrogram and no information is lost. We have created an encoder-decoder model, where the encoder is an efficient Transformer -- the Longformer -- and the decoder is a traditional Transformer decoder. Our results, which are close to the ones obtained with the standard approach, show that this is a promising research direction.

| Comments: | (c) 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2107.03069](https://arxiv.org/abs/2107.03069) [cs.CL]** |
|           | (or **[arXiv:2107.03069v1](https://arxiv.org/abs/2107.03069v1) [cs.CL]** for this version) |





<h2 id="2021-07-08-4">4. On Training Instance Selection for Few-Shot Neural Text Generation
</h2>

Title: [On Training Instance Selection for Few-Shot Neural Text Generation](https://arxiv.org/abs/2107.03176)

Authors: [Ernie Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+E), [Xiaoyu Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+X), [Hui-Syuan Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+H), [Vera Demberg](https://arxiv.org/search/cs?searchtype=author&query=Demberg%2C+V)

> Large-scale pretrained language models have led to dramatic improvements in text generation. Impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). Nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. Little to no attention has been paid to the selection strategies and how they would affect model performance. In this work, we present a study on training instance selection in few-shot neural text generation. The selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost. Based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution, we propose a simple selection strategy with K-means clustering. We show that even with the naive clustering-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. We hope that this work will call for more attention on this largely unexplored area.

| Comments: | Accepted at ACL 2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.03176](https://arxiv.org/abs/2107.03176) [cs.CL]** |
|           | (or **[arXiv:2107.03176v1](https://arxiv.org/abs/2107.03176v1) [cs.CL]** for this version) |





<h2 id="2021-07-08-5">5. Time-Aware Ancient Chinese Text Translation and Inference
</h2>

Title: [Time-Aware Ancient Chinese Text Translation and Inference](https://arxiv.org/abs/2107.03179)

Authors: [Ernie Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+E), [Yow-Ting Shiue](https://arxiv.org/search/cs?searchtype=author&query=Shiue%2C+Y), [Hui-Syuan Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+H), [Vera Demberg](https://arxiv.org/search/cs?searchtype=author&query=Demberg%2C+V)

> In this paper, we aim to address the challenges surrounding the translation of ancient Chinese text: (1) The linguistic gap due to the difference in eras results in translations that are poor in quality, and (2) most translations are missing the contextual information that is often very crucial to understanding the text. To this end, we improve upon past translation techniques by proposing the following: We reframe the task as a multi-label prediction task where the model predicts both the translation and its particular era. We observe that this helps to bridge the linguistic gap as chronological context is also used as auxiliary information. % As a natural step of generalization, we pivot on the modern Chinese translations to generate multilingual outputs. %We show experimentally the efficacy of our framework in producing quality translation outputs and also validate our framework on a collected task-specific parallel corpus. We validate our framework on a parallel corpus annotated with chronology information and show experimentally its efficacy in producing quality translation outputs. We release both the code and the data [this https URL](https://github.com/orina1123/time-aware-ancient-text-translation) for future research.

| Comments: | Accepted at LChange at ACL 2021                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.03179](https://arxiv.org/abs/2107.03179) [cs.CL]** |
|           | (or **[arXiv:2107.03179v1](https://arxiv.org/abs/2107.03179v1) [cs.CL]** for this version) |





# 2021-07-07

[Return to Index](#Index)



<h2 id="2021-07-07-1">1. Long-Short Transformer: Efficient Transformers for Language and Vision
</h2>

Title: [Long-Short Transformer: Efficient Transformers for Language and Vision](https://arxiv.org/abs/2107.02192)

Authors: [Chen Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Wei Ping](https://arxiv.org/search/cs?searchtype=author&query=Ping%2C+W), [Chaowei Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C), [Mohammad Shoeybi](https://arxiv.org/search/cs?searchtype=author&query=Shoeybi%2C+M), [Tom Goldstein](https://arxiv.org/search/cs?searchtype=author&query=Goldstein%2C+T), [Anima Anandkumar](https://arxiv.org/search/cs?searchtype=author&query=Anandkumar%2C+A), [Bryan Catanzaro](https://arxiv.org/search/cs?searchtype=author&query=Catanzaro%2C+B)

> Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3× as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results~(e.g., Top-1 accuracy 84.1% trained on 224×224 ImageNet-1K only), while being more scalable on high-resolution images. The models and source code will be released soon.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02192](https://arxiv.org/abs/2107.02192) [cs.CV]** |
|           | (or **[arXiv:2107.02192v1](https://arxiv.org/abs/2107.02192v1) [cs.CV]** for this version) |





<h2 id="2021-07-07-2">2. Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering
</h2>

Title: [Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering](https://arxiv.org/abs/2107.02331)

Authors: [Siddharth Karamcheti](https://arxiv.org/search/cs?searchtype=author&query=Karamcheti%2C+S), [Ranjay Krishna](https://arxiv.org/search/cs?searchtype=author&query=Krishna%2C+R), [Li Fei-Fei](https://arxiv.org/search/cs?searchtype=author&query=Fei-Fei%2C+L), [Christopher D. Manning](https://arxiv.org/search/cs?searchtype=author&query=Manning%2C+C+D)

> Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers -- groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.

| Comments: | Accepted at ACL-IJCNLP 2021. 17 pages, 16 Figures            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.02331](https://arxiv.org/abs/2107.02331) [cs.CL]** |
|           | (or **[arXiv:2107.02331v1](https://arxiv.org/abs/2107.02331v1) [cs.CL]** for this version) |





<h2 id="2021-07-07-3">3. An NLG pipeline for a legal expert system: a work in progress
</h2>

Title: [An NLG pipeline for a legal expert system: a work in progress](https://arxiv.org/abs/2107.02421)

Authors: [Inari Listenmaa](https://arxiv.org/search/cs?searchtype=author&query=Listenmaa%2C+I), [Jason Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris%2C+J), [Alfred Ang](https://arxiv.org/search/cs?searchtype=author&query=Ang%2C+A), [Maryam Hanafiah](https://arxiv.org/search/cs?searchtype=author&query=Hanafiah%2C+M), [Regina Cheong](https://arxiv.org/search/cs?searchtype=author&query=Cheong%2C+R)

> We present the NLG component for L4, a prototype domain-specific language (DSL) for drafting laws and contracts. As a concrete use case, we describe a pipeline for a legal expert system created from L4 code. The NLG component is used in two steps. The first step is to create an interview, whose answers are processed into a query for an automated reasoner. The second step is to render the answers of the reasoner in natural language.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02421](https://arxiv.org/abs/2107.02421) [cs.CL]** |
|           | (or **[arXiv:2107.02421v1](https://arxiv.org/abs/2107.02421v1) [cs.CL]** for this version) |





<h2 id="2021-07-07-4">4. The NiuTrans End-to-End Speech Translation System \\for IWSLT 2021 Offline Task
</h2>

Title: [The NiuTrans End-to-End Speech Translation System \\for IWSLT 2021 Offline Task](https://arxiv.org/abs/2107.02444)

Authors: [Chen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Xiaoqian Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Xiaowen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Laohu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Canan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> This paper describes the submission of the NiuTrans end-to-end speech translation system for the IWSLT 2021 offline task, which translates from the English audio to German text directly without intermediate transcription. We use the Transformer-based model architecture and enhance it by Conformer, relative position encoding, and stacked acoustic and textual encoding. To augment the training data, the English transcriptions are translated to German translations. Finally, we employ ensemble decoding to integrate the predictions from several models trained with the different datasets. Combining these techniques, we achieve 33.84 BLEU points on the MuST-C En-De test set, which shows the enormous potential of the end-to-end model.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.02444](https://arxiv.org/abs/2107.02444) [cs.CL]** |
|           | (or **[arXiv:2107.02444v1](https://arxiv.org/abs/2107.02444v1) [cs.CL]** for this version) |





<h2 id="2021-07-07-5">5. VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer
</h2>

Title: [VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer](https://arxiv.org/abs/2107.02681)

Authors: [Zineng Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Z), [Jaemin Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+J), [Hao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Since visual perception can give rich information beyond text descriptions for world understanding, there has been increasing interest in leveraging visual grounding for language learning. Recently, vokenization has attracted attention by using the predictions of a text-to-image retrieval model as labels for language model supervision. Despite its success, the method suffers from approximation error of using finite image labels and the lack of vocabulary diversity of a small image-text dataset. To overcome these limitations, we present VidLanKD, a video-language knowledge distillation method for improving language understanding. We train a multi-modal teacher model on a video-text dataset, and then transfer its knowledge to a student language model with a text dataset. To avoid approximation error, we propose to use different knowledge distillation objectives. In addition, the use of a large-scale video-text dataset helps learn diverse and richer vocabularies. In our experiments, VidLanKD achieves consistent improvements over text-only language models and vokenization models, on several downstream language understanding tasks including GLUE, SQuAD, and SWAG. We also demonstrate the improved world knowledge, physical reasoning, and temporal reasoning capabilities of our model by evaluating on the GLUE-diagnostics, PIQA, and TRACIE datasets. Lastly, we present comprehensive ablation studies as well as visualizations of the learned text-to-video grounding results of our teacher and student language models. Our code and models are available at: [this https URL](https://github.com/zinengtang/VidLanKD)

| Comments: | 18 pages (5 figures, 10 tables)                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.02681](https://arxiv.org/abs/2107.02681) [cs.CL]** |
|           | (or **[arXiv:2107.02681v1](https://arxiv.org/abs/2107.02681v1) [cs.CL]** for this version) |










# 2021-07-06

[Return to Index](#Index)



<h2 id="2021-07-06-1">1. Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition
</h2>

Title: [Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition](https://arxiv.org/abs/2107.01569)

Authors: [Tomohiro Tanaka](https://arxiv.org/search/cs?searchtype=author&query=Tanaka%2C+T), [Ryo Masumura](https://arxiv.org/search/cs?searchtype=author&query=Masumura%2C+R), [Mana Ihori](https://arxiv.org/search/cs?searchtype=author&query=Ihori%2C+M), [Akihiko Takashima](https://arxiv.org/search/cs?searchtype=author&query=Takashima%2C+A), [Takafumi Moriya](https://arxiv.org/search/cs?searchtype=author&query=Moriya%2C+T), [Takanori Ashihara](https://arxiv.org/search/cs?searchtype=author&query=Ashihara%2C+T), [Shota Orihashi](https://arxiv.org/search/cs?searchtype=author&query=Orihashi%2C+S), [Naoki Makishima](https://arxiv.org/search/cs?searchtype=author&query=Makishima%2C+N)

> We propose a cross-modal transformer-based neural correction models that refines the output of an automatic speech recognition (ASR) system so as to exclude ASR errors. Generally, neural correction models are composed of encoder-decoder networks, which can directly model sequence-to-sequence mapping problems. The most successful method is to use both input speech and its ASR output text as the input contexts for the encoder-decoder networks. However, the conventional method cannot take into account the relationships between these two different modal inputs because the input contexts are separately encoded for each modal. To effectively leverage the correlated information between the two different modal inputs, our proposed models encode two different contexts jointly on the basis of cross-modal self-attention using a transformer. We expect that cross-modal self-attention can effectively capture the relationships between two different modals for refining ASR hypotheses. We also introduce a shallow fusion technique to efficiently integrate the first-pass ASR model and our proposed neural correction model. Experiments on Japanese natural language ASR tasks demonstrated that our proposed models achieve better ASR performance than conventional neural correction models.

| Comments: | Accepted to Interspeech 2021                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.01569](https://arxiv.org/abs/2107.01569) [cs.CL]** |
|           | (or **[arXiv:2107.01569v1](https://arxiv.org/abs/2107.01569v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-2">2. IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task
</h2>

Title: [IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task](https://arxiv.org/abs/2107.01656)

Authors: [Baban Gain](https://arxiv.org/search/cs?searchtype=author&query=Gain%2C+B), [Dibyanayan Bandyopadhyay](https://arxiv.org/search/cs?searchtype=author&query=Bandyopadhyay%2C+D), [Asif Ekbal](https://arxiv.org/search/cs?searchtype=author&query=Ekbal%2C+A)

> Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the translation by removing ambiguity on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT - 2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU points for Evaluation and Challenge subset, respectively.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.01656](https://arxiv.org/abs/2107.01656) [cs.CL]** |
|           | (or **[arXiv:2107.01656v1](https://arxiv.org/abs/2107.01656v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-3">3. Packing: Towards 2x NLP BERT Acceleration
</h2>

Title: [Packing: Towards 2x NLP BERT Acceleration](https://arxiv.org/abs/2107.02027)

Authors: [Matej Kosec](https://arxiv.org/search/cs?searchtype=author&query=Kosec%2C+M), [Sheng Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+S), [Mario Michael Krell](https://arxiv.org/search/cs?searchtype=author&query=Krell%2C+M+M)

> We find that at sequence length 512 padding tokens represent in excess of 50% of the Wikipedia dataset used for pretraining BERT (Bidirectional Encoder Representations from Transformers). Therefore by removing all padding we achieve a 2x speed-up in terms of sequences/sec. To exploit this characteristic of the dataset, we develop and contrast two deterministic packing algorithms. Both algorithms rely on the assumption that sequences are interchangeable and therefore packing can be performed on the histogram of sequence lengths, rather than per sample. This transformation of the problem leads to algorithms which are fast and have linear complexity in dataset size. The shortest-pack-first histogram-packing (SPFHP) algorithm determines the packing order for the Wikipedia dataset of over 16M sequences in 0.02 seconds. The non-negative least-squares histogram-packing (NNLSHP) algorithm converges in 28.4 seconds but produces solutions which are more depth efficient, managing to get near optimal packing by combining a maximum of 3 sequences in one sample. Using the dataset with multiple sequences per sample requires additional masking in the attention layer and a modification of the MLM loss function. We demonstrate that both of these changes are straightforward to implement and have relatively little impact on the achievable performance gain on modern hardware. Finally, we pretrain BERT-Large using the packed dataset, demonstrating no loss of convergence and the desired 2x speed-up.

| Subjects:    | **Computation and Language (cs.CL)**; Computational Complexity (cs.CC); Information Theory (cs.IT); Machine Learning (cs.LG) |
| ------------ | ------------------------------------------------------------ |
| MSC classes: | 05-08                                                        |
| ACM classes: | I.2.7; G.2.1                                                 |
| Cite as:     | **[arXiv:2107.02027](https://arxiv.org/abs/2107.02027) [cs.CL]** |
|              | (or **[arXiv:2107.02027v1](https://arxiv.org/abs/2107.02027v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-4">4. Power Law Graph Transformer for Machine Translation and Representation Learning
</h2>

Title: [Power Law Graph Transformer for Machine Translation and Representation Learning](https://arxiv.org/abs/2107.02039)

Authors: [Burc Gokden](https://arxiv.org/search/cs?searchtype=author&query=Gokden%2C+B)

> We present the Power Law Graph Transformer, a transformer model with well defined deductive and inductive tasks for prediction and representation learning. The deductive task learns the dataset level (global) and instance level (local) graph structures in terms of learnable power law distribution parameters. The inductive task outputs the prediction probabilities using the deductive task output, similar to a transductive model. We trained our model with Turkish-English and Portuguese-English datasets from TED talk transcripts for machine translation and compared the model performance and characteristics to a transformer model with scaled dot product attention trained on the same experimental setup. We report BLEU scores of 17.79 and 28.33 on the Turkish-English and Portuguese-English translation tasks with our model, respectively. We also show how a duality between a quantization set and N-dimensional manifold representation can be leveraged to transform between local and global deductive-inductive outputs using successive application of linear and non-linear transformations end-to-end.

| Comments: | 55 pages, 39 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.02039](https://arxiv.org/abs/2107.02039) [cs.CL]** |
|           | (or **[arXiv:2107.02039v1](https://arxiv.org/abs/2107.02039v1) [cs.CL]** for this version) |





<h2 id="2021-07-06-5">5. ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
</h2>

Title: [ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2107.02137)

Authors: [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Shuohuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Shikun Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S), [Siyu Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+S), [Chao Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+C), [Junyuan Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+J), [Jiaxiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Xuyi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Yanbin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Yuxiang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Weixin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W), [Zhihua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Weibao Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+W), [Jianzhong Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+J), [Zhizhou Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+Z), [Peng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+P), [Wei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W), [Xuan Ouyang](https://arxiv.org/search/cs?searchtype=author&query=Ouyang%2C+X), [Dianhai Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D), [Hao Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+H), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.02137](https://arxiv.org/abs/2107.02137) [cs.CL]** |
|           | (or **[arXiv:2107.02137v1](https://arxiv.org/abs/2107.02137v1) [cs.CL]** for this version) |








# 2021-07-05

[Return to Index](#Index)



<h2 id="2021-07-05-1">1. Transformer-F: A Transformer network with effective methods for learning universal sentence representation
</h2>

Title: [Transformer-F: A Transformer network with effective methods for learning universal sentence representation](https://arxiv.org/abs/2107.00653)

Authors: [Yu Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y)

> The Transformer model is widely used in natural language processing for sentence representation. However, the previous Transformer-based models focus on function words that have limited meaning in most cases and could merely extract high-level semantic abstraction features. In this paper, two approaches are introduced to improve the performance of Transformers. We calculated the attention score by multiplying the part-of-speech weight vector with the correlation coefficient, which helps extract the words with more practical meaning. The weight vector is obtained by the input text sequence based on the importance of the part-of-speech. Furthermore, we fuse the features of each layer to make the sentence representation results more comprehensive and accurate. In experiments, we demonstrate the effectiveness of our model Transformer-F on three standard text classification datasets. Experimental results show that our proposed model significantly boosts the performance of text classification as compared to the baseline model. Specifically, we obtain a 5.28% relative improvement over the vanilla Transformer on the simple tasks.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00653](https://arxiv.org/abs/2107.00653) [cs.CL]** |
|           | (or **[arXiv:2107.00653v1](https://arxiv.org/abs/2107.00653v1) [cs.CL]** for this version) |





<h2 id="2021-07-05-2">2. A Primer on Pretrained Multilingual Language Models
</h2>

Title: [A Primer on Pretrained Multilingual Language Models](https://arxiv.org/abs/2107.00676)

Authors: [Sumanth Doddapaneni](https://arxiv.org/search/cs?searchtype=author&query=Doddapaneni%2C+S), [Gowtham Ramesh](https://arxiv.org/search/cs?searchtype=author&query=Ramesh%2C+G), [Anoop Kunchukuttan](https://arxiv.org/search/cs?searchtype=author&query=Kunchukuttan%2C+A), [Pratyush Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+P), [Mitesh M. Khapra](https://arxiv.org/search/cs?searchtype=author&query=Khapra%2C+M+M)

> Multilingual Language Models (MLLMs) such as mBERT, XLM, XLM-R, \textit{etc.} have emerged as a viable option for bringing the power of pretraining to a large number of languages. Given their success in zero shot transfer learning, there has emerged a large body of work in (i) building bigger MLLMs covering a large number of languages (ii) creating exhaustive benchmarks covering a wider variety of tasks and languages for evaluating MLLMs (iii) analysing the performance of MLLMs on monolingual, zero shot crosslingual and bilingual tasks (iv) understanding the universal language patterns (if any) learnt by MLLMs and (v) augmenting the (often) limited capacity of MLLMs to improve their performance on seen or even unseen languages. In this survey, we review the existing literature covering the above broad areas of research pertaining to MLLMs. Based on our survey, we recommend some promising directions of future research.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00676](https://arxiv.org/abs/2107.00676) [cs.CL]** |
|           | (or **[arXiv:2107.00676v1](https://arxiv.org/abs/2107.00676v1) [cs.CL]** for this version) |







<h2 id="2021-07-05-3">3. Interactive decoding of words from visual speech recognition models
</h2>

Title: [Interactive decoding of words from visual speech recognition models](https://arxiv.org/abs/2107.00692)

Authors: [Brendan Shillingford](https://arxiv.org/search/cs?searchtype=author&query=Shillingford%2C+B), [Yannis Assael](https://arxiv.org/search/cs?searchtype=author&query=Assael%2C+Y), [Misha Denil](https://arxiv.org/search/cs?searchtype=author&query=Denil%2C+M)

> This work describes an interactive decoding method to improve the performance of visual speech recognition systems using user input to compensate for the inherent ambiguity of the task. Unlike most phoneme-to-word decoding pipelines, which produce phonemes and feed these through a finite state transducer, our method instead expands words in lockstep, facilitating the insertion of interaction points at each word position. Interaction points enable us to solicit input during decoding, allowing users to interactively direct the decoding process. We simulate the behavior of user input using an oracle to give an automated evaluation, and show promise for the use of this method for text input.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.00692](https://arxiv.org/abs/2107.00692) [cs.CL]** |
|           | (or **[arXiv:2107.00692v1](https://arxiv.org/abs/2107.00692v1) [cs.CL]** for this version) |







<h2 id="2021-07-05-4">4. Data Centric Domain Adaptation for Historical Text with OCR Errors
</h2>

Title: [Data Centric Domain Adaptation for Historical Text with OCR Errors](https://arxiv.org/abs/2107.00927)

Authors: [Luisa März](https://arxiv.org/search/cs?searchtype=author&query=März%2C+L), [Stefan Schweter](https://arxiv.org/search/cs?searchtype=author&query=Schweter%2C+S), [Nina Poerner](https://arxiv.org/search/cs?searchtype=author&query=Poerner%2C+N), [Benjamin Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+B), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> We propose new methods for in-domain and cross-domain Named Entity Recognition (NER) on historical data for Dutch and French. For the cross-domain case, we address domain shift by integrating unsupervised in-domain data via contextualized string embeddings; and OCR errors by injecting synthetic OCR errors into the source domain and address data centric domain adaptation. We propose a general approach to imitate OCR errors in arbitrary input data. Our cross-domain as well as our in-domain results outperform several strong baselines and establish state-of-the-art results. We publish preprocessed versions of the French and Dutch Europeana NER corpora.

| Comments: | 14 pages, 2 figures, 6 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2107.00927](https://arxiv.org/abs/2107.00927) [cs.CL]** |
|           | (or **[arXiv:2107.00927v1](https://arxiv.org/abs/2107.00927v1) [cs.CL]** for this version) |







# 2021-07-02

[Return to Index](#Index)



<h2 id="2021-07-02-1">1. GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph
</h2>

Title: [GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph](https://arxiv.org/abs/2107.00395)

Authors: [Yunxin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yu Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Baotian Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+B), [Qingcai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Yang Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+Y), [Xiaolong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yuxin Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Y), [Lin Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+L)

> Previous works indicate that the glyph of Chinese characters contains rich semantic information and has the potential to enhance the representation of Chinese characters. The typical method to utilize the glyph features is by incorporating them into the character embedding space. Inspired by previous methods, we innovatively propose a Chinese pre-trained representation model named as GlyphCRM, which abandons the ID-based character embedding method yet solely based on sequential character images. We render each character into a binary grayscale image and design two-channel position feature maps for it. Formally, we first design a two-layer residual convolutional neural network, namely HanGlyph to generate the initial glyph representation of Chinese characters, and subsequently adopt multiple bidirectional encoder Transformer blocks as the superstructure to capture the context-sensitive information. Meanwhile, we feed the glyph features extracted from each layer of the HanGlyph module into the underlying Transformer blocks by skip-connection method to fully exploit the glyph features of Chinese characters. As the HanGlyph module can obtain a sufficient glyph representation of any Chinese character, the long-standing out-of-vocabulary problem could be effectively solved. Extensive experimental results indicate that GlyphCRM substantially outperforms the previous BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has strong transferability and generalization on specialized fields and low-resource tasks. We hope this work could spark further research beyond the realms of well-established representation of Chinese texts.

| Comments: | 11 pages, 7 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2107.00395](https://arxiv.org/abs/2107.00395) [cs.AI]** |
|           | (or **[arXiv:2107.00395v1](https://arxiv.org/abs/2107.00395v1) [cs.AI]** for this version) |





<h2 id="2021-07-02-2">2. ESPnet-ST IWSLT 2021 Offline Speech Translation System
</h2>

Title: [ESPnet-ST IWSLT 2021 Offline Speech Translation System](https://arxiv.org/abs/2107.00636)

Authors: [Hirofumi Inaguma](https://arxiv.org/search/eess?searchtype=author&query=Inaguma%2C+H), [Brian Yan](https://arxiv.org/search/eess?searchtype=author&query=Yan%2C+B), [Siddharth Dalmia](https://arxiv.org/search/eess?searchtype=author&query=Dalmia%2C+S), [Pengcheng Gu](https://arxiv.org/search/eess?searchtype=author&query=Gu%2C+P), [Jiatong Shi](https://arxiv.org/search/eess?searchtype=author&query=Shi%2C+J), [Kevin Duh](https://arxiv.org/search/eess?searchtype=author&query=Duh%2C+K), [Shinji Watanabe](https://arxiv.org/search/eess?searchtype=author&query=Watanabe%2C+S)

> This paper describes the ESPnet-ST group's IWSLT 2021 submission in the offline speech translation track. This year we made various efforts on training data, architecture, and audio segmentation. On the data side, we investigated sequence-level knowledge distillation (SeqKD) for end-to-end (E2E) speech translation. Specifically, we used multi-referenced SeqKD from multiple teachers trained on different amounts of bitext. On the architecture side, we adopted the Conformer encoder and the Multi-Decoder architecture, which equips dedicated decoders for speech recognition and translation tasks in a unified encoder-decoder model and enables search in both source and target language spaces during inference. We also significantly improved audio segmentation by using the pyannote.audio toolkit and merging multiple short segments for long context modeling. Experimental evaluations showed that each of them contributed to large improvements in translation performance. Our best E2E system combined all the above techniques with model ensembling and achieved 31.4 BLEU on the 2-ref of tst2021 and 21.2 BLEU and 19.3 BLEU on the two single references of tst2021.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2107.00636](https://arxiv.org/abs/2107.00636) [eess.AS]** |
|           | (or **[arXiv:2107.00636v1](https://arxiv.org/abs/2107.00636v1) [eess.AS]** for this version) |





<h2 id="2021-07-02-3">3. Word-Free Spoken Language Understanding for Mandarin-Chinese
</h2>

Title: [Word-Free Spoken Language Understanding for Mandarin-Chinese](https://arxiv.org/abs/2107.00186)

Authors: [Zhiyuan Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Z), [Yuexin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Guo Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Xingyu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Akshat Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A)

> Spoken dialogue systems such as Siri and Alexa provide great convenience to people's everyday life. However, current spoken language understanding (SLU) pipelines largely depend on automatic speech recognition (ASR) modules, which require a large amount of language-specific training data. In this paper, we propose a Transformer-based SLU system that works directly on phones. This acoustic-based SLU system consists of only two blocks and does not require the presence of ASR module. The first block is a universal phone recognition system, and the second block is a Transformer-based language model for phones. We verify the effectiveness of the system on an intent classification dataset in Mandarin Chinese.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00186](https://arxiv.org/abs/2107.00186) [cs.CL]** |
|           | (or **[arXiv:2107.00186v1](https://arxiv.org/abs/2107.00186v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-4">4. The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021 </h2>



Title: [The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021]()

Authors: [Dan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Mengge Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+M), [Xiaoxi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Yuchen Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y), [Lirong Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+L)

> This paper describes USTC-NELSLIP's submissions to the IWSLT2021 Simultaneous Speech Translation task. We proposed a novel simultaneous translation model, Cross Attention Augmented Transducer (CAAT), which extends conventional RNN-T to sequence-to-sequence tasks without monotonic constraints, e.g., simultaneous translation. Experiments on speech-to-text (S2T) and text-to-text (T2T) simultaneous translation tasks shows CAAT achieves better quality-latency trade-offs compared to \textit{wait-k}, one of the previous state-of-the-art approaches. Based on CAAT architecture and data augmentation, we build S2T and T2T simultaneous translation systems in this evaluation campaign. Compared to last year's optimal systems, our S2T simultaneous translation system improves by an average of 11.3 BLEU for all latency regimes, and our T2T simultaneous translation system improves by an average of 4.6 BLEU.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2107.00279](https://arxiv.org/abs/2107.00279) [cs.CL]** |
|           | (or **[arXiv:2107.00279v1](https://arxiv.org/abs/2107.00279v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-5">5. Zero-pronoun Data Augmentation for Japanese-to-English Translation
</h2>

Title: [Zero-pronoun Data Augmentation for Japanese-to-English Translation](https://arxiv.org/abs/2107.00318)

Authors: [Ryokan Ri](https://arxiv.org/search/cs?searchtype=author&query=Ri%2C+R), [Toshiaki Nakazawa](https://arxiv.org/search/cs?searchtype=author&query=Nakazawa%2C+T), [Yoshimasa Tsuruoka](https://arxiv.org/search/cs?searchtype=author&query=Tsuruoka%2C+Y)

> For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding pronoun in the target side of the English sentence. However, although fully resolving zero pronouns often needs discourse context, in some cases, the local context within a sentence gives clues to the inference of the zero pronoun. In this study, we propose a data augmentation method that provides additional training signals for the translation model to learn correlations between local context and zero pronouns. We show that the proposed method significantly improves the accuracy of zero pronoun translation with machine translation experiments in the conversational domain.

| Comments: | WAT2021                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.00318](https://arxiv.org/abs/2107.00318) [cs.CL]** |
|           | (or **[arXiv:2107.00318v1](https://arxiv.org/abs/2107.00318v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-6">6. Modeling Target-side Inflection in Placeholder Translation
</h2>

Title: [Modeling Target-side Inflection in Placeholder Translation](https://arxiv.org/abs/2107.00334)

Authors: [Ryokan Ri](https://arxiv.org/search/cs?searchtype=author&query=Ri%2C+R), [Toshiaki Nakazawa](https://arxiv.org/search/cs?searchtype=author&query=Nakazawa%2C+T), [Yoshimasa Tsuruoka](https://arxiv.org/search/cs?searchtype=author&query=Tsuruoka%2C+Y)

> Placeholder translation systems enable the users to specify how a specific phrase is translated in the output sentence. The system is trained to output special placeholder tokens, and the user-specified term is injected into the output through the context-free replacement of the placeholder token. However, this approach could result in ungrammatical sentences because it is often the case that the specified term needs to be inflected according to the context of the output, which is unknown before the translation. To address this problem, we propose a novel method of placeholder translation that can inflect specified terms according to the grammatical construction of the output sentence. We extend the sequence-to-sequence architecture with a character-level decoder that takes the lemma of a user-specified term and the words generated from the word-level decoder to output the correct inflected form of the lemma. We evaluate our approach with a Japanese-to-English translation task in the scientific writing domain, and show that our model can incorporate specified terms in the correct form more successfully than other comparable models.

| Comments: | MT Summit 2021                                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2107.00334](https://arxiv.org/abs/2107.00334) [cs.CL]** |
|           | (or **[arXiv:2107.00334v1](https://arxiv.org/abs/2107.00334v1) [cs.CL]** for this version) |





<h2 id="2021-07-02-7">7. CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding
</h2>

Title: [CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding](https://arxiv.org/abs/2107.00440)

Authors: [Dong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Ning Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Piji Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Hai-Tao Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H)

> Despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations. Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics. Different from the image processing field, the text is discrete and few word substitutions can cause significant semantic changes. To study the impact of semantics caused by small perturbations, we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes. To address this problem, we propose Contrastive Learning with semantIc Negative Examples (CLINE), which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking. By comparing with similar and opposite semantic examples, the model can effectively perceive the semantic changes caused by small perturbations. Empirical results show that our approach yields substantial improvements on a range of sentiment analysis, reasoning, and reading comprehension tasks. And CLINE also ensures the compactness within the same semantics and separability across different semantics in sentence-level.

| Comments: | ACL 2021, Main Conference, Long Paper                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2107.00440](https://arxiv.org/abs/2107.00440) [cs.CL]** |
|           | (or **[arXiv:2107.00440v1](https://arxiv.org/abs/2107.00440v1) [cs.CL]** for this version) |








# 2021-07-01

[Return to Index](#Index)



<h2 id="2021-07-01-1">1. What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?
</h2>

Title: [What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?](https://arxiv.org/abs/2106.15818)

Authors: [Kelly Marchisio](https://arxiv.org/search/cs?searchtype=author&query=Marchisio%2C+K), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [David Grangier](https://arxiv.org/search/cs?searchtype=author&query=Grangier%2C+D)

> Whereas existing literature on unsupervised machine translation (MT) focuses on exploiting unsupervised techniques for low-resource language pairs where bilingual training data is scare or unavailable, we investigate whether unsupervised MT can also improve translation quality of high-resource language pairs where sufficient bitext does exist. We compare the style of correct translations generated by either supervised or unsupervised MT and find that the unsupervised output is less monotonic and more natural than supervised output. We demonstrate a way to combine the benefits of unsupervised and supervised MT into a single system, resulting in better human evaluation of quality and fluency. Our results open the door to discussions about the potential contributions of unsupervised MT in high-resource settings, and how supervised and unsupervised systems might be mutually-beneficial.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.15818](https://arxiv.org/abs/2106.15818) [cs.CL]** |
|           | (or **[arXiv:2106.15818v1](https://arxiv.org/abs/2106.15818v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-2">2. Mixed Cross Entropy Loss for Neural Machine Translation
</h2>

Title: [Mixed Cross Entropy Loss for Neural Machine Translation](https://arxiv.org/abs/2106.15880)

Authors: [Haoran Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Wei Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+W)

> In neural machine translation, cross entropy (CE) is the standard loss function in two training methods of auto-regressive models, i.e., teacher forcing and scheduled sampling. In this paper, we propose mixed cross entropy loss (mixed CE) as a substitute for CE in both training approaches. In teacher forcing, the model trained with CE regards the translation problem as a one-to-one mapping process, while in mixed CE this process can be relaxed to one-to-many. In scheduled sampling, we show that mixed CE has the potential to encourage the training and testing behaviours to be similar to each other, more effectively mitigating the exposure bias problem. We demonstrate the superiority of mixed CE over CE on several machine translation datasets, WMT'16 Ro-En, WMT'16 Ru-En, and WMT'14 En-De in both teacher forcing and scheduled sampling setups. Furthermore, in WMT'14 En-De, we also find mixed CE consistently outperforms CE on a multi-reference set as well as a challenging paraphrased reference set. We also found the model trained with mixed CE is able to provide a better probability distribution defined over the translation output space. Our code is available at [this https URL](https://github.com/haorannlp/mix).

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | ICML2021                                                     |
| Cite as:           | **[arXiv:2106.15880](https://arxiv.org/abs/2106.15880) [cs.CL]** |
|                    | (or **[arXiv:2106.15880v1](https://arxiv.org/abs/2106.15880v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-3">3. Cross-lingual alignments of ELMo contextual embeddings
</h2>

Title: [Cross-lingual alignments of ELMo contextual embeddings](https://arxiv.org/abs/2106.15986)

Authors: [Matej Ulčar](https://arxiv.org/search/cs?searchtype=author&query=Ulčar%2C+M), [Marko Robnik-Šikonja](https://arxiv.org/search/cs?searchtype=author&query=Robnik-Šikonja%2C+M)

> Building machine learning prediction models for a specific NLP task requires sufficient training data, which can be difficult to obtain for low-resource languages. Cross-lingual embeddings map word embeddings from a low-resource language to a high-resource language so that a prediction model trained on data from the high-resource language can also be used in the low-resource language. To produce cross-lingual mappings of recent contextual embeddings, anchor points between the embedding spaces have to be words in the same context. We address this issue with a new method for creating datasets for cross-lingual contextual alignments. Based on that, we propose novel cross-lingual mapping methods for ELMo embeddings. Our linear mapping methods use existing vecmap and MUSE alignments on contextual ELMo embeddings. Our new nonlinear ELMoGAN mapping method is based on GANs and does not assume isomorphic embedding spaces. We evaluate the proposed mapping methods on nine languages, using two downstream tasks, NER and dependency parsing. The ELMoGAN method performs well on the NER task, with low cross-lingual loss compared to direct training on some languages. In the dependency parsing, linear alignment variants are more successful.

| Comments: | 26 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.15986](https://arxiv.org/abs/2106.15986) [cs.CL]** |
|           | (or **[arXiv:2106.15986v1](https://arxiv.org/abs/2106.15986v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-4">4. ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information
</h2>

Title: [ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information](https://arxiv.org/abs/2106.16038)

Authors: [Zijun Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Xiang Ao](https://arxiv.org/search/cs?searchtype=author&query=Ao%2C+X), [Qing He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Q), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding. In this work, we propose ChineseBERT, which incorporates both the {\it glyph} and {\it pinyin} information of Chinese characters into language model pretraining. The glyph embedding is obtained based on different fonts of a Chinese character, being able to capture character semantics from the visual features, and the pinyin embedding characterizes the pronunciation of Chinese characters, which handles the highly prevalent heteronym phenomenon in Chinese (the same character has different pronunciations with different meanings). Pretrained on large-scale unlabeled Chinese corpus, the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps. The porpsoed model achieves new SOTA performances on a wide range of Chinese NLP tasks, including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition. Code and pretrained models are publicly available at [this https URL](https://github.com/ShannonAI/ChineseBert).

| Comments: | To appear at ACL2021                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2106.16038](https://arxiv.org/abs/2106.16038) [cs.CL]** |
|           | (or **[arXiv:2106.16038v1](https://arxiv.org/abs/2106.16038v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-5">5. IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task
</h2>

Title: [IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task](https://arxiv.org/abs/2106.16055)

Authors: [Pavel Denisov](https://arxiv.org/search/cs?searchtype=author&query=Denisov%2C+P), [Manuel Mager](https://arxiv.org/search/cs?searchtype=author&query=Mager%2C+M), [Ngoc Thang Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+N+T)

> This paper describes the submission to the IWSLT 2021 Low-Resource Speech Translation Shared Task by IMS team. We utilize state-of-the-art models combined with several data augmentation, multi-task and transfer learning approaches for the automatic speech recognition (ASR) and machine translation (MT) steps of our cascaded system. Moreover, we also explore the feasibility of a full end-to-end speech translation (ST) model in the case of very constrained amount of ground truth labeled data. Our best system achieves the best performance among all submitted systems for Congolese Swahili to English and French with BLEU scores 7.7 and 13.7 respectively, and the second best result for Coastal Swahili to English with BLEU score 14.9.

| Comments: | IWSLT 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2106.16055](https://arxiv.org/abs/2106.16055) [cs.CL]** |
|           | (or **[arXiv:2106.16055v1](https://arxiv.org/abs/2106.16055v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-6">6. XLM-E: Cross-lingual Language Model Pre-training via ELECTRA
</h2>

Title: [XLM-E: Cross-lingual Language Model Pre-training via ELECTRA](https://arxiv.org/abs/2106.16138)

Authors: [Zewen Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi%2C+Z), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Saksham Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal%2C+S), [Payal Bajaj](https://arxiv.org/search/cs?searchtype=author&query=Bajaj%2C+P), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> In this paper, we introduce ELECTRA-style tasks to cross-lingual language model pre-training. Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora. Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost. Moreover, analysis shows that XLM-E tends to obtain better cross-lingual transferability.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2106.16138](https://arxiv.org/abs/2106.16138) [cs.CL]** |
|           | (or **[arXiv:2106.16138v1](https://arxiv.org/abs/2106.16138v1) [cs.CL]** for this version) |





<h2 id="2021-07-01-7">7. On the Power of Saturated Transformers: A View from Circuit Complexity
</h2>

Title: [On the Power of Saturated Transformers: A View from Circuit Complexity](https://arxiv.org/abs/2106.16213)

Authors: [William Merrill](https://arxiv.org/search/cs?searchtype=author&query=Merrill%2C+W), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y), [Roy Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+R), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> Transformers have become a standard architecture for many NLP problems. This has motivated theoretically analyzing their capabilities as models of language, in order to understand what makes them successful, and what their potential weaknesses might be. Recent work has shown that transformers with hard attention are quite limited in capacity, and in fact can be simulated by constant-depth circuits. However, hard attention is a restrictive assumption, which may complicate the relevance of these results for practical transformers. In this work, we analyze the circuit complexity of transformers with saturated attention: a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We show that saturated transformers transcend the limitations of hard-attention transformers. With some minor assumptions, we prove that the number of bits needed to represent a saturated transformer memory vector is O(logn), which implies saturated transformers can be simulated by log-depth circuits. Thus, the jump from hard to saturated attention can be understood as increasing the transformer's effective circuit depth by a factor of O(logn).

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computational Complexity (cs.CC); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2106.16213](https://arxiv.org/abs/2106.16213) [cs.CL]** |
|           | (or **[arXiv:2106.16213v1](https://arxiv.org/abs/2106.16213v1) [cs.CL]** for this version) |




# Daily arXiv: Machine Translation - October, 2021

# Index


- [2021-10-26](#2021-10-26)

  - [1. Alignment Attention by Matching Key and Query Distributions](#2021-10-26-1)
  - [2. Generating Watermarked Adversarial Texts](#2021-10-26-2)
  - [3. PhoMT: A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation](#2021-10-26-3)
  - [4. Sentence Punctuation for Collaborative Commentary Generation in Esports Live-Streaming](#2021-10-26-4)
  - [5. Understanding the Impact of UGC Specificities on Translation Quality](#2021-10-26-5)
  - [6. Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models](#2021-10-26-6)
  - [7. Generating artificial texts as substitution or complement of training data](#2021-10-26-7)
- [2021-10-25](#2021-10-25)
  - [1. VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing](#2021-10-25-1)
  - [2. Adaptive Bridge between Training and Inference for Dialogue](#2021-10-25-2)
  - [3. Lightweight Decoding Strategies for Increasing Specificity](#2021-10-25-3)
  - [4. Deep learning-based NLP Data Pipeline for EHR Scanned Document Information Extraction](#2021-10-25-4)
  - [5. Cleaning Dirty Books: Post-OCR Processing for Previously Scanned Texts](#2021-10-25-5)
- [2021-10-22](#2021-10-22)
  - [1. Knowledge distillation from language model to acoustic model: a hierarchical multi-task learning approach](#2021-10-22-1)
  - [2. SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training](#2021-10-22-2)
  - [3. Interpreting Deep Learning Models in Natural Language Processing: A Review](#2021-10-22-3)
  - [4. Multilingual Unsupervised Neural Machine Translation with Denoising Adapters](#2021-10-22-4)
  - [5. Continual Learning in Multilingual NMT via Language-Specific Embeddings](#2021-10-22-5)
  - [6. SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation](#2021-10-22-6)
  - [7. Contrastive Document Representation Learning with Graph Attention Networks](#2021-10-22-7)
  - [8. Improving Non-autoregressive Generation with Mixup Training](#2021-10-22-8)
- [2021-10-20](#2021-10-20)

  - [1. Unifying Multimodal Transformer for Bi-directional Image and Text Generation](#2021-10-20-1)
  - [2. A Picture is Worth a Thousand Words: A Unified System for Diverse Captions and Rich Images Generation](#2021-10-20-2)
  - [3. Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters](#2021-10-20-3)
  - [4. Entity Relation Extraction as Dependency Parsing in Visually Rich Documents](#2021-10-20-4)
- [2021-10-19](#2021-10-19)

  - [1. A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models](#2021-10-19-1)
  - [2. Invariant Language Modeling](#2021-10-19-2)
  - [3. EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks](#2021-10-19-3)
  - [4. MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](#2021-10-19-4)
  - [5. Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation](#2021-10-19-5)
  - [6. Virtual Augmentation Supported Contrastive Learning of Sentence Representations](#2021-10-19-6)
  - [7. GNN-LM: Language Modeling based on Global Contexts via GNN](#2021-10-19-7)
  - [8. Predicting the Performance of Multilingual NLP Models](#2021-10-19-8)
- [2021-10-18](#2021-10-18)
  - [1. StreaMulT: Streaming Multimodal Transformer for Heterogeneous and Arbitrary Long Sequential Data](#2021-10-18-1)
  - [2. Multitask Prompted Training Enables Zero-Shot Task Generalization](#2021-10-18-2)
  - [3. Alternative Input Signals Ease Transfer in Multilingual Machine Translation](#2021-10-18-3)
  - [4. Multilingual Neural Machine Translation:Can Linguistic Hierarchies Help?](#2021-10-18-4)
  - [5. SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer](#2021-10-18-5)
  - [6. Breaking Down Multilingual Machine Translation](#2021-10-18-6)
  - [7. Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm](#2021-10-18-7)
  - [8. Why don't people use character-level machine translation?](#2021-10-18-8)
  - [9. Incremental Speech Synthesis For Speech-To-Speech Translation](#2021-10-18-9)
  - [10. Tricks for Training Sparse Translation Models](#2021-10-18-10)
  - [11. Direct simultaneous speech to speech translation](#2021-10-18-11)
- [2021-10-15](#2021-10-15)
  - [1. Evaluating Off-the-Shelf Machine Listening and Natural Language Models for Automated Audio Captioning](#2021-10-15-1)
  - [2. Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits](#2021-10-15-2)
  - [3. LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5](#2021-10-15-3)
  - [4. An Empirical Investigation of Multi-bridge Multilingual NMT models](#2021-10-15-4)
  - [5. Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision](#2021-10-15-5)
  - [6. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning](#2021-10-15-6)
  - [7. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](#2021-10-15-7)
- [2021-10-14](#2021-10-14)

  - [1. Learning Compact Metrics for MT](#2021-10-14-1)
  - [2. Tell Me How to Survey: Literature Review Made Simple with Automatic Reading Path Generation](#2021-10-14-2)
  - [3. MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators](#2021-10-14-3)
  - [4. Maximizing Efficiency of Language Model Pre-training for Learning Representation](#2021-10-14-4)
  - [5. Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese](#2021-10-14-5)
  - [6. Semantics-aware Attention Improves Neural Machine Translation](#2021-10-14-6)
- [2021-10-13](#2021-10-13)

  - [1. Doubly-Trained Adversarial Data Augmentation for Neural Machine Translation](#2021-10-13)
- [2021-10-12](#2021-10-12)

  - [1. CLIP-Adapter: Better Vision-Language Models with Feature Adapters](#2021-10-12-1)
  - [2. The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design](#2021-10-12-2)
  - [3. Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning](#2021-10-12-3)
  - [4. WeTS: A Benchmark for Translation Suggestion](#2021-10-12-4)
  - [5. It is Not as Good as You Think! Evaluating Simultaneous Machine Translation on Interpretation Data](#2021-10-12-5)
  - [6. Unsupervised Neural Machine Translation with Generative Language Models Only](#2021-10-12-6)
- [2021-10-11](#2021-10-11)
  - [1. Speeding up Deep Model Training by Sharing Weights and Then Unsharing](#2021-10-11-1)
  - [2. QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks](#2021-10-11-2)
  - [3. M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining](#2021-10-11-3)
  - [4. Iterative Decoding for Compositional Generalization in Transformers](#2021-10-11-4)
  - [5. Machine Translation Verbosity Control for Automatic Dubbing](#2021-10-11-5)
  - [6. Text analysis and deep learning: A network approach](#2021-10-11-6)
  - [7. Contrastive String Representation Learning using Synthetic Data](#2021-10-11-7)
  - [8. Local and Global Context-Based Pairwise Models for Sentence Ordering](#2021-10-11-8)
- [2021-10-08](#2021-10-08)

  - [1. Unsupervised Multimodal Language Representations using Convolutional Autoencoders](#2021-10-08-1)
  - [2. The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation](#2021-10-08-2)
  - [3. On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation](#2021-10-08-3)
  - [4. Towards Continual Knowledge Learning of Language Models](#2021-10-08-4)
- [2021-10-07](#2021-10-07)

  - [1. Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning](#2021-10-07-1)
  - [2. How BPE Affects Memorization in Transformers](#2021-10-07-2)
  - [3. Sequence-to-Sequence Lexical Normalization with Multilingual Transformers](#2021-10-07-3)
  - [4. Using Optimal Transport as Alignment Objective for fine-tuning Multilingual Contextualized Embeddings](#2021-10-07-4)
- [2021-10-06](#2021-10-06)
  - [1. OPAD: An Optimized Policy-based Active Learning Framework for Document Content Analysis](#2021-10-06-1)
  - [2. Rerunning OCR -- A Machine Learning Approach to Quality Assessment and Enhancement Prediction](#2021-10-06-2)
  - [3. On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation](#2021-10-06-3)
  - [4. Data Augmentation Approaches in Natural Language Processing: A Survey](#2021-10-06-4)
  - [5. Sicilian Translator: A Recipe for Low-Resource NMT](#2021-10-06-5)
  - [6. Transfer Learning for Multi-lingual Tasks -- a Survey](#2021-10-06-6)
  - [7. Structured Prediction in NLP -- A survey](#2021-10-06-7)
  - [8. Interactively Generating Explanations for Transformer-based Language Models](#2021-10-06-8)
- [2021-10-05](#2021-10-05)

  - [1. Improving Zero-shot Multilingual Neural Machine Translation for Low-Resource Languages](#2021-10-05-1)
- [2021-10-04](#2021-10-04)

  - [1. Improving Punctuation Restoration for Speech Transcripts via External Data](#2021-10-04-1)
  - [2. A Survey of Knowledge Enhanced Pre-trained Models](#2021-10-04-2)
  - [3. Attention based Sequence to Sequence Learning for Machine Translation of Low Resourced Indic Languages -- A case of Sanskrit to Hindi](#2021-10-04-3)
- [2021-10-01](#2021-10-01)
  - [1. Phonetic Word Embeddings](#2021-10-01-1)
  - [2. Improved statistical machine translation using monolingual paraphrases](#2021-10-01-2)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-10-26

[Return to Index](#Index)



<h2 id="2021-10-26-1">1. Alignment Attention by Matching Key and Query Distributions
</h2>

Title: [Alignment Attention by Matching Key and Query Distributions](https://arxiv.org/abs/2110.12567)

Authors: [Shujian Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Xinjie Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+X), [Huangjie Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H), [Korawat Tanwisuth](https://arxiv.org/search/cs?searchtype=author&query=Tanwisuth%2C+K), [Mingyuan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

> The neural attention mechanism has been incorporated into deep neural networks to achieve state-of-the-art performance in various domains. Most such models use multi-head self-attention which is appealing for the ability to attend to information from different perspectives. This paper introduces alignment attention that explicitly encourages self-attention to match the distributions of the key and query within each head. The resulting alignment attention networks can be optimized as an unsupervised regularization in the existing attention framework. It is simple to convert any models with self-attention, including pre-trained ones, to the proposed alignment attention. On a variety of language understanding tasks, we show the effectiveness of our method in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our approach on graph attention and visual question answering, showing the great potential of incorporating our alignment method into various attention-related tasks.

| Comments: | NeurIPS 2021; Our code is publicly available at [this https URL](https://github.com/szhang42/alignment_attention) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2110.12567](https://arxiv.org/abs/2110.12567) [cs.LG]** |
|           | (or **[arXiv:2110.12567v1](https://arxiv.org/abs/2110.12567v1) [cs.LG]** for this version) |







<h2 id="2021-10-26-2">2. Generating Watermarked Adversarial Texts
</h2>

Title: [Generating Watermarked Adversarial Texts](https://arxiv.org/abs/2110.12948)

Authors: [Mingjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M), [Hanzhou Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Xinpeng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X)

> Adversarial example generation has been a hot spot in recent years because it can cause deep neural networks (DNNs) to misclassify the generated adversarial examples, which reveals the vulnerability of DNNs, motivating us to find good solutions to improve the robustness of DNN models. Due to the extensiveness and high liquidity of natural language over the social networks, various natural language based adversarial attack algorithms have been proposed in the literature. These algorithms generate adversarial text examples with high semantic quality. However, the generated adversarial text examples may be maliciously or illegally used. In order to tackle with this problem, we present a general framework for generating watermarked adversarial text examples. For each word in a given text, a set of candidate words are determined to ensure that all the words in the set can be used to either carry secret bits or facilitate the construction of adversarial example. By applying a word-level adversarial text generation algorithm, the watermarked adversarial text example can be finally generated. Experiments show that the adversarial text examples generated by the proposed method not only successfully fool advanced DNN models, but also carry a watermark that can effectively verify the ownership and trace the source of the adversarial examples. Moreover, the watermark can still survive after attacked with adversarial example generation algorithms, which has shown the applicability and superiority.

| Comments: | [this https URL](https://scholar.google.com/citations?user=IdiF7M0AAAAJ&hl=en) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Cryptography and Security (cs.CR)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2110.12948](https://arxiv.org/abs/2110.12948) [cs.CR]** |
|           | (or **[arXiv:2110.12948v1](https://arxiv.org/abs/2110.12948v1) [cs.CR]** for this version) |





<h2 id="2021-10-26-3">3. PhoMT: A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation
</h2>

Title: [PhoMT: A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation](https://arxiv.org/abs/2110.12199)

Authors: [Long Doan](https://arxiv.org/search/cs?searchtype=author&query=Doan%2C+L), [Linh The Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+L+T), [Nguyen Luong Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+N+L), [Thai Hoang](https://arxiv.org/search/cs?searchtype=author&query=Hoang%2C+T), [Dat Quoc Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+D+Q)

> We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs, which is 2.9M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong neural baselines and well-known automatic translation engines on our dataset and find that in both automatic and human evaluations: the best performance is obtained by fine-tuning the pre-trained sequence-to-sequence denoising auto-encoder mBART. To our best knowledge, this is the first large-scale Vietnamese-English machine translation study. We hope our publicly available dataset and study can serve as a starting point for future research and applications on Vietnamese-English machine translation.

| Comments: | To appear in Proceedings of EMNLP 2021 (main conference). The first three authors contribute equally to this work |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.12199](https://arxiv.org/abs/2110.12199) [cs.CL]** |
|           | (or **[arXiv:2110.12199v1](https://arxiv.org/abs/2110.12199v1) [cs.CL]** for this version) |





<h2 id="2021-10-26-4">4. Sentence Punctuation for Collaborative Commentary Generation in Esports Live-Streaming
</h2>

Title: [Sentence Punctuation for Collaborative Commentary Generation in Esports Live-Streaming](https://arxiv.org/abs/2110.12416)

Authors: [Hong Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Junjie H. Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J+H), [Xiaoling Ling](https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+X), [Pujana Paliyawan](https://arxiv.org/search/cs?searchtype=author&query=Paliyawan%2C+P)

> To solve the existing sentence punctuation problem for collaborative commentary generation in Esports live-streaming, this paper presents two strategies for sentence punctuation for text sequences of game commentary, that is, punctuating sentences by two or three text sequence(s) originally punctuated by Youtube to obtain a complete sentence of commentary. We conducted comparative experiments utilizing and fine-tuning a state-of-the-art pre-trained generative language model among two strategies and the baseline to generate collaborative commentary. Both objective evaluations by automatic metrics and subjective analyses showed that our strategy of punctuating sentences by two text sequences outperformed the baseline.

| Comments: | 2 pages, review manuscript, accepted by IEEE ICCE 2022       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Multimedia (cs.MM)     |
| Cite as:  | **[arXiv:2110.12416](https://arxiv.org/abs/2110.12416) [cs.CL]** |
|           | (or **[arXiv:2110.12416v1](https://arxiv.org/abs/2110.12416v1) [cs.CL]** for this version) |







<h2 id="2021-10-26-5">5. Understanding the Impact of UGC Specificities on Translation Quality
</h2>

Title: [Understanding the Impact of UGC Specificities on Translation Quality](https://arxiv.org/abs/2110.12551)

Authors: [José Carlos Rosales Núñez](https://arxiv.org/search/cs?searchtype=author&query=Núñez%2C+J+C+R), [Djamé Seddah](https://arxiv.org/search/cs?searchtype=author&query=Seddah%2C+D), [Guillaume Wisniewski](https://arxiv.org/search/cs?searchtype=author&query=Wisniewski%2C+G)

> This work takes a critical look at the evaluation of user-generated content automatic translation, the well-known specificities of which raise many challenges for MT. Our analyses show that measuring the average-case performance using a standard metric on a UGC test set falls far short of giving a reliable image of the UGC translation quality. That is why we introduce a new data set for the evaluation of UGC translation in which UGC specificities have been manually annotated using a fine-grained typology. Using this data set, we conduct several experiments to measure the impact of different kinds of UGC specificities on translation quality, more precisely than previously possible.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.12551](https://arxiv.org/abs/2110.12551) [cs.CL]** |
|           | (or **[arXiv:2110.12551v1](https://arxiv.org/abs/2110.12551v1) [cs.CL]** for this version) |





<h2 id="2021-10-26-6">6. Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models
</h2>

Title: [Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models](https://arxiv.org/abs/2110.12552)

Authors: [José Carlos Rosales Núñez](https://arxiv.org/search/cs?searchtype=author&query=Núñez%2C+J+C+R), [Guillaume Wisniewski](https://arxiv.org/search/cs?searchtype=author&query=Wisniewski%2C+G), [Djamé Seddah](https://arxiv.org/search/cs?searchtype=author&query=Seddah%2C+D)

> This work explores the capacities of character-based Neural Machine Translation to translate noisy User-Generated Content (UGC) with a strong focus on exploring the limits of such approaches to handle productive UGC phenomena, which almost by definition, cannot be seen at training time. Within a strict zero-shot scenario, we first study the detrimental impact on translation performance of various user-generated content phenomena on a small annotated dataset we developed, and then show that such models are indeed incapable of handling unknown letters, which leads to catastrophic translation failure once such characters are encountered. We further confirm this behavior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.12552](https://arxiv.org/abs/2110.12552) [cs.CL]** |
|           | (or **[arXiv:2110.12552v1](https://arxiv.org/abs/2110.12552v1) [cs.CL]** for this version) |



<h2 id="2021-10-26-7">7. Generating artificial texts as substitution or complement of training data
</h2>

Title: [Generating artificial texts as substitution or complement of training data](https://arxiv.org/abs/2110.13016)

Authors: [Vincent Claveau](https://arxiv.org/search/cs?searchtype=author&query=Claveau%2C+V), [Antoine Chaffin](https://arxiv.org/search/cs?searchtype=author&query=Chaffin%2C+A), [Ewa Kijak](https://arxiv.org/search/cs?searchtype=author&query=Kijak%2C+E)

> The quality of artificially generated texts has considerably improved with the advent of transformers. The question of using these models to generate learning data for supervised learning tasks naturally arises. In this article, this question is explored under 3 aspects: (i) are artificial data an efficient complement? (ii) can they replace the original data when those are not available or cannot be distributed for confidentiality reasons? (iii) can they improve the explainability of classifiers? Different experiments are carried out on Web-related classification tasks -- namely sentiment analysis on product reviews and Fake News detection -- using artificially generated data by fine-tuned GPT-2 models. The results show that such artificial data can be used in a certain extend but require pre-processing to significantly improve performance. We show that bag-of-word approaches benefit the most from such data augmentation.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| Cite as:  | **[arXiv:2110.13016](https://arxiv.org/abs/2110.13016) [cs.CL]** |
|           | (or **[arXiv:2110.13016v1](https://arxiv.org/abs/2110.13016v1) [cs.CL]** for this version) |







# 2021-10-25

[Return to Index](#Index)



<h2 id="2021-10-25-1">1. VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing
</h2>

Title: [VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing](https://arxiv.org/abs/2110.11338)

Authors: [Lisai Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Hongfa Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Qingcai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Yimeng Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y), [Zhonghua Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Dejiang Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+D), [Zhao Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Z), [Joanna Siebert](https://arxiv.org/search/cs?searchtype=author&query=Siebert%2C+J), [Yunpeng Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+Y)

> Vision-language transformers (VL transformers) have shown impressive accuracy in cross-modal retrieval. However, most of the existing VL transformers use early-interaction dataflow that computes a joint representation for the text-image input. In the retrieval stage, such models need to infer on all the matched text-image combinations, which causes high computing costs. The goal of this paper is to decompose the early-interaction dataflow inside the pre-trained VL transformer to achieve acceleration while maintaining its outstanding accuracy. To achieve this, we propose a novel Vision-language Transformer Decomposing (VLDeformer) to modify the VL transformer as an individual encoder for a single image or text through contrastive learning, which accelerates retrieval speed by thousands of times. Meanwhile, we propose to compose bi-modal hard negatives for the contrastive learning objective, which enables the VLDeformer to maintain the outstanding accuracy of the backbone VL transformer. Extensive experiments on COCO and Flickr30k datasets demonstrate the superior performance of the proposed method. Considering both effectiveness and efficiency, VLDeformer provides a superior selection for cross-modal retrieval in the similar pre-training datascale.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.11338](https://arxiv.org/abs/2110.11338) [cs.CV]** |
|           | (or **[arXiv:2110.11338v1](https://arxiv.org/abs/2110.11338v1) [cs.CV]** for this version) |





<h2 id="2021-10-25-2">2. Adaptive Bridge between Training and Inference for Dialogue
</h2>

Title: [Adaptive Bridge between Training and Inference for Dialogue](https://arxiv.org/abs/2110.11560)

Authors: [Haoran Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Hainan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Yanyan Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+Y), [Hongshen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H), [Zhuoye Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Z), [Yanyan Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Y)

> Although exposure bias has been widely studied in some NLP tasks, it faces its unique challenges in dialogue response generation, the representative one-to-various generation scenario. In real human dialogue, there are many appropriate responses for the same context, not only with different expressions, but also with different topics. Therefore, due to the much bigger gap between various ground-truth responses and the generated synthetic response, exposure bias is more challenging in dialogue generation task. What's more, as MLE encourages the model to only learn the common words among different ground-truth responses, but ignores the interesting and specific parts, exposure bias may further lead to the common response generation problem, such as "I don't know" and "HaHa?" In this paper, we propose a novel adaptive switching mechanism, which learns to automatically transit between ground-truth learning and generated learning regarding the word-level matching score, such as the cosine similarity. Experimental results on both Chinese STC dataset and English Reddit dataset, show that our adaptive method achieves a significant improvement in terms of metric-based evaluation and human evaluation, as compared with the state-of-the-art exposure bias approaches. Further analysis on NMT task also shows that our model can achieve a significant improvement.

| Comments: | EMNLP2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2110.11560](https://arxiv.org/abs/2110.11560) [cs.CL]** |
|           | (or **[arXiv:2110.11560v1](https://arxiv.org/abs/2110.11560v1) [cs.CL]** for this version) |







<h2 id="2021-10-25-3">3. Lightweight Decoding Strategies for Increasing Specificity
</h2>

Title: [Lightweight Decoding Strategies for Increasing Specificity](https://arxiv.org/abs/2110.11850)

Authors: [Katy Ilonka Gero](https://arxiv.org/search/cs?searchtype=author&query=Gero%2C+K+I), [Chris Kedzie](https://arxiv.org/search/cs?searchtype=author&query=Kedzie%2C+C), [Savvas Petridis](https://arxiv.org/search/cs?searchtype=author&query=Petridis%2C+S), [Lydia Chilton](https://arxiv.org/search/cs?searchtype=author&query=Chilton%2C+L)

> Language models are known to produce vague and generic outputs. We propose two unsupervised decoding strategies based on either word-frequency or point-wise mutual information to increase the specificity of any model that outputs a probability distribution over its vocabulary at generation time. We test the strategies in a prompt completion task; with human evaluations, we find that both strategies increase the specificity of outputs with only modest decreases in sensibility. We also briefly present a summarization use case, where these strategies can produce more specific summaries.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.11850](https://arxiv.org/abs/2110.11850) [cs.CL]** |
|           | (or **[arXiv:2110.11850v1](https://arxiv.org/abs/2110.11850v1) [cs.CL]** for this version) |





<h2 id="2021-10-25-4">4. Deep learning-based NLP Data Pipeline for EHR Scanned Document Information Extraction
</h2>

Title: [Deep learning-based NLP Data Pipeline for EHR Scanned Document Information Extraction](https://arxiv.org/abs/2110.11864)

Authors: [Enshuo Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+E) (1, 3, and 4), [Ioannis Malagaris](https://arxiv.org/search/cs?searchtype=author&query=Malagaris%2C+I) (1), [Yong-Fang Kuo](https://arxiv.org/search/cs?searchtype=author&query=Kuo%2C+Y) (1), [Rizwana Sultana](https://arxiv.org/search/cs?searchtype=author&query=Sultana%2C+R) (2), [Kirk Roberts](https://arxiv.org/search/cs?searchtype=author&query=Roberts%2C+K) (3) ((1) Office of Biostatistics, (2) Division of Pulmonary, Critical Care and Sleep Medicine, Department of Internal Medicine, University of Texas Medical Branch, Galveston, Texas, USA. (3) School of Biomedical Informatics, University of Texas Health Science Center at Houston, Houston, Texas, USA. (4) Center for Outcomes Research, Houston Methodist, Houston, TX, USA.)

> Scanned documents in electronic health records (EHR) have been a challenge for decades, and are expected to stay in the foreseeable future. Current approaches for processing often include image preprocessing, optical character recognition (OCR), and text mining. However, there is limited work that evaluates the choice of image preprocessing methods, the selection of NLP models, and the role of document layout. The impact of each element remains unknown. We evaluated this method on a use case of two key indicators for sleep apnea, Apnea hypopnea index (AHI) and oxygen saturation (SaO2) values, from scanned sleep study reports. Our data that included 955 manually annotated reports was secondarily utilized from a previous study in the University of Texas Medical Branch. We performed image preprocessing: gray-scaling followed by 1 iteration of dilating and erode, and 20% contrast increasing. The OCR was implemented with the Tesseract OCR engine. A total of seven Bag-of-Words models (Logistic Regression, Ridge Regression, Lasso Regression, Support Vector Machine, k-Nearest Neighbor, Naïve Bayes, and Random Forest) and three deep learning-based models (BiLSTM, BERT, and Clinical BERT) were evaluated. We also evaluated the combinations of image preprocessing methods (gray-scaling, dilate & erode, increased contrast by 20%, increased contrast by 60%), and two deep learning architectures (with and without structured input that provides document layout information). Our proposed method using Clinical BERT reached an AUROC of 0.9743 and document accuracy of 94.76% for AHI, and an AUROC of 0.9523, and document accuracy of 91.61% for SaO2. We demonstrated the proper use of image preprocessing and document layout could be beneficial to scanned document processing.

| Comments: | 6 tables, 7 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2110.11864](https://arxiv.org/abs/2110.11864) [cs.CL]** |
|           | (or **[arXiv:2110.11864v1](https://arxiv.org/abs/2110.11864v1) [cs.CL]** for this version) |



<h2 id="2021-10-25-5">5. Cleaning Dirty Books: Post-OCR Processing for Previously Scanned Texts
</h2>

Title: [Cleaning Dirty Books: Post-OCR Processing for Previously Scanned Texts](https://arxiv.org/abs/2110.11934)

Authors: Cleaning Dirty Books: Post-OCR Processing for Previously Scanned Texts

[Allen Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+A), [Charuta Pethe](https://arxiv.org/search/cs?searchtype=author&query=Pethe%2C+C), [Naoya Inoue](https://arxiv.org/search/cs?searchtype=author&query=Inoue%2C+N), [Steve Skiena](https://arxiv.org/search/cs?searchtype=author&query=Skiena%2C+S)

> Substantial amounts of work are required to clean large collections of digitized books for NLP analysis, both because of the presence of errors in the scanned text and the presence of duplicate volumes in the corpora. In this paper, we consider the issue of deduplication in the presence of optical character recognition (OCR) errors. We present methods to handle these errors, evaluated on a collection of 19,347 texts from the Project Gutenberg dataset and 96,635 texts from the HathiTrust Library. We demonstrate that improvements in language models now enable the detection and correction of OCR errors without consideration of the scanning image itself. The inconsistencies found by aligning pairs of scans of the same underlying work provides training data to build models for detecting and correcting errors. We identify the canonical version for each of 17,136 repeatedly-scanned books from 58,808 scans. Finally, we investigate methods to detect and correct errors in single-copy texts. We show that on average, our method corrects over six times as many errors as it introduces. We also provide interesting analysis on the relation between scanning quality and other factors such as location and publication year.

| Comments: | Accepted for Findings of EMNLP 2021                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.11934](https://arxiv.org/abs/2110.11934) [cs.CL]** |
|           | (or **[arXiv:2110.11934v1](https://arxiv.org/abs/2110.11934v1) [cs.CL]** for this version) |






# 2021-10-22

[Return to Index](#Index)



<h2 id="2021-10-22-1">1. Knowledge distillation from language model to acoustic model: a hierarchical multi-task learning approach
</h2>

Title: [Knowledge distillation from language model to acoustic model: a hierarchical multi-task learning approach](https://arxiv.org/abs/2110.10429)

Authors: [Mun-Hak Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+M), [Joon-Hyuk Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+J)

> The remarkable performance of the pre-trained language model (LM) using self-supervised learning has led to a major paradigm shift in the study of natural language processing. In line with these changes, leveraging the performance of speech recognition systems with massive deep learning-based LMs is a major topic of speech recognition research. Among the various methods of applying LMs to speech recognition systems, in this paper, we focus on a cross-modal knowledge distillation method that transfers knowledge between two types of deep neural networks with different modalities. We propose an acoustic model structure with multiple auxiliary output layers for cross-modal distillation and demonstrate that the proposed method effectively compensates for the shortcomings of the existing label-interpolation-based distillation method. In addition, we extend the proposed method to a hierarchical distillation method using LMs trained in different units (senones, monophones, and subwords) and reveal the effectiveness of the hierarchical distillation method through an ablation study.

| Comments: | 4page + 1page for citation + 2 pages for appendix            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2110.10429](https://arxiv.org/abs/2110.10429) [cs.LG]** |
|           | (or **[arXiv:2110.10429v1](https://arxiv.org/abs/2110.10429v1) [cs.LG]** for this version) |





<h2 id="2021-10-22-2">2. SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training
</h2>

Title: [SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training](https://arxiv.org/abs/2110.10329)

Authors: [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Yu-an Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+Y), [Nan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+N), [Anmol Gulati](https://arxiv.org/search/cs?searchtype=author&query=Gulati%2C+A), [Ye Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+Y), [Jonathan H. Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+J+H), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M), [Jason Riesa](https://arxiv.org/search/cs?searchtype=author&query=Riesa%2C+J), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A), [Yu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST~2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.10329](https://arxiv.org/abs/2110.10329) [cs.CL]** |
|           | (or **[arXiv:2110.10329v1](https://arxiv.org/abs/2110.10329v1) [cs.CL]** for this version) |





<h2 id="2021-10-22-3">3. Interpreting Deep Learning Models in Natural Language Processing: A Review
</h2>

Title: [Interpreting Deep Learning Models in Natural Language Processing: A Review](https://arxiv.org/abs/2110.10470)

Authors: [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Diyi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+D), [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Tianwei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Qiu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+Q), [Guoyin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Neural network models have achieved state-of-the-art performances in a wide range of natural language processing (NLP) tasks. However, a long-standing criticism against neural network models is the lack of interpretability, which not only reduces the reliability of neural NLP systems but also limits the scope of their applications in areas where interpretability is essential (e.g., health care applications). In response, the increasing interest in interpreting neural NLP models has spurred a diverse array of interpretation methods over recent years. In this survey, we provide a comprehensive review of various interpretation methods for neural models in NLP. We first stretch out a high-level taxonomy for interpretation methods in NLP, i.e., training-based approaches, test-based approaches, and hybrid approaches. Next, we describe sub-categories in each category in detail, e.g., influence-function based methods, KNN-based methods, attention-based models, saliency-based methods, perturbation-based methods, etc. We point out deficiencies of current methods and suggest some avenues for future research.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.10470](https://arxiv.org/abs/2110.10470) [cs.CL]** |
|           | (or **[arXiv:2110.10470v1](https://arxiv.org/abs/2110.10470v1) [cs.CL]** for this version) |





<h2 id="2021-10-22-4">4. Multilingual Unsupervised Neural Machine Translation with Denoising Adapters
</h2>

Title: [Multilingual Unsupervised Neural Machine Translation with Denoising Adapters](https://arxiv.org/abs/2110.10472)

Authors: [Ahmet Üstün](https://arxiv.org/search/cs?searchtype=author&query=Üstün%2C+A), [Alexandre Bérard](https://arxiv.org/search/cs?searchtype=author&query=Bérard%2C+A), [Laurent Besacier](https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L), [Matthias Gallé](https://arxiv.org/search/cs?searchtype=author&query=Gallé%2C+M)

> We consider the problem of multilingual unsupervised machine translation, translating to and from languages that only have monolingual data by using auxiliary parallel language pairs. For this problem the standard procedure so far to leverage the monolingual data is back-translation, which is computationally costly and hard to tune. 
> In this paper we propose instead to use denoising adapters, adapter layers with a denoising objective, on top of pre-trained mBART-50. In addition to the modularity and flexibility of such an approach we show that the resulting translations are on-par with back-translating as measured by BLEU, and furthermore it allows adding unseen languages incrementally.

| Comments: | Accepted as a long paper to EMNLP 2021                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.10472](https://arxiv.org/abs/2110.10472) [cs.CL]** |
|           | (or **[arXiv:2110.10472v1](https://arxiv.org/abs/2110.10472v1) [cs.CL]** for this version) |





<h2 id="2021-10-22-5">5. Continual Learning in Multilingual NMT via Language-Specific Embeddings
</h2>

Title: [Continual Learning in Multilingual NMT via Language-Specific Embeddings](https://arxiv.org/abs/2110.10478)

Authors: [Alexandre Berard](https://arxiv.org/search/cs?searchtype=author&query=Berard%2C+A)

> This paper proposes a technique for adding a new source or target language to an existing multilingual NMT model without re-training it on the initial set of languages. It consists in replacing the shared vocabulary with a small language-specific vocabulary and fine-tuning the new embeddings on the new language's parallel data. Some additional language-specific components may be trained to improve performance (e.g., Transformer layers or adapter modules). Because the parameters of the original model are not modified, its performance on the initial languages does not degrade. We show on two sets of experiments (small-scale on TED Talks, and large-scale on ParaCrawl) that this approach performs as well or better as the more costly alternatives; and that it has excellent zero-shot performance: training on English-centric data is enough to translate between the new language and any of the initial languages.

| Comments: | Accepted as a research paper to WMT 2021                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.10478](https://arxiv.org/abs/2110.10478) [cs.CL]** |
|           | (or **[arXiv:2110.10478v1](https://arxiv.org/abs/2110.10478v1) [cs.CL]** for this version) |





<h2 id="2021-10-22-6">6. SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation
</h2>

Title: [SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation](https://arxiv.org/abs/2110.10774)

Authors: [Hong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H), [Hiroya Takamura](https://arxiv.org/search/cs?searchtype=author&query=Takamura%2C+H), [Hideki Nakayama](https://arxiv.org/search/cs?searchtype=author&query=Nakayama%2C+H)

> Generating texts in scientific papers requires not only capturing the content contained within the given input but also frequently acquiring the external information called \textit{context}. We push forward the scientific text generation by proposing a new task, namely \textbf{context-aware text generation} in the scientific domain, aiming at exploiting the contributions of context in generated texts. To this end, we present a novel challenging large-scale \textbf{Sci}entific Paper Dataset for Conte\textbf{X}t-Aware Text \textbf{Gen}eration (SciXGen), consisting of well-annotated 205,304 papers with full references to widely-used objects (e.g., tables, figures, algorithms) in a paper. We comprehensively benchmark, using state-of-the-arts, the efficacy of our newly constructed SciXGen dataset in generating description and paragraph. Our dataset and benchmarks will be made publicly available to hopefully facilitate the scientific text generation research.

| Comments: | this paper was accepted by EMNLP2021-findings                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.10774](https://arxiv.org/abs/2110.10774) [cs.CL]** |
|           | (or **[arXiv:2110.10774v1](https://arxiv.org/abs/2110.10774v1) [cs.CL]** for this version) |







<h2 id="2021-10-22-7">7. Contrastive Document Representation Learning with Graph Attention Networks
</h2>

Title: [Contrastive Document Representation Learning with Graph Attention Networks](https://arxiv.org/abs/2110.10778)

Authors: [Peng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P), [Xinchi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Xiaofei Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Zhiheng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z), [Bing Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+B)

> Recent progress in pretrained Transformer-based language models has shown great success in learning contextual representation of text. However, due to the quadratic self-attention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this work, we propose to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings. This graph attention network allows us to leverage the high-level semantic structure of the document. In addition, based on our graph document model, we design a simple contrastive learning strategy to pretrain our models on a large amount of unlabeled corpus. Empirically, we demonstrate the effectiveness of our approaches in document classification and document retrieval tasks.

| Comments: | Findings of EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.10778](https://arxiv.org/abs/2110.10778) [cs.CL]** |
|           | (or **[arXiv:2110.10778v1](https://arxiv.org/abs/2110.10778v1) [cs.CL]** for this version) |



<h2 id="2021-10-22-8">8. Improving Non-autoregressive Generation with Mixup Training
</h2>

Title: [Improving Non-autoregressive Generation with Mixup Training](https://arxiv.org/abs/2110.11115)

Authors: [Ting Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+T), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Zihan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Deqing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Fuzhen Zhuang](https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+F), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Haizhen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Liangjie Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Qi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q)

> While pre-trained language models have achieved great success on various natural language understanding tasks, how to effectively leverage them into non-autoregressive generation tasks remains a challenge. To solve this problem, we present a non-autoregressive generation model based on pre-trained transformer models. To bridge the gap between autoregressive and non-autoregressive models, we propose a simple and effective iterative training method called MIx Source and pseudo Target (MIST). Unlike other iterative decoding methods, which sacrifice the inference speed to achieve better performance based on multiple decoding iterations, MIST works in the training stage and has no effect on inference time. Our experiments on three generation benchmarks including question generation, summarization and paraphrase generation, show that the proposed framework achieves the new state-of-the-art results for fully non-autoregressive models. We also demonstrate that our method can be used to a variety of pre-trained models. For instance, MIST based on the small pre-trained model also obtains comparable performance with seq2seq models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.11115](https://arxiv.org/abs/2110.11115) [cs.CL]** |
|           | (or **[arXiv:2110.11115v1](https://arxiv.org/abs/2110.11115v1) [cs.CL]** for this version) |



# 2021-10-20

[Return to Index](#Index)



<h2 id="2021-10-20-1">1. Unifying Multimodal Transformer for Bi-directional Image and Text Generation
</h2>

Title: [Unifying Multimodal Transformer for Bi-directional Image and Text Generation](https://arxiv.org/abs/2110.09753)

Authors: [Yupan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y), [Hongwei Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+H), [Bei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B), [Yutong Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y)

> We study the joint learning of image-to-text and text-to-image generations, which are naturally bi-directional tasks. Typical existing works design two separate task-specific models for each task, which impose expensive design efforts. In this work, we propose a unified image-and-text generative framework based on a single multimodal model to jointly study the bi-directional tasks. We adopt Transformer as our unified architecture for its strong performance and task-agnostic design. Specifically, we formulate both tasks as sequence generation tasks, where we represent images and text as unified sequences of tokens, and the Transformer learns multimodal interactions to generate sequences. We further propose two-level granularity feature representations and sequence-level training to improve the Transformer-based unified framework. Experiments show that our approach significantly improves previous Transformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for text-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for fine-tuned image-to-text generation on the MS-COCO dataset. Our code is available online.

| Comments: | ACM MM 2021 (Industrial Track). Code: [this https URL](https://github.com/researchmm/generate-it) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Multimedia (cs.MM) |
| DOI:      | [10.1145/3474085.3481540](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3474085.3481540&v=1d938f2d) |
| Cite as:  | **[arXiv:2110.09753](https://arxiv.org/abs/2110.09753) [cs.CV]** |
|           | (or **[arXiv:2110.09753v1](https://arxiv.org/abs/2110.09753v1) [cs.CV]** for this version) |





<h2 id="2021-10-20-2">2. A Picture is Worth a Thousand Words: A Unified System for Diverse Captions and Rich Images Generation
</h2>

Title: [A Picture is Worth a Thousand Words: A Unified System for Diverse Captions and Rich Images Generation](https://arxiv.org/abs/2110.09756)

Authors: [Yupan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y), [Bei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B), [Jianlong Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J), [Yutong Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y)

> A creative image-and-text generative AI system mimics humans' extraordinary abilities to provide users with diverse and comprehensive caption suggestions, as well as rich image creations. In this work, we demonstrate such an AI creation system to produce both diverse captions and rich images. When users imagine an image and associate it with multiple captions, our system paints a rich image to reflect all captions faithfully. Likewise, when users upload an image, our system depicts it with multiple diverse captions. We propose a unified multi-modal framework to achieve this goal. Specifically, our framework jointly models image-and-text representations with a Transformer network, which supports rich image creation by accepting multiple captions as input. We consider the relations among input captions to encourage diversity in training and adopt a non-autoregressive decoding strategy to enable real-time inference. Based on these, our system supports both diverse captions and rich images generations. Our code is available online.

| Comments: | ACM MM 2021 (Video and Demo Track). Code: [this https URL](https://github.com/researchmm/generate-it) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Multimedia (cs.MM) |
| DOI:      | [10.1145/3474085.3478561](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3474085.3478561&v=36cdb3a9) |
| Cite as:  | **[arXiv:2110.09756](https://arxiv.org/abs/2110.09756) [cs.CV]** |
|           | (or **[arXiv:2110.09756v1](https://arxiv.org/abs/2110.09756v1) [cs.CV]** for this version) |





<h2 id="2021-10-20-3">3. Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters
</h2>

Title: [Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters](https://arxiv.org/abs/2110.09574)

Authors: [Asa Cooper Stickland](https://arxiv.org/search/cs?searchtype=author&query=Stickland%2C+A+C), [Alexandre Bérard](https://arxiv.org/search/cs?searchtype=author&query=Bérard%2C+A), [Vassilina Nikoulina](https://arxiv.org/search/cs?searchtype=author&query=Nikoulina%2C+V)

> Adapter layers are lightweight, learnable units inserted between transformer layers. Recent work explores using such layers for neural machine translation (NMT), to adapt pre-trained models to new domains or language pairs, training only a small set of parameters for each new setting (language pair or domain). In this work we study the compositionality of language and domain adapters in the context of Machine Translation. We aim to study, 1) parameter-efficient adaptation to multiple domains and languages simultaneously (full-resource scenario) and 2) cross-lingual transfer in domains where parallel data is unavailable for certain language pairs (partial-resource scenario). We find that in the partial resource scenario a naive combination of domain-specific and language-specific adapters often results in `catastrophic forgetting' of the missing languages. We study other ways to combine the adapters to alleviate this issue and maximize cross-lingual transfer. With our best adapter combinations, we obtain improvements of 3-4 BLEU on average for source languages that do not have in-domain data. For target languages without in-domain data, we achieve a similar improvement by combining adapters with back-translation. Supplementary material is available at[this https URL](https://tinyurl.com/r66stbxj)

| Comments: | Accepted at The Sixth Conference in Machine Translation (WMT21) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.09574](https://arxiv.org/abs/2110.09574) [cs.CL]** |
|           | (or **[arXiv:2110.09574v1](https://arxiv.org/abs/2110.09574v1) [cs.CL]** for this version) |





<h2 id="2021-10-20-4">4. Entity Relation Extraction as Dependency Parsing in Visually Rich Documents
</h2>

Title: [Entity Relation Extraction as Dependency Parsing in Visually Rich Documents](https://arxiv.org/abs/2110.09915)

Authors: [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Bo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Rui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R), [Junjie Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+J), [Chen Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Zuyi Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+Z)

> Previous works on key information extraction from visually rich documents (VRDs) mainly focus on labeling the text within each bounding box (i.e., semantic entity), while the relations in-between are largely unexplored. In this paper, we adapt the popular dependency parsing model, the biaffine parser, to this entity relation extraction task. Being different from the original dependency parsing model which recognizes dependency relations between words, we identify relations between groups of words with layout information instead. We have compared different representations of the semantic entity, different VRD encoders, and different relation decoders. The results demonstrate that our proposed model achieves 65.96% F1 score on the FUNSD dataset. As for the real-world application, our model has been applied to the in-house customs data, achieving reliable performance in the production setting.

| Comments: | Accepted to EMNLP 2021 (main conference)                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2110.09915](https://arxiv.org/abs/2110.09915) [cs.CL]** |
|           | (or **[arXiv:2110.09915v1](https://arxiv.org/abs/2110.09915v1) [cs.CL]** for this version) |





# 2021-10-19

[Return to Index](#Index)



<h2 id="2021-10-19-1">1. A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models
</h2>

Title: [A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models](https://arxiv.org/abs/2110.08484)

Authors: [Woojeong Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+W), [Yu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Yelong Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W), [Xiang Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X)

> Large pretrained vision-language (VL) models can learn a new task with a handful of examples or generalize to a new task without fine-tuning. However, these gigantic VL models are hard to deploy for real-world applications due to their impractically huge model size and slow inference speed. In this work, we propose FewVLM, a few-shot prompt-based learner on vision-language tasks. We pretrain a sequence-to-sequence Transformer model with both prefix language modeling (PrefixLM) and masked language modeling (MaskedLM), and introduce simple prompts to improve zero-shot and few-shot performance on VQA and image captioning. Experimental results on five VQA and captioning datasets show that \method\xspace outperforms Frozen which is 31 times larger than ours by 18.2% point on zero-shot VQAv2 and achieves comparable results to a 246× larger model, PICa. We observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) MaskedLM helps few-shot VQA tasks while PrefixLM boosts captioning performance, and (3) performance significantly increases when training set size is small.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2110.08484](https://arxiv.org/abs/2110.08484) [cs.CV]** |
|           | (or **[arXiv:2110.08484v1](https://arxiv.org/abs/2110.08484v1) [cs.CV]** for this version) |





<h2 id="2021-10-19-2">2. Invariant Language Modeling
</h2>

Title: [Invariant Language Modeling](https://arxiv.org/abs/2110.08413)

Authors: [Maxime Peyrard](https://arxiv.org/search/cs?searchtype=author&query=Peyrard%2C+M), [Sarvjeet Singh Ghotra](https://arxiv.org/search/cs?searchtype=author&query=Ghotra%2C+S+S), [Martin Josifoski](https://arxiv.org/search/cs?searchtype=author&query=Josifoski%2C+M), [Vidhan Agarwal](https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+V), [Barun Patra](https://arxiv.org/search/cs?searchtype=author&query=Patra%2C+B), [Dean Carignan](https://arxiv.org/search/cs?searchtype=author&query=Carignan%2C+D), [Emre Kiciman](https://arxiv.org/search/cs?searchtype=author&query=Kiciman%2C+E), [Robert West](https://arxiv.org/search/cs?searchtype=author&query=West%2C+R)

> Modern pretrained language models are critical components of NLP pipelines. Yet, they suffer from spurious correlations, poor out-of-domain generalization, and biases. Inspired by recent progress in causal machine learning, in particular the invariant risk minimization (IRM) paradigm, we propose invariant language modeling, a framework for learning invariant representations that generalize better across multiple environments. In particular, we adapt a game-theoretic implementation of IRM (IRM-games) to language models, where the invariance emerges from a specific training schedule in which all the environments compete to optimize their own environment-specific loss by updating subsets of the model in a round-robin fashion. In a series of controlled experiments, we demonstrate the ability of our method to (i) remove structured noise, (ii) ignore specific spurious correlations without affecting global performance, and (iii) achieve better out-of-domain generalization. These benefits come with a negligible computational overhead compared to standard training, do not require changing the local loss, and can be applied to any language model architecture. We believe this framework is promising to help mitigate spurious correlations and biases in language models.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.08413](https://arxiv.org/abs/2110.08413) [cs.CL]** |
|           | (or **[arXiv:2110.08413v1](https://arxiv.org/abs/2110.08413v1) [cs.CL]** for this version) |





<h2 id="2021-10-19-3">3. EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks
</h2>

Title: [EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks](https://arxiv.org/abs/2110.08426)

Authors: [Frederick Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Siamak Shakeri](https://arxiv.org/search/cs?searchtype=author&query=Shakeri%2C+S), [Hongkun Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H), [Jing Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Encoder-decoder transformer architectures have become popular recently with the advent of T5 models. It is also more favorable over architectures like BERT for pre-training on language model task when it comes to large scale models which could take months to train given it's generality. While being able to generalize to more tasks, it is not evident if the proposed encoder-decoder architecture is the most efficient for fine-tuning on classification and regression tasks given the pre-trained model. In this work, we study fine-tuning pre-trained encoder-decoder models such as T5. Particularly, we propose \textbf{EncT5} as a way to efficiently fine-tune pre-trained encoder-decoder T5 models for classification and regression tasks by using the encoder layers. Our experimental results show that \textbf{EncT5} with less than half of the parameters of T5 performs similarly to T5 models on GLUE benchmark. We believe our proposed approach can be easily applied to any pre-trained encoder-decoder model.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.08426](https://arxiv.org/abs/2110.08426) [cs.CL]** |
|           | (or **[arXiv:2110.08426v1](https://arxiv.org/abs/2110.08426v1) [cs.CL]** for this version) |





<h2 id="2021-10-19-4">4. MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding
</h2>

Title: [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518)

Authors: [Junlong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Yiheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Lei Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document Understanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a large number of digital documents where the layout information is not fixed and needs to be interactively and dynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this paper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as HTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding tasks. The pre-trained model and code will be publicly available at [this https URL](https://aka.ms/markuplm).

| Comments: | Work in Progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.08518](https://arxiv.org/abs/2110.08518) [cs.CL]** |
|           | (or **[arXiv:2110.08518v1](https://arxiv.org/abs/2110.08518v1) [cs.CL]** for this version) |







<h2 id="2021-10-19-5">5. Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation
</h2>

Title: [Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation](https://arxiv.org/abs/2110.08547)

Authors: [Guanhua Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Yun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Jia Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+J), [Wenping Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> This paper demonstrates that multilingual pretraining, a proper fine-tuning method and a large-scale parallel dataset from multiple auxiliary languages are all critical for zero-shot translation, where the NMT model is tested on source languages unseen during supervised training. Following this idea, we present SixT++, a strong many-to-English NMT model that supports 100 source languages but is trained once with a parallel dataset from only six source languages. SixT++ initializes the decoder embedding and the full encoder with XLM-R large, and then trains the encoder and decoder layers with a simple two-stage training strategy. SixT++ achieves impressive performance on many-to-English translation. It significantly outperforms CRISS and m2m-100, two strong multilingual NMT systems, with an average gain of 7.2 and 5.0 BLEU respectively. Additionally, SixT++ offers a set of model parameters that can be further fine-tuned to develop unsupervised NMT models for low-resource languages. With back-translation on monolingual data of low-resource language, it outperforms all current state-of-the-art unsupervised methods on Nepali and Sinhal for both translating into and from English.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.08547](https://arxiv.org/abs/2110.08547) [cs.CL]** |
|           | (or **[arXiv:2110.08547v1](https://arxiv.org/abs/2110.08547v1) [cs.CL]** for this version) |





<h2 id="2021-10-19-6">6. Virtual Augmentation Supported Contrastive Learning of Sentence Representations
</h2>

Title: [Virtual Augmentation Supported Contrastive Learning of Sentence Representations](https://arxiv.org/abs/2110.08552)

Authors: [Dejiao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Wei Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+W), [Henghui Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+H), [Xiaofei Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Andrew O. Arnold](https://arxiv.org/search/cs?searchtype=author&query=Arnold%2C+A+O)

> Despite profound successes, contrastive representation learning relies on carefully designed data augmentations using domain specific knowledge. This challenge is magnified in natural language processing where no general rules exist for data augmentation due to the discrete nature of natural language. We tackle this challenge by presenting a Virtual augmentation Supported Contrastive Learning of sentence representations (VaSCL). Originating from the interpretation that data augmentation essentially constructs the neighborhoods of each training instance, we in turn utilize the neighborhood to generate effective data augmentations. Leveraging the large training batch size of contrastive learning, we approximate the neighborhood of an instance via its K-nearest in-batch neighbors in the representation space. We then define an instance discrimination task within this neighborhood, and generate the virtual augmentation in an adversarial training manner. We access the performance of VaSCL on a wide range of downstream tasks, and set a new state-of-the-art for unsupervised sentence representation learning.

| Comments: | 8 pages, 3 figures, 3 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2110.08552](https://arxiv.org/abs/2110.08552) [cs.CL]** |
|           | (or **[arXiv:2110.08552v1](https://arxiv.org/abs/2110.08552v1) [cs.CL]** for this version) |



<h2 id="2021-10-19-7">7. GNN-LM: Language Modeling based on Global Contexts via GNN
</h2>

Title: [GNN-LM: Language Modeling based on Global Contexts via GNN](https://arxiv.org/abs/2110.08743)

Authors: [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Shi Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+S), [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Tianwei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Inspired by the notion that ``{\it to copy is easier than to memorize}``, in this work, we introduce GNN-LM, which extends the vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 4.5 point improvement over its counterpart of the vanilla LM model) and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.08743](https://arxiv.org/abs/2110.08743) [cs.CL]** |
|           | (or **[arXiv:2110.08743v1](https://arxiv.org/abs/2110.08743v1) [cs.CL]** for this version) |





<h2 id="2021-10-19-8">8. Predicting the Performance of Multilingual NLP Models
</h2>

Title: [Predicting the Performance of Multilingual NLP Models](https://arxiv.org/abs/2110.08875)

Authors: [Anirudh Srinivasan](https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+A), [Sunayana Sitaram](https://arxiv.org/search/cs?searchtype=author&query=Sitaram%2C+S), [Tanuja Ganu](https://arxiv.org/search/cs?searchtype=author&query=Ganu%2C+T), [Sandipan Dandapat](https://arxiv.org/search/cs?searchtype=author&query=Dandapat%2C+S), [Kalika Bali](https://arxiv.org/search/cs?searchtype=author&query=Bali%2C+K), [Monojit Choudhury](https://arxiv.org/search/cs?searchtype=author&query=Choudhury%2C+M)

> Recent advancements in NLP have given us models like mBERT and XLMR that can serve over 100 languages. The languages that these models are evaluated on, however, are very few in number, and it is unlikely that evaluation datasets will cover all the languages that these models support. Potential solutions to the costly problem of dataset creation are to translate datasets to new languages or use template-filling based techniques for creation. This paper proposes an alternate solution for evaluating a model across languages which make use of the existing performance scores of the model on languages that a particular task has test sets for. We train a predictor on these performance scores and use this predictor to predict the model's performance in different evaluation settings. Our results show that our method is effective in filling the gaps in the evaluation for an existing set of languages, but might require additional improvements if we want it to generalize to unseen languages.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.08875](https://arxiv.org/abs/2110.08875) [cs.CL]** |
|           | (or **[arXiv:2110.08875v1](https://arxiv.org/abs/2110.08875v1) [cs.CL]** for this version) |








# 2021-10-18

[Return to Index](#Index)



<h2 id="2021-10-18-1">1. StreaMulT: Streaming Multimodal Transformer for Heterogeneous and Arbitrary Long Sequential Data
</h2>

Title: [StreaMulT: Streaming Multimodal Transformer for Heterogeneous and Arbitrary Long Sequential Data](https://arxiv.org/abs/2110.08021)

Authors: [Victor Pellegrain](https://arxiv.org/search/cs?searchtype=author&query=Pellegrain%2C+V) (1 and 2), [Myriam Tami](https://arxiv.org/search/cs?searchtype=author&query=Tami%2C+M) (2), [Michel Batteux](https://arxiv.org/search/cs?searchtype=author&query=Batteux%2C+M) (1), [Céline Hudelot](https://arxiv.org/search/cs?searchtype=author&query=Hudelot%2C+C) (2) ((1) Institut de Recherche Technologique SystemX, (2) Université Paris-Saclay, CentraleSupélec, MICS)

> This paper tackles the problem of processing and combining efficiently arbitrary long data streams, coming from different modalities with different acquisition frequencies. Common applications can be, for instance, long-time industrial or real-life systems monitoring from multimodal heterogeneous data (sensor data, monitoring report, images, etc.). To tackle this problem, we propose StreaMulT, a Streaming Multimodal Transformer, relying on cross-modal attention and an augmented memory bank to process arbitrary long input sequences at training time and run in a streaming way at inference. StreaMulT reproduces state-of-the-art results on CMU-MOSEI dataset, while being able to deal with much longer inputs than other models such as previous Multimodal Transformer.

| Comments: | 5 pages, 4 figures, submitted to ICASSP 2022                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Multimedia (cs.MM) |
| Cite as:  | **[arXiv:2110.08021](https://arxiv.org/abs/2110.08021) [cs.LG]** |
|           | (or **[arXiv:2110.08021v1](https://arxiv.org/abs/2110.08021v1) [cs.LG]** for this version) |





<h2 id="2021-10-18-2">2. Multitask Prompted Training Enables Zero-Shot Task Generalization
</h2>

Title: [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)

Authors: [Victor Sanh](https://arxiv.org/search/cs?searchtype=author&query=Sanh%2C+V), [Albert Webson](https://arxiv.org/search/cs?searchtype=author&query=Webson%2C+A), [Colin Raffel](https://arxiv.org/search/cs?searchtype=author&query=Raffel%2C+C), [Stephen H. Bach](https://arxiv.org/search/cs?searchtype=author&query=Bach%2C+S+H), [Lintang Sutawika](https://arxiv.org/search/cs?searchtype=author&query=Sutawika%2C+L), [Zaid Alyafeai](https://arxiv.org/search/cs?searchtype=author&query=Alyafeai%2C+Z), [Antoine Chaffin](https://arxiv.org/search/cs?searchtype=author&query=Chaffin%2C+A), [Arnaud Stiegler](https://arxiv.org/search/cs?searchtype=author&query=Stiegler%2C+A), [Teven Le Scao](https://arxiv.org/search/cs?searchtype=author&query=Scao%2C+T+L), [Arun Raja](https://arxiv.org/search/cs?searchtype=author&query=Raja%2C+A), [Manan Dey](https://arxiv.org/search/cs?searchtype=author&query=Dey%2C+M), [M Saiful Bari](https://arxiv.org/search/cs?searchtype=author&query=Bari%2C+M+S), [Canwen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Urmish Thakker](https://arxiv.org/search/cs?searchtype=author&query=Thakker%2C+U), [Shanya Sharma Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+S+S), [Eliza Szczechla](https://arxiv.org/search/cs?searchtype=author&query=Szczechla%2C+E), [Taewoon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+T), [Gunjan Chhablani](https://arxiv.org/search/cs?searchtype=author&query=Chhablani%2C+G), [Nihal Nayak](https://arxiv.org/search/cs?searchtype=author&query=Nayak%2C+N), [Debajyoti Datta](https://arxiv.org/search/cs?searchtype=author&query=Datta%2C+D), [Jonathan Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+J), [Mike Tian-Jian Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+M+T), [Han Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Matteo Manica](https://arxiv.org/search/cs?searchtype=author&query=Manica%2C+M), [Sheng Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+S), [Zheng Xin Yong](https://arxiv.org/search/cs?searchtype=author&query=Yong%2C+Z+X), [Harshit Pandey](https://arxiv.org/search/cs?searchtype=author&query=Pandey%2C+H), [Rachel Bawden](https://arxiv.org/search/cs?searchtype=author&query=Bawden%2C+R), [Thomas Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Trishala Neeraj](https://arxiv.org/search/cs?searchtype=author&query=Neeraj%2C+T), [Jos Rozen](https://arxiv.org/search/cs?searchtype=author&query=Rozen%2C+J), [Abheesht Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+A), [Andrea Santilli](https://arxiv.org/search/cs?searchtype=author&query=Santilli%2C+A), [Thibault Fevry](https://arxiv.org/search/cs?searchtype=author&query=Fevry%2C+T), [Jason Alan Fries](https://arxiv.org/search/cs?searchtype=author&query=Fries%2C+J+A), [Ryan Teehan](https://arxiv.org/search/cs?searchtype=author&query=Teehan%2C+R), [Stella Biderman](https://arxiv.org/search/cs?searchtype=author&query=Biderman%2C+S), [Leo Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+L), [Tali Bers](https://arxiv.org/search/cs?searchtype=author&query=Bers%2C+T), [Thomas Wolf](https://arxiv.org/search/cs?searchtype=author&query=Wolf%2C+T), [Alexander M. Rush](https://arxiv.org/search/cs?searchtype=author&query=Rush%2C+A+M)

> Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks. It has been hypothesized that this is a consequence of implicit multitask learning in language model training. Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping general natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts using varying natural language. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks specified in natural language. We fine-tune a pretrained encoder-decoder model on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-Bench benchmark, outperforming models 6x its size. All prompts and trained models are available at [this http URL](http://github.com/bigscience-workshop/promptsource/).

| Comments: | [this https URL](https://github.com/bigscience-workshop/promptsource/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2110.08207](https://arxiv.org/abs/2110.08207) [cs.LG]** |
|           | (or **[arXiv:2110.08207v1](https://arxiv.org/abs/2110.08207v1) [cs.LG]** for this version) |





<h2 id="2021-10-18-3">3. Alternative Input Signals Ease Transfer in Multilingual Machine Translation
</h2>

Title: [Alternative Input Signals Ease Transfer in Multilingual Machine Translation](https://arxiv.org/abs/2110.07804)

Authors: [Simeng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+S), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A), [James Cross](https://arxiv.org/search/cs?searchtype=author&query=Cross%2C+J), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Chau Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+C), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P), [Francisco Guzman](https://arxiv.org/search/cs?searchtype=author&query=Guzman%2C+F)

> Recent work in multilingual machine translation (MMT) has focused on the potential of positive transfer between languages, particularly cases where higher-resourced languages can benefit lower-resourced ones. While training an MMT model, the supervision signals learned from one language pair can be transferred to the other via the tokens shared by multiple source languages. However, the transfer is inhibited when the token overlap among source languages is small, which manifests naturally when languages use different writing systems. In this paper, we tackle inhibited transfer by augmenting the training data with alternative signals that unify different writing systems, such as phonetic, romanized, and transliterated input. We test these signals on Indic and Turkic languages, two language families where the writing systems differ but languages still share common features. Our results indicate that a straightforward multi-source self-ensemble -- training a model on a mixture of various signals and ensembling the outputs of the same model fed with different signals during inference, outperforms strong ensemble baselines by 1.3 BLEU points on both language families. Further, we find that incorporating alternative inputs via self-ensemble can be particularly effective when training set is small, leading to +5 BLEU when only 5% of the total training data is accessible. Finally, our analysis demonstrates that including alternative signals yields more consistency and translates named entities more accurately, which is crucial for increased factuality of automated systems.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.07804](https://arxiv.org/abs/2110.07804) [cs.CL]** |
|           | (or **[arXiv:2110.07804v1](https://arxiv.org/abs/2110.07804v1) [cs.CL]** for this version) |





<h2 id="2021-10-18-4">4. Multilingual Neural Machine Translation:Can Linguistic Hierarchies Help?
</h2>

Title: [Multilingual Neural Machine Translation:Can Linguistic Hierarchies Help?](https://arxiv.org/abs/2110.07816)

Authors: [Fahimeh Saleh](https://arxiv.org/search/cs?searchtype=author&query=Saleh%2C+F), [Wray Buntine](https://arxiv.org/search/cs?searchtype=author&query=Buntine%2C+W), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G), [Lan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+L)

> Multilingual Neural Machine Translation (MNMT) trains a single NMT model that supports translation between multiple languages, rather than training separate models for different languages. Learning a single model can enhance the low-resource translation by leveraging data from multiple languages. However, the performance of an MNMT model is highly dependent on the type of languages used in training, as transferring knowledge from a diverse set of languages degrades the translation performance due to negative transfer. In this paper, we propose a Hierarchical Knowledge Distillation (HKD) approach for MNMT which capitalises on language groups generated according to typological features and phylogeny of languages to overcome the issue of negative transfer. HKD generates a set of multilingual teacher-assistant models via a selective knowledge distillation mechanism based on the language groups, and then distils the ultimate multilingual model from those assistants in an adaptive way. Experimental results derived from the TED dataset with 53 languages demonstrate the effectiveness of our approach in avoiding the negative transfer effect in MNMT, leading to an improved translation performance (about 1 BLEU score on average) compared to strong baselines.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.07816](https://arxiv.org/abs/2110.07816) [cs.CL]** |
|           | (or **[arXiv:2110.07816v1](https://arxiv.org/abs/2110.07816v1) [cs.CL]** for this version) |





<h2 id="2021-10-18-5">5. SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer
</h2>

Title: [SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer](https://arxiv.org/abs/2110.07904)

Authors: [Tu Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+T), [Brian Lester](https://arxiv.org/search/cs?searchtype=author&query=Lester%2C+B), [Noah Constant](https://arxiv.org/search/cs?searchtype=author&query=Constant%2C+N), [Rami Al-Rfou](https://arxiv.org/search/cs?searchtype=author&query=Al-Rfou%2C+R), [Daniel Cer](https://arxiv.org/search/cs?searchtype=author&query=Cer%2C+D)

> As pre-trained language models have gotten larger, there has been growing interest in parameter-efficient methods to apply these models to downstream tasks. Building on the PromptTuning approach of Lester et al. (2021), which learns task-specific soft prompts to condition a frozen language model to perform downstream tasks, we propose a novel prompt-based transfer learning approach called SPoT: Soft Prompt Transfer. SPoT first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. We show that SPoT significantly boosts the performance of PromptTuning across many tasks. More importantly, SPoT either matches or outperforms ModelTuning, which fine-tunes the entire model on each individual task, across all model sizes while being more parameter-efficient (up to 27,000x fewer task-specific parameters). We further conduct a large-scale study on task transferability with 26 NLP tasks and 160 combinations of source-target tasks, and demonstrate that tasks can often benefit each other via prompt transfer. Finally, we propose a simple yet efficient retrieval approach that interprets task prompts as task embeddings to identify the similarity between tasks and predict the most transferable source tasks for a given novel target task.

| Comments: | 20 pages, 6 figures, 5 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.07904](https://arxiv.org/abs/2110.07904) [cs.CL]** |
|           | (or **[arXiv:2110.07904v1](https://arxiv.org/abs/2110.07904v1) [cs.CL]** for this version) |





<h2 id="2021-10-18-6">6. Breaking Down Multilingual Machine Translation
</h2>

Title: [Breaking Down Multilingual Machine Translation](https://arxiv.org/abs/2110.08130)

Authors: [Ting-Rui Chiang](https://arxiv.org/search/cs?searchtype=author&query=Chiang%2C+T), [Yi-Pei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yi-Ting Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+Y), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> While multilingual training is now an essential ingredient in machine translation (MT) systems, recent work has demonstrated that it has different effects in different multilingual settings, such as many-to-one, one-to-many, and many-to-many learning. These training settings expose the encoder and the decoder in a machine translation model with different data distributions. In this paper, we examine how different varieties of multilingual training contribute to learning these two components of the MT model. Specifically, we compare bilingual models with encoders and/or decoders initialized by multilingual training. We show that multilingual training is beneficial to encoders in general, while it only benefits decoders for low-resource languages (LRLs). We further find the important attention heads for each language pair and compare their correlations during inference. Our analysis sheds light on how multilingual translation models work and also enables us to propose methods to improve performance by training with highly related languages. Our many-to-one models for high-resource languages and one-to-many models for LRL outperform the best results reported by Aharoni et al. (2019).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.08130](https://arxiv.org/abs/2110.08130) [cs.CL]** |
|           | (or **[arXiv:2110.08130v1](https://arxiv.org/abs/2110.08130v1) [cs.CL]** for this version) |





<h2 id="2021-10-18-7">7. Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm
</h2>

Title: [Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm](https://arxiv.org/abs/2110.08190)

Authors: [Shaoyi Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Dongkuan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+D), [Ian E.H. Yen](https://arxiv.org/search/cs?searchtype=author&query=Yen%2C+I+E), [Sung-en Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+S), [Bingbing Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Shiyang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Mimi Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+M), [Hang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Caiwen Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+C)

> Various pruning approaches have been proposed to reduce the footprint requirements of Transformer-based language models. Conventional wisdom is that pruning reduces the model expressiveness and thus is more likely to underfit than overfit compared to the original model. However, under the trending pretrain-and-finetune paradigm, we argue that pruning increases the risk of overfitting if pruning was performed at the fine-tuning phase, as it increases the amount of information a model needs to learn from the downstream task, resulting in relative data deficiency. In this paper, we aim to address the overfitting issue under the pretrain-and-finetune paradigm to improve pruning performance via progressive knowledge distillation (KD) and sparse pruning. Furthermore, to mitigate the interference between different strategies of learning rate, pruning and distillation, we propose a three-stage learning framework. We show for the first time that reducing the risk of overfitting can help the effectiveness of pruning under the pretrain-and-finetune paradigm. Experiments on multiple datasets of GLUE benchmark show that our method achieves highly competitive pruning performance over the state-of-the-art competitors across different pruning ratio constraints.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.08190](https://arxiv.org/abs/2110.08190) [cs.CL]** |
|           | (or **[arXiv:2110.08190v1](https://arxiv.org/abs/2110.08190v1) [cs.CL]** for this version) |







<h2 id="2021-10-18-8">8. Why don't people use character-level machine translation?
</h2>

Title: [Why don't people use character-level machine translation?](https://arxiv.org/abs/2110.08191)

Authors: [Jindřich Libovický](https://arxiv.org/search/cs?searchtype=author&query=Libovický%2C+J), [Helmut Schmid](https://arxiv.org/search/cs?searchtype=author&query=Schmid%2C+H), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts both in terms of translation quality and training and inference speed. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. On the other hand, they tend to be more robust towards source side noise and the translation quality does not degrade with increasing beam size at decoding time.

| Comments: | 16 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.08191](https://arxiv.org/abs/2110.08191) [cs.CL]** |
|           | (or **[arXiv:2110.08191v1](https://arxiv.org/abs/2110.08191v1) [cs.CL]** for this version) |





<h2 id="2021-10-18-9">9. Incremental Speech Synthesis For Speech-To-Speech Translation
</h2>

Title: [Incremental Speech Synthesis For Speech-To-Speech Translation](https://arxiv.org/abs/2110.08214)

Authors: [Danni Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Hongyu Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H), [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J)

> In a speech-to-speech translation (S2ST) pipeline, the text-to-speech (TTS) module is an important component for delivering the translated speech to users. To enable incremental S2ST, the TTS module must be capable of synthesizing and playing utterances while its input text is still streaming in. In this work, we focus on improving the incremental synthesis performance of TTS models. With a simple data augmentation strategy based on prefixes, we are able to improve the incremental TTS quality to approach offline performance. Furthermore, we bring our incremental TTS system to the practical scenario in combination with an upstream simultaneous speech translation system, and show the gains also carry over to this use-case. In addition, we propose latency metrics tailored to S2ST applications, and investigate methods for latency reduction in this context.

| Comments: | Work-in-progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2110.08214](https://arxiv.org/abs/2110.08214) [cs.CL]** |
|           | (or **[arXiv:2110.08214v1](https://arxiv.org/abs/2110.08214v1) [cs.CL]** for this version) |





<h2 id="2021-10-18-10">10. Tricks for Training Sparse Translation Models
</h2>

Title: [Tricks for Training Sparse Translation Models](https://arxiv.org/abs/2110.08246)

Authors: [Dheeru Dua](https://arxiv.org/search/cs?searchtype=author&query=Dua%2C+D), [Shruti Bhosale](https://arxiv.org/search/cs?searchtype=author&query=Bhosale%2C+S), [Vedanuj Goswami](https://arxiv.org/search/cs?searchtype=author&query=Goswami%2C+V), [James Cross](https://arxiv.org/search/cs?searchtype=author&query=Cross%2C+J), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+A)

> Multi-task learning with an unbalanced data distribution skews model learning towards high resource tasks, especially when model capacity is fixed and fully shared across all tasks. Sparse scaling architectures, such as BASELayers, provide flexible mechanisms for different tasks to have a variable number of parameters, which can be useful to counterbalance skewed data distributions. We find that that sparse architectures for multilingual machine translation can perform poorly out of the box, and propose two straightforward techniques to mitigate this - a temperature heating mechanism and dense pre-training. Overall, these methods improve performance on two multilingual translation benchmarks compared to standard BASELayers and Dense scaling baselines, and in combination, more than 2x model convergence speed.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.08246](https://arxiv.org/abs/2110.08246) [cs.CL]** |
|           | (or **[arXiv:2110.08246v1](https://arxiv.org/abs/2110.08246v1) [cs.CL]** for this version) |



<h2 id="2021-10-18-11">11. Direct simultaneous speech to speech translation
</h2>

Title: [Direct simultaneous speech to speech translation](https://arxiv.org/abs/2110.08250)

Authors: [Xutai Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Hongyu Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H), [Danni Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Ann Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+A), [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Wei-Ning Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+W), [Kenneth Heafield](https://arxiv.org/search/cs?searchtype=author&query=Heafield%2C+K), [Phillip Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J)

> We present the first direct simultaneous speech-to-speech translation (Simul-S2ST) model, with the ability to start generating translation in the target speech before consuming the full source speech content and independently from intermediate text representations. Our approach leverages recent progress on direct speech-to-speech translation with discrete units. Instead of continuous spectrogram features, a sequence of direct representations, which are learned in a unsupervised manner, are predicted from the model and passed directly to a vocoder for speech synthesis. The simultaneous policy then operates on source speech features and target discrete units. Finally, a vocoder synthesize the target speech from discrete units on-the-fly. We carry out numerical studies to compare cascaded and direct approach on Fisher Spanish-English dataset.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.08250](https://arxiv.org/abs/2110.08250) [cs.CL]** |
|           | (or **[arXiv:2110.08250v1](https://arxiv.org/abs/2110.08250v1) [cs.CL]** for this version) |








# 2021-10-15

[Return to Index](#Index)



<h2 id="2021-10-15-1">1. Evaluating Off-the-Shelf Machine Listening and Natural Language Models for Automated Audio Captioning
</h2>

Title: [Evaluating Off-the-Shelf Machine Listening and Natural Language Models for Automated Audio Captioning](https://arxiv.org/abs/2110.07410)

Authors: [Benno Weck](https://arxiv.org/search/cs?searchtype=author&query=Weck%2C+B), [Xavier Favory](https://arxiv.org/search/cs?searchtype=author&query=Favory%2C+X), [Konstantinos Drossos](https://arxiv.org/search/cs?searchtype=author&query=Drossos%2C+K), [Xavier Serra](https://arxiv.org/search/cs?searchtype=author&query=Serra%2C+X)

> Automated audio captioning (AAC) is the task of automatically generating textual descriptions for general audio signals. A captioning system has to identify various information from the input signal and express it with natural language. Existing works mainly focus on investigating new methods and try to improve their performance measured on existing datasets. Having attracted attention only recently, very few works on AAC study the performance of existing pre-trained audio and natural language processing resources. In this paper, we evaluate the performance of off-the-shelf models with a Transformer-based captioning approach. We utilize the freely available Clotho dataset to compare four different pre-trained machine listening models, four word embedding models, and their combinations in many different settings. Our evaluation suggests that YAMNet combined with BERT embeddings produces the best captions. Moreover, in general, fine-tuning pre-trained word embeddings can lead to better performance. Finally, we show that sequences of audio embeddings can be processed using a Transformer encoder to produce higher-quality captions.

| Comments: | 5 pages, 4 figures. Accepted at Detection and Classification of Acoustic Scenes and Events 2021 (DCASE2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2110.07410](https://arxiv.org/abs/2110.07410) [cs.LG]** |
|           | (or **[arXiv:2110.07410v1](https://arxiv.org/abs/2110.07410v1) [cs.LG]** for this version) |





<h2 id="2021-10-15-2">2. Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits
</h2>

Title: [Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits](https://arxiv.org/abs/2110.06997)

Authors: [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J), [David Vilar](https://arxiv.org/search/cs?searchtype=author&query=Vilar%2C+D), [Artem Sokolov](https://arxiv.org/search/cs?searchtype=author&query=Sokolov%2C+A)

> Training data for machine translation (MT) is often sourced from a multitude of large corpora that are multi-faceted in nature, e.g. containing contents from multiple domains or different levels of quality or complexity. Naturally, these facets do not occur with equal frequency, nor are they equally important for the test scenario at hand. In this work, we propose to optimize this balance jointly with MT model parameters to relieve system developers from manual schedule design. A multi-armed bandit is trained to dynamically choose between facets in a way that is most beneficial for the MT system. We evaluate it on three different multi-facet applications: balancing translationese and natural training data, or data from multiple domains or multiple language pairs. We find that bandit learning leads to competitive MT systems across tasks, and our analysis provides insights into its learned strategies and the underlying data sets.

| Comments: | EMNLP Findings 2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2110.06997](https://arxiv.org/abs/2110.06997) [cs.CL]** |
|           | (or **[arXiv:2110.06997v1](https://arxiv.org/abs/2110.06997v1) [cs.CL]** for this version) |





<h2 id="2021-10-15-3">3. LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5
</h2>

Title: [LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5](https://arxiv.org/abs/2110.07298)

Authors: [Chengwei Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+C), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S)

> Existing approaches to lifelong language learning rely on plenty of labeled data for learning a new task, which is hard to obtain in most real scenarios. Considering that humans can continually learn new tasks from a handful of examples, we expect the models also to be able to generalize well on new few-shot tasks without forgetting the previous ones. In this work, we define this more challenging yet practical problem as Lifelong Few-shot Language Learning (LFLL) and propose a unified framework for it based on prompt tuning of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot learning ability, and simultaneously trains the model as a task solver and a data generator. Before learning a new domain of the same task type, LFPT5 generates pseudo (labeled) samples of previously learned domains, and later gets trained on those samples to alleviate forgetting of previous knowledge as it learns the new domain. In addition, a KL divergence loss is minimized to achieve label consistency between the previous and the current model. While adapting to a new task type, LFPT5 includes and tunes additional prompt embeddings for the new task. With extensive experiments, we demonstrate that LFPT5 can be applied to various different types of tasks and significantly outperform previous methods in different LFLL settings.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.07298](https://arxiv.org/abs/2110.07298) [cs.CL]** |
|           | (or **[arXiv:2110.07298v1](https://arxiv.org/abs/2110.07298v1) [cs.CL]** for this version) |







<h2 id="2021-10-15-4">4. An Empirical Investigation of Multi-bridge Multilingual NMT models
</h2>

Title: [An Empirical Investigation of Multi-bridge Multilingual NMT models](https://arxiv.org/abs/2110.07304)

Authors: [Anoop Kunchukuttan](https://arxiv.org/search/cs?searchtype=author&query=Kunchukuttan%2C+A)

> In this paper, we present an extensive investigation of multi-bridge, many-to-many multilingual NMT models (MB-M2M) ie., models trained on non-English language pairs in addition to English-centric language pairs. In addition to validating previous work which shows that MB-M2M models can overcome zeroshot translation problems, our analysis reveals the following results about multibridge models: (1) it is possible to extract a reasonable amount of parallel corpora between non-English languages for low-resource languages (2) with limited non-English centric data, MB-M2M models are competitive with or outperform pivot models, (3) MB-M2M models can outperform English-Any models and perform at par with Any-English models, so a single multilingual NMT system can serve all translation directions.

| Comments: | 6 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.07304](https://arxiv.org/abs/2110.07304) [cs.CL]** |
|           | (or **[arXiv:2110.07304v1](https://arxiv.org/abs/2110.07304v1) [cs.CL]** for this version) |





<h2 id="2021-10-15-5">5. Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision
</h2>

Title: [Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision](https://arxiv.org/abs/2110.07515)

Authors: [Chenyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+C), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Osmar R. Zaïane](https://arxiv.org/search/cs?searchtype=author&query=Zaïane%2C+O+R), [Lili Mou](https://arxiv.org/search/cs?searchtype=author&query=Mou%2C+L), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> How do we perform efficient inference while retaining high translation quality? Existing neural machine translation models, such as Transformer, achieve high performance, but they decode words one by one, which is inefficient. Recent non-autoregressive translation models speed up the inference, but their quality is still inferior. In this work, we propose DSLP, a highly efficient and high-performance model for machine translation. The key insight is to train a non-autoregressive Transformer with Deep Supervision and feed additional Layer-wise Predictions. We conducted extensive experiments on four translation tasks (both directions of WMT'14 EN-DE and WMT'16 EN-RO). Results show that our approach consistently improves the BLEU scores compared with respective base models. Specifically, our best variant outperforms the autoregressive model on three translation tasks, while being 14.8 times more efficient in inference.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.07515](https://arxiv.org/abs/2110.07515) [cs.CL]** |
|           | (or **[arXiv:2110.07515v1](https://arxiv.org/abs/2110.07515v1) [cs.CL]** for this version) |





<h2 id="2021-10-15-6">6. UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning
</h2>

Title: [UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning](https://arxiv.org/abs/2110.07577)

Authors: [Yuning Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+Y), [Lambert Mathias](https://arxiv.org/search/cs?searchtype=author&query=Mathias%2C+L), [Rui Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+R), [Amjad Almahairi](https://arxiv.org/search/cs?searchtype=author&query=Almahairi%2C+A), [Hao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+H), [Jiawei Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J), [Wen-tau Yih](https://arxiv.org/search/cs?searchtype=author&query=Yih%2C+W), [Madian Khabsa](https://arxiv.org/search/cs?searchtype=author&query=Khabsa%2C+M)

> Conventional fine-tuning of pre-trained language models tunes all model parameters and stores a full model copy for each downstream task, which has become increasingly infeasible as the model size grows larger. Recent parameter-efficient language model tuning (PELT) methods manage to match the performance of fine-tuning with much fewer trainable parameters and perform especially well when the training data is limited. However, different PELT methods may perform rather differently on the same task, making it nontrivial to select the most appropriate method for a specific task, especially considering the fast-growing number of new PELT methods and downstream tasks. In light of model diversity and the difficulty of model selection, we propose a unified framework, UniPELT, which incorporates different PELT methods as submodules and learns to activate the ones that best suit the current data or task setup. Remarkably, on the GLUE benchmark, UniPELT consistently achieves 1~3pt gains compared to the best individual PELT method that it incorporates and even outperforms fine-tuning under different setups. Moreover, UniPELT often surpasses the upper bound when taking the best performance of all its submodules used individually on each task, indicating that a mixture of multiple PELT methods may be inherently more effective than single methods.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.07577](https://arxiv.org/abs/2110.07577) [cs.CL]** |
|           | (or **[arXiv:2110.07577v1](https://arxiv.org/abs/2110.07577v1) [cs.CL]** for this version) |



<h2 id="2021-10-15-7">7. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
</h2>

Title: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/abs/2110.07602)

Authors: [Xiao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Kaixuan Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+K), [Yicheng Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y), [Zhengxiao Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Z), [Zhilin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Jie Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J)

> Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work and our results reveal that existing methods of prompt tuning do not perform well for normal-sized pre-trained models and for hard sequence tasks, indicating lack of universality. We present a novel empirical finding that properly-optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks, where it matches the performance of fine-tuning while having only 0.1\%-3\% tuned parameters. Our method P-Tuning v2 is not a new method but a version of prefix-tuning \cite{li2021prefix} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative for fine-tuning and a strong baseline for future research.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.07602](https://arxiv.org/abs/2110.07602) [cs.CL]** |
|           | (or **[arXiv:2110.07602v1](https://arxiv.org/abs/2110.07602v1) [cs.CL]** for this version) |





# 2021-10-14

[Return to Index](#Index)



<h2 id="2021-10-14-1">1. Learning Compact Metrics for MT
</h2>

Title: [Learning Compact Metrics for MT](https://arxiv.org/abs/2110.06341)

Authors: [Amy Pu](https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+A), [Hyung Won Chung](https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+H+W), [Ankur P. Parikh](https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A+P), [Sebastian Gehrmann](https://arxiv.org/search/cs?searchtype=author&query=Gehrmann%2C+S), [Thibault Sellam](https://arxiv.org/search/cs?searchtype=author&query=Sellam%2C+T)

> Recent developments in machine translation and multilingual text generation have led researchers to adopt trained metrics such as COMET or BLEURT, which treat evaluation as a regression problem and use representations from multilingual pre-trained models such as XLM-RoBERTa or mBERT. Yet studies on related tasks suggest that these models are most efficient when they are large, which is costly and impractical for evaluation. We investigate the trade-off between multilinguality and model capacity with RemBERT, a state-of-the-art multilingual language model, using data from the WMT Metrics Shared Task. We present a series of experiments which show that model size is indeed a bottleneck for cross-lingual transfer, then demonstrate how distillation can help addressing this bottleneck, by leveraging synthetic data generation and transferring knowledge from one teacher to multiple students trained on related languages. Our method yields up to 10.5% improvement over vanilla fine-tuning and reaches 92.6% of RemBERT's performance using only a third of its parameters.

| Comments: | Accepted at EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.06341](https://arxiv.org/abs/2110.06341) [cs.CL]** |
|           | (or **[arXiv:2110.06341v1](https://arxiv.org/abs/2110.06341v1) [cs.CL]** for this version) |





<h2 id="2021-10-14-2">2. Tell Me How to Survey: Literature Review Made Simple with Automatic Reading Path Generation
</h2>

Title: [Tell Me How to Survey: Literature Review Made Simple with Automatic Reading Path Generation](https://arxiv.org/abs/2110.06354)

Authors: [Jiayuan Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+J), [Tong Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+T), [Zijing Ou](https://arxiv.org/search/cs?searchtype=author&query=Ou%2C+Z), [Wangyang Zuo](https://arxiv.org/search/cs?searchtype=author&query=Zuo%2C+W), [Ruihui Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+R), [Chenghua Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+C), [Yefeng Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y), [Bang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B)

> Recent years have witnessed the dramatic growth of paper volumes with plenty of new research papers published every day, especially in the area of computer science. How to glean papers worth reading from the massive literature to do a quick survey or keep up with the latest advancement about a specific research topic has become a challenging task. Existing academic search engines such as Google Scholar return relevant papers by individually calculating the relevance between each paper and query. However, such systems usually omit the prerequisite chains of a research topic and cannot form a meaningful reading path. In this paper, we introduce a new task named Reading Path Generation (RPG) which aims at automatically producing a path of papers to read for a given query. To serve as a research benchmark, we further propose SurveyBank, a dataset consisting of large quantities of survey papers in the field of computer science as well as their citation relationships. Each survey paper contains key phrases extracted from its title and multi-level reading lists inferred from its references. Furthermore, we propose a graph-optimization-based approach for reading path generation which takes the relationship between papers into account. Extensive evaluations demonstrate that our approach outperforms other baselines. A Real-time Reading Path Generation System (RePaGer) has been also implemented with our designed model. To the best of our knowledge, we are the first to target this important research problem. Our source code of RePaGer system and SurveyBank dataset can be found on here.

| Comments: | 16 pages, 12 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2110.06354](https://arxiv.org/abs/2110.06354) [cs.CL]** |
|           | (or **[arXiv:2110.06354v1](https://arxiv.org/abs/2110.06354v1) [cs.CL]** for this version) |





<h2 id="2021-10-14-3">3. MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators
</h2>

Title: [MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators](https://arxiv.org/abs/2110.06609)

Authors: [Zhixing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z), [Xiangwen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Shuo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y)

> Pre-trained language models have recently been shown to be able to perform translation without finetuning via prompting. Inspired by these findings, we study improving the performance of pre-trained language models on translation tasks, where training neural machine translation models is the current de facto approach. We present Multi-Stage Prompting, a simple and lightweight approach for better adapting pre-trained language models to translation tasks. To make pre-trained language models better translators, we divide the translation process via pre-trained language models into three separate stages: the encoding stage, the re-encoding stage, and the decoding stage. During each stage, we independently apply different continuous prompts for allowing pre-trained language models better adapting to translation tasks. We conduct extensive experiments on low-, medium-, and high-resource translation tasks. Experiments show that our method can significantly improve the translation performance of pre-trained language models.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.06609](https://arxiv.org/abs/2110.06609) [cs.CL]** |
|           | (or **[arXiv:2110.06609v1](https://arxiv.org/abs/2110.06609v1) [cs.CL]** for this version) |





<h2 id="2021-10-14-4">4. Maximizing Efficiency of Language Model Pre-training for Learning Representation
</h2>

Title: [Maximizing Efficiency of Language Model Pre-training for Learning Representation](https://arxiv.org/abs/2110.06620)

Authors: [Junmo Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+J), [Suwon Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+S), [Jeonghwan Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J), [Jaeyoung Jo](https://arxiv.org/search/cs?searchtype=author&query=Jo%2C+J), [Sung-Hyon Myaeng](https://arxiv.org/search/cs?searchtype=author&query=Myaeng%2C+S)

> Pre-trained language models in the past years have shown exponential growth in model parameters and compute time. ELECTRA is a novel approach for improving the compute efficiency of pre-trained language models (e.g. BERT) based on masked language modeling (MLM) by addressing the sample inefficiency problem with the replaced token detection (RTD) task. Our work proposes adaptive early exit strategy to maximize the efficiency of the pre-training process by relieving the model's subsequent layers of the need to process latent features by leveraging earlier layer representations. Moreover, we evaluate an initial approach to the problem that has not succeeded in maintaining the accuracy of the model while showing a promising compute efficiency by thoroughly investigating the necessity of the generator module of ELECTRA.

| Comments: | Published in KSC 2020                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2110.06620](https://arxiv.org/abs/2110.06620) [cs.CL]** |
|           | (or **[arXiv:2110.06620v1](https://arxiv.org/abs/2110.06620v1) [cs.CL]** for this version) |





<h2 id="2021-10-14-5">5. Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese
</h2>

Title: [Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese](https://arxiv.org/abs/2110.06696)

Authors: [Zhuosheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Hanqing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Keming Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Yuhang Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y), [Jingyun Hua](https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+J), [Yulong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M)

> Although pre-trained models (PLMs) have achieved remarkable improvements in a wide range of NLP tasks, they are expensive in terms of time and resources. This calls for the study of training more efficient models with less computation but still ensures impressive performance. Instead of pursuing a larger scale, we are committed to developing lightweight yet more powerful models trained with equal or less computation and friendly to rapid deployment. This technical report releases our pre-trained model called Mengzi, which stands for a family of discriminative, generative, domain-specific, and multimodal pre-trained model variants, capable of a wide range of language and vision tasks. Compared with public Chinese PLMs, Mengzi is simple but more powerful. Our lightweight model has achieved new state-of-the-art results on the widely-used CLUE benchmark with our optimized pre-training and fine-tuning techniques. Without modifying the model architecture, our model can be easily employed as an alternative to existing PLMs. Our sources are available at [this https URL](https://github.com/Langboat/Mengzi).

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.06696](https://arxiv.org/abs/2110.06696) [cs.CL]** |
|           | (or **[arXiv:2110.06696v1](https://arxiv.org/abs/2110.06696v1) [cs.CL]** for this version) |





<h2 id="2021-10-14-6">6. Semantics-aware Attention Improves Neural Machine Translation
</h2>

Title: [Semantics-aware Attention Improves Neural Machine Translation](https://arxiv.org/abs/2110.06920)

Authors: [Aviv Slobodkin](https://arxiv.org/search/cs?searchtype=author&query=Slobodkin%2C+A), [Leshem Choshen](https://arxiv.org/search/cs?searchtype=author&query=Choshen%2C+L), [Omri Abend](https://arxiv.org/search/cs?searchtype=author&query=Abend%2C+O)

> The integration of syntactic structures into Transformer machine translation has shown positive results, but to our knowledge, no work has attempted to do so with semantic structures. In this work we propose two novel parameter-free methods for injecting semantic information into Transformers, both rely on semantics-aware masking of (some of) the attention heads. One such method operates on the encoder, through a Scene-Aware Self-Attention (SASA) head. Another on the decoder, through a Scene-Aware Cross-Attention (SACrA) head. We show a consistent improvement over the vanilla Transformer and syntax-aware models for four language pairs. We further show an additional gain when using both semantic and syntactic structures in some language pairs.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.06920](https://arxiv.org/abs/2110.06920) [cs.CL]** |
|           | (or **[arXiv:2110.06920v1](https://arxiv.org/abs/2110.06920v1) [cs.CL]** for this version) |





# 2021-10-13

[Return to Index](#Index)



<h2 id="2021-10-13-1">1. Doubly-Trained Adversarial Data Augmentation for Neural Machine Translation
</h2>

Title: [Doubly-Trained Adversarial Data Augmentation for Neural Machine Translation](https://arxiv.org/abs/2110.05691)

Authors: [Weiting Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+W), [Shuoyang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+S), [Huda Khayrallah](https://arxiv.org/search/cs?searchtype=author&query=Khayrallah%2C+H), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

> Neural Machine Translation (NMT) models are known to suffer from noisy inputs. To make models robust, we generate adversarial augmentation samples that attack the model and preserve the source-side semantic meaning at the same time. To generate such samples, we propose a doubly-trained architecture that pairs two NMT models of opposite translation directions with a joint loss function, which combines the target-side attack and the source-side semantic similarity constraint. The results from our experiments across three different language pairs and two evaluation metrics show that these adversarial samples improve the model robustness.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.05691](https://arxiv.org/abs/2110.05691) [cs.CL]** |
|           | (or **[arXiv:2110.05691v1](https://arxiv.org/abs/2110.05691v1) [cs.CL]** for this version) |











# 2021-10-12

[Return to Index](#Index)



<h2 id="2021-10-12-1">1. CLIP-Adapter: Better Vision-Language Models with Feature Adapters
</h2>

Title: [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/abs/2110.04544)

Authors: [Peng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+P), [Shijie Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+S), [Renrui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Teli Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+T), [Rongyao Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+R), [Yongfeng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Hongsheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Yu Qiao](https://arxiv.org/search/cs?searchtype=author&query=Qiao%2C+Y)

> Large-scale contrastive vision-language pre-training has shown significant progress in visual representation learning. Unlike traditional visual systems trained by a fixed set of discrete labels, a new paradigm was introduced in \cite{radford2021learning} to directly learn to align images with raw texts in an open-vocabulary setting. On downstream tasks, a carefully chosen text prompt is employed to make zero-shot predictions.~To avoid non-trivial prompt engineering, context optimization \cite{zhou2021coop} has been proposed to learn continuous vectors as task-specific prompts with few-shot training examples.~In this paper, we show that there is an alternative path to achieve better vision-language models other than prompt tuning.~While prompt tuning is for the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with feature adapters on either visual or language branch. Specifically, CLIP-Adapter adopts an additional bottleneck layer to learn new features and performs residual-style feature blending with the original pre-trained features.~As a consequence, CLIP-Adapter is able to outperform context optimization while maintains a simple design. Experiments and extensive ablation studies on various visual classification tasks demonstrate the effectiveness of our approach.

| Comments: | Technical Report                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2110.04544](https://arxiv.org/abs/2110.04544) [cs.CV]** |
|           | (or **[arXiv:2110.04544v1](https://arxiv.org/abs/2110.04544v1) [cs.CV]** for this version) |





<h2 id="2021-10-12-2">2. The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design
</h2>

Title: [The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design](https://arxiv.org/abs/2110.04541)

Authors: [Yoav Levine](https://arxiv.org/search/cs?searchtype=author&query=Levine%2C+Y), [Noam Wies](https://arxiv.org/search/cs?searchtype=author&query=Wies%2C+N), [Daniel Jannai](https://arxiv.org/search/cs?searchtype=author&query=Jannai%2C+D), [Dan Navon](https://arxiv.org/search/cs?searchtype=author&query=Navon%2C+D), [Yedid Hoshen](https://arxiv.org/search/cs?searchtype=author&query=Hoshen%2C+Y), [Amnon Shashua](https://arxiv.org/search/cs?searchtype=author&query=Shashua%2C+A)

> Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose "kNN-Pretraining": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities. This theoretically motivated degree of freedom for "pretraining example design" indicates new training schemes for self-improving representations.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.04541](https://arxiv.org/abs/2110.04541) [cs.CL]** |
|           | (or **[arXiv:2110.04541v1](https://arxiv.org/abs/2110.04541v1) [cs.CL]** for this version) |





<h2 id="2021-10-12-3">3. Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning
</h2>

Title: [Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning](https://arxiv.org/abs/2110.04725)

Authors: [Shaohua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Xudong Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+X), [Tong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+T), [Rongguo Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Chong Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+C), [Hongli Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Feng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+F), [Hong Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+H), [Jiangang Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J), [Liang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+L), [Xuanwei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Jun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.04725](https://arxiv.org/abs/2110.04725) [cs.CL]** |
|           | (or **[arXiv:2110.04725v1](https://arxiv.org/abs/2110.04725v1) [cs.CL]** for this version) |





<h2 id="2021-10-12-4">4. WeTS: A Benchmark for Translation Suggestion
</h2>

Title: [WeTS: A Benchmark for Translation Suggestion](https://arxiv.org/abs/2110.05151)

Authors: [Zhen Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Yingxue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Ernan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+E), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Translation Suggestion (TS), which provides alternatives for specific words or phrases given the entire documents translated by machine translation (MT) \cite{lee2021intellicat}, has been proven to play a significant role in post editing (PE). However, there is still no publicly available data set to support in-depth research for this problem, and no reproducible experimental results can be followed by researchers in this community. To break this limitation, we create a benchmark data set for TS, called \emph{WeTS}, which contains golden corpus annotated by expert translators on four translation directions. Apart from the human-annotated golden corpus, we also propose several novel methods to generate synthetic corpus which can substantially improve the performance of TS. With the corpus we construct, we introduce the Transformer-based model for TS, and experimental results show that our model achieves State-Of-The-Art (SOTA) results on all four translation directions, including English-to-German, German-to-English, Chinese-to-English and English-to-Chinese. Codes and corpus can be found at \url{[this https URL](https://github.com/ZhenYangIACAS/WeTS.git)}.

| Comments: | Translation suggestion, Transformer                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.05151](https://arxiv.org/abs/2110.05151) [cs.CL]** |
|           | (or **[arXiv:2110.05151v1](https://arxiv.org/abs/2110.05151v1) [cs.CL]** for this version) |





<h2 id="2021-10-12-5">5. It is Not as Good as You Think! Evaluating Simultaneous Machine Translation on Interpretation Data
</h2>

Title: [It is Not as Good as You Think! Evaluating Simultaneous Machine Translation on Interpretation Data](https://arxiv.org/abs/2110.05213)

Authors: [Jinming Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J), [Philip Arthur](https://arxiv.org/search/cs?searchtype=author&query=Arthur%2C+P), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G), [Trevor Cohn](https://arxiv.org/search/cs?searchtype=author&query=Cohn%2C+T), [Ehsan Shareghi](https://arxiv.org/search/cs?searchtype=author&query=Shareghi%2C+E)

> Most existing simultaneous machine translation (SiMT) systems are trained and evaluated on offline translation corpora. We argue that SiMT systems should be trained and tested on real interpretation data. To illustrate this argument, we propose an interpretation test set and conduct a realistic evaluation of SiMT trained on offline translations. Our results, on our test set along with 3 existing smaller scale language pairs, highlight the difference of up-to 13.83 BLEU score when SiMT models are evaluated on translation vs interpretation data. In the absence of interpretation training data, we propose a translation-to-interpretation (T2I) style transfer method which allows converting existing offline translations into interpretation-style data, leading to up-to 2.8 BLEU improvement. However, the evaluation gap remains notable, calling for constructing large-scale interpretation corpora better suited for evaluating and developing SiMT systems.

| Comments: | EMNLP2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2110.05213](https://arxiv.org/abs/2110.05213) [cs.CL]** |
|           | (or **[arXiv:2110.05213v1](https://arxiv.org/abs/2110.05213v1) [cs.CL]** for this version) |





<h2 id="2021-10-12-6">6. Unsupervised Neural Machine Translation with Generative Language Models Only
</h2>

Title: [Unsupervised Neural Machine Translation with Generative Language Models Only](https://arxiv.org/abs/2110.05448)

Authors: [Jesse Michael Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J+M), [Igor Babuschkin](https://arxiv.org/search/cs?searchtype=author&query=Babuschkin%2C+I), [Harrison Edwards](https://arxiv.org/search/cs?searchtype=author&query=Edwards%2C+H), [Arvind Neelakantan](https://arxiv.org/search/cs?searchtype=author&query=Neelakantan%2C+A), [Tao Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+T), [Stanislas Polu](https://arxiv.org/search/cs?searchtype=author&query=Polu%2C+S), [Alex Ray](https://arxiv.org/search/cs?searchtype=author&query=Ray%2C+A), [Pranav Shyam](https://arxiv.org/search/cs?searchtype=author&query=Shyam%2C+P), [Aditya Ramesh](https://arxiv.org/search/cs?searchtype=author&query=Ramesh%2C+A), [Alec Radford](https://arxiv.org/search/cs?searchtype=author&query=Radford%2C+A), [Ilya Sutskever](https://arxiv.org/search/cs?searchtype=author&query=Sutskever%2C+I)

> We show how to derive state-of-the-art unsupervised neural machine translation systems from generatively pre-trained language models. Our method consists of three steps: few-shot amplification, distillation, and backtranslation. We first use the zero-shot translation ability of large pre-trained language models to generate translations for a small set of unlabeled sentences. We then amplify these zero-shot translations by using them as few-shot demonstrations for sampling a larger synthetic dataset. This dataset is distilled by discarding the few-shot demonstrations and then fine-tuning. During backtranslation, we repeatedly generate translations for a set of inputs and then fine-tune a single language model on both directions of the translation task at once, ensuring cycle-consistency by swapping the roles of gold monotext and generated translations when fine-tuning. By using our method to leverage GPT-3's zero-shot translation capability, we achieve a new state-of-the-art in unsupervised translation on the WMT14 English-French benchmark, attaining a BLEU score of 42.1.

| Comments: | 10 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2110.05448](https://arxiv.org/abs/2110.05448) [cs.CL]** |
|           | (or **[arXiv:2110.05448v1](https://arxiv.org/abs/2110.05448v1) [cs.CL]** for this version) |






# 2021-10-11

[Return to Index](#Index)



<h2 id="2021-10-11-1">1. Speeding up Deep Model Training by Sharing Weights and Then Unsharing
</h2>

Title: [Speeding up Deep Model Training by Sharing Weights and Then Unsharing](https://arxiv.org/abs/2110.03848)

Authors: [Shuo Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Le Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+L), [Xiaodan Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Qiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Denny Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+D)

> We propose a simple and efficient approach for training the BERT model. Our approach exploits the special structure of BERT that contains a stack of repeated modules (i.e., transformer encoders). Our proposed approach first trains BERT with the weights shared across all the repeated modules till some point. This is for learning the commonly shared component of weights across all repeated layers. We then stop weight sharing and continue training until convergence. We present theoretic insights for training by sharing weights then unsharing with analysis for simplified models. Empirical experiments on the BERT model show that our method yields better performance of trained models, and significantly reduces the number of training iterations.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.03848](https://arxiv.org/abs/2110.03848) [cs.LG]** |
|           | (or **[arXiv:2110.03848v1](https://arxiv.org/abs/2110.03848v1) [cs.LG]** for this version) |





<h2 id="2021-10-11-2">2. QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks
</h2>

Title: [QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks](https://arxiv.org/abs/2110.03861)

Authors: [Jun Qi](https://arxiv.org/search/quant-ph?searchtype=author&query=Qi%2C+J), [Chao-Han Huck Yang](https://arxiv.org/search/quant-ph?searchtype=author&query=Yang%2C+C+H), [Pin-Yu Chen](https://arxiv.org/search/quant-ph?searchtype=author&query=Chen%2C+P)

> The advent of noisy intermediate-scale quantum (NISQ) computers raises a crucial challenge to design quantum neural networks for fully quantum learning tasks. To bridge the gap, this work proposes an end-to-end learning framework named QTN-VQC, by introducing a trainable quantum tensor network (QTN) for quantum embedding on a variational quantum circuit (VQC). The architecture of QTN is composed of a parametric tensor-train network for feature extraction and a tensor product encoding for quantum encoding. We highlight the QTN for quantum embedding in terms of two perspectives: (1) we theoretically characterize QTN by analyzing its representation power of input features; (2) QTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the generation of quantum embedding to the output measurement. Our experiments on the MNIST dataset demonstrate the advantages of QTN for quantum embedding over other quantum embedding approaches.

| Subjects: | **Quantum Physics (quant-ph)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.03861](https://arxiv.org/abs/2110.03861) [quant-ph]** |
|           | (or **[arXiv:2110.03861v1](https://arxiv.org/abs/2110.03861v1) [quant-ph]** for this version) |





<h2 id="2021-10-11-3">3. M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining
</h2>

Title: [M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining](https://arxiv.org/abs/2110.03888)

Authors: [Junyang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J), [An Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+A), [Jinze Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+J), [Chang Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Le Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+L), [Xianyan Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+X), [Ang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+A), [Jie Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Yong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Wei Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+W), [Jingren Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Hongxia Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H)

> Recent expeditious developments in deep learning algorithms, distributed training, and even hardware design for large models have enabled training extreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of billions or even trillions of parameters. However, under limited resources, extreme-scale model training that requires enormous amounts of computes and memory footprint suffers from frustratingly low efficiency in model convergence. In this paper, we propose a simple training strategy called "Pseudo-to-Real" for high-memory-footprint-required large models. Pseudo-to-Real is compatible with large models with architecture of sequential layers. We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the application of Pseudo-to-Real, we also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities. Fast training of extreme-scale models on a decent amount of resources can bring much smaller carbon footprint and contribute to greener AI.

| Comments: | 14 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2110.03888](https://arxiv.org/abs/2110.03888) [cs.LG]** |
|           | (or **[arXiv:2110.03888v1](https://arxiv.org/abs/2110.03888v1) [cs.LG]** for this version) |





<h2 id="2021-10-11-4">4. Iterative Decoding for Compositional Generalization in Transformers
</h2>

Title: [Iterative Decoding for Compositional Generalization in Transformers](https://arxiv.org/abs/2110.04169)

Authors: [Luana Ruiz](https://arxiv.org/search/cs?searchtype=author&query=Ruiz%2C+L), [Joshua Ainslie](https://arxiv.org/search/cs?searchtype=author&query=Ainslie%2C+J), [Santiago Ontañón](https://arxiv.org/search/cs?searchtype=author&query=Ontañón%2C+S)

> Deep learning models do well at generalizing to in-distribution data but struggle to generalize compositionally, i.e., to combine a set of learned primitives to solve more complex tasks. In particular, in sequence-to-sequence (seq2seq) learning, transformers are often unable to predict correct outputs for even marginally longer examples than those seen during training. This paper introduces iterative decoding, an alternative to seq2seq learning that (i) improves transformer compositional generalization and (ii) evidences that, in general, seq2seq transformers do not learn iterations that are not unrolled. Inspired by the idea of compositionality -- that complex tasks can be solved by composing basic primitives -- training examples are broken down into a sequence of intermediate steps that the transformer then learns iteratively. At inference time, the intermediate outputs are fed back to the transformer as intermediate inputs until an end-of-iteration token is predicted. Through numerical experiments, we show that transfomers trained via iterative decoding outperform their seq2seq counterparts on the PCFG dataset, and solve the problem of calculating Cartesian products between vectors longer than those seen during training with 100% accuracy, a task at which seq2seq models have been shown to fail. We also illustrate a limitation of iterative decoding, specifically, that it can make sorting harder to learn on the CFQ dataset.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.04169](https://arxiv.org/abs/2110.04169) [cs.LG]** |
|           | (or **[arXiv:2110.04169v1](https://arxiv.org/abs/2110.04169v1) [cs.LG]** for this version) |





<h2 id="2021-10-11-5">5. Machine Translation Verbosity Control for Automatic Dubbing
</h2>

Title: [Machine Translation Verbosity Control for Automatic Dubbing](https://arxiv.org/abs/2110.03847)

Authors: [Surafel M. Lakew](https://arxiv.org/search/cs?searchtype=author&query=Lakew%2C+S+M), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M), [Yue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Cuong Hoang](https://arxiv.org/search/cs?searchtype=author&query=Hoang%2C+C), [Yogesh Virkar](https://arxiv.org/search/cs?searchtype=author&query=Virkar%2C+Y), [Roberto Barra-Chicote](https://arxiv.org/search/cs?searchtype=author&query=Barra-Chicote%2C+R), [Robert Enyedi](https://arxiv.org/search/cs?searchtype=author&query=Enyedi%2C+R)

> Automatic dubbing aims at seamlessly replacing the speech in a video document with synthetic speech in a different language. The task implies many challenges, one of which is generating translations that not only convey the original content, but also match the duration of the corresponding utterances. In this paper, we focus on the problem of controlling the verbosity of machine translation output, so that subsequent steps of our automatic dubbing pipeline can generate dubs of better quality. We propose new methods to control the verbosity of MT output and compare them against the state of the art with both intrinsic and extrinsic evaluations. For our experiments we use a public data set to dub English speeches into French, Italian, German and Spanish. Finally, we report extensive subjective tests that measure the impact of MT verbosity control on the final quality of dubbed video clips.

| Comments: | Accepted at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2110.03847](https://arxiv.org/abs/2110.03847) [cs.CL]** |
|           | (or **[arXiv:2110.03847v1](https://arxiv.org/abs/2110.03847v1) [cs.CL]** for this version) |





<h2 id="2021-10-11-6">6. Text analysis and deep learning: A network approach
</h2>

Title: [Text analysis and deep learning: A network approach](https://arxiv.org/abs/2110.04151)

Authors: [Ingo Marquart](https://arxiv.org/search/cs?searchtype=author&query=Marquart%2C+I)

> Much information available to applied researchers is contained within written language or spoken text. Deep language models such as BERT have achieved unprecedented success in many applications of computational linguistics. However, much less is known about how these models can be used to analyze existing text. We propose a novel method that combines transformer models with network analysis to form a self-referential representation of language use within a corpus of interest. Our approach produces linguistic relations strongly consistent with the underlying model as well as mathematically well-defined operations on them, while reducing the amount of discretionary choices of representation and distance measures. It represents, to the best of our knowledge, the first unsupervised method to extract semantic networks directly from deep language models. We illustrate our approach in a semantic analysis of the term "founder". Using the entire corpus of Harvard Business Review from 1980 to 2020, we find that ties in our network track the semantics of discourse over time, and across contexts, identifying and relating clusters of semantic and syntactic relations. Finally, we discuss how this method can also complement and inform analyses of the behavior of deep learning models.

| Subjects:    | **Computation and Language (cs.CL)**; Social and Information Networks (cs.SI) |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7; I.5.4; J.4                                            |
| Cite as:     | **[arXiv:2110.04151](https://arxiv.org/abs/2110.04151) [cs.CL]** |
|              | (or **[arXiv:2110.04151v1](https://arxiv.org/abs/2110.04151v1) [cs.CL]** for this version) |





<h2 id="2021-10-11-7">7. Contrastive String Representation Learning using Synthetic Data
</h2>

Title: [Contrastive String Representation Learning using Synthetic Data](https://arxiv.org/abs/2110.04217)

Authors: [Urchade Zaratiana](https://arxiv.org/search/cs?searchtype=author&query=Zaratiana%2C+U)

> String representation Learning (SRL) is an important task in the field of Natural Language Processing, but it remains under-explored. The goal of SRL is to learn dense and low-dimensional vectors (or embeddings) for encoding character sequences. The learned representation from this task can be used in many downstream application tasks such as string similarity matching or lexical normalization. In this paper, we propose a new method for to train a SRL model by only using synthetic data. Our approach makes use of Contrastive Learning in order to maximize similarity between related strings while minimizing it for unrelated strings. We demonstrate the effectiveness of our approach by evaluating the learned representation on the task of string similarity matching. Codes, data and pretrained models will be made publicly available.

| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.04217](https://arxiv.org/abs/2110.04217) [cs.CL]** |
|           | (or **[arXiv:2110.04217v1](https://arxiv.org/abs/2110.04217v1) [cs.CL]** for this version) |





<h2 id="2021-10-11-8">8. Local and Global Context-Based Pairwise Models for Sentence Ordering
</h2>

Title: [Local and Global Context-Based Pairwise Models for Sentence Ordering](https://arxiv.org/abs/2110.04291)

Authors: [Ruskin Raj Manku](https://arxiv.org/search/cs?searchtype=author&query=Manku%2C+R+R), [Aditya Jyoti Paul](https://arxiv.org/search/cs?searchtype=author&query=Paul%2C+A+J)

> Sentence Ordering refers to the task of rearranging a set of sentences into the appropriate coherent order. For this task, most previous approaches have explored global context-based end-to-end methods using Sequence Generation techniques. In this paper, we put forward a set of robust local and global context-based pairwise ordering strategies, leveraging which our prediction strategies outperform all previous works in this domain. Our proposed encoding method utilizes the paragraph's rich global contextual information to predict the pairwise order using novel transformer architectures. Analysis of the two proposed decoding strategies helps better explain error propagation in pairwise models. This approach is the most accurate pure pairwise model and our encoding strategy also significantly improves the performance of other recent approaches that use pairwise models, including the previous state-of-the-art, demonstrating the research novelty and generalizability of this work. Additionally, we show how the pre-training task for ALBERT helps it to significantly outperform BERT, despite having considerably lesser parameters. The extensive experimental results, architectural analysis and ablation studies demonstrate the effectiveness and superiority of the proposed models compared to the previous state-of-the-art, besides providing a much better understanding of the functioning of pairwise models.

| Comments: | Under review by Knowledge-Based Systems                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Logic in Computer Science (cs.LO) |
| Cite as:  | **[arXiv:2110.04291](https://arxiv.org/abs/2110.04291) [cs.CL]** |
|           | (or **[arXiv:2110.04291v1](https://arxiv.org/abs/2110.04291v1) [cs.CL]** for this version) |





# 2021-10-08

[Return to Index](#Index)



<h2 id="2021-10-08-1">1. Unsupervised Multimodal Language Representations using Convolutional Autoencoders
</h2>

Title: [Unsupervised Multimodal Language Representations using Convolutional Autoencoders](https://arxiv.org/abs/2110.03007)

Authors: [Panagiotis Koromilas](https://arxiv.org/search/cs?searchtype=author&query=Koromilas%2C+P), [Theodoros Giannakopoulos](https://arxiv.org/search/cs?searchtype=author&query=Giannakopoulos%2C+T)

> Multimodal Language Analysis is a demanding area of research, since it is associated with two requirements: combining different modalities and capturing temporal information. During the last years, several works have been proposed in the area, mostly centered around supervised learning in downstream tasks. In this paper we propose extracting unsupervised Multimodal Language representations that are universal and can be applied to different tasks. Towards this end, we map the word-level aligned multimodal sequences to 2-D matrices and then use Convolutional Autoencoders to learn embeddings by combining multiple datasets. Extensive experimentation on Sentiment Analysis (MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned representations can achieve near-state-of-the-art performance with just the use of a Logistic Regression algorithm for downstream classification. It is also shown that our method is extremely lightweight and can be easily generalized to other tasks and unseen data with small performance drop and almost the same number of parameters. The proposed multimodal representation models are open-sourced and will help grow the applicability of Multimodal Language.

| Comments: | 5 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Multimedia (cs.MM) |
| Cite as:  | **[arXiv:2110.03007](https://arxiv.org/abs/2110.03007) [cs.CL]** |
|           | (or **[arXiv:2110.03007v1](https://arxiv.org/abs/2110.03007v1) [cs.CL]** for this version) |







<h2 id="2021-10-08-2">2. The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation
</h2>

Title: [The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation](https://arxiv.org/abs/2110.03036)

Authors: [Orevaoghene Ahia](https://arxiv.org/search/cs?searchtype=author&query=Ahia%2C+O), [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J), [Sara Hooker](https://arxiv.org/search/cs?searchtype=author&query=Hooker%2C+S)

> A "bigger is better" explosion in the number of parameters in deep neural networks has made it increasingly challenging to make state-of-the-art networks accessible in compute-restricted environments. Compression techniques have taken on renewed importance as a way to bridge the gap. However, evaluation of the trade-offs incurred by popular compression techniques has been centered on high-resource datasets. In this work, we instead consider the impact of compression in a data-limited regime. We introduce the term low-resource double bind to refer to the co-occurrence of data limitations and compute resource constraints. This is a common setting for NLP for low-resource languages, yet the trade-offs in performance are poorly studied. Our work offers surprising insights into the relationship between capacity and generalization in data-limited regimes for the task of machine translation. Our experiments on magnitude pruning for translations from English into Yoruba, Hausa, Igbo and German show that in low-resource regimes, sparsity preserves performance on frequent sentences but has a disparate impact on infrequent ones. However, it improves robustness to out-of-distribution shifts, especially for datasets that are very distinct from the training distribution. Our findings suggest that sparsity can play a beneficial role at curbing memorization of low frequency attributes, and therefore offers a promising solution to the low-resource double bind.

| Comments: | Accepted to Findings of EMNLP 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2110.03036](https://arxiv.org/abs/2110.03036) [cs.CL]** |
|           | (or **[arXiv:2110.03036v1](https://arxiv.org/abs/2110.03036v1) [cs.CL]** for this version) |







<h2 id="2021-10-08-3">3. On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation
</h2>

Title: [On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation](https://arxiv.org/abs/2110.03067)

Authors: [Gal Patel](https://arxiv.org/search/cs?searchtype=author&query=Patel%2C+G), [Leshem Choshen](https://arxiv.org/search/cs?searchtype=author&query=Choshen%2C+L), [Omri Abend](https://arxiv.org/search/cs?searchtype=author&query=Abend%2C+O)

> To gain insight into the role neurons play, we study the activation patterns corresponding to meaning-preserving paraphrases (e.g., active-passive). We compile a dataset of controlled syntactic paraphrases in English with their reference German translations and demonstrate our model-agnostic approach with the Transformer translation model. First, we identify neurons that correlate across paraphrases and dissect the observed correlation into possible confounds. Although lower-level components are found as the cause of similar activations, no sentence-level semantics or syntax are detected locally. Later, we manipulate neuron activations to influence translation towards a particular syntactic form. We find that a simple value shift is effective, and more so when many neurons are modified. These suggest that complex syntactic constructions are indeed encoded in the model. We conclude by discussing how to better manipulate it using the correlations we first obtained.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.03067](https://arxiv.org/abs/2110.03067) [cs.CL]** |
|           | (or **[arXiv:2110.03067v1](https://arxiv.org/abs/2110.03067v1) [cs.CL]** for this version) |







<h2 id="2021-10-08-4">4. Towards Continual Knowledge Learning of Language Models
</h2>

Title: [Towards Continual Knowledge Learning of Language Models](https://arxiv.org/abs/2110.03215)

Authors: [Joel Jang](https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+J), [Seonghyeon Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+S), [Sohee Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Joongbo Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+J), [Janghoon Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J), [Gyeonghun Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+G), [Stanley Jungkyu Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+S+J), [Minjoon Seo](https://arxiv.org/search/cs?searchtype=author&query=Seo%2C+M)

> Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.03215](https://arxiv.org/abs/2110.03215) [cs.CL]** |
|           | (or **[arXiv:2110.03215v1](https://arxiv.org/abs/2110.03215v1) [cs.CL]** for this version) |





# 2021-10-07

[Return to Index](#Index)



<h2 id="2021-10-07-1">1. Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning
</h2>

Title: [Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning](https://arxiv.org/abs/2110.02600)

Authors: [Seanie Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S), [Hae Beom Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H+B), [Juho Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J), [Sung Ju Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+S+J)

> Multilingual models jointly pretrained on multiple languages have achieved remarkable performance on various multilingual downstream tasks. Moreover, models finetuned on a single monolingual downstream task have shown to generalize to unseen languages. In this paper, we first show that it is crucial for those tasks to align gradients between them in order to maximize knowledge transfer while minimizing negative transfer. Despite its importance, the existing methods for gradient alignment either have a completely different purpose, ignore inter-task alignment, or aim to solve continual learning problems in rather inefficient ways. As a result of the misaligned gradients between tasks, the model suffers from severe negative transfer in the form of catastrophic forgetting of the knowledge acquired from the pretraining. To overcome the limitations, we propose a simple yet effective method that can efficiently align gradients between tasks. Specifically, we perform each inner-optimization by sequentially sampling batches from all the tasks, followed by a Reptile outer update. Thanks to the gradients aligned between tasks by our method, the model becomes less vulnerable to negative transfer and catastrophic forgetting. We extensively validate our method on various multi-task learning and zero-shot cross-lingual transfer tasks, where our method largely outperforms all the relevant baselines we consider.

| Comments: | preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.02600](https://arxiv.org/abs/2110.02600) [cs.CL]** |
|           | (or **[arXiv:2110.02600v1](https://arxiv.org/abs/2110.02600v1) [cs.CL]** for this version) |





<h2 id="2021-10-07-2">2. How BPE Affects Memorization in Transformers
</h2>

Title: [How BPE Affects Memorization in Transformers](https://arxiv.org/abs/2110.02782)

Authors: [Eugene Kharitonov](https://arxiv.org/search/cs?searchtype=author&query=Kharitonov%2C+E), [Marco Baroni](https://arxiv.org/search/cs?searchtype=author&query=Baroni%2C+M), [Dieuwke Hupkes](https://arxiv.org/search/cs?searchtype=author&query=Hupkes%2C+D)

> Training data memorization in NLP can both be beneficial (e.g., closed-book QA) and undesirable (personal data extraction). In any case, successful model training requires a non-trivial amount of memorization to store word spellings, various linguistic idiosyncrasies and common knowledge. However, little is known about what affects the memorization behavior of NLP models, as the field tends to focus on the equally important question of generalization. In this work, we demonstrate that the size of the subword vocabulary learned by Byte-Pair Encoding (BPE) greatly affects both ability and tendency of standard Transformer models to memorize training data, even when we control for the number of learned parameters. We find that with a large subword vocabulary size, Transformer models fit random mappings more easily and are more vulnerable to membership inference attacks. Similarly, given a prompt, Transformer-based language models with large subword vocabularies reproduce the training data more often. We conjecture this effect is caused by reduction in the sequences' length that happens as the BPE vocabulary grows. Our findings can allow a more informed choice of hyper-parameters, that is better tailored for a particular use-case.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.02782](https://arxiv.org/abs/2110.02782) [cs.CL]** |
|           | (or **[arXiv:2110.02782v1](https://arxiv.org/abs/2110.02782v1) [cs.CL]** for this version) |





<h2 id="2021-10-07-3">3. Sequence-to-Sequence Lexical Normalization with Multilingual Transformers
</h2>

Title: [Sequence-to-Sequence Lexical Normalization with Multilingual Transformers](https://arxiv.org/abs/2110.02869)

Authors: [Ana-Maria Bucur](https://arxiv.org/search/cs?searchtype=author&query=Bucur%2C+A), [Adrian Cosma](https://arxiv.org/search/cs?searchtype=author&query=Cosma%2C+A), [Liviu P. Dinu](https://arxiv.org/search/cs?searchtype=author&query=Dinu%2C+L+P)

> Current benchmark tasks for natural language processing contain text that is qualitatively different from the text used in informal day to day digital communication. This discrepancy has led to severe performance degradation of state-of-the-art NLP models when fine-tuned on real-world data. One way to resolve this issue is through lexical normalization, which is the process of transforming non-standard text, usually from social media, into a more standardized form. In this work, we propose a sentence-level sequence-to-sequence model based on mBART, which frames the problem as a machine translation problem. As the noisy text is a pervasive problem across languages, not just English, we leverage the multi-lingual pre-training of mBART to fine-tune it to our data. While current approaches mainly operate at the word or subword level, we argue that this approach is straightforward from a technical standpoint and builds upon existing pre-trained transformer networks. Our results show that while word-level, intrinsic, performance evaluation is behind other methods, our model improves performance on extrinsic, downstream tasks through normalization compared to models operating on raw, unprocessed, social media text.

| Comments: | Accepted to Proceedings of the 7th Workshop on Noisy User-generated Text (WNUT 2021), EMNLP 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.02869](https://arxiv.org/abs/2110.02869) [cs.CL]** |
|           | (or **[arXiv:2110.02869v2](https://arxiv.org/abs/2110.02869v2) [cs.CL]** for this version) |





<h2 id="2021-10-07-4">4. Using Optimal Transport as Alignment Objective for fine-tuning Multilingual Contextualized Embeddings
</h2>

Title: [Using Optimal Transport as Alignment Objective for fine-tuning Multilingual Contextualized Embeddings](https://arxiv.org/abs/2110.02887)

Authors: [Sawsan Alqahtani](https://arxiv.org/search/cs?searchtype=author&query=Alqahtani%2C+S), [Garima Lalwani](https://arxiv.org/search/cs?searchtype=author&query=Lalwani%2C+G), [Yi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Salvatore Romeo](https://arxiv.org/search/cs?searchtype=author&query=Romeo%2C+S), [Saab Mansour](https://arxiv.org/search/cs?searchtype=author&query=Mansour%2C+S)

> Recent studies have proposed different methods to improve multilingual word representations in contextualized settings including techniques that align between source and target embedding spaces. For contextualized embeddings, alignment becomes more complex as we additionally take context into consideration. In this work, we propose using Optimal Transport (OT) as an alignment objective during fine-tuning to further improve multilingual contextualized representations for downstream cross-lingual transfer. This approach does not require word-alignment pairs prior to fine-tuning that may lead to sub-optimal matching and instead learns the word alignments within context in an unsupervised manner. It also allows different types of mappings due to soft matching between source and target sentences. We benchmark our proposed method on two tasks (XNLI and XQuAD) and achieve improvements over baselines as well as competitive results compared to similar recent works.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | EMNLP 2021                                                   |
| Cite as:           | **[arXiv:2110.02887](https://arxiv.org/abs/2110.02887) [cs.CL]** |
|                    | (or **[arXiv:2110.02887v1](https://arxiv.org/abs/2110.02887v1) [cs.CL]** for this version) |








# 2021-10-06

[Return to Index](#Index)



<h2 id="2021-10-06-1">1. OPAD: An Optimized Policy-based Active Learning Framework for Document Content Analysis
</h2>

Title: [OPAD: An Optimized Policy-based Active Learning Framework for Document Content Analysis](https://arxiv.org/abs/2110.02069)

Authors: [Sumit Shekhar](https://arxiv.org/search/cs?searchtype=author&query=Shekhar%2C+S), [Bhanu Prakash Reddy Guda](https://arxiv.org/search/cs?searchtype=author&query=Guda%2C+B+P+R), [Ashutosh Chaubey](https://arxiv.org/search/cs?searchtype=author&query=Chaubey%2C+A), [Ishan Jindal](https://arxiv.org/search/cs?searchtype=author&query=Jindal%2C+I), [Avanish Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+A)

> Documents are central to many business systems, and include forms, reports, contracts, invoices or purchase orders. The information in documents is typically in natural language, but can be organized in various layouts and formats. There have been recent spurt of interest in understanding document content with novel deep learning architectures. However, document understanding tasks need dense information annotations, which are costly to scale and generalize. Several active learning techniques have been proposed to reduce the overall budget of annotation while maintaining the performance of the underlying deep learning model. However, most of these techniques work only for classification problems. But content detection is a more complex task, and has been scarcely explored in active learning literature. In this paper, we propose \textit{OPAD}, a novel framework using reinforcement policy for active learning in content detection tasks for documents. The proposed framework learns the acquisition function to decide the samples to be selected while optimizing performance metrics that the tasks typically have. Furthermore, we extend to weak labelling scenarios to further reduce the cost of annotation significantly. We propose novel rewards to account for class imbalance and user feedback in the annotation interface, to improve the active learning method. We show superior performance of the proposed \textit{OPAD} framework for active learning for various tasks related to document understanding like layout parsing, object detection and named entity recognition. Ablation studies for human feedback and class imbalance rewards are presented, along with a comparison of annotation times for different approaches.

| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.02069](https://arxiv.org/abs/2110.02069) [cs.IR]** |
|           | (or **[arXiv:2110.02069v1](https://arxiv.org/abs/2110.02069v1) [cs.IR]** for this version) |





<h2 id="2021-10-06-2">2. Rerunning OCR -- A Machine Learning Approach to Quality Assessment and Enhancement Prediction
</h2>

Title: [Rerunning OCR -- A Machine Learning Approach to Quality Assessment and Enhancement Prediction](https://arxiv.org/abs/2110.01661)

Authors: [Pit Schneider](https://arxiv.org/search/cs?searchtype=author&query=Schneider%2C+P)

> Iterating with new and improved OCR solutions enforces decisions to be taken when it comes to targeting the right reprocessing candidates. This especially applies when the underlying data collection is of considerable size and rather diverse in terms of fonts, languages, periods of publication and consequently OCR quality. This article captures the efforts of the National Library of Luxembourg to support those exact decisions. They are crucial in order to guarantee low computational overhead and reduced quality degradation risks, combined with a more quantifiable OCR improvement. In particular, this work explains the methodology of the library with respect to text block level quality assessment. As an extension of this technique, another contribution comes in the form of a regression model that takes the enhancement potential of a new OCR engine into account. They both mark promising approaches, especially for cultural institutions dealing with historic data of lower quality.

| Comments:    | Journal of Data Mining and Digital Humanities                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2110.01661](https://arxiv.org/abs/2110.01661) [cs.CL]** |
|              | (or **[arXiv:2110.01661v1](https://arxiv.org/abs/2110.01661v1) [cs.CL]** for this version) |





<h2 id="2021-10-06-3">3. On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation
</h2>

Title: [On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation](https://arxiv.org/abs/2110.01811)

Authors: [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Longyue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Pre-training (PT) and back-translation (BT) are two simple and powerful methods to utilize monolingual data for improving the model performance of neural machine translation (NMT). This paper takes the first step to investigate the complementarity between PT and BT. We introduce two probing tasks for PT and BT respectively and find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder. Experimental results show that PT and BT are nicely complementary to each other, establishing state-of-the-art performances on the WMT16 English-Romanian and English-Russian benchmarks. Through extensive analyses on sentence originality and word frequency, we also demonstrate that combining Tagged BT with PT is more helpful to their complementarity, leading to better translation quality. Source code is freely available at [this https URL](https://github.com/SunbowLiu/PTvsBT).

| Comments: | Accepted to Findings of EMNLP 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2110.01811](https://arxiv.org/abs/2110.01811) [cs.CL]** |
|           | (or **[arXiv:2110.01811v1](https://arxiv.org/abs/2110.01811v1) [cs.CL]** for this version) |





<h2 id="2021-10-06-4">4. Data Augmentation Approaches in Natural Language Processing: A Survey
</h2>

Title: [Data Augmentation Approaches in Natural Language Processing: A Survey](https://arxiv.org/abs/2110.01852)

Authors: [Bohan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Yutai Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+Y), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W)

> As an effective strategy, data augmentation (DA) alleviates data scarcity scenarios where deep learning techniques may fail. It is widely applied in computer vision then introduced to natural language processing and achieves improvements in many tasks. One of the main focuses of the DA methods is to improve the diversity of training data, thereby helping the model to better generalize to unseen testing data. In this survey, we frame DA methods into three categories based on the diversity of augmented data, including paraphrasing, noising, and sampling. Our paper sets out to analyze DA methods in detail according to the above categories. Further, we also introduce their applications in NLP tasks as well as the challenges.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.01852](https://arxiv.org/abs/2110.01852) [cs.CL]** |
|           | (or **[arXiv:2110.01852v1](https://arxiv.org/abs/2110.01852v1) [cs.CL]** for this version) |





<h2 id="2021-10-06-5">5. Sicilian Translator: A Recipe for Low-Resource NMT
</h2>

Title: [Sicilian Translator: A Recipe for Low-Resource NMT](https://arxiv.org/abs/2110.01938)

Authors: [Eryk Wdowiak](https://arxiv.org/search/cs?searchtype=author&query=Wdowiak%2C+E)

> With 17,000 pairs of Sicilian-English translated sentences, Arba Sicula developed the first neural machine translator for the Sicilian language. Using small subword vocabularies, we trained small Transformer models with high dropout parameters and achieved BLEU scores in the upper 20s. Then we supplemented our dataset with backtranslation and multilingual translation and pushed our scores into the mid 30s. We also attribute our success to incorporating theoretical information in our dataset. Prior to training, we biased the subword vocabulary towards the desinences one finds in a textbook. And we included textbook exercises in our dataset.

| Comments:    | 7 pages, 2 tables                                            |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2110.01938](https://arxiv.org/abs/2110.01938) [cs.CL]** |
|              | (or **[arXiv:2110.01938v1](https://arxiv.org/abs/2110.01938v1) [cs.CL]** for this version) |





<h2 id="2021-10-06-6">6. Transfer Learning for Multi-lingual Tasks -- a Survey
</h2>

Title: [Transfer Learning for Multi-lingual Tasks -- a Survey](https://arxiv.org/abs/2110.02052)

Authors: [Amir Reza Jafari](https://arxiv.org/search/cs?searchtype=author&query=Jafari%2C+A+R), [Behnam Heidary](https://arxiv.org/search/cs?searchtype=author&query=Heidary%2C+B), [Reza Farahbakhsh](https://arxiv.org/search/cs?searchtype=author&query=Farahbakhsh%2C+R), [Mostafa Salehi](https://arxiv.org/search/cs?searchtype=author&query=Salehi%2C+M), [Mahdi Jalili](https://arxiv.org/search/cs?searchtype=author&query=Jalili%2C+M)

> These days different platforms such as social media provide their clients from different backgrounds and languages the possibility to connect and exchange information. It is not surprising anymore to see comments from different languages in posts published by international celebrities or data providers. In this era, understanding cross languages content and multilingualism in natural language processing (NLP) are hot topics, and multiple efforts have tried to leverage existing technologies in NLP to tackle this challenging research problem. In this survey, we provide a comprehensive overview of the existing literature with a focus on transfer learning techniques in multilingual tasks. We also identify potential opportunities for further research in this domain.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.02052](https://arxiv.org/abs/2110.02052) [cs.CL]** |
|           | (or **[arXiv:2110.02052v1](https://arxiv.org/abs/2110.02052v1) [cs.CL]** for this version) |





<h2 id="2021-10-06-7">7. Structured Prediction in NLP -- A survey
</h2>

Title: [Structured Prediction in NLP -- A survey](https://arxiv.org/abs/2110.02057)

Authors: [Chauhan Dev](https://arxiv.org/search/cs?searchtype=author&query=Dev%2C+C), [Naman Biyani](https://arxiv.org/search/cs?searchtype=author&query=Biyani%2C+N), [Nirmal P. Suthar](https://arxiv.org/search/cs?searchtype=author&query=Suthar%2C+N+P), [Prashant Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+P), [Priyanshu Agarwal](https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+P)

> Over the last several years, the field of Structured prediction in NLP has had seen huge advancements with sophisticated probabilistic graphical models, energy-based networks, and its combination with deep learning-based approaches. This survey provides a brief of major techniques in structured prediction and its applications in the NLP domains like parsing, sequence labeling, text generation, and sequence to sequence tasks. We also deep-dived into energy-based and attention-based techniques in structured prediction, identified some relevant open issues and gaps in the current state-of-the-art research, and have come up with some detailed ideas for future research in these fields.

| Comments: | 6 pages, 0 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2110.02057](https://arxiv.org/abs/2110.02057) [cs.CL]** |
|           | (or **[arXiv:2110.02057v1](https://arxiv.org/abs/2110.02057v1) [cs.CL]** for this version) |





<h2 id="2021-10-06-8">8. Interactively Generating Explanations for Transformer-based Language Models
</h2>

Title: [Interactively Generating Explanations for Transformer-based Language Models](https://arxiv.org/abs/2110.02058)

Authors: [Patrick Schramowski](https://arxiv.org/search/cs?searchtype=author&query=Schramowski%2C+P), [Felix Friedrich](https://arxiv.org/search/cs?searchtype=author&query=Friedrich%2C+F), [Christopher Tauchmann](https://arxiv.org/search/cs?searchtype=author&query=Tauchmann%2C+C), [Kristian Kersting](https://arxiv.org/search/cs?searchtype=author&query=Kersting%2C+K)

> Transformer language models are state-of-the-art in a multitude of NLP tasks. Despite these successes, their opaqueness remains problematic. Recent methods aiming to provide interpretability and explainability to black-box models primarily focus on post-hoc explanations of (sometimes spurious) input-output correlations. Instead, we emphasize using prototype networks directly incorporated into the model architecture and hence explain the reasoning process behind the network's decisions. Moreover, while our architecture performs on par with several language models, it enables one to learn from user interactions. This not only offers a better understanding of language models, but uses human capabilities to incorporate knowledge outside of the rigid range of purely data-driven approaches.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.02058](https://arxiv.org/abs/2110.02058) [cs.CL]** |
|           | (or **[arXiv:2110.02058v1](https://arxiv.org/abs/2110.02058v1) [cs.CL]** for this version) |









# 2021-10-05

[Return to Index](#Index)



<h2 id="2021-10-05-1">1. Improving Zero-shot Multilingual Neural Machine Translation for Low-Resource Languages
</h2>

Title: [Improving Zero-shot Multilingual Neural Machine Translation for Low-Resource Languages](https://arxiv.org/abs/2110.00712)

Authors: [Chenyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Gongxu Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+G)

> Although the multilingual Neural Machine Translation(NMT), which extends Google's multilingual NMT, has ability to perform zero-shot translation and the iterative self-learning algorithm can improve the quality of zero-shot translation, it confronts with two problems: the multilingual NMT model is prone to generate wrong target language when implementing zero-shot translation; the self-learning algorithm, which uses beam search to generate synthetic parallel data, demolishes the diversity of the generated source language and amplifies the impact of the same noise during the iterative learning process. In this paper, we propose the tagged-multilingual NMT model and improve the self-learning algorithm to handle these two problems. Firstly, we extend the Google's multilingual NMT model and add target tokens to the target languages, which associates the start tag with the target language to ensure that the source language can be translated to the required target language. Secondly, we improve the self-learning algorithm by replacing beam search with random sample to increases the diversity of the generated data and makes it properly cover the true data distribution. Experimental results on IWSLT show that the adjusted tagged-multilingual NMT separately obtains 9.41 and 7.85 BLEU scores over the multilingual NMT on 2010 and 2017 Romanian-Italian test sets. Similarly, it obtains 9.08 and 7.99 BLEU scores on Italian-Romanian zero-shot translation. Furthermore, the improved self-learning algorithm shows its superiorities over the conventional self-learning algorithm on zero-shot translations.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.00712](https://arxiv.org/abs/2110.00712) [cs.CL]** |
|           | (or **[arXiv:2110.00712v1](https://arxiv.org/abs/2110.00712v1) [cs.CL]** for this version) |









# 2021-10-04

[Return to Index](#Index)



<h2 id="2021-10-04-1">1. Improving Punctuation Restoration for Speech Transcripts via External Data
</h2>

Title: [Improving Punctuation Restoration for Speech Transcripts via External Data](https://arxiv.org/abs/2110.00560)

Authors:[Xue-Yong Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+X), [Cheng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Md Tahmid Rahman Laskar](https://arxiv.org/search/cs?searchtype=author&query=Laskar%2C+M+T+R), [Shashi Bhushan TN](https://arxiv.org/search/cs?searchtype=author&query=TN%2C+S+B), [Simon Corston-Oliver](https://arxiv.org/search/cs?searchtype=author&query=Corston-Oliver%2C+S)

> Automatic Speech Recognition (ASR) systems generally do not produce punctuated transcripts. To make transcripts more readable and follow the expected input format for downstream language models, it is necessary to add punctuation marks. In this paper, we tackle the punctuation restoration problem specifically for the noisy text (e.g., phone conversation scenarios). To leverage the available written text datasets, we introduce a data sampling technique based on an n-gram language model to sample more training data that are similar to our in-domain data. Moreover, we propose a two-stage fine-tuning approach that utilizes the sampled external data as well as our in-domain dataset for models based on BERT. Extensive experiments show that the proposed approach outperforms the baseline with an improvement of 1:12% F1 score.

| Comments: | Accepted by W-NUT at EMNLP 2021                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2110.00560](https://arxiv.org/abs/2110.00560) [cs.CL]** |
|           | (or **[arXiv:2110.00560v1](https://arxiv.org/abs/2110.00560v1) [cs.CL]** for this version) |





<h2 id="2021-10-04-2">2. A Survey of Knowledge Enhanced Pre-trained Models
</h2>

Title: [A Survey of Knowledge Enhanced Pre-trained Models](https://arxiv.org/abs/2110.00269)

Authors:[Jian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Gang Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+G), [Yulong Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Wei Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+W), [Xinyu Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X), [Ying Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Jinghui Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+J)

> Pre-trained models learn contextualized word representations on large-scale text corpus through a self-supervised learning method, which has achieved promising performance after fine-tuning. These models, however, suffer from poor robustness and lack of interpretability. Pre-trained models with knowledge injection, which we call knowledge enhanced pre-trained models (KEPTMs), possess deep understanding and logical reasoning and introduce interpretability to some extent. In this survey, we provide a comprehensive overview of KEPTMs for natural language processing. We first introduce the progress of pre-trained models and knowledge representation learning. Then we systematically categorize existing KEPTMs from three different perspectives. Finally, we outline some potential directions of KEPTMs for future research.

| Comments: | 16 pages, 11 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2110.00269](https://arxiv.org/abs/2110.00269) [cs.CL]** |
|           | (or **[arXiv:2110.00269v1](https://arxiv.org/abs/2110.00269v1) [cs.CL]** for this version) |





<h2 id="2021-10-04-3">3. Attention based Sequence to Sequence Learning for Machine Translation of Low Resourced Indic Languages -- A case of Sanskrit to Hindi
</h2>

Title: [Attention based Sequence to Sequence Learning for Machine Translation of Low Resourced Indic Languages -- A case of Sanskrit to Hindi](https://arxiv.org/abs/2110.00435)

Authors:[Vishvajit Bakarola](https://arxiv.org/search/cs?searchtype=author&query=Bakarola%2C+V), [Jitendra Nasriwala](https://arxiv.org/search/cs?searchtype=author&query=Nasriwala%2C+J)

> Deep Learning techniques are powerful in mimicking humans in a particular set of problems. They have achieved a remarkable performance in complex learning tasks. Deep learning inspired Neural Machine Translation (NMT) is a proficient technique that outperforms traditional machine translation. Performing machine-aided translation on Indic languages has always been a challenging task considering their rich and diverse grammar. The neural machine translation has shown quality results compared to the traditional machine translation approaches. The fully automatic machine translation becomes problematic when it comes to low-resourced languages, especially with Sanskrit. This paper presents attention mechanism based neural machine translation by selectively focusing on a particular part of language sentences during translation. The work shows the construction of Sanskrit to Hindi bilingual parallel corpus with nearly 10K samples and having 178,000 tokens. The neural translation model equipped with an attention mechanism has been trained on Sanskrit to Hindi parallel corpus. The approach has shown the significance of attention mechanisms to overcome long-term dependencies, primarily associated with low resources Indic languages. The paper shows the attention plots on testing data to demonstrate the alignment between source and translated words. For the evaluation of the translated sentences, manual score based human evaluation and automatic evaluation metric based techniques have been adopted. The attention mechanism based neural translation has achieved 88% accuracy in human evaluation and a BLEU score of 0.92 on Sanskrit to Hindi translation.

| Comments:          | Published with International Journal of Engineering Trends and Technology (IJETT) |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Journal reference: | International Journal of Engineering Trends and Technology (IJETT) 69.9(2021):230-235 |
| DOI:               | [10.14445/22315381/IJETT-V69I9P227](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.14445%2F22315381%2FIJETT-V69I9P227&v=cf20c5d7) |
| Cite as:           | **[arXiv:2110.00435](https://arxiv.org/abs/2110.00435) [cs.CL]** |
|                    | (or **[arXiv:2110.00435v1](https://arxiv.org/abs/2110.00435v1) [cs.CL]** for this version) |





# 2021-10-01

[Return to Index](#Index)



<h2 id="2021-10-01-1">1. Phonetic Word Embeddings
</h2>

Title: [Phonetic Word Embeddings](https://arxiv.org/abs/2109.14796)

Authors: [Rahul Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+R), [Kunal Dhawan](https://arxiv.org/search/cs?searchtype=author&query=Dhawan%2C+K), [Balakrishna Pailla](https://arxiv.org/search/cs?searchtype=author&query=Pailla%2C+B)

> This work presents a novel methodology for calculating the phonetic similarity between words taking motivation from the human perception of sounds. This metric is employed to learn a continuous vector embedding space that groups similar sounding words together and can be used for various downstream computational phonology tasks. The efficacy of the method is presented for two different languages (English, Hindi) and performance gains over previous reported works are discussed on established tests for predicting phonetic similarity. To address limited benchmarking mechanisms in this field, we also introduce a heterographic pun dataset based evaluation methodology to compare the effectiveness of acoustic similarity algorithms. Further, a visualization of the embedding space is presented with a discussion on the various possible use-cases of this novel algorithm. An open-source implementation is also shared to aid reproducibility and enable adoption in related tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.14796](https://arxiv.org/abs/2109.14796) [cs.CL]** |
|           | (or **[arXiv:2109.14796v1](https://arxiv.org/abs/2109.14796v1) [cs.CL]** for this version) |





<h2 id="2021-10-01-2">2. Improved statistical machine translation using monolingual paraphrases
</h2>

Title: [Improved statistical machine translation using monolingual paraphrases](https://arxiv.org/abs/2109.15119)

Authors: [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P)

> We propose a novel monolingual sentence paraphrasing method for augmenting the training data for statistical machine translation systems "for free" -- by creating it from data that is already available rather than having to create more aligned data. Starting with a syntactic tree, we recursively generate new sentence variants where noun compounds are paraphrased using suitable prepositions, and vice-versa -- preposition-containing noun phrases are turned into noun compounds. The evaluation shows an improvement equivalent to 33%-50% of that of doubling the amount of training data.

| Comments:          | machine translation, SMT, paraphrasing, data augmentation. arXiv admin note: substantial text overlap with [arXiv:1912.01113](https://arxiv.org/abs/1912.01113) |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| MSC classes:       | 68T50                                                        |
| ACM classes:       | F.2.2; I.2.7                                                 |
| Journal reference: | ECAI-2008                                                    |
| Cite as:           | **[arXiv:2109.15119](https://arxiv.org/abs/2109.15119) [cs.CL]** |
|                    | (or **[arXiv:2109.15119v1](https://arxiv.org/abs/2109.15119v1) [cs.CL]** for this version) |


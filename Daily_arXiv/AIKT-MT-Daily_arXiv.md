# Daily arXiv: Machine Translation - September, 2021

# Index


- [2021-09-17](#2021-09-17)

  - [1. Scaling Laws for Neural Machine Translation](#2021-09-17-1)
  - [2. CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning](#2021-09-17-2)
  - [3. Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification](#2021-09-17-3)
  - [4. Language Models are Few-shot Multilingual Learners](#2021-09-17-4)
  - [5. Improving Neural Machine Translation by Bidirectional Training](#2021-09-17-5)
  - [6. The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation](#2021-09-17-6)
  - [7. The NiuTrans System for the WMT21 Efficiency Task](#2021-09-17-7)
- [2021-09-16](#2021-09-16-1)

  - [1. fairseq S^2: A Scalable and Integrable Speech Synthesis Toolkit](#2021-09-16-1)
  - [2. On the Language-specificity of Multilingual BERT and the Impact of Fine-tuning](#2021-09-16-2)
  - [3. The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders](#2021-09-16-3)
  - [4. Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality Estimation for Neural Machine Translation](#2021-09-16-4)
  - [5. Regressive Ensemble for Machine Translation Quality Evaluation](#2021-09-16-5)
  - [6. Sequence Length is a Domain: Length-based Overfitting in Transformer Models](#2021-09-16-6)
  - [7. What Vision-Language Models `See' when they See Scenes](#2021-09-16-7)
  - [8.Cross-lingual Transfer of Monolingual Models](#2021-09-16-8)
  - [9. UniST: Unified End-to-end Model for Streaming and Non-streaming Speech Translation](#2021-09-16-9)
  - [10. SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations](#2021-09-16-10)
  - [11. Is "moby dick" a Whale or a Bird? Named Entities and Terminology in Speech Translation](#2021-09-16-11)
  - [12. When Does Translation Require Context? A Data-driven, Multilingual Exploration](#2021-09-16-12)
  - [13. On the Limits of Minimal Pairs in Contrastive Evaluation](#2021-09-16-13)
- [2021-09-15](#2021-09-15)
  - [1. Scalable Font Reconstruction with Dual Latent Manifolds](#2021-09-15-1)
  - [2. Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation](#2021-09-15-2)
  - [3. Evaluating Multiway Multilingual NMT in the Turkic Languages](#2021-09-15-3)
  - [4. Post-OCR Document Correction with large Ensembles of Character Sequence Models](#2021-09-15-4)
  - [5. Mitigating Catastrophic Forgetting in Scheduled Sampling with Elastic Weight Consolidation in Neural Machine Translation](#2021-09-15-5)
  - [6. Uncertainty-Aware Machine Translation Evaluation](#2021-09-15-6)
  - [7. AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate](#2021-09-15-7)
  - [8. Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation](#2021-09-15-8)
  - [9. Efficient Inference for Multilingual Neural Machine Translation](#2021-09-15-9)
  - [10. LM-Critic: Language Models for Unsupervised Grammatical Error Correction](#2021-09-15-10)
- [2021-09-14](#2021-09-14)

  - [1. MURAL: Multimodal, Multitask Retrieval Across Languages](#2021-09-14-1)
  - [2. GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks](#2021-09-14-2)
  - [3. Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy](#2021-09-14-3)
  - [4. Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model](#2021-09-14-4)
  - [5. Multilingual Translation via Grafting Pre-trained Language Models](#2021-09-14-5)
  - [6. Pairwise Supervised Contrastive Learning of Sentence Representations](#2021-09-14-6)
  - [7. Contrastive Learning for Context-aware Neural Machine TranslationUsing Coreference Information](#2021-09-14-7)
  - [8. CPT: A Pre-Trained Unbalanced Transformerfor Both Chinese Language Understanding and Generation](#2021-09-14-8)
  - [9. Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions](#2021-09-14-9)
  - [10. Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning](#2021-09-14-10)
  - [11. Tamizhi-Net OCR: Creating A Quality Large Scale Tamil-Sinhala-English Parallel Corpus Using Deep Learning Based Printed Character Recognition (PCR)](#2021-09-14-11)
  - [12. xGQA: Cross-Lingual Visual Question Answering](#2021-09-14-12)
- [2021-09-13](#2021-09-13)

  - [1. LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation](#2021-09-13-1)
  - [2. Speechformer: Reducing Information Loss in Direct Speech Translation](#2021-09-13-2)
  - [3. BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation](#2021-09-13-3)
  - [4. A Large-Scale Study of Machine Translation in the Turkic Languages](#2021-09-13-4)
  - [5. Rule-based Morphological Inflection Improves Neural Terminology Translation](#2021-09-13-5)
  - [6. EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling](#2021-09-13-6)
  - [7. Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables](#2021-09-13-7)
  - [8. Pre-train or Annotate? Domain Adaptation with a Constrained Budget](#2021-09-13-8)
  - [9. AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages](#2021-09-13-9)
  - [10. A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations](#2021-09-13-10)
  - [11. Improving Multilingual Translation by Representation and Gradient Regularization](#2021-09-13-11)
  - [12. Artificial Text Detection via Examining the Topology of Attention Maps](#2021-09-13-12)
  - [13. Box Embeddings: An open-source library for representation learning using geometric structures](#2021-09-13-13)
  - [14. BiSECT: Learning to Split and Rephrase Sentences with Bitexts](#2021-09-13-14)
  - [15. Neural Machine Translation Quality and Post-Editing Performance](#2021-09-13-15)
- [2021-09-10](#2021-09-10)
  - [1. Non-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring](#2021-09-10-1)
  - [2. TxT: Crossmodal End-to-End Learning with Transformers](#2021-09-10-2)
  - [3. Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation](#2021-09-10-3)
  - [4. Ensemble Fine-tuned mBERT for Translation Quality Estimation](#2021-09-10-4)
  - [5. Competence-based Curriculum Learning for Multilingual Machine Translation](#2021-09-10-5)
  - [6. Distributionally Robust Multilingual Machine Translation](#2021-09-10-6)
  - [7. Efficient Nearest Neighbor Language Models](#2021-09-10-7)
  - [8. Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection](#2021-09-10-8)
  - [9. MATE: Multi-view Attention for Table Transformer Efficiency](#2021-09-10-9)
  - [10. Smoothed Contrastive Learning for Unsupervised Sentence Embedding](#2021-09-10-10)
  - [11. PPT: Pre-trained Prompt Tuning for Few-shot Learning](#2021-09-10-11)
  - [12. ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding](#2021-09-10-12)
  - [13. HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints](#2021-09-10-13)
  - [14. Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers](#2021-09-10-14)
- [2021-09-09](#2021-09-09)

  - [1. Mixup Decoding for Diverse Machine Translation](#2021-09-09-1)
  - [2. Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models](#2021-09-09-2)
  - [3. Sequence Level Contrastive Learning for Text Summarization](#2021-09-09-3)
  - [4. RefineCap: Concept-Aware Refinement for Image Captioning](#2021-09-09-4)
  - [5. Discrete and Soft Prompting for Multilingual Models](#2021-09-09-5)
  - [6. Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach](#2021-09-09-6)
  - [7. Active Learning by Acquiring Contrastive Examples](#2021-09-09-7)
- [2021-09-08](#2021-09-08)

  - [1. Paraphrase Generation as Unsupervised Machine Translation](#2021-09-08-1)
  - [2. Don't Go Far Off: An Empirical Study on Neural Poetry Translation](#2021-09-08-2)
  - [3. Revisiting Context Choices for Context-aware Machine Translation](#2021-09-08-3)
  - [4. Generate & Rank: A Multi-task Framework for Math Word Problems](#2021-09-08-4)
  - [5. NumGPT: Improving Numeracy Ability of Generative Pre-trained Models](#2021-09-08-5)
- [2021-09-07](#2021-09-07)

  - [1. Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models](#2021-09-07-1)
  - [2. On the ability of monolingual models to learn language-agnostic representations](#2021-09-07-2)
  - [3. Counterfactual Evaluation for Explainable AI](#2021-09-07-3)
  - [4. Data Efficient Masked Language Modeling for Vision and Language](#2021-09-07-4)
  - [5. Teaching Autoregressive Language Models Complex Tasks By Demonstration](#2021-09-07-5)
  - [6. Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack](#2021-09-07-6)
  - [7. Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training](#2021-09-07-7)
- [2021-09-06](#2021-09-06)
  - [1. Ranking Scientific Papers Using Preference Learning](#2021-09-06-1)
  - [2. Establishing Interlingua in Multilingual Language Models](#2021-09-06-2)
  - [3. Quantifying Reproducibility in NLP and ML](#2021-09-06-3)
  - [4. Multimodal Conditionality for Natural Language Generation](#2021-09-06-4)
  - [5. Do Prompt-Based Models Really Understand the Meaning of their Prompts?](#2021-09-06-5)
  - [6. Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT](#2021-09-06-6)
  - [7. Finetuned Language Models Are Zero-Shot Learners](#2021-09-06-7)
- [2021-09-03](#2021-09-03)
  - [1. Skim-Attention: Learning to Focus via Document Layout](#2021-09-03-1)
  - [2. How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?](#2021-09-03-2)
  - [3. Sequence-to-Sequence Learning with Latent Neural Grammars](#2021-09-03-3)
  - [4. Knowledge Perceived Multi-modal Pretraining in E-commerce](#2021-09-03-4)
  - [5. Improving Multimodal fusion via Mutual Dependency Maximisation](#2021-09-03-5)
  - [6. Towards Improving Adversarial Training of NLP Models](#2021-09-03-6)
  - [7. Point-of-Interest Type Prediction using Text and Images](#2021-09-03-7)
  - [8. Towards Making the Most of Dialogue Characteristics for Neural Chat Translation](#2021-09-03-8)
- [2021-09-02](#2021-09-02)

  - [1. Sentence Bottleneck Autoencoders from Transformer Language Models](#2021-09-02-1)
  - [2. It's not Rocket Science : Interpreting Figurative Language in Narratives](#2021-09-02-2)
  - [3. Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast](#2021-09-02-3)
  - [4. Discovering Representation Sprachbund For Multilingual Pre-Training](#2021-09-02-4)
  - [5. ∞-former: Infinite Memory Transformer](#2021-09-02-5)
  - [6. Masked Adversarial Generation for Neural Machine Translation](#2021-09-02-6)
  - [7. Position Masking for Improved Layout-Aware Document Understanding](#2021-09-02-7)
  - [8. Survey of Low-Resource Machine Translation](#2021-09-02-8)
- [2021-09-01](#2021-09-01)
  - [1. SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory](#2021-09-01-1)
  - [2. Want To Reduce Labeling Cost? GPT-3 Can Help](#2021-09-01-2)
  - [3. T3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP](#2021-09-01-3)
  - [4. Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience](#2021-09-01-4)
  - [5. Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools](#2021-09-01-5)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MT-Daily_arXiv-index.md)



# 2021-09-17

[Return to Index](#Index)



<h2 id="2021-09-17-1">1. Scaling Laws for Neural Machine Translation
</h2>

Title: [Scaling Laws for Neural Machine Translation](https://arxiv.org/abs/2109.07740)

Authors: [Behrooz Ghorbani](https://arxiv.org/search/cs?searchtype=author&query=Ghorbani%2C+B), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Maxim Krikun](https://arxiv.org/search/cs?searchtype=author&query=Krikun%2C+M), [Xavier Garcia](https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+X), [Ciprian Chelba](https://arxiv.org/search/cs?searchtype=author&query=Chelba%2C+C), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C)

> We present an empirical study of scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Specifically (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size, and show that it gives accurate predictions under a variety of scaling approaches and languages; we show that the total number of parameters alone is not sufficient for such purposes. (ii) We observe different power law exponents when scaling the decoder vs scaling the encoder, and provide recommendations for optimal allocation of encoder/decoder capacity based on this observation. (iii) We also report that the scaling behavior of the model is acutely influenced by composition bias of the train/test sets, which we define as any deviation from naturally generated text (either via machine generated or human translated text). We observe that natural text on the target side enjoys scaling, which manifests as successful reduction of the cross-entropy loss. (iv) Finally, we investigate the relationship between the cross-entropy loss and the quality of the generated translations. We find two different behaviors, depending on the nature of the test data. For test sets which were originally translated from target language to source language, both loss and BLEU score improve as model size increases. In contrast, for test sets originally translated from source language to target language, the loss improves, but the BLEU score stops improving after a certain threshold. We release generated text from all models used in this study.

| Comments: | 31 pages, 23 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2109.07740](https://arxiv.org/abs/2109.07740) [cs.LG]** |
|           | (or **[arXiv:2109.07740v1](https://arxiv.org/abs/2109.07740v1) [cs.LG]** for this version) |





<h2 id="2021-09-17-2">2. CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning
</h2>

Title: [CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning](https://arxiv.org/abs/2109.07589)

Authors: [Sarkar Snigdha Sarathi Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+S+S+S), [Arzoo Katiyar](https://arxiv.org/search/cs?searchtype=author&query=Katiyar%2C+A), [Rebecca J. Passonneau](https://arxiv.org/search/cs?searchtype=author&query=Passonneau%2C+R+J), [Rui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R)

> Named Entity Recognition (NER) in Few-Shot setting is imperative for entity tagging in low resource domains. Existing approaches only learn class-specific semantic features and intermediate representations from source domains. This affects generalizability to unseen target domains, resulting in suboptimal performances. To this end, we present CONTaiNER, a novel contrastive learning technique that optimizes the inter-token distribution distance for Few-Shot NER. Instead of optimizing class-specific attributes, CONTaiNER optimizes a generalized objective of differentiating between token categories based on their Gaussian-distributed embeddings. This effectively alleviates overfitting issues originating from training domains. Our experiments in several traditional test domains (OntoNotes, CoNLL'03, WNUT '17, GUM) and a new large scale Few-Shot NER dataset (Few-NERD) demonstrate that on average, CONTaiNER outperforms previous methods by 3%-13% absolute F1 points while showing consistent performance trends, even in challenging scenarios where previous approaches could not achieve appreciable performance.

| Comments: | 10 pages, 6 tables, 2 figures                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.07589](https://arxiv.org/abs/2109.07589) [cs.CL]** |
|           | (or **[arXiv:2109.07589v1](https://arxiv.org/abs/2109.07589v1) [cs.CL]** for this version) |





<h2 id="2021-09-17-3">3. Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification
</h2>

Title: [Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification](https://arxiv.org/abs/2109.07604)

Authors: [Daria Pylypenko](https://arxiv.org/search/cs?searchtype=author&query=Pylypenko%2C+D), [Kwabena Amponsah-Kaakyire](https://arxiv.org/search/cs?searchtype=author&query=Amponsah-Kaakyire%2C+K), [Koel Dutta Chowdhury](https://arxiv.org/search/cs?searchtype=author&query=Chowdhury%2C+K+D), [Josef van Genabith](https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J), [Cristina España-Bonet](https://arxiv.org/search/cs?searchtype=author&query=España-Bonet%2C+C)

> Traditional hand-crafted linguistically-informed features have often been used for distinguishing between translated and original non-translated texts. By contrast, to date, neural architectures without manual feature engineering have been less explored for this task. In this work, we (i) compare the traditional feature-engineering-based approach to the feature-learning-based one and (ii) analyse the neural architectures in order to investigate how well the hand-crafted features explain the variance in the neural models' predictions. We use pre-trained neural word embeddings, as well as several end-to-end neural architectures in both monolingual and multilingual settings and compare them to feature-engineering-based SVM classifiers. We show that (i) neural architectures outperform other approaches by more than 20 accuracy points, with the BERT-based model performing the best in both the monolingual and multilingual settings; (ii) while many individual hand-crafted translationese features correlate with neural model predictions, feature importance analysis shows that the most important features for neural and classical architectures differ; and (iii) our multilingual experiments provide empirical evidence for translationese universals across languages.

| Comments: | 9 pages, 5 pages appendix, 2 figures, 7 tables. The first 3 authors contributed equally. Accepted to EMNLP 2021, Main Conference |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.07604](https://arxiv.org/abs/2109.07604) [cs.CL]** |
|           | (or **[arXiv:2109.07604v1](https://arxiv.org/abs/2109.07604v1) [cs.CL]** for this version) |





<h2 id="2021-09-17-4">4. Language Models are Few-shot Multilingual Learners
</h2>

Title: [Language Models are Few-shot Multilingual Learners](https://arxiv.org/abs/2109.07684)

Authors: [Genta Indra Winata](https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I), [Andrea Madotto](https://arxiv.org/search/cs?searchtype=author&query=Madotto%2C+A), [Zhaojiang Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Rosanne Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+R), [Jason Yosinski](https://arxiv.org/search/cs?searchtype=author&query=Yosinski%2C+J), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

> General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models.

| Comments: | 14 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.07684](https://arxiv.org/abs/2109.07684) [cs.CL]** |
|           | (or **[arXiv:2109.07684v1](https://arxiv.org/abs/2109.07684v1) [cs.CL]** for this version) |





<h2 id="2021-09-17-5">5. Improving Neural Machine Translation by Bidirectional Training
</h2>

Title: [Improving Neural Machine Translation by Bidirectional Training](https://arxiv.org/abs/2109.07780)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Di Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+D), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D)

> We present a simple and effective pretraining strategy -- bidirectional training (BiT) for neural machine translation. Specifically, we bidirectionally update the model parameters at the early stage and then tune the model normally. To achieve bidirectional updating, we simply reconstruct the training samples from "src→tgt" to "src+tgt→tgt+src" without any complicated model modifications. Notably, our approach does not increase any parameters or training steps, requiring the parallel data merely. Experimental results show that BiT pushes the SOTA neural machine translation performance across 15 translation tasks on 8 language pairs (data sizes range from 160K to 38M) significantly higher. Encouragingly, our proposed model can complement existing data manipulation strategies, i.e. back translation, data distillation, and data diversification. Extensive analyses show that our approach functions as a novel bilingual code-switcher, obtaining better bilingual alignment.

| Comments: | EMNLP 2021. arXiv admin note: text overlap with [arXiv:2107.11572](https://arxiv.org/abs/2107.11572) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.07780](https://arxiv.org/abs/2109.07780) [cs.CL]** |
|           | (or **[arXiv:2109.07780v1](https://arxiv.org/abs/2109.07780v1) [cs.CL]** for this version) |





<h2 id="2021-09-17-6">6. The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation
</h2>

Title: [The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation](https://arxiv.org/abs/2109.07848)

Authors: [Laura Aina](https://arxiv.org/search/cs?searchtype=author&query=Aina%2C+L), [Tal Linzen](https://arxiv.org/search/cs?searchtype=author&query=Linzen%2C+T)

> Temporary syntactic ambiguities arise when the beginning of a sentence is compatible with multiple syntactic analyses. We inspect to which extent neural language models (LMs) exhibit uncertainty over such analyses when processing temporarily ambiguous inputs, and how that uncertainty is modulated by disambiguating cues. We probe the LM's expectations by generating from it: we use stochastic decoding to derive a set of sentence completions, and estimate the probability that the LM assigns to each interpretation based on the distribution of parses across completions. Unlike scoring-based methods for targeted syntactic evaluation, this technique makes it possible to explore completions that are not hypothesized in advance by the researcher. We apply this method to study the behavior of two LMs (GPT2 and an LSTM) on three types of temporary ambiguity, using materials from human sentence processing experiments. We find that LMs can track multiple analyses simultaneously; the degree of uncertainty varies across constructions and contexts. As a response to disambiguating cues, the LMs often select the correct interpretation, but occasional errors point to potential areas of improvement.

| Comments: | To appear in Proceedings of BlackboxNLP 2021: Analyzing and Interpreting Neural Networks for NLP |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.07848](https://arxiv.org/abs/2109.07848) [cs.CL]** |
|           | (or **[arXiv:2109.07848v1](https://arxiv.org/abs/2109.07848v1) [cs.CL]** for this version) |





<h2 id="2021-09-17-7">7. The NiuTrans System for the WMT21 Efficiency Task
</h2>

Title: [The NiuTrans System for the WMT21 Efficiency Task](https://arxiv.org/abs/2109.08003)

Authors: [Chenglong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Chi Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+C), [Yongyu Mu](https://arxiv.org/search/cs?searchtype=author&query=Mu%2C+Y), [Zhongxiang Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Z), [Siming Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Minyi Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+M), [Hang Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+H), [Bei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Ye Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Tong Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T), [Jingbo Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J)

> This paper describes the NiuTrans system for the WMT21 translation efficiency task ([this http URL](http://statmt.org/wmt21/efficiency-task.html)). Following last year's work, we explore various techniques to improve efficiency while maintaining translation quality. We investigate the combinations of lightweight Transformer architectures and knowledge distillation strategies. Also, we improve the translation efficiency with graph optimization, low precision, dynamic batching, and parallel pre/post-processing. Our system can translate 247,000 words per second on an NVIDIA A100, being 3× faster than last year's system. Our system is the fastest and has the lowest memory consumption on the GPU-throughput track. The code, model, and pipeline will be available at NiuTrans.NMT ([this https URL](https://github.com/NiuTrans/NiuTrans.NMT)).

| Comments: | NiuTrans at the WMT21 Translation Efficiency Task            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.08003](https://arxiv.org/abs/2109.08003) [cs.CL]** |
|           | (or **[arXiv:2109.08003v1](https://arxiv.org/abs/2109.08003v1) [cs.CL]** for this version) |





# 2021-09-16

[Return to Index](#Index)



<h2 id="2021-09-16-1">1. fairseq S^2: A Scalable and Integrable Speech Synthesis Toolkit
</h2>

Title: [fairseq S^2: A Scalable and Integrable Speech Synthesis Toolkit](https://arxiv.org/abs/2109.06912)

Authors: [Changhan Wang](https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+C), [Wei-Ning Hsu](https://arxiv.org/search/eess?searchtype=author&query=Hsu%2C+W), [Yossi Adi](https://arxiv.org/search/eess?searchtype=author&query=Adi%2C+Y), [Adam Polyak](https://arxiv.org/search/eess?searchtype=author&query=Polyak%2C+A), [Ann Lee](https://arxiv.org/search/eess?searchtype=author&query=Lee%2C+A), [Peng-Jen Chen](https://arxiv.org/search/eess?searchtype=author&query=Chen%2C+P), [Jiatao Gu](https://arxiv.org/search/eess?searchtype=author&query=Gu%2C+J), [Juan Pino](https://arxiv.org/search/eess?searchtype=author&query=Pino%2C+J)

> This paper presents fairseq S^2, a fairseq extension for speech synthesis. We implement a number of autoregressive (AR) and non-AR text-to-speech models, and their multi-speaker variants. To enable training speech synthesis models with less curated data, a number of preprocessing tools are built and their importance is shown empirically. To facilitate faster iteration of development and analysis, a suite of automatic metrics is included. Apart from the features added specifically for this extension, fairseq S^2 also benefits from the scalability offered by fairseq and can be easily integrated with other state-of-the-art systems provided in this framework. The code, documentation, and pre-trained models are available at [this https URL](https://github.com/pytorch/fairseq/tree/master/examples/speech_synthesis).

| Comments: | Accepted to EMNLP 2021 Demo                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2109.06912](https://arxiv.org/abs/2109.06912) [eess.AS]** |
|           | (or **[arXiv:2109.06912v1](https://arxiv.org/abs/2109.06912v1) [eess.AS]** for this version) |





<h2 id="2021-09-16-2">2. On the Language-specificity of Multilingual BERT and the Impact of Fine-tuning
</h2>

Title: [On the Language-specificity of Multilingual BERT and the Impact of Fine-tuning](https://arxiv.org/abs/2109.06935)

Authors: [Marc Tanti](https://arxiv.org/search/cs?searchtype=author&query=Tanti%2C+M), [Lonneke van der Plas](https://arxiv.org/search/cs?searchtype=author&query=van+der+Plas%2C+L), [Claudia Borg](https://arxiv.org/search/cs?searchtype=author&query=Borg%2C+C), [Albert Gatt](https://arxiv.org/search/cs?searchtype=author&query=Gatt%2C+A)

> Recent work has shown evidence that the knowledge acquired by multilingual BERT (mBERT) has two components: a language-specific and a language-neutral one. This paper analyses the relationship between them, in the context of fine-tuning on two tasks -- POS tagging and natural language inference -- which require the model to bring to bear different degrees of language-specific knowledge. Visualisations reveal that mBERT loses the ability to cluster representations by language after fine-tuning, a result that is supported by evidence from language identification experiments. However, further experiments on 'unlearning' language-specific representations using gradient reversal and iterative adversarial learning are shown not to add further improvement to the language-independent component over and above the effect of fine-tuning. The results presented here suggest that the process of fine-tuning causes a reorganisation of the model's limited representational capacity, enhancing language-independent representations at the expense of language-specific ones.

| Comments: | 22 pages, 6 figures, 5 tables, to appear in BlackBoxNLP 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| Cite as:  | **[arXiv:2109.06935](https://arxiv.org/abs/2109.06935) [cs.CL]** |
|           | (or **[arXiv:2109.06935v1](https://arxiv.org/abs/2109.06935v1) [cs.CL]** for this version) |







<h2 id="2021-09-16-3">3. The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders
</h2>

Title: [The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders](https://arxiv.org/abs/2109.06939)

Authors: [Han He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+H), [Jinho D. Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+J+D)

> Multi-task learning with transformer encoders (MTL) has emerged as a powerful technique to improve performance on closely-related tasks for both accuracy and efficiency while a question still remains whether or not it would perform as well on tasks that are distinct in nature. We first present MTL results on five NLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over single-task learning. We then conduct an extensive pruning analysis to show that a certain set of attention heads get claimed by most tasks during MTL, who interfere with one another to fine-tune those heads for their own objectives. Based on this finding, we propose the Stem Cell Hypothesis to reveal the existence of attention heads naturally talented for many tasks that cannot be jointly trained to create adequate embeddings for all of those tasks. Finally, we design novel parameter-free probes to justify our hypothesis and demonstrate how attention heads are transformed across the five tasks during MTL through label analysis.

| Comments: | Accepted to EMNLP 2021: The 2021 Conference on Empirical Methods in Natural Language Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.06939](https://arxiv.org/abs/2109.06939) [cs.CL]** |
|           | (or **[arXiv:2109.06939v1](https://arxiv.org/abs/2109.06939v1) [cs.CL]** for this version) |





<h2 id="2021-09-16-4">4. Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality Estimation for Neural Machine Translation
</h2>

Title: [Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality Estimation for Neural Machine Translation](https://arxiv.org/abs/2109.07141)

Authors: [Ke Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+K), [Yangbin Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y), [Jiayi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Yuqi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Yu Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Xiaolin Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X)

> Quality Estimation (QE) plays an essential role in applications of Machine Translation (MT). Traditionally, a QE system accepts the original source text and translation from a black-box MT system as input. Recently, a few studies indicate that as a by-product of translation, QE benefits from the model and training data's information of the MT system where the translations come from, and it is called the "glass-box QE". In this paper, we extend the definition of "glass-box QE" generally to uncertainty quantification with both "black-box" and "glass-box" approaches and design several features deduced from them to blaze a new trial in improving QE's performance. We propose a framework to fuse the feature engineering of uncertainty quantification into a pre-trained cross-lingual language model to predict the translation quality. Experiment results show that our method achieves state-of-the-art performances on the datasets of WMT 2020 QE shared task.

| Comments: | Accepted by Findings of EMNLP 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.07141](https://arxiv.org/abs/2109.07141) [cs.CL]** |
|           | (or **[arXiv:2109.07141v1](https://arxiv.org/abs/2109.07141v1) [cs.CL]** for this version) |



<h2 id="2021-09-16-5">5. Regressive Ensemble for Machine Translation Quality Evaluation
</h2>

Title: [Regressive Ensemble for Machine Translation Quality Evaluation](https://arxiv.org/abs/2109.07242)

Authors: [Michal Štefánik](https://arxiv.org/search/cs?searchtype=author&query=Štefánik%2C+M), [Vít Novotný](https://arxiv.org/search/cs?searchtype=author&query=Novotný%2C+V), [Petr Sojka](https://arxiv.org/search/cs?searchtype=author&query=Sojka%2C+P)

> This work introduces a simple regressive ensemble for evaluating machine translation quality based on a set of novel and established metrics. We evaluate the ensemble using a correlation to expert-based MQM scores of the WMT 2021 Metrics workshop. In both monolingual and zero-shot cross-lingual settings, we show a significant performance improvement over single metrics. In the cross-lingual settings, we also demonstrate that an ensemble approach is well-applicable to unseen languages. Furthermore, we identify a strong reference-free baseline that consistently outperforms the commonly-used BLEU and METEOR measures and significantly improves our ensemble's performance.

| Comments: | 8 pages incl. references, Proceedings of EMNLP 2021 Sixth Conference on Machine Translation (WMT 21) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.07242](https://arxiv.org/abs/2109.07242) [cs.CL]** |
|           | (or **[arXiv:2109.07242v1](https://arxiv.org/abs/2109.07242v1) [cs.CL]** for this version) |





<h2 id="2021-09-16-6">6. Sequence Length is a Domain: Length-based Overfitting in Transformer Models
</h2>

Title: [Sequence Length is a Domain: Length-based Overfitting in Transformer Models](https://arxiv.org/abs/2109.07276)

Authors: [Dušan Variš](https://arxiv.org/search/cs?searchtype=author&query=Variš%2C+D), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

> Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in machine translation, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017).
> We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing task and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.07276](https://arxiv.org/abs/2109.07276) [cs.CL]** |
|           | (or **[arXiv:2109.07276v1](https://arxiv.org/abs/2109.07276v1) [cs.CL]** for this version) |





<h2 id="2021-09-16-7">7. What Vision-Language Models `See' when they See Scenes
</h2>

Title: [What Vision-Language Models `See' when they See Scenes](https://arxiv.org/abs/2109.07301)

Authors: [Michele Cafagna](https://arxiv.org/search/cs?searchtype=author&query=Cafagna%2C+M), [Kees van Deemter](https://arxiv.org/search/cs?searchtype=author&query=van+Deemter%2C+K), [Albert Gatt](https://arxiv.org/search/cs?searchtype=author&query=Gatt%2C+A)

> Images can be described in terms of the objects they contain, or in terms of the types of scene or place that they instantiate. In this paper we address to what extent pretrained Vision and Language models can learn to align descriptions of both types with images. We compare 3 state-of-the-art models, VisualBERT, LXMERT and CLIP. We find that (i) V&L models are susceptible to stylistic biases acquired during pretraining; (ii) only CLIP performs consistently well on both object- and scene-level descriptions. A follow-up ablation study shows that CLIP uses object-level information in the visual modality to align with scene-level textual descriptions.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.07301](https://arxiv.org/abs/2109.07301) [cs.CL]** |
|           | (or **[arXiv:2109.07301v1](https://arxiv.org/abs/2109.07301v1) [cs.CL]** for this version) |





<h2 id="2021-09-16-8">8. Cross-lingual Transfer of Monolingual Models
</h2>

Title: [Cross-lingual Transfer of Monolingual Models](https://arxiv.org/abs/2109.07348)

Authors: [Evangelia Gogoulou](https://arxiv.org/search/cs?searchtype=author&query=Gogoulou%2C+E), [Ariel Ekgren](https://arxiv.org/search/cs?searchtype=author&query=Ekgren%2C+A), [Tim Isbister](https://arxiv.org/search/cs?searchtype=author&query=Isbister%2C+T), [Magnus Sahlgren](https://arxiv.org/search/cs?searchtype=author&query=Sahlgren%2C+M)

> Recent studies in zero-shot cross-lingual learning using multilingual models have falsified the previous hypothesis that shared vocabulary and joint pre-training are the keys to cross-lingual generalization. Inspired by this advancement, we introduce a cross-lingual transfer method for monolingual models based on domain adaptation. We study the effects of such transfer from four different languages to English. Our experimental results on GLUE show that the transferred models outperform the native English model independently of the source language. After probing the English linguistic knowledge encoded in the representations before and after transfer, we find that semantic information is retained from the source language, while syntactic information is learned during transfer. Additionally, the results of evaluating the transferred models in source language tasks reveal that their performance in the source domain deteriorates after transfer.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.07348](https://arxiv.org/abs/2109.07348) [cs.CL]** |
|           | (or **[arXiv:2109.07348v1](https://arxiv.org/abs/2109.07348v1) [cs.CL]** for this version) |







<h2 id="2021-09-16-9">9. UniST: Unified End-to-end Model for Streaming and Non-streaming Speech Translation
</h2>

Title: [UniST: Unified End-to-end Model for Streaming and Non-streaming Speech Translation](https://arxiv.org/abs/2109.07368)

Authors: [Qianqian Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Q), [Yaoming Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Y), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> This paper presents a unified end-to-end frame-work for both streaming and non-streamingspeech translation. While the training recipes for non-streaming speech translation have been mature, the recipes for streaming speechtranslation are yet to be built. In this work, wefocus on developing a unified model (UniST) which supports streaming and non-streaming ST from the perspective of fundamental components, including training objective, attention mechanism and decoding policy. Experiments on the most popular speech-to-text translation benchmark dataset, MuST-C, show that UniST achieves significant improvement for non-streaming ST, and a better-learned trade-off for BLEU score and latency metrics for streaming ST, compared with end-to-end baselines and the cascaded models. We will make our codes and evaluation tools publicly available.

| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.07368](https://arxiv.org/abs/2109.07368) [cs.CL]** |
|           | (or **[arXiv:2109.07368v1](https://arxiv.org/abs/2109.07368v1) [cs.CL]** for this version) |





<h2 id="2021-09-16-10">10. SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations
</h2>

Title: [SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations](https://arxiv.org/abs/2109.07424)

Authors: [Hooman Sedghamiz](https://arxiv.org/search/cs?searchtype=author&query=Sedghamiz%2C+H), [Shivam Raval](https://arxiv.org/search/cs?searchtype=author&query=Raval%2C+S), [Enrico Santus](https://arxiv.org/search/cs?searchtype=author&query=Santus%2C+E), [Tuka Alhanai](https://arxiv.org/search/cs?searchtype=author&query=Alhanai%2C+T), [Mohammad Ghassemi](https://arxiv.org/search/cs?searchtype=author&query=Ghassemi%2C+M)

> While contrastive learning is proven to be an effective training strategy in computer vision, Natural Language Processing (NLP) is only recently adopting it as a self-supervised alternative to Masked Language Modeling (MLM) for improving sequence representations. This paper introduces SupCL-Seq, which extends the supervised contrastive learning from computer vision to the optimization of sequence representations in NLP. By altering the dropout mask probability in standard Transformer architectures, for every representation (anchor), we generate augmented altered views. A supervised contrastive loss is then utilized to maximize the system's capability of pulling together similar samples (e.g., anchors and their altered views) and pushing apart the samples belonging to the other classes. Despite its simplicity, SupCLSeq leads to large gains in many sequence classification tasks on the GLUE benchmark compared to a standard BERTbase, including 6% absolute improvement on CoLA, 5.4% on MRPC, 4.7% on RTE and 2.6% on STSB. We also show consistent gains over self supervised contrastively learned representations, especially in non-semantic tasks. Finally we show that these gains are not solely due to augmentation, but rather to a downstream optimized sequence representation. Code: [this https URL](https://github.com/hooman650/SupCL-Seq)

| Comments: | short paper, EMNLP 2021, Findings                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.07424](https://arxiv.org/abs/2109.07424) [cs.CL]** |
|           | (or **[arXiv:2109.07424v1](https://arxiv.org/abs/2109.07424v1) [cs.CL]** for this version) |





<h2 id="2021-09-16-11">11. Is "moby dick" a Whale or a Bird? Named Entities and Terminology in Speech Translation
</h2>

Title: [Is "moby dick" a Whale or a Bird? Named Entities and Terminology in Speech Translation](https://arxiv.org/abs/2109.07439)

Authors: [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Susana Rodríguez](https://arxiv.org/search/cs?searchtype=author&query=Rodríguez%2C+S), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Luisa Bentivogli](https://arxiv.org/search/cs?searchtype=author&query=Bentivogli%2C+L), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Automatic translation systems are known to struggle with rare words. Among these, named entities (NEs) and domain-specific terms are crucial, since errors in their translation can lead to severe meaning distortions. Despite their importance, previous speech translation (ST) studies have neglected them, also due to the dearth of publicly available resources tailored to their specific evaluation. To fill this gap, we i) present the first systematic analysis of the behavior of state-of-the-art ST systems in translating NEs and terminology, and ii) release NEuRoparl-ST, a novel benchmark built from European Parliament speeches annotated with NEs and terminology. Our experiments on the three language directions covered by our benchmark (en->es/fr/it) show that ST systems correctly translate 75-80% of terms and 65-70% of NEs, with very low performance (37-40%) on person names.

| Comments: | Accepted at EMNLP2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.07439](https://arxiv.org/abs/2109.07439) [cs.CL]** |
|           | (or **[arXiv:2109.07439v1](https://arxiv.org/abs/2109.07439v1) [cs.CL]** for this version) |



<h2 id="2021-09-16-12">12. When Does Translation Require Context? A Data-driven, Multilingual Exploration
</h2>

Title: [When Does Translation Require Context? A Data-driven, Multilingual Exploration](https://arxiv.org/abs/2109.07446)

Authors: [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K), [Patrick Fernandes](https://arxiv.org/search/cs?searchtype=author&query=Fernandes%2C+P), [André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Although proper handling of discourse phenomena significantly contributes to the quality of machine translation (MT), common translation quality metrics do not adequately capture them. Recent works in context-aware MT attempt to target a small set of these phenomena during evaluation. In this paper, we propose a new metric, P-CXMI, which allows us to identify translations that require context systematically and confirm the difficulty of previously studied phenomena as well as uncover new ones that have not been addressed in previous work. We then develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers for these phenomena in 14 different language pairs, which we use to evaluate context-aware MT. We find that state-of-the-art context-aware MT models find marginal improvements over context-agnostic models on our benchmark, which suggests current models do not handle these ambiguities effectively. We release code and data to invite the MT research community to increase efforts on context-aware translation on discourse phenomena and languages that are currently overlooked.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.07446](https://arxiv.org/abs/2109.07446) [cs.CL]** |
|           | (or **[arXiv:2109.07446v1](https://arxiv.org/abs/2109.07446v1) [cs.CL]** for this version) |





<h2 id="2021-09-16-13">13. On the Limits of Minimal Pairs in Contrastive Evaluation
</h2>

Title: [On the Limits of Minimal Pairs in Contrastive Evaluation](https://arxiv.org/abs/2109.07465)

Authors: [Jannis Vamvas](https://arxiv.org/search/cs?searchtype=author&query=Vamvas%2C+J), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Minimal sentence pairs are frequently used to analyze the behavior of language models. It is often assumed that model behavior on contrastive pairs is predictive of model behavior at large. We argue that two conditions are necessary for this assumption to hold: First, a tested hypothesis should be well-motivated, since experiments show that contrastive evaluation can lead to false positives. Secondly, test data should be chosen such as to minimize distributional discrepancy between evaluation time and deployment time. For a good approximation of deployment-time decoding, we recommend that minimal pairs are created based on machine-generated text, as opposed to human-written references. We present a contrastive evaluation suite for English-German MT that implements this recommendation.

| Comments: | BlackboxNLP 2021                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.07465](https://arxiv.org/abs/2109.07465) [cs.CL]** |
|           | (or **[arXiv:2109.07465v1](https://arxiv.org/abs/2109.07465v1) [cs.CL]** for this version) |








# 2021-09-15

[Return to Index](#Index)



<h2 id="2021-09-15-1">1. Scalable Font Reconstruction with Dual Latent Manifolds
</h2>

Title: [Scalable Font Reconstruction with Dual Latent Manifolds](https://arxiv.org/abs/2109.06627)

Authors: [Nikita Srivatsan](https://arxiv.org/search/cs?searchtype=author&query=Srivatsan%2C+N), [Si Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Jonathan T. Barron](https://arxiv.org/search/cs?searchtype=author&query=Barron%2C+J+T), [Taylor Berg-Kirkpatrick](https://arxiv.org/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T)

> We propose a deep generative model that performs typography analysis and font reconstruction by learning disentangled manifolds of both font style and character shape. Our approach enables us to massively scale up the number of character types we can effectively model compared to previous methods. Specifically, we infer separate latent variables representing character and font via a pair of inference networks which take as input sets of glyphs that either all share a character type, or belong to the same font. This design allows our model to generalize to characters that were not observed during training time, an important task in light of the relative sparsity of most fonts. We also put forward a new loss, adapted from prior work that measures likelihood using an adaptive distribution in a projected space, resulting in more natural images without requiring a discriminator. We evaluate on the task of font reconstruction over various datasets representing character types of many languages, and compare favorably to modern style transfer systems according to both automatic and manually-evaluated metrics.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.06627](https://arxiv.org/abs/2109.06627) [cs.CV]** |
|           | (or **[arXiv:2109.06627v1](https://arxiv.org/abs/2109.06627v1) [cs.CV]** for this version) |





<h2 id="2021-09-15-2">2. Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation
</h2>

Title: [Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation](https://arxiv.org/abs/2109.06253)

Authors: [Ivan Provilkov](https://arxiv.org/search/cs?searchtype=author&query=Provilkov%2C+I), [Andrey Malinin](https://arxiv.org/search/cs?searchtype=author&query=Malinin%2C+A)

> Neural Machine Translation (NMT) is known to suffer from a beam-search problem: after a certain point, increasing beam size causes an overall drop in translation quality. This effect is especially pronounced for long sentences. While much work was done analyzing this phenomenon, primarily for autoregressive NMT models, there is still no consensus on its underlying cause. In this work, we analyze errors that cause major quality degradation with large beams in NMT and Automatic Speech Recognition (ASR). We show that a factor that strongly contributes to the quality degradation with large beams is \textit{dataset length-bias} - \textit{NMT datasets are strongly biased towards short sentences}. To mitigate this issue, we propose a new data augmentation technique -- \textit{Multi-Sentence Resampling (MSR)}. This technique extends the training examples by concatenating several sentences from the original dataset to make a long training example. We demonstrate that MSR significantly reduces degradation with growing beam size and improves final translation quality on the IWSTL15 En-Vi, IWSTL17 En-Fr, and WMT14 En-De datasets.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.06253](https://arxiv.org/abs/2109.06253) [cs.CL]** |
|           | (or **[arXiv:2109.06253v1](https://arxiv.org/abs/2109.06253v1) [cs.CL]** for this version) |





<h2 id="2021-09-15-3">3. Evaluating Multiway Multilingual NMT in the Turkic Languages
</h2>

Title: [Evaluating Multiway Multilingual NMT in the Turkic Languages](https://arxiv.org/abs/2109.06262)

Authors: [Jamshidbek Mirzakhalov](https://arxiv.org/search/cs?searchtype=author&query=Mirzakhalov%2C+J), [Anoop Babu](https://arxiv.org/search/cs?searchtype=author&query=Babu%2C+A), [Aigiz Kunafin](https://arxiv.org/search/cs?searchtype=author&query=Kunafin%2C+A), [Ahsan Wahab](https://arxiv.org/search/cs?searchtype=author&query=Wahab%2C+A), [Behzod Moydinboyev](https://arxiv.org/search/cs?searchtype=author&query=Moydinboyev%2C+B), [Sardana Ivanova](https://arxiv.org/search/cs?searchtype=author&query=Ivanova%2C+S), [Mokhiyakhon Uzokova](https://arxiv.org/search/cs?searchtype=author&query=Uzokova%2C+M), [Shaxnoza Pulatova](https://arxiv.org/search/cs?searchtype=author&query=Pulatova%2C+S), [Duygu Ataman](https://arxiv.org/search/cs?searchtype=author&query=Ataman%2C+D), [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J), [Francis Tyers](https://arxiv.org/search/cs?searchtype=author&query=Tyers%2C+F), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [John Licato](https://arxiv.org/search/cs?searchtype=author&query=Licato%2C+J), [Sriram Chellappan](https://arxiv.org/search/cs?searchtype=author&query=Chellappan%2C+S)

> Despite the increasing number of large and comprehensive machine translation (MT) systems, evaluation of these methods in various languages has been restrained by the lack of high-quality parallel corpora as well as engagement with the people that speak these languages. In this study, we present an evaluation of state-of-the-art approaches to training and evaluating MT systems in 22 languages from the Turkic language family, most of which being extremely under-explored. First, we adopt the TIL Corpus with a few key improvements to the training and the evaluation sets. Then, we train 26 bilingual baselines as well as a multi-way neural MT (MNMT) model using the corpus and perform an extensive analysis using automatic metrics as well as human evaluations. We find that the MNMT model outperforms almost all bilingual baselines in the out-of-domain test sets and finetuning the model on a downstream task of a single pair also results in a huge performance boost in both low- and high-resource scenarios. Our attentive analysis of evaluation criteria for MT models in Turkic languages also points to the necessity for further research in this direction. We release the corpus splits, test sets as well as models to the public.

| Comments: | 9 pages, 3 figures, 7 tables. To be presented at WMT 2021    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.06262](https://arxiv.org/abs/2109.06262) [cs.CL]** |
|           | (or **[arXiv:2109.06262v1](https://arxiv.org/abs/2109.06262v1) [cs.CL]** for this version) |







<h2 id="2021-09-15-4">4. Post-OCR Document Correction with large Ensembles of Character Sequence Models
</h2>

Title: [Post-OCR Document Correction with large Ensembles of Character Sequence Models](https://arxiv.org/abs/2109.06264)

Authors: [Juan Ramirez-Orta](https://arxiv.org/search/cs?searchtype=author&query=Ramirez-Orta%2C+J), [Eduardo Xamena](https://arxiv.org/search/cs?searchtype=author&query=Xamena%2C+E), [Ana Maguitman](https://arxiv.org/search/cs?searchtype=author&query=Maguitman%2C+A), [Evangelos Milios](https://arxiv.org/search/cs?searchtype=author&query=Milios%2C+E), [Axel J. Soto](https://arxiv.org/search/cs?searchtype=author&query=Soto%2C+A+J)

> In this paper, we propose a novel method based on character sequence-to-sequence models to correct documents already processed with Optical Character Recognition (OCR) systems. The main contribution of this paper is a set of strategies to accurately process strings much longer than the ones used to train the sequence model while being sample- and resource-efficient, supported by thorough experimentation. The strategy with the best performance involves splitting the input document in character n-grams and combining their individual corrections into the final output using a voting scheme that is equivalent to an ensemble of a large number of sequence models. We further investigate how to weigh the contributions from each one of the members of this ensemble. We test our method on nine languages of the ICDAR 2019 competition on post-OCR text correction and achieve a new state-of-the-art performance in five of them. Our code for post-OCR correction is shared at [this https URL](https://github.com/jarobyte91/post_ocr_correction).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.06264](https://arxiv.org/abs/2109.06264) [cs.CL]** |
|           | (or **[arXiv:2109.06264v1](https://arxiv.org/abs/2109.06264v1) [cs.CL]** for this version) |





<h2 id="2021-09-15-5">5. Mitigating Catastrophic Forgetting in Scheduled Sampling with Elastic Weight Consolidation in Neural Machine Translation
</h2>

Title: [Mitigating Catastrophic Forgetting in Scheduled Sampling with Elastic Weight Consolidation in Neural Machine Translation](https://arxiv.org/abs/2109.06308)

Authors: [Michalis Korakakis](https://arxiv.org/search/cs?searchtype=author&query=Korakakis%2C+M), [Andreas Vlachos](https://arxiv.org/search/cs?searchtype=author&query=Vlachos%2C+A)

> Despite strong performance in many sequence-to-sequence tasks, autoregressive models trained with maximum likelihood estimation suffer from exposure bias, i.e. a discrepancy between the ground-truth prefixes used during training and the model-generated prefixes used at inference time. Scheduled sampling is a simple and often empirically successful approach which addresses this issue by incorporating model-generated prefixes into the training process. However, it has been argued that it is an inconsistent training objective leading to models ignoring the prefixes altogether. In this paper, we conduct systematic experiments and find that it ameliorates exposure bias by increasing model reliance on the input sequence. We also observe that as a side-effect, it worsens performance when the model-generated prefix is correct, a form of catastrophic forgetting. We propose using Elastic Weight Consolidation as trade-off between mitigating exposure bias and retaining output quality. Experiments on two IWSLT'14 translation tasks demonstrate that our approach alleviates catastrophic forgetting and significantly improves BLEU compared to standard scheduled sampling.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.06308](https://arxiv.org/abs/2109.06308) [cs.CL]** |
|           | (or **[arXiv:2109.06308v1](https://arxiv.org/abs/2109.06308v1) [cs.CL]** for this version) |





<h2 id="2021-09-15-6">6. Uncertainty-Aware Machine Translation Evaluation
</h2>

Title: [Uncertainty-Aware Machine Translation Evaluation](https://arxiv.org/abs/2109.06352)

Authors: [Taisiya Glushkova](https://arxiv.org/search/cs?searchtype=author&query=Glushkova%2C+T), [Chrysoula Zerva](https://arxiv.org/search/cs?searchtype=author&query=Zerva%2C+C), [Ricardo Rei](https://arxiv.org/search/cs?searchtype=author&query=Rei%2C+R), [André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T)

> Several neural-based metrics have been recently proposed to evaluate machine translation quality. However, all of them resort to point estimates, which provide limited information at segment level. This is made worse as they are trained on noisy, biased and scarce human judgements, often resulting in unreliable quality predictions. In this paper, we introduce uncertainty-aware MT evaluation and analyze the trustworthiness of the predicted quality. We combine the COMET framework with two uncertainty estimation methods, Monte Carlo dropout and deep ensembles, to obtain quality scores along with confidence intervals. We compare the performance of our uncertainty-aware MT evaluation methods across multiple language pairs from the QT21 dataset and the WMT20 metrics task, augmented with MQM annotations. We experiment with varying numbers of references and further discuss the usefulness of uncertainty-aware quality estimation (without references) to flag possibly critical translation mistakes.

| Comments: | Accepted to Findings of EMNLP 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.06352](https://arxiv.org/abs/2109.06352) [cs.CL]** |
|           | (or **[arXiv:2109.06352v1](https://arxiv.org/abs/2109.06352v1) [cs.CL]** for this version) |





<h2 id="2021-09-15-7">7. AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate
</h2>

Title: [AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate](https://arxiv.org/abs/2109.06481)

Authors: [Jongyoon Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+J), [Sungwon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Sungroh Yoon](https://arxiv.org/search/cs?searchtype=author&query=Yoon%2C+S)

> Non-autoregressive neural machine translation (NART) models suffer from the multi-modality problem which causes translation inconsistency such as token repetition. Most recent approaches have attempted to solve this problem by implicitly modeling dependencies between outputs. In this paper, we introduce AligNART, which leverages full alignment information to explicitly reduce the modality of the target distribution. AligNART divides the machine translation task into (i) alignment estimation and (ii) translation with aligned decoder inputs, guiding the decoder to focus on simplified one-to-one translation. To alleviate the alignment estimation problem, we further propose a novel alignment decomposition method. Our experiments show that AligNART outperforms previous non-iterative NART models that focus on explicit modality reduction on WMT14 En↔De and WMT16 Ro→En. Furthermore, AligNART achieves BLEU scores comparable to those of the state-of-the-art connectionist temporal classification based models on WMT14 En↔De. We also observe that AligNART effectively addresses the token repetition problem even without sequence-level knowledge distillation.

| Comments: | Accepted by EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.06481](https://arxiv.org/abs/2109.06481) [cs.CL]** |
|           | (or **[arXiv:2109.06481v1](https://arxiv.org/abs/2109.06481v1) [cs.CL]** for this version) |





<h2 id="2021-09-15-8">8. Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation
</h2>

Title: [Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation](https://arxiv.org/abs/2109.06604)

Authors: [Xin Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Shujian Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Jun Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+J), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W), [Jiajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J)

> Recently, kNN-MT has shown the promising capability of directly incorporating the pre-trained neural machine translation (NMT) model with domain-specific token-level k-nearest-neighbor (kNN) retrieval to achieve domain adaptation without retraining. Despite being conceptually attractive, it heavily relies on high-quality in-domain parallel corpora, limiting its capability on unsupervised domain adaptation, where in-domain parallel corpora are scarce or nonexistent. In this paper, we propose a novel framework that directly uses in-domain monolingual sentences in the target language to construct an effective datastore for k-nearest-neighbor retrieval. To this end, we first introduce an autoencoder task based on the target language, and then insert lightweight adapters into the original NMT model to map the token-level representation of this task to the ideal representation of translation task. Experiments on multi-domain datasets demonstrate that our proposed approach significantly improves the translation accuracy with target-side monolingual data, while achieving comparable performance with back-translation.

| Comments: | Findings of EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.06604](https://arxiv.org/abs/2109.06604) [cs.CL]** |
|           | (or **[arXiv:2109.06604v1](https://arxiv.org/abs/2109.06604v1) [cs.CL]** for this version) |





<h2 id="2021-09-15-9">9. Efficient Inference for Multilingual Neural Machine Translation
</h2>

Title: [Efficient Inference for Multilingual Neural Machine Translation](https://arxiv.org/abs/2109.06679)

Authors: [Alexandre Berard](https://arxiv.org/search/cs?searchtype=author&query=Berard%2C+A), [Dain Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+D), [Stéphane Clinchant](https://arxiv.org/search/cs?searchtype=author&query=Clinchant%2C+S), [Kweonwoo Jung](https://arxiv.org/search/cs?searchtype=author&query=Jung%2C+K), [Vassilina Nikoulina](https://arxiv.org/search/cs?searchtype=author&query=Nikoulina%2C+V)

> Multilingual NMT has become an attractive solution for MT deployment in production. But to match bilingual quality, it comes at the cost of larger and slower models. In this work, we consider several ways to make multilingual NMT faster at inference without degrading its quality. We experiment with several "light decoder" architectures in two 20-language multi-parallel settings: small-scale on TED Talks and large-scale on ParaCrawl. Our experiments demonstrate that combining a shallow decoder with vocabulary filtering leads to more than twice faster inference with no loss in translation quality. We validate our findings with BLEU and chrF (on 380 language pairs), robustness evaluation and human evaluation.

| Comments: | Accepted as a long paper to EMNLP 2021                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.06679](https://arxiv.org/abs/2109.06679) [cs.CL]** |
|           | (or **[arXiv:2109.06679v1](https://arxiv.org/abs/2109.06679v1) [cs.CL]** for this version) |



<h2 id="2021-09-15-10">10. LM-Critic: Language Models for Unsupervised Grammatical Error Correction
</h2>

Title: [LM-Critic: Language Models for Unsupervised Grammatical Error Correction](https://arxiv.org/abs/2109.06822)

Authors: [Michihiro Yasunaga](https://arxiv.org/search/cs?searchtype=author&query=Yasunaga%2C+M), [Jure Leskovec](https://arxiv.org/search/cs?searchtype=author&query=Leskovec%2C+J), [Percy Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P)

> Training a model for grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs, but manually annotating such pairs can be expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program without any labeled examples, but this relies on a perfect critic (e.g., a compiler) that returns whether an example is valid or not, which does not exist for the GEC task. In this work, we show how to leverage a pretrained language model (LM) in defining an LM-Critic, which judges a sentence to be grammatical if the LM assigns it a higher probability than its local perturbations. We apply this LM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap realistic ungrammatical / grammatical pairs for training a corrector. We evaluate our approach on GEC datasets across multiple domains (CoNLL-2014, BEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing methods in both the unsupervised setting (+7.7 F0.5) and the supervised setting (+0.5 F0.5).

| Comments: | EMNLP 2021. Code & data available at [this https URL](https://github.com/michiyasunaga/LM-Critic) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.06822](https://arxiv.org/abs/2109.06822) [cs.CL]** |
|           | (or **[arXiv:2109.06822v1](https://arxiv.org/abs/2109.06822v1) [cs.CL]** for this version) |





# 2021-09-14

[Return to Index](#Index)



<h2 id="2021-09-14-1">1. MURAL: Multimodal, Multitask Retrieval Across Languages
</h2>

Title: [MURAL: Multimodal, Multitask Retrieval Across Languages](https://arxiv.org/abs/2109.05125)

Authors: [Aashi Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+A), [Mandy Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+M), [Krishna Srinivasan](https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+K), [Ting Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T), [Sneha Kudugunta](https://arxiv.org/search/cs?searchtype=author&query=Kudugunta%2C+S), [Chao Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+C), [Yinfei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Jason Baldridge](https://arxiv.org/search/cs?searchtype=author&query=Baldridge%2C+J)

> Both image-caption pairs and translation pairs provide the means to learn deep representations of and connections between languages. We use both types of pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual encoder that solves two tasks: 1) image-text matching and 2) translation pair matching. By incorporating billions of translation pairs, MURAL extends ALIGN (Jia et al. PMLR'21)--a state-of-the-art dual encoder learned from 1.8 billion noisy image-text pairs. When using the same encoders, MURAL's performance matches or exceeds ALIGN's cross-modal retrieval performance on well-resourced languages across several datasets. More importantly, it considerably improves performance on under-resourced languages, showing that text-text learning can overcome a paucity of image-caption examples for these languages. On the Wikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean recall by 8.1% on average for eight under-resourced languages and by 6.8% on average when fine-tuning. We additionally show that MURAL's text representations cluster not only with respect to genealogical connections but also based on areal linguistics, such as the Balkan Sprachbund.

| Subjects: | **Information Retrieval (cs.IR)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.05125](https://arxiv.org/abs/2109.05125) [cs.IR]** |
|           | (or **[arXiv:2109.05125v1](https://arxiv.org/abs/2109.05125v1) [cs.IR]** for this version) |





<h2 id="2021-09-14-2">2. GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks
</h2>

Title: [GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks](https://arxiv.org/abs/2109.05748)

Authors: [Weicheng Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+W), [Renze Lou](https://arxiv.org/search/cs?searchtype=author&query=Lou%2C+R), [Kai Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+K), [Lili Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Soroush Vosoughi](https://arxiv.org/search/cs?searchtype=author&query=Vosoughi%2C+S)

> A key problem in multi-task learning (MTL) research is how to select high-quality auxiliary tasks automatically. This paper presents GradTS, an automatic auxiliary task selection method based on gradient calculation in Transformer-based models. Compared to AUTOSEM, a strong baseline method, GradTS improves the performance of MT-DNN with a bert-base-cased backend model, from 0.33% to 17.93% on 8 natural language understanding (NLU) tasks in the GLUE benchmarks. GradTS is also time-saving since (1) its gradient calculations are based on single-task experiments and (2) the gradients are re-used without additional experiments when the candidate task set changes. On the 8 GLUE classification tasks, for example, GradTS costs on average 21.32% less time than AUTOSEM with comparable GPU consumption. Further, we show the robustness of GradTS across various task settings and model selections, e.g. mixed objectives among candidate tasks. The efficiency and efficacy of GradTS in these case studies illustrate its general applicability in MTL research without requiring manual task filtering or costly parameter tuning.

| Comments: | In EMNLP 2021                                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2109.05748](https://arxiv.org/abs/2109.05748) [cs.LG]** |
|           | (or **[arXiv:2109.05748v1](https://arxiv.org/abs/2109.05748v1) [cs.LG]** for this version) |





<h2 id="2021-09-14-3">3. Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy
</h2>

Title: [Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy](https://arxiv.org/abs/2109.05238)

Authors: [Shaolei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y)

> Simultaneous machine translation (SiMT) generates translation before reading the entire source sentence and hence it has to trade off between translation quality and latency. To fulfill the requirements of different translation quality and latency in practical applications, the previous methods usually need to train multiple SiMT models for different latency levels, resulting in large computational costs. In this paper, we propose a universal SiMT model with Mixture-of-Experts Wait-k Policy to achieve the best translation quality under arbitrary latency with only one trained model. Specifically, our method employs multi-head attention to accomplish the mixture of experts where each head is treated as a wait-k expert with its own waiting words number, and given a test latency and source inputs, the weights of the experts are accordingly adjusted to produce the best translation. Experiments on three datasets show that our method outperforms all the strong baselines under different latency, including the state-of-the-art adaptive policy.

| Comments: | Accepted by EMNLP2021. 12 pages, 7 figures, 4 tables         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.05238](https://arxiv.org/abs/2109.05238) [cs.CL]** |
|           | (or **[arXiv:2109.05238v1](https://arxiv.org/abs/2109.05238v1) [cs.CL]** for this version) |







<h2 id="2021-09-14-4">4. Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model
</h2>

Title: [Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model](https://arxiv.org/abs/2109.05244)

Authors: [Shaolei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y)

> Cross-attention is an important component of neural machine translation (NMT), which is always realized by dot-product attention in previous methods. However, dot-product attention only considers the pair-wise correlation between words, resulting in dispersion when dealing with long sentences and neglect of source neighboring relationships. Inspired by linguistics, the above issues are caused by ignoring a type of cross-attention, called concentrated attention, which focuses on several central words and then spreads around them. In this work, we apply Gaussian Mixture Model (GMM) to model the concentrated attention in cross-attention. Experiments and analyses we conducted on three datasets show that the proposed method outperforms the baseline and has significant improvement on alignment quality, N-gram accuracy, and long sentence translation.

| Comments: | Accepted by the Findings of EMNLP2021. 11 pages, 7 figures, 7 tables |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.05244](https://arxiv.org/abs/2109.05244) [cs.CL]** |
|           | (or **[arXiv:2109.05244v1](https://arxiv.org/abs/2109.05244v1) [cs.CL]** for this version) |





<h2 id="2021-09-14-5">5. Multilingual Translation via Grafting Pre-trained Language Models
</h2>

Title: [Multilingual Translation via Grafting Pre-trained Language Models](https://arxiv.org/abs/2109.05256)

Authors: [Zewei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Z), [Mingxuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> Can pre-trained BERT for one language and GPT for another be glued together to translate texts? Self-supervised training using only monolingual data has led to the success of pre-trained (masked) language models in many NLP tasks. However, directly connecting BERT as an encoder and GPT as a decoder can be challenging in machine translation, for GPT-like models lack a cross-attention component that is needed in seq2seq decoders. In this paper, we propose Graformer to graft separately pre-trained (masked) language models for machine translation. With monolingual data for pre-training and parallel data for grafting training, we maximally take advantage of the usage of both types of data. Experiments on 60 directions show that our method achieves average improvements of 5.8 BLEU in x2en and 2.9 BLEU in en2x directions comparing with the multilingual Transformer of the same size.

| Comments: | Accepted in EMNLP 2021 (Findings)                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.05256](https://arxiv.org/abs/2109.05256) [cs.CL]** |
|           | (or **[arXiv:2109.05256v1](https://arxiv.org/abs/2109.05256v1) [cs.CL]** for this version) |





<h2 id="2021-09-14-6">6. Pairwise Supervised Contrastive Learning of Sentence Representations
</h2>

Title: [Pairwise Supervised Contrastive Learning of Sentence Representations](https://arxiv.org/abs/2109.05424)

Authors: [Dejiao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Shang-Wen Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Wei Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+W), [Henghui Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+H), [Ramesh Nallapati](https://arxiv.org/search/cs?searchtype=author&query=Nallapati%2C+R), [Andrew O. Arnold](https://arxiv.org/search/cs?searchtype=author&query=Arnold%2C+A+O), [Bing Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+B)

> Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with triplet loss or siamese loss. Nevertheless, they share a common weakness: sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by the fact that the vanilla siamese or triplet losses only learn from individual sentence pairs or triplets, which often suffer from bad local optima. In this paper, we propose PairSupCon, an instance discrimination based approach aiming to bridge semantic entailment and contradiction understanding with high-level categorical concept encoding. We evaluate PairSupCon on various downstream tasks that involve understanding sentence semantics at different granularities. We outperform the previous state-of-the-art method with 10%--13% averaged improvement on eight clustering tasks, and 5%--6% averaged improvement on seven semantic textual similarity (STS) tasks.

| Comments: | 9 pages, EMNLP 2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.05424](https://arxiv.org/abs/2109.05424) [cs.CL]** |
|           | (or **[arXiv:2109.05424v1](https://arxiv.org/abs/2109.05424v1) [cs.CL]** for this version) |





<h2 id="2021-09-14-7">7. Contrastive Learning for Context-aware Neural Machine TranslationUsing Coreference Information
</h2>

Title: [Contrastive Learning for Context-aware Neural Machine TranslationUsing Coreference Information](https://arxiv.org/abs/2109.05712)

Authors: [Yongkeun Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+Y), [Hyungu Yun](https://arxiv.org/search/cs?searchtype=author&query=Yun%2C+H), [Kyomin Jung](https://arxiv.org/search/cs?searchtype=author&query=Jung%2C+K)

> Context-aware neural machine translation (NMT) incorporates contextual information of surrounding texts, that can improve the translation quality of document-level machine translation. Many existing works on context-aware NMT have focused on developing new model architectures for incorporating additional contexts and have shown some promising results. However, most existing works rely on cross-entropy loss, resulting in limited use of contextual information. In this paper, we propose CorefCL, a novel data augmentation and contrastive learning scheme based on coreference between the source and contextual sentences. By corrupting automatically detected coreference mentions in the contextual sentence, CorefCL can train the model to be sensitive to coreference inconsistency. We experimented with our method on common context-aware NMT models and two document-level translation tasks. In the experiments, our method consistently improved BLEU of compared models on English-German and English-Korean tasks. We also show that our method significantly improves coreference resolution in the English-German contrastive test suite.

| Comments: | WMT 2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.05712](https://arxiv.org/abs/2109.05712) [cs.CL]** |
|           | (or **[arXiv:2109.05712v1](https://arxiv.org/abs/2109.05712v1) [cs.CL]** for this version) |





<h2 id="2021-09-14-8">8. CPT: A Pre-Trained Unbalanced Transformerfor Both Chinese Language Understanding and Generation
</h2>

Title: [CPT: A Pre-Trained Unbalanced Transformerfor Both Chinese Language Understanding and Generation](https://arxiv.org/abs/2109.05729)

Authors: [Yunfan Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+Y), [Zhichao Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+Z), [Yitao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Junqi Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+J), [Fei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+F), [Li Zhe](https://arxiv.org/search/cs?searchtype=author&query=Zhe%2C+L), [Hujun Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+H), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X)

> In this paper, we take the advantage of previous pre-trained models (PTMs) and propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different from previous Chinese PTMs, CPT is designed for both natural language understanding (NLU) and natural language generation (NLG) tasks. CPT consists of three parts: a shared encoder, an understanding decoder, and a generation decoder. Two specific decoders with a shared encoder are pre-trained with masked language modeling (MLM) and denoising auto-encoding (DAE) tasks, respectively. With the partially shared architecture and multi-task pre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that fully exploits the potential of the model. Moreover, the unbalanced Transformer saves the computational and storage cost, which makes CPT competitive and greatly accelerates the inference of text generation. Experimental results on a wide range of Chinese NLU and NLG tasks show the effectiveness of CPT.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.05729](https://arxiv.org/abs/2109.05729) [cs.CL]** |
|           | (or **[arXiv:2109.05729v1](https://arxiv.org/abs/2109.05729v1) [cs.CL]** for this version) |





<h2 id="2021-09-14-9">9. Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions
</h2>

Title: [Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions](https://arxiv.org/abs/2109.05853)

Authors: [Javier Ferrando](https://arxiv.org/search/cs?searchtype=author&query=Ferrando%2C+J), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

> This work proposes an extensive analysis of the Transformer architecture in the Neural Machine Translation (NMT) setting. Focusing on the encoder-decoder attention mechanism, we prove that attention weights systematically make alignment errors by relying mainly on uninformative tokens from the source sequence. However, we observe that NMT models assign attention to these tokens to regulate the contribution in the prediction of the two contexts, the source and the prefix of the target sequence. We provide evidence about the influence of wrong alignments on the model behavior, demonstrating that the encoder-decoder attention mechanism is well suited as an interpretability method for NMT. Finally, based on our analysis, we propose methods that largely reduce the word alignment error rate compared to standard induced alignments from attention weights.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.05853](https://arxiv.org/abs/2109.05853) [cs.CL]** |
|           | (or **[arXiv:2109.05853v1](https://arxiv.org/abs/2109.05853v1) [cs.CL]** for this version) |





<h2 id="2021-09-14-10">10. Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning
</h2>

Title: [Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning](https://arxiv.org/abs/2109.05941)

Authors: [Seonghyeon Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+S), [Jiseon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J), [Alice Oh](https://arxiv.org/search/cs?searchtype=author&query=Oh%2C+A)

> We introduce EfficientCL, a memory-efficient continual pretraining method that applies contrastive learning with novel data augmentation and curriculum learning. For data augmentation, we stack two types of operation sequentially: cutoff and PCA jittering. While pretraining steps proceed, we apply curriculum learning by incrementing the augmentation degree for each difficulty step. After data augmentation is finished, contrastive learning is applied on projected embeddings of original and augmented examples. When finetuned on GLUE benchmark, our model outperforms baseline models, especially for sentence-level tasks. Additionally, this improvement is capable with only 70% of computational memory compared to the baseline model.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.05941](https://arxiv.org/abs/2109.05941) [cs.CL]** |
|           | (or **[arXiv:2109.05941v1](https://arxiv.org/abs/2109.05941v1) [cs.CL]** for this version) |





<h2 id="2021-09-14-11">11. Tamizhi-Net OCR: Creating A Quality Large Scale Tamil-Sinhala-English Parallel Corpus Using Deep Learning Based Printed Character Recognition (PCR)
</h2>

Title: [Tamizhi-Net OCR: Creating A Quality Large Scale Tamil-Sinhala-English Parallel Corpus Using Deep Learning Based Printed Character Recognition (PCR)](https://arxiv.org/abs/2109.05952)

Authors: [Charangan Vasantharajan](https://arxiv.org/search/cs?searchtype=author&query=Vasantharajan%2C+C), [Uthayasanker Thayasivam](https://arxiv.org/search/cs?searchtype=author&query=Thayasivam%2C+U)

> Most of the low resource languages do not have the necessary resources to create even a substantial monolingual corpus. These languages may often be found in government proceedings but mostly in the form of Portable Document Formats (PDFs) that contains legacy fonts. Extracting text from these documents to create a monolingual corpus is challenging due to legacy font usage and printer-friendly encoding which are not optimized for text extraction. Therefore, we propose a simple, automatic, and novel idea that can scale for Tamil, Sinhala, and English languages and many documents. For this purpose, we enhanced the performance of Tesseract 4.1.1 by employing LSTM-based training on many legacy fonts to recognize printed characters in the above languages. Especially, our model detects code-mix text, numbers, and special characters from the printed document. It is shown that this approach can boost the character-level accuracy of Tesseract 4.1.1 from 85.5 to 98.2 for Tamil (+12.9% relative change) and 91.8 to 94.8 for Sinhala (+3.26% relative change) on a dataset that is considered as challenging by its authors.

| Comments: | 7 Pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.05952](https://arxiv.org/abs/2109.05952) [cs.CL]** |
|           | (or **[arXiv:2109.05952v1](https://arxiv.org/abs/2109.05952v1) [cs.CL]** for this version) |



<h2 id="2021-09-14-12">12. xGQA: Cross-Lingual Visual Question Answering
</h2>

Title: [xGQA: Cross-Lingual Visual Question Answering](https://arxiv.org/abs/2109.06082)

Authors: [Jonas Pfeiffer](https://arxiv.org/search/cs?searchtype=author&query=Pfeiffer%2C+J), [Gregor Geigle](https://arxiv.org/search/cs?searchtype=author&query=Geigle%2C+G), [Aishwarya Kamath](https://arxiv.org/search/cs?searchtype=author&query=Kamath%2C+A), [Jan-Martin O. Steitz](https://arxiv.org/search/cs?searchtype=author&query=Steitz%2C+J+O), [Stefan Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+S), [Ivan Vulić](https://arxiv.org/search/cs?searchtype=author&query=Vulić%2C+I), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and -- vice versa -- multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling. The xGQA dataset is available online at: [this https URL](https://github.com/Adapter-Hub/xGQA).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.06082](https://arxiv.org/abs/2109.06082) [cs.CL]** |
|           | (or **[arXiv:2109.06082v1](https://arxiv.org/abs/2109.06082v1) [cs.CL]** for this version) |






# 2021-09-13

[Return to Index](#Index)



<h2 id="2021-09-13-1">1. LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation
</h2>

Title: [LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation](https://arxiv.org/abs/2109.04993)

Authors: [Mohammad Abuzar Shaikh](https://arxiv.org/search/cs?searchtype=author&query=Shaikh%2C+M+A), [Zhanghexuan Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+Z), [Dana Moukheiber](https://arxiv.org/search/cs?searchtype=author&query=Moukheiber%2C+D), [Sargur Srihari](https://arxiv.org/search/cs?searchtype=author&query=Srihari%2C+S), [Mingchen Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+M)

> Pre-training visual and textual representations from large-scale image-text pairs is becoming a standard approach for many downstream vision-language tasks. The transformer-based models learn inter and intra-modal attention through a list of self-supervised learning tasks. This paper proposes LAViTeR, a novel architecture for visual and textual representation learning. The main module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks, GAN-based image synthesis and Image Captioning. We also propose a new evaluation metric measuring the similarity between the learnt visual and textual embedding. The experimental results on two public datasets, CUB and MS-COCO, demonstrate superior visual and textual representation alignment in the joint feature embedding space

| Comments: | 14 pages, 10 Figures, 5 Tables                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2109.04993](https://arxiv.org/abs/2109.04993) [cs.CV]** |
|           | (or **[arXiv:2109.04993v1](https://arxiv.org/abs/2109.04993v1) [cs.CV]** for this version) |





<h2 id="2021-09-13-2">2. Speechformer: Reducing Information Loss in Direct Speech Translation
</h2>

Title: [Speechformer: Reducing Information Loss in Direct Speech Translation](https://arxiv.org/abs/2109.04574)

Authors: [Sara Papi](https://arxiv.org/search/cs?searchtype=author&query=Papi%2C+S), [Marco Gaido](https://arxiv.org/search/cs?searchtype=author&query=Gaido%2C+M), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including speech translation. However, Transformer's quadratic complexity with respect to the input sequence length prevents its adoption as is with audio signals, which are typically represented by long sequences. Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features. Therefore, potentially useful linguistic information is not accessible to higher-level layers in the architecture. To solve this issue, we propose Speechformer, an architecture that, thanks to reduced memory usage in the attention layers, avoids the initial lossy compression and aggregates information only at a higher level according to more informed linguistic criteria. Experiments on three language pairs (en->de/es/nl) show the efficacy of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and of up to 4.0 BLEU in a low resource scenario.

| Comments: | Accepted to EMNLP 2021 Main Conference                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04574](https://arxiv.org/abs/2109.04574) [cs.CL]** |
|           | (or **[arXiv:2109.04574v1](https://arxiv.org/abs/2109.04574v1) [cs.CL]** for this version) |







<h2 id="2021-09-13-3">3. BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation
</h2>

Title: [BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation](https://arxiv.org/abs/2109.04588)

Authors: [Haoran Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Benjamin Van Durme](https://arxiv.org/search/cs?searchtype=author&query=Van+Durme%2C+B), [Kenton Murray](https://arxiv.org/search/cs?searchtype=author&query=Murray%2C+K)

> The success of bidirectional encoders using masked language models, such as BERT, on numerous natural language processing tasks has prompted researchers to attempt to incorporate these pre-trained models into neural machine translation (NMT) systems. However, proposed methods for incorporating pre-trained models are non-trivial and mainly focus on BERT, which lacks a comparison of the impact that other pre-trained models may have on translation performance. In this paper, we demonstrate that simply using the output (contextualized embeddings) of a tailored and suitable bilingual pre-trained language model (dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art translation performance. Moreover, we also propose a stochastic layer selection approach and a concept of dual-directional translation model to ensure the sufficient utilization of contextualized embeddings. In the case of without using back translation, our best models achieve BLEU scores of 30.45 for En->De and 38.61 for De->En on the IWSLT'14 dataset, and 31.26 for En->De and 34.94 for De->En on the WMT'14 dataset, which exceeds all published numbers.

| Comments:          | EMNLP 2021                                                   |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | EMNLP 2021                                                   |
| Cite as:           | **[arXiv:2109.04588](https://arxiv.org/abs/2109.04588) [cs.CL]** |
|                    | (or **[arXiv:2109.04588v1](https://arxiv.org/abs/2109.04588v1) [cs.CL]** for this version) |







<h2 id="2021-09-13-4">4. A Large-Scale Study of Machine Translation in the Turkic Languages
</h2>

Title: [A Large-Scale Study of Machine Translation in the Turkic Languages](https://arxiv.org/abs/2109.04593)

Authors: [Jamshidbek Mirzakhalov](https://arxiv.org/search/cs?searchtype=author&query=Mirzakhalov%2C+J), [Anoop Babu](https://arxiv.org/search/cs?searchtype=author&query=Babu%2C+A), [Duygu Ataman](https://arxiv.org/search/cs?searchtype=author&query=Ataman%2C+D), [Sherzod Kariev](https://arxiv.org/search/cs?searchtype=author&query=Kariev%2C+S), [Francis Tyers](https://arxiv.org/search/cs?searchtype=author&query=Tyers%2C+F), [Otabek Abduraufov](https://arxiv.org/search/cs?searchtype=author&query=Abduraufov%2C+O), [Mammad Hajili](https://arxiv.org/search/cs?searchtype=author&query=Hajili%2C+M), [Sardana Ivanova](https://arxiv.org/search/cs?searchtype=author&query=Ivanova%2C+S), [Abror Khaytbaev](https://arxiv.org/search/cs?searchtype=author&query=Khaytbaev%2C+A), [Antonio Laverghetta Jr.](https://arxiv.org/search/cs?searchtype=author&query=Laverghetta%2C+A), [Behzodbek Moydinboyev](https://arxiv.org/search/cs?searchtype=author&query=Moydinboyev%2C+B), [Esra Onal](https://arxiv.org/search/cs?searchtype=author&query=Onal%2C+E), [Shaxnoza Pulatova](https://arxiv.org/search/cs?searchtype=author&query=Pulatova%2C+S), [Ahsan Wahab](https://arxiv.org/search/cs?searchtype=author&query=Wahab%2C+A), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Sriram Chellappan](https://arxiv.org/search/cs?searchtype=author&query=Chellappan%2C+S)

> Recent advances in neural machine translation (NMT) have pushed the quality of machine translation systems to the point where they are becoming widely adopted to build competitive systems. However, there is still a large number of languages that are yet to reap the benefits of NMT. In this paper, we provide the first large-scale case study of the practical application of MT in the Turkic language family in order to realize the gains of NMT for Turkic languages under high-resource to extremely low-resource scenarios. In addition to presenting an extensive analysis that identifies the bottlenecks towards building competitive systems to ameliorate data scarcity, our study has several key contributions, including, i) a large parallel corpus covering 22 Turkic languages consisting of common public datasets in combination with new datasets of approximately 2 million parallel sentences, ii) bilingual baselines for 26 language pairs, iii) novel high-quality test sets in three different translation domains and iv) human evaluation scores. All models, scripts, and data will be released to the public.

| Comments: | 9 pages, 1 figure, 8 tables. Main proceedings of EMNLP 2021  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.04593](https://arxiv.org/abs/2109.04593) [cs.CL]** |
|           | (or **[arXiv:2109.04593v1](https://arxiv.org/abs/2109.04593v1) [cs.CL]** for this version) |









<h2 id="2021-09-13-5">5. Rule-based Morphological Inflection Improves Neural Terminology Translation
</h2>

Title: [Rule-based Morphological Inflection Improves Neural Terminology Translation](https://arxiv.org/abs/2109.04620)

Authors: [Weijia Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Marine Carpuat](https://arxiv.org/search/cs?searchtype=author&query=Carpuat%2C+M)

> Current approaches to incorporating terminology constraints in machine translation (MT) typically assume that the constraint terms are provided in their correct morphological forms. This limits their application to real-world scenarios where constraint terms are provided as lemmas. In this paper, we introduce a modular framework for incorporating lemma constraints in neural MT (NMT) in which linguistic knowledge and diverse types of NMT models can be flexibly applied. It is based on a novel cross-lingual inflection module that inflects the target lemma constraints based on the source context. We explore linguistically motivated rule-based and data-driven neural-based inflection modules and design English-German health and English-Lithuanian news test suites to evaluate them in domain adaptation and low-resource MT settings. Results show that our rule-based inflection module helps NMT models incorporate lemma constraints more accurately than a neural module and outperforms the existing end-to-end approach with lower training costs.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04620](https://arxiv.org/abs/2109.04620) [cs.CL]** |
|           | (or **[arXiv:2109.04620v1](https://arxiv.org/abs/2109.04620v1) [cs.CL]** for this version) |







<h2 id="2021-09-13-6">6. EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling
</h2>

Title: [EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling](https://arxiv.org/abs/2109.04699)

Authors: [Jue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Haofan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Jincan Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+J), [Weijia Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+W), [Debing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D)

> While large scale pre-training has achieved great achievements in bridging the gap between vision and language, it still faces several challenges. First, the cost for pre-training is expensive. Second, there is no efficient way to handle the data noise which degrades model performance. Third, previous methods only leverage limited image-text paired data, while ignoring richer single-modal data, which may result in poor generalization to single-modal downstream tasks. In this work, we propose an EfficientCLIP method via Ensemble Confident Learning to obtain a less noisy data subset. Extra rich non-paired single-modal text data is used for boosting the generalization of text branch. We achieve the state-of-the-art performance on Chinese cross-modal retrieval tasks with only 1/10 training resources compared to CLIP and WenLan, while showing excellent generalization to single-modal tasks, including text retrieval and text classification.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.04699](https://arxiv.org/abs/2109.04699) [cs.CL]** |
|           | (or **[arXiv:2109.04699v1](https://arxiv.org/abs/2109.04699v1) [cs.CL]** for this version) |







<h2 id="2021-09-13-7">7. Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables
</h2>

Title: [Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables](https://arxiv.org/abs/2109.04705)

Authors: [Weizhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Yichao Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Jun Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+J), [Weihua Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W)

> Zero-shot translation, directly translating between language pairs unseen in training, is a promising capability of multilingual neural machine translation (NMT). However, it usually suffers from capturing spurious correlations between the output language and language invariant semantics due to the maximum likelihood training objective, leading to poor transfer performance on zero-shot translation. In this paper, we introduce a denoising autoencoder objective based on pivot language into traditional training objective to improve the translation accuracy on zero-shot directions. The theoretical analysis from the perspective of latent variables shows that our approach actually implicitly maximizes the probability distributions for zero-shot directions. On two benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively eliminate the spurious correlations and significantly outperforms state-of-the-art methods with a remarkable performance. Our code is available at [this https URL](https://github.com/Victorwz/zs-nmt-dae).

| Comments: | EMNLP Findings 2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04705](https://arxiv.org/abs/2109.04705) [cs.CL]** |
|           | (or **[arXiv:2109.04705v1](https://arxiv.org/abs/2109.04705v1) [cs.CL]** for this version) |







<h2 id="2021-09-13-8">8. Pre-train or Annotate? Domain Adaptation with a Constrained Budget
</h2>

Title: [Pre-train or Annotate? Domain Adaptation with a Constrained Budget](https://arxiv.org/abs/2109.04711)

Authors: [Fan Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+F), [Alan Ritter](https://arxiv.org/search/cs?searchtype=author&query=Ritter%2C+A), [Wei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W)

> Recent work has demonstrated that pre-training in-domain language models can boost performance when adapting to a new domain. However, the costs associated with pre-training raise an important question: given a fixed budget, what steps should an NLP practitioner take to maximize performance? In this paper, we study domain adaptation under budget constraints, and approach it as a customer choice problem between data annotation and pre-training. Specifically, we measure the annotation cost of three procedural text datasets and the pre-training cost of three in-domain language models. Then we evaluate the utility of different combinations of pre-training and data annotation under varying budget constraints to assess which combination strategy works best. We find that, for small budgets, spending all funds on annotation leads to the best performance; once the budget becomes large enough, a combination of data annotation and in-domain pre-training works more optimally. We therefore suggest that task-specific data annotation should be part of an economical strategy when adapting an NLP model to a new domain.

| Comments: | Accepted to EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04711](https://arxiv.org/abs/2109.04711) [cs.CL]** |
|           | (or **[arXiv:2109.04711v1](https://arxiv.org/abs/2109.04711v1) [cs.CL]** for this version) |







<h2 id="2021-09-13-9">9. AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages
</h2>

Title: [AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages](https://arxiv.org/abs/2109.04715)

Authors: [Machel Reid](https://arxiv.org/search/cs?searchtype=author&query=Reid%2C+M), [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Yutaka Matsuo](https://arxiv.org/search/cs?searchtype=author&query=Matsuo%2C+Y)

> Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AfroMT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account the unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong baselines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for African languages.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04715](https://arxiv.org/abs/2109.04715) [cs.CL]** |
|           | (or **[arXiv:2109.04715v1](https://arxiv.org/abs/2109.04715v1) [cs.CL]** for this version) |



<h2 id="2021-09-13-10">10. A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations
</h2>

Title: [A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations](https://arxiv.org/abs/2109.04727)

Authors: [Ziyi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Yinfei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Daniel Cer](https://arxiv.org/search/cs?searchtype=author&query=Cer%2C+D), [Eric Darve](https://arxiv.org/search/cs?searchtype=author&query=Darve%2C+E)

> Language agnostic and semantic-language information isolation is an emerging research direction for multilingual representations models. We explore this problem from a novel angle of geometric algebra and semantic space. A simple but highly effective method "Language Information Removal (LIR)" factors out language identity information from semantic related components in multilingual representations pre-trained on multi-monolingual data. A post-training and model-agnostic method, LIR only uses simple linear operations, e.g. matrix factorization and orthogonal projection. LIR reveals that for weak-alignment multilingual systems, the principal components of semantic spaces primarily encodes language identity information. We first evaluate the LIR on a cross-lingual question answer retrieval task (LAReQA), which requires the strong alignment for the multilingual embedding space. Experiment shows that LIR is highly effectively on this task, yielding almost 100% relative improvement in MAP for weak-alignment models. We then evaluate the LIR on Amazon Reviews and XEVAL dataset, with the observation that removing language information is able to improve the cross-lingual transfer performance.

| Comments: | Accepted to the 2021 Conference on Empirical Methods in Natural Language Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.04727](https://arxiv.org/abs/2109.04727) [cs.CL]** |
|           | (or **[arXiv:2109.04727v1](https://arxiv.org/abs/2109.04727v1) [cs.CL]** for this version) |







<h2 id="2021-09-13-11">11. Improving Multilingual Translation by Representation and Gradient Regularization
</h2>

Title: [Improving Multilingual Translation by Representation and Gradient Regularization](https://arxiv.org/abs/2109.04778)

Authors: [Yilin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Akiko Eriguchi](https://arxiv.org/search/cs?searchtype=author&query=Eriguchi%2C+A), [Alexandre Muzio](https://arxiv.org/search/cs?searchtype=author&query=Muzio%2C+A), [Prasad Tadepalli](https://arxiv.org/search/cs?searchtype=author&query=Tadepalli%2C+P), [Stefan Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S), [Hany Hassan](https://arxiv.org/search/cs?searchtype=author&query=Hassan%2C+H)

> Multilingual Neural Machine Translation (NMT) enables one model to serve all translation directions, including ones that are unseen during training, i.e. zero-shot translation. Despite being theoretically attractive, current models often produce low quality translations -- commonly failing to even produce outputs in the right target language. In this work, we observe that off-target translation is dominant even in strong multilingual systems, trained on massive multilingual corpora. To address this issue, we propose a joint approach to regularize NMT models at both representation-level and gradient-level. At the representation level, we leverage an auxiliary target language prediction task to regularize decoder outputs to retain information about the target language. At the gradient level, we leverage a small amount of direct data (in thousands of sentence pairs) to regularize model gradients. Our results demonstrate that our approach is highly effective in both reducing off-target translation occurrences and improving zero-shot translation performance by +5.59 and +10.38 BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our method also works well when the small amount of direct data is not available.

| Comments: | EMNLP 2021 (Long)                                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.04778](https://arxiv.org/abs/2109.04778) [cs.CL]** |
|           | (or **[arXiv:2109.04778v1](https://arxiv.org/abs/2109.04778v1) [cs.CL]** for this version) |







<h2 id="2021-09-13-12">12. Artificial Text Detection via Examining the Topology of Attention Maps
</h2>

Title: [Artificial Text Detection via Examining the Topology of Attention Maps](https://arxiv.org/abs/2109.04825)

Authors: [Laida Kushnareva](https://arxiv.org/search/cs?searchtype=author&query=Kushnareva%2C+L), [Daniil Cherniavskii](https://arxiv.org/search/cs?searchtype=author&query=Cherniavskii%2C+D), [Vladislav Mikhailov](https://arxiv.org/search/cs?searchtype=author&query=Mikhailov%2C+V), [Ekaterina Artemova](https://arxiv.org/search/cs?searchtype=author&query=Artemova%2C+E), [Serguei Barannikov](https://arxiv.org/search/cs?searchtype=author&query=Barannikov%2C+S), [Alexander Bernstein](https://arxiv.org/search/cs?searchtype=author&query=Bernstein%2C+A), [Irina Piontkovskaya](https://arxiv.org/search/cs?searchtype=author&query=Piontkovskaya%2C+I), [Dmitri Piontkovski](https://arxiv.org/search/cs?searchtype=author&query=Piontkovski%2C+D), [Evgeny Burnaev](https://arxiv.org/search/cs?searchtype=author&query=Burnaev%2C+E)

> The impressive capabilities of recent generative models to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text detection, they still lack interpretability and robustness towards unseen models. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data Analysis (TDA) which is currently understudied in the field of NLP. We empirically show that the features derived from the BERT model outperform count- and neural-based baselines up to 10\% on three common datasets, and tend to be the most robust towards unseen GPT-style generation models as opposed to existing methods. The probing analysis of the features reveals their sensitivity to the surface and syntactic properties. The results demonstrate that TDA is a promising line with respect to NLP tasks, specifically the ones that incorporate surface and structural information.

| Comments: | Accepted to EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.04825](https://arxiv.org/abs/2109.04825) [cs.CL]** |
|           | (or **[arXiv:2109.04825v1](https://arxiv.org/abs/2109.04825v1) [cs.CL]** for this version) |







<h2 id="2021-09-13-13">13. Box Embeddings: An open-source library for representation learning using geometric structures
</h2>

Title: [Box Embeddings: An open-source library for representation learning using geometric structures](https://arxiv.org/abs/2109.04997)

Authors: [Tejas Chheda](https://arxiv.org/search/cs?searchtype=author&query=Chheda%2C+T), [Purujit Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+P), [Trang Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+T), [Dhruvesh Patel](https://arxiv.org/search/cs?searchtype=author&query=Patel%2C+D), [Michael Boratko](https://arxiv.org/search/cs?searchtype=author&query=Boratko%2C+M), [Shib Sankar Dasgupta](https://arxiv.org/search/cs?searchtype=author&query=Dasgupta%2C+S+S), [Andrew McCallum](https://arxiv.org/search/cs?searchtype=author&query=McCallum%2C+A)

> A major factor contributing to the success of modern representation learning is the ease of performing various vector operations. Recently, objects with geometric structures (eg. distributions, complex or hyperbolic vectors, or regions such as cones, disks, or boxes) have been explored for their alternative inductive biases and additional representational capacities. In this work, we introduce Box Embeddings, a Python library that enables researchers to easily apply and extend probabilistic box embeddings.

| Comments: | The source code and the usage and API documentation for the library is available at [this https URL](https://github.com/iesl/box-embeddings) and [this https URL](https://www.iesl.cs.umass.edu/box-embeddings/main/index.html) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.04997](https://arxiv.org/abs/2109.04997) [cs.CL]** |
|           | (or **[arXiv:2109.04997v1](https://arxiv.org/abs/2109.04997v1) [cs.CL]** for this version) |







<h2 id="2021-09-13-14">14. BiSECT: Learning to Split and Rephrase Sentences with Bitexts
</h2>

Title: [BiSECT: Learning to Split and Rephrase Sentences with Bitexts](https://arxiv.org/abs/2109.05006)

Authors: [Joongwon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J), [Mounica Maddela](https://arxiv.org/search/cs?searchtype=author&query=Maddela%2C+M), [Reno Kriz](https://arxiv.org/search/cs?searchtype=author&query=Kriz%2C+R), [Wei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Chris Callison-Burch](https://arxiv.org/search/cs?searchtype=author&query=Callison-Burch%2C+C)

> An important task in NLP applications such as sentence simplification is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel dataset and a new model for this `split and rephrase' task. Our BiSECT training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using machine translation to convert both sides of the corpus into the same language. BiSECT contains higher quality training examples than previous Split and Rephrase corpora, with sentence splits that require more significant modifications. We categorize examples in our corpus, and use these categories in a novel model that allows us to target specific regions of the input sentence to be split and edited. Moreover, we show that models trained on BiSECT can perform a wider variety of split operations and improve upon previous state-of-the-art approaches in automatic and human evaluations.

| Comments: | 9 pages, 9 figures. Long paper to appear in Empirical Methods in Natural Language Processing 2021 (EMNLP 2021) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.05006](https://arxiv.org/abs/2109.05006) [cs.CL]** |
|           | (or **[arXiv:2109.05006v1](https://arxiv.org/abs/2109.05006v1) [cs.CL]** for this version) |





<h2 id="2021-09-13-15">15. Neural Machine Translation Quality and Post-Editing Performance
</h2>

Title: [Neural Machine Translation Quality and Post-Editing Performance](https://arxiv.org/abs/2109.05016)

Authors: [Vilém Zouhar](https://arxiv.org/search/cs?searchtype=author&query=Zouhar%2C+V), [Aleš Tamchyna](https://arxiv.org/search/cs?searchtype=author&query=Tamchyna%2C+A), [Martin Popel](https://arxiv.org/search/cs?searchtype=author&query=Popel%2C+M), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

> We test the natural expectation that using MT in professional translation saves human processing time. The last such study was carried out by Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the translation quality. In contrast, we focus on neural MT (NMT) of high quality, which has become the state-of-the-art approach since then and also got adopted by most translation companies.
> Through an experimental study involving over 30 professional translators for English -> Czech translation, we examine the relationship between NMT performance and post-editing time and quality. Across all models, we found that better MT systems indeed lead to fewer changes in the sentences in this industry setting. The relation between system quality and post-editing time is however not straightforward and, contrary to the results on phrase-based MT, BLEU is definitely not a stable predictor of the time or final output quality.

| Comments: | 9 pages, 1 page appendix. To be presented at EMNLP2021       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2109.05016](https://arxiv.org/abs/2109.05016) [cs.CL]** |
|           | (or **[arXiv:2109.05016v1](https://arxiv.org/abs/2109.05016v1) [cs.CL]** for this version) |










# 2021-09-10

[Return to Index](#Index)



<h2 id="2021-09-10-1">1. Non-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring
</h2>

Title: [Non-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring](https://arxiv.org/abs/2109.04411)

Authors: [Hirofumi Inaguma](https://arxiv.org/search/eess?searchtype=author&query=Inaguma%2C+H), [Yosuke Higuchi](https://arxiv.org/search/eess?searchtype=author&query=Higuchi%2C+Y), [Kevin Duh](https://arxiv.org/search/eess?searchtype=author&query=Duh%2C+K), [Tatsuya Kawahara](https://arxiv.org/search/eess?searchtype=author&query=Kawahara%2C+T), [Shinji Watanabe](https://arxiv.org/search/eess?searchtype=author&query=Watanabe%2C+S)

> This article describes an efficient end-to-end speech translation (E2E-ST) framework based on non-autoregressive (NAR) models. End-to-end speech translation models have several advantages over traditional cascade systems such as inference latency reduction. However, conventional AR decoding methods are not fast enough because each token is generated incrementally. NAR models, however, can accelerate the decoding speed by generating multiple tokens in parallel on the basis of the token-wise conditional independence assumption. We propose a unified NAR E2E-ST framework called Orthros, which has an NAR decoder and an auxiliary shallow AR decoder on top of the shared encoder. The auxiliary shallow AR decoder selects the best hypothesis by rescoring multiple candidates generated from the NAR decoder in parallel (parallel AR rescoring). We adopt conditional masked language model (CMLM) and a connectionist temporal classification (CTC)-based model as NAR decoders for Orthros, referred to as Orthros-CMLM and Orthros-CTC, respectively. We also propose two training methods to enhance the CMLM decoder. Experimental evaluations on three benchmark datasets with six language directions demonstrated that Orthros achieved large improvements in translation quality with a very small overhead compared with the baseline NAR model. Moreover, the Conformer encoder architecture enabled large quality improvements, especially for CTC-based models. Orthros-CTC with the Conformer encoder increased decoding speed by 3.63x on CPU with translation quality comparable to that of an AR model.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.04411](https://arxiv.org/abs/2109.04411) [eess.AS]** |
|           | (or **[arXiv:2109.04411v1](https://arxiv.org/abs/2109.04411v1) [eess.AS]** for this version) |





<h2 id="2021-09-10-2">2. TxT: Crossmodal End-to-End Learning with Transformers
</h2>

Title: [TxT: Crossmodal End-to-End Learning with Transformers](https://arxiv.org/abs/2109.04422)

Authors: [Jan-Martin O. Steitz](https://arxiv.org/search/cs?searchtype=author&query=Steitz%2C+J+O), [Jonas Pfeiffer](https://arxiv.org/search/cs?searchtype=author&query=Pfeiffer%2C+J), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I), [Stefan Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+S)

> Reasoning over multiple modalities, e.g. in Visual Question Answering (VQA), requires an alignment of semantic concepts across domains. Despite the widespread success of end-to-end learning, today's multimodal pipelines by and large leverage pre-extracted, fixed features from object detectors, typically Faster R-CNN, as representations of the visual world. The obvious downside is that the visual representation is not specifically tuned to the multimodal task at hand. At the same time, while transformer-based object detectors have gained popularity, they have not been employed in today's multimodal pipelines. We address both shortcomings with TxT, a transformer-based crossmodal pipeline that enables fine-tuning both language and visual components on the downstream task in a fully end-to-end manner. We overcome existing limitations of transformer-based detectors for multimodal reasoning regarding the integration of global context and their scalability. Our transformer-based multimodal model achieves considerable gains from end-to-end learning for multimodal question answering.

| Comments: | To appear at the 43rd DAGM German Conference on Pattern Recognition (GCPR) 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2109.04422](https://arxiv.org/abs/2109.04422) [cs.CV]** |
|           | (or **[arXiv:2109.04422v1](https://arxiv.org/abs/2109.04422v1) [cs.CV]** for this version) |





<h2 id="2021-09-10-3">3. Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation
</h2>

Title: [Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation](https://arxiv.org/abs/2109.03858)

Authors: [Shahar Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+S), [Koren Lazar](https://arxiv.org/search/cs?searchtype=author&query=Lazar%2C+K), [abriel Stanovsky](https://arxiv.org/search/cs?searchtype=author&query=Stanovsky%2C+a)

> Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains, resulting in a first large-scale gender bias dataset of 108K diverse real-world English sentences. We manually verify the quality of our corpus and use it to evaluate gender bias in various coreference resolution and machine translation models. We find that all tested models tend to over-rely on gender stereotypes when presented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at [this http URL](http://www.github.com/SLAB-NLP/BUG). We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings.

| Comments: | Accepted to Findings of EMNLP 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03858](https://arxiv.org/abs/2109.03858) [cs.CL]** |
|           | (or **[arXiv:2109.03858v1](https://arxiv.org/abs/2109.03858v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-4">4. Ensemble Fine-tuned mBERT for Translation Quality Estimation
</h2>

Title: [Ensemble Fine-tuned mBERT for Translation Quality Estimation](https://arxiv.org/abs/2109.03914)

Authors: [Shaika Chowdhury](https://arxiv.org/search/cs?searchtype=author&query=Chowdhury%2C+S), [Naouel Baili](https://arxiv.org/search/cs?searchtype=author&query=Baili%2C+N), [Brian Vannah](https://arxiv.org/search/cs?searchtype=author&query=Vannah%2C+B)

> Quality Estimation (QE) is an important component of the machine translation workflow as it assesses the quality of the translated output without consulting reference translations. In this paper, we discuss our submission to the WMT 2021 QE Shared Task. We participate in Task 2 sentence-level sub-task that challenge participants to predict the HTER score for sentence-level post-editing effort. Our proposed system is an ensemble of multilingual BERT (mBERT)-based regression models, which are generated by fine-tuning on different input settings. It demonstrates comparable performance with respect to the Pearson's correlation and beats the baseline system in MAE/ RMSE for several language pairs. In addition, we adapt our system for the zero-shot setting by exploiting target language-relevant language pairs and pseudo-reference translations.

| Comments: | The Sixth Conference on Machine Translation, WMT 2021        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03914](https://arxiv.org/abs/2109.03914) [cs.CL]** |
|           | (or **[arXiv:2109.03914v1](https://arxiv.org/abs/2109.03914v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-5">5. Competence-based Curriculum Learning for Multilingual Machine Translation
</h2>

Title: [Competence-based Curriculum Learning for Multilingual Machine Translation](https://arxiv.org/abs/2109.04002)

Authors: [Mingliang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yunhai Tong](https://arxiv.org/search/cs?searchtype=author&query=Tong%2C+Y), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Currently, multilingual machine translation is receiving more and more attention since it brings better performance for low resource languages (LRLs) and saves more space. However, existing multilingual machine translation models face a severe challenge: imbalance. As a result, the translation performance of different languages in multilingual translation models are quite different. We argue that this imbalance problem stems from the different learning competencies of different languages. Therefore, we focus on balancing the learning competencies of different languages and propose Competence-based Curriculum Learning for Multilingual Machine Translation, named CCL-M. Specifically, we firstly define two competencies to help schedule the high resource languages (HRLs) and the low resource languages: 1) Self-evaluated Competence, evaluating how well the language itself has been learned; and 2) HRLs-evaluated Competence, evaluating whether an LRL is ready to be learned according to HRLs' Self-evaluated Competence. Based on the above competencies, we utilize the proposed CCL-M algorithm to gradually add new languages into the training set in a curriculum learning manner. Furthermore, we propose a novel competenceaware dynamic balancing sampling strategy for better selecting training samples in multilingual training. Experimental results show that our approach has achieved a steady and significant performance gain compared to the previous state-of-the-art approach on the TED talks dataset.

| Comments: | Accepted by Findings of EMNLP 2021. We release the codes at [this https URL](https://github.com/zml24/ccl-m) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04002](https://arxiv.org/abs/2109.04002) [cs.CL]** |
|           | (or **[arXiv:2109.04002v1](https://arxiv.org/abs/2109.04002v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-6">6. Distributionally Robust Multilingual Machine Translation
</h2>

Title: [Distributionally Robust Multilingual Machine Translation](https://arxiv.org/abs/2109.04020)

Authors: [Chunting Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Daniel Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+D), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Multilingual neural machine translation (MNMT) learns to translate multiple language pairs with a single model, potentially improving both the accuracy and the memory-efficiency of deployed models. However, the heavy data imbalance between languages hinders the model from performing uniformly across language pairs. In this paper, we propose a new learning objective for MNMT based on distributionally robust optimization, which minimizes the worst-case expected loss over the set of language pairs. We further show how to practically optimize this objective for large translation corpora using an iterated best response scheme, which is both effective and incurs negligible additional computational cost compared to standard empirical risk minimization. We perform extensive experiments on three sets of languages from two datasets and show that our method consistently outperforms strong baseline methods in terms of average and per-language performance under both many-to-one and one-to-many translation settings.

| Comments: | Long paper accepted by EMNLP2021 main conference             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.04020](https://arxiv.org/abs/2109.04020) [cs.CL]** |
|           | (or **[arXiv:2109.04020v1](https://arxiv.org/abs/2109.04020v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-7">7. Efficient Nearest Neighbor Language Models
</h2>

Title: [Efficient Nearest Neighbor Language Models](https://arxiv.org/abs/2109.04212)

Authors: [Junxian He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+J), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Taylor Berg-Kirkpatrick](https://arxiv.org/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T)

> Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model (Khandelwal et al., 2019) as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04212](https://arxiv.org/abs/2109.04212) [cs.CL]** |
|           | (or **[arXiv:2109.04212v1](https://arxiv.org/abs/2109.04212v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-8">8. Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection
</h2>

Title: [Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection](https://arxiv.org/abs/2109.04292)

Authors: [Thuy-Trang Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+T), [Xuanli He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X), [Dinh Phung](https://arxiv.org/search/cs?searchtype=author&query=Phung%2C+D), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G)

> This paper considers the unsupervised domain adaptation problem for neural machine translation (NMT), where we assume the access to only monolingual text in either the source or target language in the new domain. We propose a cross-lingual data selection method to extract in-domain sentences in the missing language side from a large generic monolingual corpus. Our proposed method trains an adaptive layer on top of multilingual BERT by contrastive learning to align the representation between the source and target language. This then enables the transferability of the domain classifier between the languages in a zero-shot manner. Once the in-domain data is detected by the classifier, the NMT model is then adapted to the new domain by jointly learning translation and domain discrimination tasks. We evaluate our cross-lingual data selection method on NMT across five diverse domains in three language pairs, as well as a real-world scenario of translation for COVID-19. The results show that our proposed method outperforms other selection baselines up to +1.5 BLEU score.

| Comments: | EMNLP2021                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04292](https://arxiv.org/abs/2109.04292) [cs.CL]** |
|           | (or **[arXiv:2109.04292v1](https://arxiv.org/abs/2109.04292v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-9">9. MATE: Multi-view Attention for Table Transformer Efficiency
</h2>

Title: [MATE: Multi-view Attention for Table Transformer Efficiency](https://arxiv.org/abs/2109.04312)

Authors: [Julian Martin Eisenschlos](https://arxiv.org/search/cs?searchtype=author&query=Eisenschlos%2C+J+M), [Maharshi Gor](https://arxiv.org/search/cs?searchtype=author&query=Gor%2C+M), [Thomas Müller](https://arxiv.org/search/cs?searchtype=author&query=Müller%2C+T), [William W. Cohen](https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+W+W)

> This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20% of relational tables on the web have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose MATE, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This architecture scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate inductive bias for tabular data, and sets a new state-of-the-art for three table reasoning datasets. For HybridQA (Chen et al., 2020b), a dataset that involves large documents containing tables, we improve the best prior result by 19 points.

| Comments: | Accepted to EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.04312](https://arxiv.org/abs/2109.04312) [cs.CL]** |
|           | (or **[arXiv:2109.04312v1](https://arxiv.org/abs/2109.04312v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-10">10. Smoothed Contrastive Learning for Unsupervised Sentence Embedding
</h2>

Title: [Smoothed Contrastive Learning for Unsupervised Sentence Embedding](https://arxiv.org/abs/2109.04321)

Authors: [Xing Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Chaochen Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C), [Liangjun Zang](https://arxiv.org/search/cs?searchtype=author&query=Zang%2C+L), [Jizhong Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J), [Zhongyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Songlin Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S)

> Contrastive learning has been gradually applied to learn high-quality unsupervised sentence embedding. Among the previous un-supervised methods, the latest state-of-the-art method, as far as we know, is unsupervised SimCSE (unsup-SimCSE). Unsup-SimCSE uses the InfoNCE1loss function in the training stage by pulling semantically similar sentences together and pushing apart dis-similar ones.Theoretically, we expect to use larger batches in unsup-SimCSE to get more adequate comparisons among samples and avoid overfitting. However, increasing the batch size does not always lead to improvements, but instead even lead to performance degradation when the batch size exceeds a threshold. Through statistical observation, we find that this is probably due to the introduction of low-confidence negative pairs after in-creasing the batch size. To alleviate this problem, we introduce a simple smoothing strategy upon the InfoNCE loss function, termedGaussian Smoothing InfoNCE (GS-InfoNCE).Specifically, we add random Gaussian noise vectors as negative samples, which act asa smoothing of the negative sample space.Though being simple, the proposed smooth-ing strategy brings substantial improvements to unsup-SimCSE. We evaluate GS-InfoNCEon the standard semantic text similarity (STS)task. GS-InfoNCE outperforms the state-of-the-art unsup-SimCSE by an average Spear-man correlation of 1.38%, 0.72%, 1.17% and0.28% on the base of BERT-base, BERT-large,RoBERTa-base and RoBERTa-large, respectively.

| Comments: | 6 pages, 2 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.04321](https://arxiv.org/abs/2109.04321) [cs.CL]** |
|           | (or **[arXiv:2109.04321v1](https://arxiv.org/abs/2109.04321v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-11">11. PPT: Pre-trained Prompt Tuning for Few-shot Learning
</h2>

Title: [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://arxiv.org/abs/2109.04332)

Authors: [Yuxian Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+Y), [Xu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Minlie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M)

> Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model fine-tuning when downstream data are sufficient, whereas it performs much worse under few-shot learning settings, which may hinder the application of prompt tuning in practice. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework "PPT". To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice.

| Comments: | 10 pages, 4 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.04332](https://arxiv.org/abs/2109.04332) [cs.CL]** |
|           | (or **[arXiv:2109.04332v1](https://arxiv.org/abs/2109.04332v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-12">12. ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding
</h2>

Title: [ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding](https://arxiv.org/abs/2109.04380)

Authors: [Xing Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Chaochen Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C), [Liangjun Zang](https://arxiv.org/search/cs?searchtype=author&query=Zang%2C+L), [Jizhong Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J), [Zhongyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Songlin Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S)

> Contrastive learning has been attracting much attention for learning unsupervised sentence embeddings. The current state-of-the-art unsupervised method is the unsupervised SimCSE (unsup-SimCSE). Unsup-SimCSE takes dropout as a minimal data augmentation method, and passes the same input sentence to a pre-trained Transformer encoder (with dropout turned on) twice to obtain the two corresponding embeddings to build a positive pair. As the length information of a sentence will generally be encoded into the sentence embeddings due to the usage of position embedding in Transformer, each positive pair in unsup-SimCSE actually contains the same length information. And thus unsup-SimCSE trained with these positive pairs is probably biased, which would tend to consider that sentences of the same or similar length are more similar in semantics. Through statistical observations, we find that unsup-SimCSE does have such a problem. To alleviate it, we apply a simple repetition operation to modify the input sentence, and then pass the input sentence and its modified counterpart to the pre-trained Transformer encoder, respectively, to get the positive pair. Additionally, we draw inspiration from the community of computer vision and introduce a momentum contrast, enlarging the number of negative pairs without additional calculations. The proposed two modifications are applied on positive and negative pairs separately, and build a new sentence embedding method, termed Enhanced Unsup-SimCSE (ESimCSE). We evaluate the proposed ESimCSE on several benchmark datasets w.r.t the semantic text similarity (STS) task. Experimental results show that ESimCSE outperforms the state-of-the-art unsup-SimCSE by an average Spearman correlation of 2.02% on BERT-base.

| Comments: | 9 pages, 2 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.04380](https://arxiv.org/abs/2109.04380) [cs.CL]** |
|           | (or **[arXiv:2109.04380v1](https://arxiv.org/abs/2109.04380v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-13">13. HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints
</h2>

Title: [HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints](https://arxiv.org/abs/2109.04443)

Authors: [Sahana Ramnath](https://arxiv.org/search/cs?searchtype=author&query=Ramnath%2C+S), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M), [Abhirut Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Aravindan Raghuveer](https://arxiv.org/search/cs?searchtype=author&query=Raghuveer%2C+A)

> Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT -- a family of techniques which provides hints (through tags) to the encoder and decoder. First, we propose a novel method of using both high and low quality BT data by providing hints (as source tags on the encoder) to the model about the quality of each source-target pair. We don't filter out low quality data but instead show that these hints enable the model to learn effectively from noisy data. Second, we address the problem of predicting whether a source token needs to be translated or transliterated to the target language, which is common in cross-script translation tasks (i.e., where source and target do not share the written script). For such cases, we propose training the model with additional hints (as target tags on the decoder) that provide information about the operation required on the source (translation or both translation and transliteration). We conduct experiments and detailed analyses on standard WMT benchmarks for three cross-script low/medium-resource language pairs: {Hindi,Gujarati,Tamil}-to-English. Our methods compare favorably with five strong and well established baselines. We show that using these hints, both separately and together, significantly improves translation quality and leads to state-of-the-art performance in all three language pairs in corresponding bilingual settings.

| Comments: | 17 pages including references and appendix. Accepted at EMNLP 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.04443](https://arxiv.org/abs/2109.04443) [cs.CL]** |
|           | (or **[arXiv:2109.04443v1](https://arxiv.org/abs/2109.04443v1) [cs.CL]** for this version) |





<h2 id="2021-09-10-14">14. Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers
</h2>

Title: [Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers](https://arxiv.org/abs/2109.04448)

Authors: [Stella Frank](https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+S), [Emanuele Bugliarello](https://arxiv.org/search/cs?searchtype=author&query=Bugliarello%2C+E), [Desmond Elliott](https://arxiv.org/search/cs?searchtype=author&query=Elliott%2C+D)

> Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed models have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these models are not symmetrically cross-modal.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2109.04448](https://arxiv.org/abs/2109.04448) [cs.CL]** |
|           | (or **[arXiv:2109.04448v1](https://arxiv.org/abs/2109.04448v1) [cs.CL]** for this version) |





# 2021-09-09

[Return to Index](#Index)



<h2 id="2021-09-09-1">1. Mixup Decoding for Diverse Machine Translation
</h2>

Title: [Mixup Decoding for Diverse Machine Translation](https://arxiv.org/abs/2109.03402)

Authors: [Jicheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Pengzhi Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+P), [Xuanfu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y), [Zhongjun He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Z), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Diverse machine translation aims at generating various target language translations for a given source language sentence. Leveraging the linear relationship in the sentence latent space introduced by the mixup training, we propose a novel method, MixDiversity, to generate different translations for the input sentence by linearly interpolating it with different sentence pairs sampled from the training corpus when decoding. To further improve the faithfulness and diversity of the translations, we propose two simple but effective approaches to select diverse sentence pairs in the training corpus and adjust the interpolation weight for each pair correspondingly. Moreover, by controlling the interpolation weight, our method can achieve the trade-off between faithfulness and diversity without any additional training, which is required in most of the previous methods. Experiments on WMT'16 en-ro, WMT'14 en-de, and WMT'17 zh-en are conducted to show that our method substantially outperforms all previous diverse machine translation methods.

| Comments: | Findings of EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03402](https://arxiv.org/abs/2109.03402) [cs.CL]** |
|           | (or **[arXiv:2109.03402v1](https://arxiv.org/abs/2109.03402v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-2">2. Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models
</h2>

Title: [Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models](https://arxiv.org/abs/2109.03415)

Authors: [Jiaoda Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Duygu Ataman](https://arxiv.org/search/cs?searchtype=author&query=Ataman%2C+D), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Multimodal machine translation (MMT) systems have been shown to outperform their text-only neural machine translation (NMT) counterparts when visual context is available. However, recent studies have also shown that the performance of MMT models is only marginally impacted when the associated image is replaced with an unrelated image or noise, which suggests that the visual context might not be exploited by the model at all. We hypothesize that this might be caused by the nature of the commonly used evaluation benchmark, also known as Multi30K, where the translations of image captions were prepared without actually showing the images to human translators. In this paper, we present a qualitative study that examines the role of datasets in stimulating the leverage of visual modality and we propose methods to highlight the importance of visual signals in the datasets which demonstrate improvements in reliance of models on the source images. Our findings suggest the research on effective MMT architectures is currently impaired by the lack of suitable datasets and careful consideration must be taken in creation of future MMT datasets, for which we also provide useful insights.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03415](https://arxiv.org/abs/2109.03415) [cs.CL]** |
|           | (or **[arXiv:2109.03415v1](https://arxiv.org/abs/2109.03415v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-3">3. Sequence Level Contrastive Learning for Text Summarization
</h2>

Title: [Sequence Level Contrastive Learning for Text Summarization](https://arxiv.org/abs/2109.03481)

Authors: [Shusheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S), [Xingxing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Yi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Contrastive learning models have achieved great success in unsupervised visual representation learning, which maximize the similarities between feature representations of different views of the same image, while minimize the similarities between feature representations of views of different images. In text summarization, the output summary is a shorter form of the input document and they have similar meanings. In this paper, we propose a contrastive learning model for supervised abstractive text summarization, where we view a document, its gold summary and its model generated summaries as different views of the same mean representation and maximize the similarities between them during training. We improve over a strong sequence-to-sequence text generation model (i.e., BART) on three different summarization datasets. Human evaluation also shows that our model achieves better faithfulness ratings compared to its counterpart without contrastive objectives.

| Comments: | 2 figures, 12 tables                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03481](https://arxiv.org/abs/2109.03481) [cs.CL]** |
|           | (or **[arXiv:2109.03481v1](https://arxiv.org/abs/2109.03481v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-4">4. RefineCap: Concept-Aware Refinement for Image Captioning
</h2>

Title: [RefineCap: Concept-Aware Refinement for Image Captioning](https://arxiv.org/abs/2109.03529)

Authors: [Yekun Chai](https://arxiv.org/search/cs?searchtype=author&query=Chai%2C+Y), [Shuo Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+S), [Junliang Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+J)

> Automatically translating images to texts involves image scene understanding and language modeling. In this paper, we propose a novel model, termed RefineCap, that refines the output vocabulary of the language decoder using decoder-guided visual semantics, and implicitly learns the mapping between visual tag words and images. The proposed Visual-Concept Refinement method can allow the generator to attend to semantic details in the image, thereby generating more semantically descriptive captions. Our model achieves superior performance on the MS-COCO dataset in comparison with previous visual-concept based models.

| Comments: | Accepted at ViGIL @NAACL 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03529](https://arxiv.org/abs/2109.03529) [cs.CL]** |
|           | (or **[arXiv:2109.03529v1](https://arxiv.org/abs/2109.03529v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-5">5. Discrete and Soft Prompting for Multilingual Models
</h2>

Title: [Discrete and Soft Prompting for Multilingual Models](https://arxiv.org/abs/2109.03630)

Authors: [Mengjie Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+M), [Hinrich Schütze](https://arxiv.org/search/cs?searchtype=author&query=Schütze%2C+H)

> It has been shown for English that discrete and soft prompting perform strongly in few-shot learning with pretrained language models (PLMs). In this paper, we show that discrete and soft prompting perform better than finetuning in multilingual cases: Crosslingual transfer and in-language training of multilingual natural language inference. For example, with 48 English training examples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely surpassing the majority baseline (33.33%). In contrast, discrete and soft prompting outperform finetuning, achieving 36.43% and 38.79%. We also demonstrate good performance of prompting with training data in multiple languages other than English.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03630](https://arxiv.org/abs/2109.03630) [cs.CL]** |
|           | (or **[arXiv:2109.03630v1](https://arxiv.org/abs/2109.03630v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-6">6. Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach
</h2>

Title: [Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach](https://arxiv.org/abs/2109.03645)

Authors: [Víctor M. Sánchez-Cartagena](https://arxiv.org/search/cs?searchtype=author&query=Sánchez-Cartagena%2C+V+M), [Miquel Esplà-Gomis](https://arxiv.org/search/cs?searchtype=author&query=Esplà-Gomis%2C+M), [Juan Antonio Pérez-Ortiz](https://arxiv.org/search/cs?searchtype=author&query=Pérez-Ortiz%2C+J+A), [Felipe Sánchez-Martínez](https://arxiv.org/search/cs?searchtype=author&query=Sánchez-Martínez%2C+F)

> In the context of neural machine translation, data augmentation (DA) techniques may be used for generating additional training samples when the available parallel data are scarce. Many DA approaches aim at expanding the support of the empirical data distribution by generating new sentence pairs that contain infrequent words, thus making it closer to the true data distribution of parallel sentences. In this paper, we propose to follow a completely different approach and present a multi-task DA approach in which we generate new sentence pairs with transformations, such as reversing the order of the target sentence, which produce unfluent target sentences. During training, these augmented sentences are used as auxiliary tasks in a multi-task framework with the aim of providing new contexts where the target prefix is not informative enough to predict the next word. This strengthens the encoder and forces the decoder to pay more attention to the source representations of the encoder. Experiments carried out on six low-resource translation tasks show consistent improvements over the baseline and over DA methods aiming at extending the support of the empirical data distribution. The systems trained with our approach rely more on the source tokens, are more robust against domain shift and suffer less hallucinations.

| Comments: | To be published as long paper in EMNLP 2021                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.03645](https://arxiv.org/abs/2109.03645) [cs.CL]** |
|           | (or **[arXiv:2109.03645v1](https://arxiv.org/abs/2109.03645v1) [cs.CL]** for this version) |





<h2 id="2021-09-09-7">7. Active Learning by Acquiring Contrastive Examples
</h2>

Title: [Active Learning by Acquiring Contrastive Examples](https://arxiv.org/abs/2109.03764)

Authors: [Katerina Margatina](https://arxiv.org/search/cs?searchtype=author&query=Margatina%2C+K), [Giorgos Vernikos](https://arxiv.org/search/cs?searchtype=author&query=Vernikos%2C+G), [Loïc Barrault](https://arxiv.org/search/cs?searchtype=author&query=Barrault%2C+L), [Nikolaos Aletras](https://arxiv.org/search/cs?searchtype=author&query=Aletras%2C+N)

> Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting \textit{contrastive examples}, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.

| Comments: | Accepted at EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.03764](https://arxiv.org/abs/2109.03764) [cs.CL]** |
|           | (or **[arXiv:2109.03764v1](https://arxiv.org/abs/2109.03764v1) [cs.CL]** for this version) |





# 2021-09-08

[Return to Index](#Index)



<h2 id="2021-09-08-1">1. Paraphrase Generation as Unsupervised Machine Translation
</h2>

Title: [Paraphrase Generation as Unsupervised Machine Translation](https://arxiv.org/abs/2109.02950)

Authors: [Chun Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+C), [Yufei Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Y), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Nanyun Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+N), [Xiaofei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X), [Fei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> In this paper, we propose a new paradigm for paraphrase generation by treating the task as unsupervised machine translation (UMT) based on the assumption that there must be pairs of sentences expressing the same meaning in a large-scale unlabeled monolingual corpus. The proposed paradigm first splits a large unlabeled corpus into multiple clusters, and trains multiple UMT models using pairs of these clusters. Then based on the paraphrase pairs produced by these UMT models, a unified surrogate model can be trained to serve as the final Seq2Seq model to generate paraphrases, which can be directly used for test in the unsupervised setup, or be finetuned on labeled datasets in the supervised setup. The proposed method offers merits over machine-translation-based paraphrase generation methods, as it avoids reliance on bilingual sentence pairs. It also allows human intervene with the model so that more diverse paraphrases can be generated using different filtering criteria. Extensive experiments on existing paraphrase dataset for both the supervised and unsupervised setups demonstrate the effectiveness the proposed paradigm.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.02950](https://arxiv.org/abs/2109.02950) [cs.CL]** |
|           | (or **[arXiv:2109.02950v1](https://arxiv.org/abs/2109.02950v1) [cs.CL]** for this version) |





<h2 id="2021-09-08-2">2. Don't Go Far Off: An Empirical Study on Neural Poetry Translation
</h2>

Title: [Don't Go Far Off: An Empirical Study on Neural Poetry Translation](https://arxiv.org/abs/2109.02972)

Authors: [Tuhin Chakrabarty](https://arxiv.org/search/cs?searchtype=author&query=Chakrabarty%2C+T), [Arkadiy Saakyan](https://arxiv.org/search/cs?searchtype=author&query=Saakyan%2C+A), [Smaranda Muresan](https://arxiv.org/search/cs?searchtype=author&query=Muresan%2C+S)

> Despite constant improvements in machine translation quality, automatic poetry translation remains a challenging problem due to the lack of open-sourced parallel poetic corpora, and to the intrinsic complexities involved in preserving the semantics, style, and figurative nature of poetry. We present an empirical investigation for poetry translation along several dimensions: 1) size and style of training data (poetic vs. non-poetic), including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3) language-family-specific models vs. mixed-multilingual models. To accomplish this, we contribute a parallel dataset of poetry translations for several language pairs. Our results show that multilingual fine-tuning on poetic text significantly outperforms multilingual fine-tuning on non-poetic text that is 35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and human evaluation metrics such as faithfulness (meaning and poetic style). Moreover, multilingual fine-tuning on poetic data outperforms \emph{bilingual} fine-tuning on poetic data.

| Comments: | EMNLP 2021 Camera ready                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.02972](https://arxiv.org/abs/2109.02972) [cs.CL]** |
|           | (or **[arXiv:2109.02972v1](https://arxiv.org/abs/2109.02972v1) [cs.CL]** for this version) |





<h2 id="2021-09-08-3">3. Revisiting Context Choices for Context-aware Machine Translation
</h2>

Title: [Revisiting Context Choices for Context-aware Machine Translation](https://arxiv.org/abs/2109.02995)

Authors: [Matīss Rikters](https://arxiv.org/search/cs?searchtype=author&query=Rikters%2C+M), [Toshiaki Nakazawa](https://arxiv.org/search/cs?searchtype=author&query=Nakazawa%2C+T)

> One of the most popular methods for context-aware machine translation (MT) is to use separate encoders for the source sentence and context as multiple sources for one target sentence. Recent work has cast doubt on whether these models actually learn useful signals from the context or are improvements in automatic evaluation metrics just a side-effect. We show that multi-source transformer models improve MT over standard transformer-base models even with empty lines provided as context, but the translation quality improves significantly (1.51 - 2.65 BLEU) when a sufficient amount of correct context is provided. We also show that even though randomly shuffling in-domain context can also improve over baselines, the correct context further improves translation quality and random out-of-domain context further degrades it.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.02995](https://arxiv.org/abs/2109.02995) [cs.CL]** |
|           | (or **[arXiv:2109.02995v1](https://arxiv.org/abs/2109.02995v1) [cs.CL]** for this version) |





<h2 id="2021-09-08-4">4. Generate & Rank: A Multi-task Framework for Math Word Problems
</h2>

Title: [Generate & Rank: A Multi-task Framework for Math Word Problems](https://arxiv.org/abs/2109.03034)

Authors: [Jianhao Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J), [Yichun Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Lin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Lifeng Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+L), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Ming Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate & Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7% (78.4% → 85.4%) higher than the state-of-the-art.

| Comments: | Findings of EMNLP2021                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.03034](https://arxiv.org/abs/2109.03034) [cs.CL]** |
|           | (or **[arXiv:2109.03034v1](https://arxiv.org/abs/2109.03034v1) [cs.CL]** for this version) |





<h2 id="2021-09-08-5">5. NumGPT: Improving Numeracy Ability of Generative Pre-trained Models
</h2>

Title: [NumGPT: Improving Numeracy Ability of Generative Pre-trained Models](https://arxiv.org/abs/2109.03137)

Authors: [Zhihua Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Z), [Xin Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [Xingbo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Yong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Xiaozhe Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Huamin Qu](https://arxiv.org/search/cs?searchtype=author&query=Qu%2C+H)

> Existing generative pre-trained language models (e.g., GPT) focus on modeling the language structure and semantics of general texts. However, those models do not consider the numerical properties of numbers and cannot perform robustly on numerical reasoning tasks (e.g., math word problems and measurement estimation). In this paper, we propose NumGPT, a generative pre-trained model that explicitly models the numerical properties of numbers in texts. Specifically, it leverages a prototype-based numeral embedding to encode the mantissa of the number and an individual embedding to encode the exponent of the number. A numeral-aware loss function is designed to integrate numerals into the pre-training objective of NumGPT. We conduct extensive experiments on four different datasets to evaluate the numeracy ability of NumGPT. The experiment results show that NumGPT outperforms baseline models (e.g., GPT and GPT with DICE) on a range of numerical reasoning tasks such as measurement estimation, number comparison, math word problems, and magnitude classification. Ablation studies are also conducted to evaluate the impact of pre-training and model hyperparameters on the performance.

| Comments: | 8 pages, 3 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.03137](https://arxiv.org/abs/2109.03137) [cs.CL]** |
|           | (or **[arXiv:2109.03137v1](https://arxiv.org/abs/2109.03137v1) [cs.CL]** for this version) |





# 2021-09-07

[Return to Index](#Index)



<h2 id="2021-09-07-1">1. Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models
</h2>

Title: [Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models](https://arxiv.org/abs/2109.01754)

Authors: [Rakesh Chada](https://arxiv.org/search/cs?searchtype=author&query=Chada%2C+R), [Pradeep Natarajan](https://arxiv.org/search/cs?searchtype=author&query=Natarajan%2C+P), [Darshan Fofadiya](https://arxiv.org/search/cs?searchtype=author&query=Fofadiya%2C+D), [Prathap Ramachandra](https://arxiv.org/search/cs?searchtype=author&query=Ramachandra%2C+P)

> Large-scale conversational assistants like Alexa, Siri, Cortana and Google Assistant process every utterance using multiple models for domain, intent and named entity recognition. Given the decoupled nature of model development and large traffic volumes, it is extremely difficult to identify utterances processed erroneously by such systems. We address this challenge to detect domain classification errors using offline Transformer models. We combine utterance encodings from a RoBERTa model with the Nbest hypothesis produced by the production system. We then fine-tune end-to-end in a multitask setting using a small dataset of humanannotated utterances with domain classification errors. We tested our approach for detecting misclassifications from one domain that accounts for <0.5% of the traffic in a large-scale conversational AI system. Our approach achieves an F1 score of 30% outperforming a bi- LSTM baseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this further by 2.2% to 32.2% by ensembling multiple models.

| Comments: | Accepted to ACL Findings 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.01754](https://arxiv.org/abs/2109.01754) [cs.CL]** |
|           | (or **[arXiv:2109.01754v1](https://arxiv.org/abs/2109.01754v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-2">2. On the ability of monolingual models to learn language-agnostic representations
</h2>

Title: [On the ability of monolingual models to learn language-agnostic representations](https://arxiv.org/abs/2109.01942)

Authors: [Leandro Rodrigues de Souza](https://arxiv.org/search/cs?searchtype=author&query=de+Souza%2C+L+R), [Rodrigo Nogueira](https://arxiv.org/search/cs?searchtype=author&query=Nogueira%2C+R), [Roberto Lotufo](https://arxiv.org/search/cs?searchtype=author&query=Lotufo%2C+R)

> Pretrained multilingual models have become a de facto default approach for zero-shot cross-lingual transfer. Previous work has shown that these models are able to achieve cross-lingual representations when pretrained on two or more languages with shared parameters. In this work, we provide evidence that a model can achieve language-agnostic representations even when pretrained on a single language. That is, we find that monolingual models pretrained and finetuned on different languages achieve competitive performance compared to the ones that use the same target language. Surprisingly, the models show a similar performance on a same task regardless of the pretraining language. For example, models pretrained on distant languages such as German and Portuguese perform similarly on English tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01942](https://arxiv.org/abs/2109.01942) [cs.CL]** |
|           | (or **[arXiv:2109.01942v1](https://arxiv.org/abs/2109.01942v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-3">3. Counterfactual Evaluation for Explainable AI
</h2>

Title: [Counterfactual Evaluation for Explainable AI](https://arxiv.org/abs/2109.01962)

Authors: [Yingqiang Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+Y), [Shuchang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Zelong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Shuyuan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S), [Shijie Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+S), [Yunqi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Juntao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+J), [Fei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+F), [Yongfeng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> While recent years have witnessed the emergence of various explainable methods in machine learning, to what degree the explanations really represent the reasoning process behind the model prediction -- namely, the faithfulness of explanation -- is still an open problem. One commonly used way to measure faithfulness is \textit{erasure-based} criteria. Though conceptually simple, erasure-based criterion could inevitably introduce biases and artifacts. We propose a new methodology to evaluate the faithfulness of explanations from the \textit{counterfactual reasoning} perspective: the model should produce substantially different outputs for the original input and its corresponding counterfactual edited on a faithful feature. Specially, we introduce two algorithms to find the proper counterfactuals in both discrete and continuous scenarios and then use the acquired counterfactuals to measure faithfulness. Empirical results on several datasets show that compared with existing metrics, our proposed counterfactual evaluation method can achieve top correlation with the ground truth under diffe

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01962](https://arxiv.org/abs/2109.01962) [cs.CL]** |
|           | (or **[arXiv:2109.01962v1](https://arxiv.org/abs/2109.01962v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-4">4. Data Efficient Masked Language Modeling for Vision and Language
</h2>

Title: [Data Efficient Masked Language Modeling for Vision and Language](https://arxiv.org/abs/2109.02040)

Authors: [Yonatan Bitton](https://arxiv.org/search/cs?searchtype=author&query=Bitton%2C+Y), [Gabriel Stanovsky](https://arxiv.org/search/cs?searchtype=author&query=Stanovsky%2C+G), [Michael Elhadad](https://arxiv.org/search/cs?searchtype=author&query=Elhadad%2C+M), [Roy Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+R)

> Masked language modeling (MLM) is one of the key sub-tasks in vision-language pretraining. In the cross-modal setting, tokens in the sentence are masked at random, and the model predicts the masked tokens given the image and the text. In this paper, we observe several key disadvantages of MLM in this setting. First, as captions tend to be short, in a third of the sentences no token is sampled. Second, the majority of masked tokens are stop-words and punctuation, leading to under-utilization of the image. We investigate a range of alternative masking strategies specific to the cross-modal setting that address these shortcomings, aiming for better fusion of text and image in the learned representation. When pre-training the LXMERT model, our alternative masking strategies consistently improve over the original masking strategy on three downstream tasks, especially in low resource settings. Further, our pre-training approach substantially outperforms the baseline model on a prompt-based probing task designed to elicit image objects. These results and our analysis indicate that our method allows for better utilization of the training data.

| Comments: | Accepted to Findings of EMNLP 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.02040](https://arxiv.org/abs/2109.02040) [cs.CL]** |
|           | (or **[arXiv:2109.02040v1](https://arxiv.org/abs/2109.02040v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-5">5. Teaching Autoregressive Language Models Complex Tasks By Demonstration
</h2>

Title: [Teaching Autoregressive Language Models Complex Tasks By Demonstration](https://arxiv.org/abs/2109.02102)

Authors: [Gabriel Recchia](https://arxiv.org/search/cs?searchtype=author&query=Recchia%2C+G)

> This paper demonstrates that by fine-tuning an autoregressive language model (GPT-Neo) on appropriately structured step-by-step demonstrations, it is possible to teach it to execute a mathematical task that has previously proved difficult for Transformers - longhand modulo operations - with a relatively small number of examples. Specifically, we fine-tune GPT-Neo to solve the numbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et al. ([arXiv:1904.01557](https://arxiv.org/abs/1904.01557)) reported below 40% accuracy on this task with 2 million training examples. We show that after fine-tuning on 200 appropriately structured demonstrations of solving long division problems and reporting the remainders, the smallest available GPT-Neo model achieves over 80% accuracy. This is achieved by constructing an appropriate dataset for fine-tuning, with no changes to the learning algorithm. These results suggest that fine-tuning autoregressive language models on small sets of well-crafted demonstrations may be a useful paradigm for enabling individuals without training in machine learning to coax such models to perform some kinds of complex multi-step tasks.

| Comments:    | 15 pages, 2 tables, 4 figures. Associated code and data available at [this https URL](https://github.com/mesotron/teaching_transformers) |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ACM classes: | I.2.0; I.2.6                                                 |
| Cite as:     | **[arXiv:2109.02102](https://arxiv.org/abs/2109.02102) [cs.CL]** |
|              | (or **[arXiv:2109.02102v1](https://arxiv.org/abs/2109.02102v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-6">6. Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack
</h2>

Title: [Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack](https://arxiv.org/abs/2109.02229)

Authors: [Shengcai Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Ning Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+N), [Cheng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Ke Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+K)

> Over the past few years, various word-level textual attack approaches have been proposed to reveal the vulnerability of deep neural networks used in natural language processing. Typically, these approaches involve an important optimization step to determine which substitute to be used for each word in the original input. However, current research on this step is still rather limited, from the perspectives of both problem-understanding and problem-solving. In this paper, we address these issues by uncovering the theoretical properties of the problem and proposing an efficient local search algorithm (LS) to solve it. We establish the first provable approximation guarantee on solving the problem in general cases. Notably, for adversarial textual attack, it is even better than the previous bound which only holds in special case. Extensive experiments involving five NLP tasks, six datasets and eleven NLP models show that LS can largely reduce the number of queries usually by an order of magnitude to achieve high attack success rates. Further experiments show that the adversarial examples crafted by LS usually have higher quality, exhibit better transferability, and can bring more robustness improvement to victim models by adversarial training.

| Subjects: | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.02229](https://arxiv.org/abs/2109.02229) [cs.CL]** |
|           | (or **[arXiv:2109.02229v1](https://arxiv.org/abs/2109.02229v1) [cs.CL]** for this version) |





<h2 id="2021-09-07-7">7. Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training
</h2>

Title: [Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training](https://arxiv.org/abs/2109.02284)

Authors: [Minghao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+M), [Yitong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Meng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Liangyou Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Gholamreza Haffari](https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q)

> Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the model converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the high resource ones. However, automatic balancing methods usually depend on the intra- and inter-dataset characteristics, which is usually agnostic or requires human priors. In this work, we propose an approach, MultiUAT, that dynamically adjusts the training data usage based on the model's uncertainty on a small set of trusted clean data for multi-corpus machine translation. We experiments with two classes of uncertainty measures on multilingual (16 languages with 4 settings) and multi-domain settings (4 for in-domain and 2 for out-of-domain on English-German translation) and demonstrate our approach MultiUAT substantially outperforms its baselines, including both static and dynamic strategies. We analyze the cross-domain transfer and show the deficiency of static and similarity based methods.

| Comments: | 15 pages, 4 figures, to appear at EMNLP 2021 main conference |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.02284](https://arxiv.org/abs/2109.02284) [cs.CL]** |
|           | (or **[arXiv:2109.02284v1](https://arxiv.org/abs/2109.02284v1) [cs.CL]** for this version) |








# 2021-09-06

[Return to Index](#Index)



<h2 id="2021-09-06-1">1. Ranking Scientific Papers Using Preference Learning
</h2>

Title: [Ranking Scientific Papers Using Preference Learning](https://arxiv.org/abs/2109.01190)

Authors: [Nils Dycke](https://arxiv.org/search/cs?searchtype=author&query=Dycke%2C+N), [Edwin Simpson](https://arxiv.org/search/cs?searchtype=author&query=Simpson%2C+E), [Ilia Kuznetsov](https://arxiv.org/search/cs?searchtype=author&query=Kuznetsov%2C+I), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> Peer review is the main quality control mechanism in academia. Quality of scientific work has many dimensions; coupled with the subjective nature of the reviewing task, this makes final decision making based on the reviews and scores therein very difficult and time-consuming. To assist with this important task, we cast it as a paper ranking problem based on peer review texts and reviewer scores. We introduce a novel, multi-faceted generic evaluation framework for making final decisions based on peer reviews that takes into account effectiveness, efficiency and fairness of the evaluated system. We propose a novel approach to paper ranking based on Gaussian Process Preference Learning (GPPL) and evaluate it on peer review data from the ACL-2018 conference. Our experiments demonstrate the superiority of our GPPL-based approach over prior work, while highlighting the importance of using both texts and review scores for paper ranking during peer review aggregation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01190](https://arxiv.org/abs/2109.01190) [cs.CL]** |
|           | (or **[arXiv:2109.01190v1](https://arxiv.org/abs/2109.01190v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-2">2. Establishing Interlingua in Multilingual Language Models
</h2>

Title: [Establishing Interlingua in Multilingual Language Models](https://arxiv.org/abs/2109.01207)

Authors: [Maksym Del](https://arxiv.org/search/cs?searchtype=author&query=Del%2C+M), [Mark Fishel](https://arxiv.org/search/cs?searchtype=author&query=Fishel%2C+M)

> Large multilingual language models show remarkable zero-shot cross-lingual transfer performance on a range of tasks. Follow-up works hypothesized that these models internally project representations of different languages into a shared interlingual space. However, they produced contradictory results. In this paper, we correct %one of the previous works the famous prior work claiming that "BERT is not an Interlingua" and show that with the proper choice of sentence representation different languages actually do converge to a shared space in such language models. Furthermore, we demonstrate that this convergence pattern is robust across four measures of correlation similarity and six mBERT-like models. We then extend our analysis to 28 diverse languages and find that the interlingual space exhibits a particular structure similar to the linguistic relatedness of languages. We also highlight a few outlier languages that seem to fail to converge to the shared space. The code for replicating our results is available at the following URL: [this https URL](https://github.com/maksym-del/interlingua).

| Comments:    | 8 pages, 10 figures                                          |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7; I.2.6                                                 |
| Cite as:     | **[arXiv:2109.01207](https://arxiv.org/abs/2109.01207) [cs.CL]** |
|              | (or **[arXiv:2109.01207v1](https://arxiv.org/abs/2109.01207v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-3">3. Quantifying Reproducibility in NLP and ML
</h2>

Title: [Quantifying Reproducibility in NLP and ML](https://arxiv.org/abs/2109.01211)

Authors: [Anya Belz](https://arxiv.org/search/cs?searchtype=author&query=Belz%2C+A)

> Reproducibility has become an intensely debated topic in NLP and ML over recent years, but no commonly accepted way of assessing reproducibility, let alone quantifying it, has so far emerged. The assumption has been that wider scientific reproducibility terminology and definitions are not applicable to NLP/ML, with the result that many different terms and definitions have been proposed, some diametrically opposed. In this paper, we test this assumption, by taking the standard terminology and definitions from metrology and applying them directly to NLP/ML. We find that we are able to straightforwardly derive a practical framework for assessing reproducibility which has the desirable property of yielding a quantified degree of reproducibility that is comparable across different reproduction studies.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01211](https://arxiv.org/abs/2109.01211) [cs.CL]** |
|           | (or **[arXiv:2109.01211v1](https://arxiv.org/abs/2109.01211v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-4">4. Multimodal Conditionality for Natural Language Generation
</h2>

Title: [Multimodal Conditionality for Natural Language Generation](https://arxiv.org/abs/2109.01229)

Authors: [Michael Sollami](https://arxiv.org/search/cs?searchtype=author&query=Sollami%2C+M), [Aashish Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+A)

> Large scale pretrained language models have demonstrated state-of-the-art performance in language understanding tasks. Their application has recently expanded into multimodality learning, leading to improved representations combining vision and language. However, progress in adapting language models towards conditional Natural Language Generation (NLG) has been limited to a single modality, generally text. We propose MAnTiS, Multimodal Adaptation for Text Synthesis, a general approach for multimodal conditionality in transformer-based NLG models. In this method, we pass inputs from each modality through modality-specific encoders, project to textual token space, and finally join to form a conditionality prefix. We fine-tune the pretrained language model and encoders with the conditionality prefix guiding the generation. We apply MAnTiS to the task of product description generation, conditioning a network on both product images and titles to generate descriptive text. We demonstrate that MAnTiS outperforms strong baseline approaches on standard NLG scoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS can generate human quality descriptions consistent with given multimodal inputs.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01229](https://arxiv.org/abs/2109.01229) [cs.CL]** |
|           | (or **[arXiv:2109.01229v1](https://arxiv.org/abs/2109.01229v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-5">5. Do Prompt-Based Models Really Understand the Meaning of their Prompts?
</h2>

Title: [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/abs/2109.01247)

Authors: [Albert Webson](https://arxiv.org/search/cs?searchtype=author&query=Webson%2C+A), [Ellie Pavlick](https://arxiv.org/search/cs?searchtype=author&query=Pavlick%2C+E)

> Recently, a boom of papers have shown extraordinary progress in few-shot learning with various prompt-based models. Such success can give the impression that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompts manually written for natural language inference (NLI). We find that models learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively "good" prompts. Additionally, we find that model performance is more dependent on the choice of the LM target words (a.k.a. the "verbalizer" that converts LM vocabulary prediction to class labels) than on the text of the prompt itself. In sum, we find little evidence that suggests existing prompt-based models truly understand the meaning of their given prompts.

| Comments: | Code available at [this https URL](https://github.com/awebson/prompt_semantics) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.01247](https://arxiv.org/abs/2109.01247) [cs.CL]** |
|           | (or **[arXiv:2109.01247v1](https://arxiv.org/abs/2109.01247v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-6">6. Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT
</h2>

Title: [Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT](https://arxiv.org/abs/2109.01396)

Authors: [Elena Voita](https://arxiv.org/search/cs?searchtype=author&query=Voita%2C+E), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R), [Ivan Titov](https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I)

> Differently from the traditional statistical MT that decomposes the translation task into distinct separately learned components, neural machine translation uses a single neural network to model the entire translation process. Despite neural machine translation being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different models in traditional SMT. In this work, we look at the competences related to three core SMT components and find that during training, NMT first focuses on learning target-side language modeling, then improves translation quality approaching word-by-word translation, and finally learns more complicated reordering patterns. We show that this behavior holds for several models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla non-autoregressive neural machine translation by guiding teacher model selection.

| Comments: | EMNLP 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.01396](https://arxiv.org/abs/2109.01396) [cs.CL]** |
|           | (or **[arXiv:2109.01396v1](https://arxiv.org/abs/2109.01396v1) [cs.CL]** for this version) |





<h2 id="2021-09-06-7">7. Finetuned Language Models Are Zero-Shot Learners
</h2>

Title: [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)

Authors: [Jason Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+J), [Maarten Bosma](https://arxiv.org/search/cs?searchtype=author&query=Bosma%2C+M), [Vincent Y. Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+V+Y), [Kelvin Guu](https://arxiv.org/search/cs?searchtype=author&query=Guu%2C+K), [Adams Wei Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+A+W), [Brian Lester](https://arxiv.org/search/cs?searchtype=author&query=Lester%2C+B), [Nan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+N), [Andrew M. Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+A+M), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V)

> This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially boosts zero-shot performance on unseen tasks.
> We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 19 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of tasks and model scale are key components to the success of instruction tuning.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.01652](https://arxiv.org/abs/2109.01652) [cs.CL]** |
|           | (or **[arXiv:2109.01652v1](https://arxiv.org/abs/2109.01652v1) [cs.CL]** for this version) |








# 2021-09-03

[Return to Index](#Index)



<h2 id="2021-09-03-1">1. Skim-Attention: Learning to Focus via Document Layout
</h2>

Title: [Skim-Attention: Learning to Focus via Document Layout](https://arxiv.org/abs/2109.01078)

Authors: [Laura Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+L), [Thomas Scialom](https://arxiv.org/search/cs?searchtype=author&query=Scialom%2C+T), [Jacopo Staiano](https://arxiv.org/search/cs?searchtype=author&query=Staiano%2C+J), [Benjamin Piwowarski](https://arxiv.org/search/cs?searchtype=author&query=Piwowarski%2C+B)

> Transformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success, multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies, this paper presents Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout. Skim-Attention only attends to the 2-dimensional position of the words in a document. Our experiments show that Skim-Attention obtains a lower perplexity than prior works, while being more computationally efficient. Skim-Attention can be further combined with long-range Transformers to efficiently process long documents. We also show how Skim-Attention can be used off-the-shelf as a mask for any Pre-trained Language Model, allowing to improve their performance while restricting attention. Finally, we show the emergence of a document structure representation in Skim-Attention.

| Comments: | 15 pages, 6 figures, to be published in EMNLP 2021 Findings  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.01078](https://arxiv.org/abs/2109.01078) [cs.CL]** |
|           | (or **[arXiv:2109.01078v1](https://arxiv.org/abs/2109.01078v1) [cs.CL]** for this version) |





<h2 id="2021-09-03-2">2. How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?
</h2>

Title: [How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?](https://arxiv.org/abs/2109.01100)

Authors: [Chantal Amrhein](https://arxiv.org/search/cs?searchtype=author&query=Amrhein%2C+C), [Rico Sennrich](https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R)

> Data-driven subword segmentation has become the default strategy for open-vocabulary machine translation and other NLP tasks, but may not be sufficiently generic for optimal learning of non-concatenative morphology. We design a test suite to evaluate segmentation strategies on different types of morphological phenomena in a controlled, semi-synthetic setting. In our experiments, we compare how well machine translation models trained on subword- and character-level can translate these morphological phenomena. We find that learning to analyse and generate morphologically complex surface representations is still challenging, especially for non-concatenative morphological phenomena like reduplication or vowel harmony and for rare word stems. Based on our results, we recommend that novel text representation strategies be tested on a range of typologically diverse languages to minimise the risk of adopting a strategy that inadvertently disadvantages certain languages.

| Comments:    | Findings of EMNLP 2021                                       |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2109.01100](https://arxiv.org/abs/2109.01100) [cs.CL]** |
|              | (or **[arXiv:2109.01100v1](https://arxiv.org/abs/2109.01100v1) [cs.CL]** for this version) |





<h2 id="2021-09-03-3">3. Sequence-to-Sequence Learning with Latent Neural Grammars
</h2>

Title: [Sequence-to-Sequence Learning with Latent Neural Grammars](https://arxiv.org/abs/2109.01135)

Authors: [Yoon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y)

> Sequence-to-sequence learning with neural networks has become the de facto standard for sequence prediction tasks. This approach typically models the local distribution over the next word with a powerful neural network that can condition on arbitrary context. While flexible and performant, these models often require large datasets for training and can fail spectacularly on benchmarks designed to test for compositional generalization. This work explores an alternative, hierarchical approach to sequence-to-sequence learning with quasi-synchronous grammars, where each node in the target tree is transduced by a node in the source tree. Both the source and target trees are treated as latent and induced during training. We develop a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. We apply this latent neural grammar to various domains -- a diagnostic language navigation task designed to test for compositional generalization (SCAN), style transfer, and small-scale machine translation -- and find that it performs respectably compared to standard baselines.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.01135](https://arxiv.org/abs/2109.01135) [cs.CL]** |
|           | (or **[arXiv:2109.01135v1](https://arxiv.org/abs/2109.01135v1) [cs.CL]** for this version) |





<h2 id="2021-09-03-4">4. Knowledge Perceived Multi-modal Pretraining in E-commerce
</h2>

Title: [Knowledge Perceived Multi-modal Pretraining in E-commerce](https://arxiv.org/abs/2109.00895)

Authors: [Yushan Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Y), [Huaixiao Tou](https://arxiv.org/search/cs?searchtype=author&query=Tou%2C+H), [Wen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Ganqiang Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+G), [Hui Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H), [Ningyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+N), [Huajun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H)

> In this paper, we address multi-modal pretraining of product data in the field of E-commerce. Current multi-modal pretraining methods proposed for image and text modalities lack robustness in the face of modality-missing and modality-noise, which are two pervasive problems of multi-modal product data in real E-commerce scenarios. To this end, we propose a novel method, K3M, which introduces knowledge modality in multi-modal pretraining to correct the noise and supplement the missing of image and text modalities. The modal-encoding layer extracts the features of each modality. The modal-interaction layer is capable of effectively modeling the interaction of multiple modalities, where an initial-interactive feature fusion model is designed to maintain the independence of image modality and text modality, and a structure aggregation module is designed to fuse the information of image, text, and knowledge modalities. We pretrain K3M with three pretraining tasks, including masked object modeling (MOM), masked language modeling (MLM), and link prediction modeling (LPM). Experimental results on a real-world E-commerce dataset and a series of product-based downstream tasks demonstrate that K3M achieves significant improvements in performances than the baseline and state-of-the-art methods when modality-noise or modality-missing exists.

| Comments: | Accepted to ACM MM 2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| DOI:      | [10.1145/3474085.3475648](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3474085.3475648&v=072100ff) |
| Cite as:  | **[arXiv:2109.00895](https://arxiv.org/abs/2109.00895) [cs.CV]** |
|           | (or **[arXiv:2109.00895v1](https://arxiv.org/abs/2109.00895v1) [cs.CV]** for this version) |





<h2 id="2021-09-03-5">5. Improving Multimodal fusion via Mutual Dependency Maximisation
</h2>

Title: [Improving Multimodal fusion via Mutual Dependency Maximisation](https://arxiv.org/abs/2109.00922)

Authors: [Pierre Colombo](https://arxiv.org/search/cs?searchtype=author&query=Colombo%2C+P), [Emile Chapuis](https://arxiv.org/search/cs?searchtype=author&query=Chapuis%2C+E), [Matthieu Labeau](https://arxiv.org/search/cs?searchtype=author&query=Labeau%2C+M), [Chloe Clavel](https://arxiv.org/search/cs?searchtype=author&query=Clavel%2C+C)

> Multimodal sentiment analysis is a trending area of research, and the multimodal fusion is one of its most active topic. Acknowledging humans communicate through a variety of channels (i.e visual, acoustic, linguistic), multimodal systems aim at integrating different unimodal representations into a synthetic one. So far, a consequent effort has been made on developing complex architectures allowing the fusion of these modalities. However, such systems are mainly trained by minimising simple losses such as L1 or cross-entropy. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to 4.3 on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets: \texttt{CMU-MOSI} and \texttt{CMU-MOSEI}. Our method not only achieves a new SOTA on both datasets but also produces representations that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the model.

| Subjects:          | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | EMNLP 2021                                                   |
| Cite as:           | **[arXiv:2109.00922](https://arxiv.org/abs/2109.00922) [cs.LG]** |
|                    | (or **[arXiv:2109.00922v1](https://arxiv.org/abs/2109.00922v1) [cs.LG]** for this version) |





<h2 id="2021-09-03-6">6. Towards Improving Adversarial Training of NLP Models
</h2>

Title: [Towards Improving Adversarial Training of NLP Models](https://arxiv.org/abs/2109.00544)

Authors: [Jin Yong Yoo](https://arxiv.org/search/cs?searchtype=author&query=Yoo%2C+J+Y), [Yanjun Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+Y)

> Adversarial training, a method for learning robust deep neural networks, constructs adversarial examples during training. However, recent methods for generating NLP adversarial examples involve combinatorial search and expensive sentence encoders for constraining the generated instances. As a result, it remains challenging to use vanilla adversarial training to improve NLP models' performance, and the benefits are mainly uninvestigated. This paper proposes a simple and improved vanilla adversarial training process for NLP, which we name Attacking to Training (𝙰𝟸𝚃). The core part of 𝙰𝟸𝚃 is a new and cheaper word substitution attack optimized for vanilla adversarial training. We use 𝙰𝟸𝚃 to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI datasets. Our results show that it is possible to train empirically robust NLP models using a much cheaper adversary. We demonstrate that vanilla adversarial training with 𝙰𝟸𝚃 can improve an NLP model's robustness to the attack it was originally trained with and also defend the model against other types of attacks. Furthermore, we show that 𝙰𝟸𝚃 can improve NLP models' standard accuracy, cross-domain generalization, and interpretability. Code is available at [this http URL](http://github.com/jinyongyoo/A2T) .

| Comments: | EMNLP Findings 2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.00544](https://arxiv.org/abs/2109.00544) [cs.CL]** |
|           | (or **[arXiv:2109.00544v1](https://arxiv.org/abs/2109.00544v1) [cs.CL]** for this version) |





<h2 id="2021-09-03-7">7. Point-of-Interest Type Prediction using Text and Images
</h2>

Title: [Point-of-Interest Type Prediction using Text and Images](https://arxiv.org/abs/2109.00602)

Authors: [Danae Sánchez Villegas](https://arxiv.org/search/cs?searchtype=author&query=Villegas%2C+D+S), [Nikolaos Aletras](https://arxiv.org/search/cs?searchtype=author&query=Aletras%2C+N)

> Point-of-interest (POI) type prediction is the task of inferring the type of a place from where a social media post was shared. Inferring a POI's type is useful for studies in computational social science including sociolinguistics, geosemiotics, and cultural geography, and has applications in geosocial networking technologies such as recommendation and visualization systems. Prior efforts in POI type prediction focus solely on text, without taking visual information into account. However in reality, the variety of modalities, as well as their semiotic relationships with one another, shape communication and interactions in social media. This paper presents a study on POI type prediction using multimodal information from text and images available at posting time. For that purpose, we enrich a currently available data set for POI type prediction with the images that accompany the text messages. Our proposed method extracts relevant information from each modality to effectively capture interactions between text and image achieving a macro F1 of 47.21 across eight categories significantly outperforming the state-of-the-art method for POI type prediction based on text-only methods. Finally, we provide a detailed analysis to shed light on cross-modal interactions and the limitations of our best performing model.

| Comments: | Accepted at EMNLP 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.00602](https://arxiv.org/abs/2109.00602) [cs.CL]** |
|           | (or **[arXiv:2109.00602v1](https://arxiv.org/abs/2109.00602v1) [cs.CL]** for this version) |





<h2 id="2021-09-03-8">8. Towards Making the Most of Dialogue Characteristics for Neural Chat Translation
</h2>

Title: [Towards Making the Most of Dialogue Characteristics for Neural Chat Translation](https://arxiv.org/abs/2109.00668)

Authors: [Yunlong Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Chulun Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jinan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Yufeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Neural Chat Translation (NCT) aims to translate conversational text between speakers of different languages. Despite the promising performance of sentence-level and context-aware neural machine translation models, there still remain limitations in current NCT models because the inherent dialogue characteristics of chat, such as dialogue coherence and speaker personality, are neglected. In this paper, we propose to promote the chat translation by introducing the modeling of dialogue characteristics into the NCT model. To this end, we design four auxiliary tasks including monolingual response generation, cross-lingual response generation, next utterance discrimination, and speaker identification. Together with the main chat translation task, we optimize the NCT model through the training objectives of all these tasks. By this means, the NCT model can be enhanced by capturing the inherent dialogue characteristics, thus generating more coherent and speaker-relevant translations. Comprehensive experiments on four language directions (English-German and English-Chinese) verify the effectiveness and superiority of the proposed approach.

| Comments: | Accepted as a long paper at EMNLP 2021 main conference. The first two authors contributed equally. Code: [this https URL](https://github.com/XL2248/CSA-NCT) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.00668](https://arxiv.org/abs/2109.00668) [cs.CL]** |
|           | (or **[arXiv:2109.00668v1](https://arxiv.org/abs/2109.00668v1) [cs.CL]** for this version) |





# 2021-09-02

[Return to Index](#Index)



<h2 id="2021-09-02-1">1. Sentence Bottleneck Autoencoders from Transformer Language Models
</h2>

Title: [Sentence Bottleneck Autoencoders from Transformer Language Models](https://arxiv.org/abs/2109.00055)

Authors: [Ivan Montero](https://arxiv.org/search/cs?searchtype=author&query=Montero%2C+I), [Nikolaos Pappas](https://arxiv.org/search/cs?searchtype=author&query=Pappas%2C+N), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> Representation learning for text via pretraining a language model on a large corpus has become a standard starting point for building NLP systems. This approach stands in contrast to autoencoders, also trained on raw text, but with the objective of learning to encode each input as a vector that allows full reconstruction. Autoencoders are attractive because of their latent space structure and generative properties. We therefore explore the construction of a sentence-level autoencoder from a pretrained, frozen transformer language model. We adapt the masked language modeling objective as a generative, denoising one, while only training a sentence bottleneck and a single-layer modified transformer decoder. We demonstrate that the sentence representations discovered by our model achieve better quality than previous methods that extract representations from pretrained transformers on text similarity tasks, style transfer (an example of controlled generation), and single-sentence classification tasks in the GLUE benchmark, while using fewer parameters than large pretrained models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.00055](https://arxiv.org/abs/2109.00055) [cs.CL]** |
|           | (or **[arXiv:2109.00055v1](https://arxiv.org/abs/2109.00055v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-2">2. It's not Rocket Science : Interpreting Figurative Language in Narratives
</h2>

Title: [It's not Rocket Science : Interpreting Figurative Language in Narratives](https://arxiv.org/abs/2109.00087)

Authors: [Tuhin Chakrabarty](https://arxiv.org/search/cs?searchtype=author&query=Chakrabarty%2C+T), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Vered Shwartz](https://arxiv.org/search/cs?searchtype=author&query=Shwartz%2C+V)

> Figurative language is ubiquitous in English. Yet, the vast majority of NLP research focuses on literal language. Existing text representations by design rely on compositionality, while figurative language is often non-compositional. In this paper, we study the interpretation of two non-compositional figurative languages (idioms and similes). We collected datasets of fictional narratives containing a figurative expression along with crowd-sourced plausible and implausible continuations relying on the correct interpretation of the expression. We then trained models to choose or generate the plausible continuation. Our experiments show that models based solely on pre-trained language models perform substantially worse than humans on these tasks. We additionally propose knowledge-enhanced models, adopting human strategies for interpreting figurative language: inferring meaning from the context and relying on the constituent word's literal meanings. The knowledge-enhanced models improve the performance on both the discriminative and generative tasks, further bridging the gap from human performance.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.00087](https://arxiv.org/abs/2109.00087) [cs.CL]** |
|           | (or **[arXiv:2109.00087v1](https://arxiv.org/abs/2109.00087v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-3">3. Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast
</h2>

Title: [Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast](https://arxiv.org/abs/2109.00253)

Authors: [Liang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Wei Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Jingming Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> In this paper, we propose to align sentence representations from different languages into a unified embedding space, where semantic similarities (both cross-lingual and monolingual) can be computed with a simple dot product. Pre-trained language models are fine-tuned with the translation ranking task. Existing work (Feng et al., 2020) uses sentences within the same batch as negatives, which can suffer from the issue of easy negatives. We adapt MoCo (He et al., 2020) to further improve the quality of alignment. As the experimental results show, the sentence representations produced by our model achieve the new state-of-the-art on several tasks, including Tatoeba en-zh similarity search (Artetxe and Schwenk, 2019b), BUCC en-zh bitext mining, and semantic textual similarity on 7 datasets.

| Comments: | Accepted to EMNLP 2021 main conference                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Information Retrieval (cs.IR) |
| Cite as:  | **[arXiv:2109.00253](https://arxiv.org/abs/2109.00253) [cs.CL]** |
|           | (or **[arXiv:2109.00253v1](https://arxiv.org/abs/2109.00253v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-4">4. Discovering Representation Sprachbund For Multilingual Pre-Training
</h2>

Title: [Discovering Representation Sprachbund For Multilingual Pre-Training](https://arxiv.org/abs/2109.00271)

Authors: [Yimin Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+Y), [Yaobo Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y), [Alexandre Muzio](https://arxiv.org/search/cs?searchtype=author&query=Muzio%2C+A), [Hany Hassan](https://arxiv.org/search/cs?searchtype=author&query=Hassan%2C+H), [Houqiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M), [Nan Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N)

> Multilingual pre-trained models have demonstrated their effectiveness in many multilingual NLP tasks and enabled zero-shot or few-shot transfer from high-resource languages to low resource ones. However, due to significant typological differences and contradictions between some languages, such models usually perform poorly on many languages and cross-lingual settings, which shows the difficulty of learning a single model to handle massive diverse languages well at the same time. To alleviate this issue, we present a new multilingual pre-training pipeline. We propose to generate language representation from multilingual pre-trained models and conduct linguistic analysis to show that language representation similarity reflect linguistic similarity from multiple perspectives, including language family, geographical sprachbund, lexicostatistics and syntax. Then we cluster all the target languages into multiple groups and name each group as a representation sprachbund. Thus, languages in the same representation sprachbund are supposed to boost each other in both pre-training and fine-tuning as they share rich linguistic similarity. We pre-train one multilingual model for each representation sprachbund. Experiments are conducted on cross-lingual benchmarks and significant improvements are achieved compared to strong baselines.

| Comments: | To Appear at the Findings of EMNLP2021                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2109.00271](https://arxiv.org/abs/2109.00271) [cs.CL]** |
|           | (or **[arXiv:2109.00271v1](https://arxiv.org/abs/2109.00271v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-5">5. ∞-former: Infinite Memory Transformer
</h2>

Title: [∞-former: Infinite Memory Transformer](https://arxiv.org/abs/2109.00301)

Authors: [Pedro Henrique Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+P+H), [Zita Marinho](https://arxiv.org/search/cs?searchtype=author&query=Marinho%2C+Z), [André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T)

> Transformers struggle when attending to long contexts, since the amount of computation grows with the context length, and therefore they cannot model long-term memories effectively. Several variations have been proposed to alleviate this problem, but they all have a finite memory capacity, being forced to drop old information. In this paper, we propose the ∞-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the ∞-former's attention complexity becomes independent of the context length. Thus, it is able to model arbitrarily long contexts and maintain "sticky memories" while keeping a fixed computation budget. Experiments on a synthetic sorting task demonstrate the ability of the ∞-former to retain information from long sequences. We also perform experiments on language modeling, by training a model from scratch and by fine-tuning a pre-trained language model, which show benefits of unbounded long-term memories.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.00301](https://arxiv.org/abs/2109.00301) [cs.CL]** |
|           | (or **[arXiv:2109.00301v1](https://arxiv.org/abs/2109.00301v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-6">6. Masked Adversarial Generation for Neural Machine Translation
</h2>

Title: [Masked Adversarial Generation for Neural Machine Translation](https://arxiv.org/abs/2109.00417)

Authors: [Badr Youbi Idrissi](https://arxiv.org/search/cs?searchtype=author&query=Idrissi%2C+B+Y), [Stéphane Clinchant](https://arxiv.org/search/cs?searchtype=author&query=Clinchant%2C+S)

> Attacking Neural Machine Translation models is an inherently combinatorial task on discrete sequences, solved with approximate heuristics. Most methods use the gradient to attack the model on each sample independently. Instead of mechanically applying the gradient, could we learn to produce meaningful adversarial attacks ? In contrast to existing approaches, we learn to attack a model by training an adversarial generator based on a language model. We propose the Masked Adversarial Generation (MAG) model, that learns to perturb the translation model throughout the training process. The experiments show that it improves the robustness of machine translation models, while being faster than competing methods.

| Comments: | 5 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2109.00417](https://arxiv.org/abs/2109.00417) [cs.CL]** |
|           | (or **[arXiv:2109.00417v1](https://arxiv.org/abs/2109.00417v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-7">7. Position Masking for Improved Layout-Aware Document Understanding
</h2>

Title: [Position Masking for Improved Layout-Aware Document Understanding](https://arxiv.org/abs/2109.00442)

Authors: [Anik Saha](https://arxiv.org/search/cs?searchtype=author&query=Saha%2C+A), [Catherine Finegan-Dollak](https://arxiv.org/search/cs?searchtype=author&query=Finegan-Dollak%2C+C), [Ashish Verma](https://arxiv.org/search/cs?searchtype=author&query=Verma%2C+A)

> Natural language processing for document scans and PDFs has the potential to enormously improve the efficiency of business processes. Layout-aware word embeddings such as LayoutLM have shown promise for classification of and information extraction from such documents. This paper proposes a new pre-training task called that can improve performance of layout-aware word embeddings that incorporate 2-D position embeddings. We compare models pre-trained with only language masking against models pre-trained with both language masking and position masking, and we find that position masking improves performance by over 5% on a form understanding task.

| Comments: | Document Intelligence Workshop at KDD, 2021                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2109.00442](https://arxiv.org/abs/2109.00442) [cs.CL]** |
|           | (or **[arXiv:2109.00442v1](https://arxiv.org/abs/2109.00442v1) [cs.CL]** for this version) |





<h2 id="2021-09-02-8">8. Survey of Low-Resource Machine Translation
</h2>

Title: [Survey of Low-Resource Machine Translation](https://arxiv.org/abs/2109.00486)

Authors: [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B), [Rachel Bawden](https://arxiv.org/search/cs?searchtype=author&query=Bawden%2C+R), [Antonio Valerio Miceli Barone](https://arxiv.org/search/cs?searchtype=author&query=Barone%2C+A+V+M), [Jindřich Helcl](https://arxiv.org/search/cs?searchtype=author&query=Helcl%2C+J), [Alexandra Birch](https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A)

> We present a survey covering the state of the art in low-resource machine translation. There are currently around 7000 languages spoken in the world and almost all language pairs lack significant resources for training machine translation models. There has been increasing interest in research addressing the challenge of producing useful translation models when very little translated training data is available. We present a high level summary of this topical field and provide an overview of best practices.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2109.00486](https://arxiv.org/abs/2109.00486) [cs.CL]** |
|           | (or **[arXiv:2109.00486v1](https://arxiv.org/abs/2109.00486v1) [cs.CL]** for this version) |








# 2021-09-01

[Return to Index](#Index)



<h2 id="2021-09-01-1">1. SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory
</h2>


Title: [SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory](https://arxiv.org/abs/2108.13630)

Authors: [Zhijie Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Zhou Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Haoyuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H), [Jinglin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Meng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Xingshan Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X), [Xiaofei He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X)

> Lip reading, aiming to recognize spoken sentences according to the given video of lip movements without relying on the audio stream, has attracted great interest due to its application in many scenarios. Although prior works that explore lip reading have obtained salient achievements, they are all trained in a non-simultaneous manner where the predictions are generated requiring access to the full video. To breakthrough this constraint, we study the task of simultaneous lip reading and devise SimulLR, a simultaneous lip Reading transducer with attention-guided adaptive memory from three aspects: (1) To address the challenge of monotonic alignments while considering the syntactic structure of the generated sentences under simultaneous setting, we build a transducer-based model and design several effective training strategies including CTC pre-training, model warm-up and curriculum learning to promote the training of the lip reading transducer. (2) To learn better spatio-temporal representations for simultaneous encoder, we construct a truncated 3D convolution and time-restricted self-attention layer to perform the frame-to-frame interaction within a video segment containing fixed number of frames. (3) The history information is always limited due to the storage in real-time scenarios, especially for massive video data. Therefore, we devise a novel attention-guided adaptive memory to organize semantic information of history segments and enhance the visual representations with acceptable computation-aware latency. The experiments show that the SimulLR achieves the translation speedup 9.10× compared with the state-of-the-art non-simultaneous methods, and also obtains competitive results, which indicates the effectiveness of our proposed methods.

| Comments: | ACMMM 2021                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2108.13630](https://arxiv.org/abs/2108.13630) [cs.CV]** |
|           | (or **[arXiv:2108.13630v1](https://arxiv.org/abs/2108.13630v1) [cs.CV]** for this version) |





<h2 id="2021-09-01-2">2. Want To Reduce Labeling Cost? GPT-3 Can Help
</h2>


Title: [Want To Reduce Labeling Cost? GPT-3 Can Help](https://arxiv.org/abs/2108.13487)

Authors: [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Yichong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M)

> Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 175 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that, to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance with limited labeling budget. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.

| Comments: | Findings of EMNLP 2021, 11 pages                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2108.13487](https://arxiv.org/abs/2108.13487) [cs.CL]** |
|           | (or **[arXiv:2108.13487v1](https://arxiv.org/abs/2108.13487v1) [cs.CL]** for this version) |





<h2 id="2021-09-01-3">3. T3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP
</h2>


Title: [T3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP](https://arxiv.org/abs/2108.13587)

Authors: [Raymond Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+R) (1), [Wen Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+W) (1), [Lanjun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L) (2), [Hyeju Jang](https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+H) (1), [Giuseppe Carenini](https://arxiv.org/search/cs?searchtype=author&query=Carenini%2C+G) (1) ((1) University of British Columbia, (2) Huawei Cananda Technologies Co. Ltd.)

> Transformers are the dominant architecture in NLP, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the model's intrinsic properties and behaviours. Our framework offers an intuitive overview that allows the user to explore different facets of the model (e.g., hidden states, attention) through interactive visualization, and allows a suite of built-in algorithms that compute the importance of model components and different parts of the input sequence. Case studies and feedback from a user focus group indicate that the framework is useful, and suggest several improvements.

| Comments: | 10 pages, 4 figures, accepted to EMNLP 2021 System Demonstration |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2108.13587](https://arxiv.org/abs/2108.13587) [cs.CL]** |
|           | (or **[arXiv:2108.13587v1](https://arxiv.org/abs/2108.13587v1) [cs.CL]** for this version) |





<h2 id="2021-09-01-4">4. Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience
</h2>


Title: [Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience](https://arxiv.org/abs/2108.13759)

Authors: [George Chrysostomou](https://arxiv.org/search/cs?searchtype=author&query=Chrysostomou%2C+G), [Nikolaos Aletras](https://arxiv.org/search/cs?searchtype=author&query=Aletras%2C+N)

> Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this paper, we hypothesize that salient information extracted a priori from the training data can complement the task-specific information learned by the model during fine-tuning on a downstream task. In this way, we aim to help BERT not to forget assigning importance to informative input tokens when making predictions by proposing SaLoss; an auxiliary loss function for guiding the multi-head attention mechanism during training to be close to salient information extracted a priori using TextRank. Experiments for explanation faithfulness across five datasets, show that models trained with SaLoss consistently provide more faithful explanations across four different feature attribution methods compared to vanilla BERT. Using the rationales extracted from vanilla BERT and SaLoss models to train inherently faithful classifiers, we further show that the latter result in higher predictive performance in downstream tasks.

| Comments: | EMNLP 2021 Pre-print                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.13759](https://arxiv.org/abs/2108.13759) [cs.CL]** |
|           | (or **[arXiv:2108.13759v1](https://arxiv.org/abs/2108.13759v1) [cs.CL]** for this version) |





<h2 id="2021-09-01-5">5. Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools
</h2>


Title: [Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools](https://arxiv.org/abs/2108.13961)

Authors: [Nils Feldhus](https://arxiv.org/search/cs?searchtype=author&query=Feldhus%2C+N), [Robert Schwarzenberg](https://arxiv.org/search/cs?searchtype=author&query=Schwarzenberg%2C+R), [Sebastian Möller](https://arxiv.org/search/cs?searchtype=author&query=Möller%2C+S)

> In the language domain, as in other domains, neural explainability takes an ever more important role, with feature attribution methods on the forefront. Many such methods require considerable computational resources and expert knowledge about implementation details and parameter choices. To facilitate research, we present Thermostat which consists of a large collection of model explanations and accompanying analysis tools. Thermostat allows easy access to over 200k explanations for the decisions of prominent state-of-the-art models spanning across different NLP tasks, generated with multiple explainers. The dataset took over 10k GPU hours (> one year) to compile; compute time that the community now saves. The accompanying software tools allow to analyse explanations instance-wise but also accumulatively on corpus level. Users can investigate and compare models, datasets and explainers without the need to orchestrate implementation details. Thermostat is fully open source, democratizes explainability research in the language domain, circumvents redundant computations and increases comparability and replicability.

| Comments: | Accepted to EMNLP 2021 System Demonstrations                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2108.13961](https://arxiv.org/abs/2108.13961) [cs.CL]** |
|           | (or **[arXiv:2108.13961v1](https://arxiv.org/abs/2108.13961v1) [cs.CL]** for this version) |




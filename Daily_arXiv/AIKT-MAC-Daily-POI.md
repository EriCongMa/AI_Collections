# MA C.'s Daily Paper Of Interest - November, 2021

# Index


- [2021-11-23](#2021-11-23)

  - [1. L-Verse: Bidirectional Generation Between Image and Text](#2021-11-23-1)
  - [2. Data Processing Matters: SRPH-Konvergen AI's Machine Translation System for WMT'21](#2021-11-23-2)
  - [3. RDF-to-Text Generation with Reinforcement Learning Based Graph-augmented Structural Neural Encoders](#2021-11-23-3)
  - [4. Capitalization and Punctuation Restoration: a Survey](#2021-11-23-4)
  - [5. TraVLR: Now You See It, Now You Don't! Evaluating Cross-Modal Transfer of Visio-Linguistic Reasoning](#2021-11-23-5)
  - [6. ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning](#2021-11-23-6)
  - [7. Knowledge Based Multilingual Language Model](#2021-11-23-7)
  
- [2021-11-22](#2021-11-22)

  - [1. Combined Scaling for Zero-shot Transfer Learning](#2021-11-22-1)
  - [2. Lattention: Lattice-attention in ASR rescoring](#2021-11-22-2)

- [2021-11-19](#2021-11-19)
  - [1. Minimum Bayes Risk Decoding with Neural Metrics of Translation Quality](#2021-11-19-1)
  - [2. SDCUP: Schema Dependency-Enhanced Curriculum Pre-Training for Table Semantic Parsing](#2021-11-19-2)
  - [3. To Augment or Not to Augment? A Comparative Study on Text Augmentation Techniques for Low-Resource NLP](#2021-11-19-3)
  - [4. Dynamic-TinyBERT: Boost TinyBERT's Inference Efficiency by Dynamic Sequence Length](#2021-11-19-4)
- [2021-11-18](#2021-11-18)
  - [1. Transparent Human Evaluation for Image Captioning](#2021-11-18-1)
  - [2. Character Transformations for Non-Autoregressive GEC Tagging](#2021-11-18-2)
  - [3. XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](#2021-11-18-3)
- [2021-11-17](#2021-11-17)
  - [1. Joint Unsupervised and Supervised Training for Multilingual ASR](#2021-11-17-1)
  - [2. Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts](#2021-11-17-2)
  - [3. Few-Shot Self-Rationalization with Natural Language Prompts](#2021-11-17-3)
  - [4. Integrated Semantic and Phonetic Post-correction for Chinese Speech Recognition](#2021-11-17-4)
  - [5. Generative Pre-Trained Transformer for Design Concept Generation: An Exploration](#2021-11-17-5)
  - [6. CVSS-BERT: Explainable Natural Language Processing to Determine the Severity of a Computer Security Vulnerability from its Description](#2021-11-17-6)
  - [7. NVIDIA NeMo Neural Machine Translation Systems for English-German and English-Russian News and Biomedical Tasks at WMT21](#2021-11-17-7)
  - [8. Document AI: Benchmarks, Models and Applications](#2021-11-17-8)
- [2021-11-16](#2021-11-16)

  - [1. Curriculum Learning for Vision-and-Language Navigation](#2021-11-16-1)
  - [2. LiT: Zero-Shot Transfer with Locked-image Text Tuning](#2021-11-16-2)
  - [3. Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning](#2021-11-16-3)
  - [4. DEEP: DEnoising Entity Pre-training for Neural Machine Translation](#2021-11-16-4)
  - [5. Time Waits for No One! Analysis and Challenges of Temporal Misalignment](#2021-11-16-5)
  - [6. Measuring Uncertainty in Translation Quality Evaluation (TQE)](#2021-11-16-6)
  - [7. Data Augmentation for Speech Recognition in Maltese: A Low-Resource Perspective](#2021-11-16-7)
  - [8. Evaluating Metrics for Bias in Word Embeddings](#2021-11-16-8)
  - [9. CoLLIE: Continual Learning of Language Grounding from Language-Image Embeddings](#2021-11-16-9)
- [2021-11-15](#2021-11-15)

  - [1. On Transferability of Prompt Tuning for Natural Language Understanding](#2021-11-15-1)
  - [2. BitextEdit: Automatic Bitext Editing for Improved Low-Resource Machine Translation](#2021-11-15-2)
  - [3. Deciphering Speech: a Zero-Resource Approach to Cross-Lingual Transfer in ASR](#2021-11-15-3)
- [2021-11-12](#2021-11-12)
  - [1. Self-Normalized Importance Sampling for Neural Language Modeling](#2021-11-12-1)
- [2021-11-11](#2021-11-11)

  - [1. MNet-Sim: A Multi-layered Semantic Similarity Network to Evaluate Sentence Similarity](#2021-11-11-1)
  - [2. Prune Once for All: Sparse Pre-Trained Language Models](#2021-11-11-2)
  - [3. Cross-lingual Adaption Model-Agnostic Meta-Learning for Natural Language Understanding](#2021-11-11-3)
- [2021-11-10](#2021-11-10)
  - [1. Speaker Generation](#2021-11-10-1)
  - [2. A Survey on Green Deep Learning](#2021-11-10-2)
  - [3. FPM: A Collection of Large-scale Foundation Pre-trained Language Models](#2021-11-10-3)
- [2021-11-09](#2021-11-09)

  - [1. Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling](#2021-11-09-1)
  - [2. Analyzing Architectures for Neural Machine Translation Using Low Computational Resources](#2021-11-09-2)
  - [3. Information Extraction from Visually Rich Documents with Font Style Embeddings](#2021-11-09-3)
  - [4. Variance-Aware Machine Translation Test Sets](#2021-11-09-4)
  - [5. Developing neural machine translation models for Hungarian-English](#2021-11-09-5)
  - [6. NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework](#2021-11-09-6)
  - [7. Machine-in-the-Loop Rewriting for Creative Image Captioning](#2021-11-09-7)
  - [8. TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning](#2021-11-09-8)
- [2021-11-08](#2021-11-08)

  - [1. StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis](#2021-11-08-1)
  - [2. The Curious Layperson: Fine-Grained Image Recognition without Expert Labels](#2021-11-08-2)
  - [3. How Do Neural Sequence Models Generalize? Local and Global Context Cues for Out-of-Distribution Prediction](#2021-11-08-3)
  - [4. A Syntax-Guided Grammatical Error Correction Model with Dependency Tree Correction](#2021-11-08-4)
- [2021-11-05](#2021-11-05)
  - [1. Benchmarking Multimodal AutoML for Tabular Data with Text Fields](#2021-11-05-1)
  - [2. Lexically Aware Semi-Supervised Learning for OCR Post-Correction](#2021-11-05-2)
  - [3. Response Generation with Context-Aware Prompt Learning](#2021-11-05-3)
  - [4. A text autoencoder from transformer for fast encoding language representation](#2021-11-05-4)
  - [5. CoreLM: Coreference-aware Language Model Fine-Tuning](#2021-11-05-5)
- [2021-11-04](#2021-11-04)

  - [1. LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs](#2021-11-04-1)
  - [2. VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts](#2021-11-04-2)
  - [3. An Empirical Study of Training End-to-End Vision-and-Language Transformers](#2021-11-04-3)
  - [4. OpenPrompt: An Open-source Framework for Prompt-learning](#2021-11-04-4)
  - [5. Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task](#2021-11-04-5)
  - [6. Lingua Custodia's participation at the WMT 2021 Machine Translation using Terminologies shared task](#2021-11-04-6)
  - [7. BERT-DRE: BERT with Deep Recursive Encoder for Natural Language Sentence Matching](#2021-11-04-7)
- [2021-11-03](#2021-11-03)

  - [1. Recent Advances in End-to-End Automatic Speech Recognition](#2021-11-03-1)
  - [2. Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey](#2021-11-03-2)
  - [3. Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP](#2021-11-03-3)
  - [4. Adapting to the Long Tail: A Meta-Analysis of Transfer Learning Research for Language Understanding Tasks](#2021-11-03-4)
  - [5. System Combination for Grammatical Error Correction Based on Integer Programming](#2021-11-03-5)
  - [6. Zero-Shot Translation using Diffusion Models](#2021-11-03-6)
  - [7. HydraText: Multi-objective Optimization for Adversarial Textual Attack](#2021-11-03-7)
  - [8. LMdiff: A Visual Diff Tool to Compare Language Models](#2021-11-03-8)
- [2021-11-02](#2021-11-02)
  - [1. Introspective Distillation for Robust Question Answering](#2021-11-02-1)
  - [2. TransAug: Translate as Augmentation for Sentence Embeddings](#2021-11-02-2)
  - [3. How should human translation coexist with NMT? Efficient tool for building high quality parallel corpus](#2021-11-02-3)
  - [4. Visualization: the missing factor in Simultaneous Speech Translation](#2021-11-02-4)
  - [5. Quality Estimation Using Round-trip Translation with Sentence Embeddings](#2021-11-02-5)
  - [6. Unsupervised Domain Adaptation with Adapter](#2021-11-02-6)
  - [7. Interpretable contrastive word mover's embedding](#2021-11-02-7)
- [2021-11-01](#2021-11-01)
  - [1. Decision Attentive Regularization to Improve Simultaneous Speech Translation Systems](#2021-11-01-1)
  - [2. Analysing the Effect of Masking Length Distribution of MLM: An Evaluation Framework and Case Study on Chinese MRC Datasets](#2021-11-01-2)
  - [3. Analysing the Effect of Masking Length Distribution of MLM: An Evaluation Framework and Case Study on Chinese MRC Datasets](#2021-11-01-3)
  - [4. Batch-Softmax Contrastive Loss for Pairwise Sentence Scoring Tasks](#2021-11-01-4)
  - [5. BERMo: What can BERT learn from ELMo?](#2021-11-01-5)
  - [6. MetaICL: Learning to Learn In Context](#2021-11-01-6)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MAC-Daily-POI-index.md)



# 2021-11-23

[Return to Index](#Index)



<h2 id="2021-11-23-1">1. L-Verse: Bidirectional Generation Between Image and Text
</h2>

Title: [L-Verse: Bidirectional Generation Between Image and Text](https://arxiv.org/abs/2111.11133)

Authors: [Taehoon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+T), [Gwangmo Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+G), [Sihaeng Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S), [Sangyun Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Yewon Seo](https://arxiv.org/search/cs?searchtype=author&query=Seo%2C+Y), [Soonyoung Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S), [Seung Hwan Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S+H), [Honglak Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H), [Kyunghoon Bae](https://arxiv.org/search/cs?searchtype=author&query=Bae%2C+K)

> Far beyond learning long-range interactions of natural language, transformers are becoming the de-facto standard for many vision tasks with their power and scalabilty. Especially with cross-modal tasks between image and text, vector quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB image into a sequence of feature vectors. To better leverage the correlation between image and text, we propose L-Verse, a novel architecture consisting of feature-augmented variational autoencoder (AugVAE) and bidirectional auto-regressive transformer (BiART) for text-to-image and image-to-text generation. Our AugVAE shows the state-of-the-art reconstruction performance on ImageNet1K validation set, along with the robustness to unseen images in the wild. Unlike other models, BiART can distinguish between image (or text) as a conditional reference and a generation target. L-Verse can be directly used for image-to-text or text-to-image generation tasks without any finetuning or extra object detection frameworks. In quantitative and qualitative experiments, L-Verse shows impressive results against previous methods in both image-to-text and text-to-image generation on MS-COCO Captions. We furthermore assess the scalability of L-Verse architecture on Conceptual Captions and present the initial results of bidirectional vision-language representation learning on general domain. Codes available at: [this https URL](https://github.com/tgisaturday/L-Verse)

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.11133](https://arxiv.org/abs/2111.11133) [cs.CV]** |
|           | (or **[arXiv:2111.11133v1](https://arxiv.org/abs/2111.11133v1) [cs.CV]** for this version) |





<h2 id="2021-11-23-2">2. Data Processing Matters: SRPH-Konvergen AI's Machine Translation System for WMT'21
</h2>

Title: [Data Processing Matters: SRPH-Konvergen AI's Machine Translation System for WMT'21](https://arxiv.org/abs/2111.10513)

Authors: [Lintang Sutawika](https://arxiv.org/search/cs?searchtype=author&query=Sutawika%2C+L), [Jan Christian Blaise Cruz](https://arxiv.org/search/cs?searchtype=author&query=Cruz%2C+J+C+B)

> In this paper, we describe the submission of the joint Samsung Research Philippines-Konvergen AI team for the WMT'21 Large Scale Multilingual Translation Task - Small Track 2. We submit a standard Seq2Seq Transformer model to the shared task without any training or architecture tricks, relying mainly on the strength of our data preprocessing techniques to boost performance. Our final submission model scored 22.92 average BLEU on the FLORES-101 devtest set, and scored 22.97 average BLEU on the contest's hidden test set, ranking us sixth overall. Despite using only a standard Transformer, our model ranked first in Indonesian to Javanese, showing that data preprocessing matters equally, if not more, than cutting edge model architectures and training techniques.

| Comments: | In Proceedings of the Sixth Conference on Machine Translation (WMT) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.10513](https://arxiv.org/abs/2111.10513) [cs.CL]** |
|           | (or **[arXiv:2111.10513v1](https://arxiv.org/abs/2111.10513v1) [cs.CL]** for this version) |





<h2 id="2021-11-23-3">3. RDF-to-Text Generation with Reinforcement Learning Based Graph-augmented Structural Neural Encoders
</h2>

Title: [RDF-to-Text Generation with Reinforcement Learning Based Graph-augmented Structural Neural Encoders](https://arxiv.org/abs/2111.10545)

Authors: [Hanning Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+H), [Lingfei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Po Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+P), [Zhihua Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Z), [Fangli Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+F), [Bo Long](https://arxiv.org/search/cs?searchtype=author&query=Long%2C+B)

> Considering a collection of RDF triples, the RDF-to-text generation task aims to generate a text description. Most previous methods solve this task using a sequence-to-sequence model or using a graph-based model to encode RDF triples and to generate a text sequence. Nevertheless, these approaches fail to clearly model the local and global structural information between and within RDF triples. Moreover, the previous methods also face the non-negligible problem of low faithfulness of the generated text, which seriously affects the overall performance of these models. To solve these problems, we propose a model combining two new graph-augmented structural neural encoders to jointly learn both local and global structural information in the input RDF triples. To further improve text faithfulness, we innovatively introduce a reinforcement learning (RL) reward based on information extraction (IE). We first extract triples from the generated text using a pretrained IE model and regard the correct number of the extracted triples as the additional RL reward. Experimental results on two benchmark datasets demonstrate that our proposed model outperforms the state-of-the-art baselines, and the additional reinforcement learning reward does help to improve the faithfulness of the generated text.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.10545](https://arxiv.org/abs/2111.10545) [cs.CL]** |
|           | (or **[arXiv:2111.10545v1](https://arxiv.org/abs/2111.10545v1) [cs.CL]** for this version) |





<h2 id="2021-11-23-4">4. Capitalization and Punctuation Restoration: a Survey
</h2>

Title: [Capitalization and Punctuation Restoration: a Survey](https://arxiv.org/abs/2111.10746)

Authors: [Vasile Păiş](https://arxiv.org/search/cs?searchtype=author&query=Păiş%2C+V), [Dan Tufiş](https://arxiv.org/search/cs?searchtype=author&query=Tufiş%2C+D)

> Ensuring proper punctuation and letter casing is a key pre-processing step towards applying complex natural language processing algorithms. This is especially significant for textual sources where punctuation and casing are missing, such as the raw output of automatic speech recognition systems. Additionally, short text messages and micro-blogging platforms offer unreliable and often wrong punctuation and casing. This survey offers an overview of both historical and state-of-the-art techniques for restoring punctuation and correcting word casing. Furthermore, current challenges and research directions are highlighted.

| Comments:          | An improved version of this paper was published in Artificial Intelligence Review. This is the article version prior to any reviewer comments and improvements |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | Păiş, V., Tufiş, D. Capitalization and punctuation restoration: a survey. Artif Intell Rev (2021) |
| DOI:               | [10.1007/s10462-021-10051-x](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1007%2Fs10462-021-10051-x&v=0c2611e9) |
| Cite as:           | **[arXiv:2111.10746](https://arxiv.org/abs/2111.10746) [cs.CL]** |
|                    | (or **[arXiv:2111.10746v1](https://arxiv.org/abs/2111.10746v1) [cs.CL]** for this version) |





<h2 id="2021-11-23-5">5. TraVLR: Now You See It, Now You Don't! Evaluating Cross-Modal Transfer of Visio-Linguistic Reasoning
</h2>

Title: [TraVLR: Now You See It, Now You Don't! Evaluating Cross-Modal Transfer of Visio-Linguistic Reasoning](https://arxiv.org/abs/2111.10756)

Authors: [Keng Ji Chow](https://arxiv.org/search/cs?searchtype=author&query=Chow%2C+K+J), [Samson Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+S), [Min-Yen Kan](https://arxiv.org/search/cs?searchtype=author&query=Kan%2C+M)

> Numerous visio-linguistic (V+L) representation learning methods have been developed, yet existing datasets do not evaluate the extent to which they represent visual and linguistic concepts in a unified space. Inspired by the crosslingual transfer and psycholinguistics literature, we propose a novel evaluation setting for V+L models: zero-shot cross-modal transfer. Existing V+L benchmarks also often report global accuracy scores on the entire dataset, rendering it difficult to pinpoint the specific reasoning tasks that models fail and succeed at. To address this issue and enable the evaluation of cross-modal transfer, we present TraVLR, a synthetic dataset comprising four V+L reasoning tasks. Each example encodes the scene bimodally such that either modality can be dropped during training/testing with no loss of relevant information. TraVLR's training and testing distributions are also constrained along task-relevant dimensions, enabling the evaluation of out-of-distribution generalisation. We evaluate four state-of-the-art V+L models and find that although they perform well on the test set from the same modality, all models fail to transfer cross-modally and have limited success accommodating the addition or deletion of one modality. In alignment with prior work, we also find these models to require large amounts of data to learn simple spatial relationships. We release TraVLR as an open challenge for the research community.

| Comments: | The first two authors contributed equally                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.10756](https://arxiv.org/abs/2111.10756) [cs.CL]** |
|           | (or **[arXiv:2111.10756v1](https://arxiv.org/abs/2111.10756v1) [cs.CL]** for this version) |







<h2 id="2021-11-23-6">6. ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning
</h2>

Title: [ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning](https://arxiv.org/abs/2111.10952)

Authors: [Vamsi Aribandi](https://arxiv.org/search/cs?searchtype=author&query=Aribandi%2C+V), [Yi Tay](https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y), [Tal Schuster](https://arxiv.org/search/cs?searchtype=author&query=Schuster%2C+T), [Jinfeng Rao](https://arxiv.org/search/cs?searchtype=author&query=Rao%2C+J), [Huaixiu Steven Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H+S), [Sanket Vaibhav Mehta](https://arxiv.org/search/cs?searchtype=author&query=Mehta%2C+S+V), [Honglei Zhuang](https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+H), [Vinh Q. Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+V+Q), [Dara Bahri](https://arxiv.org/search/cs?searchtype=author&query=Bahri%2C+D), [Jianmo Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+J), [Jai Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+J), [Kai Hui](https://arxiv.org/search/cs?searchtype=author&query=Hui%2C+K), [Sebastian Ruder](https://arxiv.org/search/cs?searchtype=author&query=Ruder%2C+S), [Donald Metzler](https://arxiv.org/search/cs?searchtype=author&query=Metzler%2C+D)

> Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExMix (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using ExMix, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose ExT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised ExMix. Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of ExMix. ExT5 also significantly improves sample efficiency while pre-training.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.10952](https://arxiv.org/abs/2111.10952) [cs.CL]** |
|           | (or **[arXiv:2111.10952v1](https://arxiv.org/abs/2111.10952v1) [cs.CL]** for this version) |





<h2 id="2021-11-23-7">7. Knowledge Based Multilingual Language Model
</h2>

Title: [Knowledge Based Multilingual Language Model](https://arxiv.org/abs/2111.10962)

Authors: [Linlin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L), [Xin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Ruidan He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+R), [Lidong Bing](https://arxiv.org/search/cs?searchtype=author&query=Bing%2C+L), [Shafiq Joty](https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S), [Luo Si](https://arxiv.org/search/cs?searchtype=author&query=Si%2C+L)

> Knowledge enriched language representation learning has shown promising performance across various knowledge-intensive NLP tasks. However, existing knowledge based language models are all trained with monolingual knowledge graph data, which limits their application to more languages. In this work, we present a novel framework to pretrain knowledge based multilingual language models (KMLMs). We first generate a large amount of code-switched synthetic sentences and reasoning-based multilingual training data using the Wikidata knowledge graphs. Then based on the intra- and inter-sentence structures of the generated data, we design pretraining tasks to facilitate knowledge learning, which allows the language models to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual NLP tasks, including named entity recognition, factual knowledge retrieval, relation classification, and a new task designed by us, namely, logic reasoning. Our code and pretrained language models will be made publicly available.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.10962](https://arxiv.org/abs/2111.10962) [cs.CL]** |
|           | (or **[arXiv:2111.10962v1](https://arxiv.org/abs/2111.10962v1) [cs.CL]** for this version) |





# 2021-11-22

[Return to Index](#Index)



<h2 id="2021-11-22-1">1. Combined Scaling for Zero-shot Transfer Learning
</h2>

Title: [Combined Scaling for Zero-shot Transfer Learning](https://arxiv.org/abs/2111.10050)

Authors: [Hieu Pham](https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+H), [Zihang Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+Z), [Golnaz Ghiasi](https://arxiv.org/search/cs?searchtype=author&query=Ghiasi%2C+G), [Hanxiao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H), [Adams Wei Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+A+W), [Minh-Thang Luong](https://arxiv.org/search/cs?searchtype=author&query=Luong%2C+M), [Mingxing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V)

> We present a combined scaling method called BASIC that achieves 85.7% top-1 zero-shot accuracy on the ImageNet ILSVRC-2012 validation set, surpassing the best-published zero-shot models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 83.7% top-1 average accuracy, only a small drop from the its original ImageNet accuracy. 
> To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN. The main challenge with scaling is the limited memory of our accelerators such as GPUs and TPUs. We hence propose a simple method of online gradient caching to overcome this limit.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.10050](https://arxiv.org/abs/2111.10050) [cs.LG]** |
|           | (or **[arXiv:2111.10050v1](https://arxiv.org/abs/2111.10050v1) [cs.LG]** for this version) |







<h2 id="2021-11-22-2">2. Lattention: Lattice-attention in ASR rescoring
</h2>

Title: [Lattention: Lattice-attention in ASR rescoring](https://arxiv.org/abs/2111.10157)

Authors: [Prabhat Pandey](https://arxiv.org/search/cs?searchtype=author&query=Pandey%2C+P), [Sergio Duarte Torres](https://arxiv.org/search/cs?searchtype=author&query=Torres%2C+S+D), [Ali Orkan Bayer](https://arxiv.org/search/cs?searchtype=author&query=Bayer%2C+A+O), [Ankur Gandhe](https://arxiv.org/search/cs?searchtype=author&query=Gandhe%2C+A), [Volker Leutnant](https://arxiv.org/search/cs?searchtype=author&query=Leutnant%2C+V)

> Lattices form a compact representation of multiple hypotheses generated from an automatic speech recognition system and have been shown to improve performance of downstream tasks like spoken language understanding and speech translation, compared to using one-best hypothesis. In this work, we look into the effectiveness of lattice cues for rescoring n-best lists in second-pass. We encode lattices with a recurrent network and train an attention encoder-decoder model for n-best rescoring. The rescoring model with attention to lattices achieves 4-5% relative word error rate reduction over first-pass and 6-8% with attention to both lattices and acoustic features. We show that rescoring models with attention to lattices outperform models with attention to n-best hypotheses. We also study different ways to incorporate lattice weights in the lattice encoder and demonstrate their importance for n-best rescoring.

| Comments:    | Submitted to ICASSP 2022                                     |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2111.10157](https://arxiv.org/abs/2111.10157) [cs.CL]** |
|              | (or **[arXiv:2111.10157v1](https://arxiv.org/abs/2111.10157v1) [cs.CL]** for this version) |



# 2021-11-19

[Return to Index](#Index)



<h2 id="2021-11-19-1">1. Minimum Bayes Risk Decoding with Neural Metrics of Translation Quality
</h2>

Title: [Minimum Bayes Risk Decoding with Neural Metrics of Translation Quality](https://arxiv.org/abs/2111.09388)

Authors:[Markus Freitag](https://arxiv.org/search/cs?searchtype=author&query=Freitag%2C+M), [David Grangier](https://arxiv.org/search/cs?searchtype=author&query=Grangier%2C+D), [Qijun Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Q), [Bowen Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+B)

> This work applies Minimum Bayes Risk (MBR) decoding to optimize diverse automated metrics of translation quality. Automatic metrics in machine translation have made tremendous progress recently. In particular, neural metrics, fine-tuned on human ratings (e.g. BLEURT, or COMET) are outperforming surface metrics in terms of correlations to human judgements. Our experiments show that the combination of a neural translation model with a neural reference-based metric, BLEURT, results in significant improvement in automatic and human evaluations. This improvement is obtained with translations different from classical beam-search output: these translations have much lower likelihood and are less favored by surface metrics like BLEU.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.09388](https://arxiv.org/abs/2111.09388) [cs.CL]** |
|           | (or **[arXiv:2111.09388v1](https://arxiv.org/abs/2111.09388v1) [cs.CL]** for this version) |





<h2 id="2021-11-19-2">2. SDCUP: Schema Dependency-Enhanced Curriculum Pre-Training for Table Semantic Parsing
</h2>

Title: [SDCUP: Schema Dependency-Enhanced Curriculum Pre-Training for Table Semantic Parsing](https://arxiv.org/abs/2111.09486)

Authors: [Bowen Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+B), [Lihan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Binyuan Hui](https://arxiv.org/search/cs?searchtype=author&query=Hui%2C+B), [Ruiying Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+R), [Zheng Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Z), [Min Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+M), [Jian Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J), [Yongbin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y)

> Recently pre-training models have significantly improved the performance of various NLP tasks by leveraging large-scale text corpora to improve the contextual representation ability of the neural network. The large pre-training language model has also been applied in the area of table semantic parsing. However, existing pre-training approaches have not carefully explored explicit interaction relationships between a question and the corresponding database schema, which is a key ingredient for uncovering their semantic and structural correspondence. Furthermore, the question-aware representation learning in the schema grounding context has received less attention in pre-training [this http URL](http://objective.to/) alleviate these issues, this paper designs two novel pre-training objectives to impose the desired inductive bias into the learned representations for table pre-training. We further propose a schema-aware curriculum learning approach to mitigate the impact of noise and learn effectively from the pre-training data in an easy-to-hard manner. We evaluate our pre-trained framework by fine-tuning it on two benchmarks, Spider and SQUALL. The results demonstrate the effectiveness of our pre-training objective and curriculum compared to a variety of baselines.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.09486](https://arxiv.org/abs/2111.09486) [cs.CL]** |
|           | (or **[arXiv:2111.09486v1](https://arxiv.org/abs/2111.09486v1) [cs.CL]** for this version) |



<h2 id="2021-11-19-3">3. To Augment or Not to Augment? A Comparative Study on Text Augmentation Techniques for Low-Resource NLP
</h2>

Title: [To Augment or Not to Augment? A Comparative Study on Text Augmentation Techniques for Low-Resource NLP](https://arxiv.org/abs/2111.09618)

Authors: [Gözde Gül Şahin](https://arxiv.org/search/cs?searchtype=author&query=Şahin%2C+G+G)

> Data-hungry deep neural networks have established themselves as the standard for many NLP tasks including the traditional sequence tagging ones. Despite their state-of-the-art performance on high-resource languages, they still fall behind of their statistical counter-parts in low-resource scenarios. One methodology to counter attack this problem is text augmentation, i.e., generating new synthetic training data points from existing data. Although NLP has recently witnessed a load of textual augmentation techniques, the field still lacks a systematic performance analysis on a diverse set of languages and sequence tagging tasks. To fill this gap, we investigate three categories of text augmentation methodologies which perform changes on the syntax (e.g., cropping sub-sentences), token (e.g., random word insertion) and character (e.g., character swapping) levels. We systematically compare them on part-of-speech tagging, dependency parsing and semantic role labeling for a diverse set of language families using various models including the architectures that rely on pretrained multilingual contextualized language models such as mBERT. Augmentation most significantly improves dependency parsing, followed by part-of-speech tagging and semantic role labeling. We find the experimented techniques to be effective on morphologically rich languages in general rather than analytic languages such as Vietnamese. Our results suggest that the augmentation techniques can further improve over strong baselines based on mBERT. We identify the character-level methods as the most consistent performers, while synonym replacement and syntactic augmenters provide inconsistent improvements. Finally, we discuss that the results most heavily depend on the task, language pair, and the model type.

| Comments: | Accepted to Computational Linguistics                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2111.09618](https://arxiv.org/abs/2111.09618) [cs.CL]** |
|           | (or **[arXiv:2111.09618v1](https://arxiv.org/abs/2111.09618v1) [cs.CL]** for this version) |



<h2 id="2021-11-19-4">4. Dynamic-TinyBERT: Boost TinyBERT's Inference Efficiency by Dynamic Sequence Length
</h2>

Title: [Dynamic-TinyBERT: Boost TinyBERT's Inference Efficiency by Dynamic Sequence Length](https://arxiv.org/abs/2111.09645)

Authors:[Shira Guskin](https://arxiv.org/search/cs?searchtype=author&query=Guskin%2C+S), [Moshe Wasserblat](https://arxiv.org/search/cs?searchtype=author&query=Wasserblat%2C+M), [Ke Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+K), [Gyuwan Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+G)

> Limited computational budgets often prevent transformers from being used in production and from having their high accuracy utilized. TinyBERT addresses the computational efficiency by self-distilling BERT into a smaller transformer representation having fewer layers and smaller internal embedding. However, TinyBERT's performance drops when we reduce the number of layers by 50%, and drops even more abruptly when we reduce the number of layers by 75% for advanced NLP tasks such as span question answering. Additionally, a separate model must be trained for each inference scenario with its distinct computational budget. In this work we present Dynamic-TinyBERT, a TinyBERT model that utilizes sequence-length reduction and Hyperparameter Optimization for enhanced inference efficiency per any computational budget. Dynamic-TinyBERT is trained only once, performing on-par with BERT and achieving an accuracy-speedup trade-off superior to any other efficient approaches (up to 3.3x with <1% loss-drop). Upon publication, the code to reproduce our work will be open-sourced.

| Comments: | ENLSP NeurIPS Workshop 2021, 7 pages                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.09645](https://arxiv.org/abs/2111.09645) [cs.CL]** |
|           | (or **[arXiv:2111.09645v1](https://arxiv.org/abs/2111.09645v1) [cs.CL]** for this version) |








# 2021-11-18

[Return to Index](#Index)



<h2 id="2021-11-18-1">1. Transparent Human Evaluation for Image Captioning
</h2>
Title: [Transparent Human Evaluation for Image Captioning](https://arxiv.org/abs/2111.08940)
Authors: [Jungo Kasai](https://arxiv.org/search/cs?searchtype=author&query=Kasai%2C+J), [Keisuke Sakaguchi](https://arxiv.org/search/cs?searchtype=author&query=Sakaguchi%2C+K), [Lavinia Dunagan](https://arxiv.org/search/cs?searchtype=author&query=Dunagan%2C+L), [Jacob Morrison](https://arxiv.org/search/cs?searchtype=author&query=Morrison%2C+J), [Ronan Le Bras](https://arxiv.org/search/cs?searchtype=author&query=Bras%2C+R+L), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> We establish a rubric-based human evaluation protocol for image captioning models. Our scoring rubrics and their definitions are carefully developed based on machine- and human-generated captions on the MSCOCO dataset. Each caption is evaluated along two main dimensions in a tradeoff (precision and recall) as well as other aspects that measure the text quality (fluency, conciseness, and inclusive language). Our evaluations demonstrate several critical problems of the current evaluation practice. Human-generated captions show substantially higher quality than machine-generated ones, especially in coverage of salient information (i.e., recall), while all automatic metrics say the opposite. Our rubric-based results reveal that CLIPScore, a recent metric that uses image features, better correlates with human judgments than conventional text-only metrics because it is more sensitive to recall. We hope that this work will promote a more transparent evaluation protocol for image captioning and its automatic metrics.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.08940](https://arxiv.org/abs/2111.08940) [cs.CL]** |
|           | (or **[arXiv:2111.08940v1](https://arxiv.org/abs/2111.08940v1) [cs.CL]** for this version) |





<h2 id="2021-11-18-2">2. Character Transformations for Non-Autoregressive GEC Tagging
</h2>

Title: [Character Transformations for Non-Autoregressive GEC Tagging](https://arxiv.org/abs/2111.09280)
Authors: [Milan Straka](https://arxiv.org/search/cs?searchtype=author&query=Straka%2C+M), [Jakub Náplava](https://arxiv.org/search/cs?searchtype=author&query=Náplava%2C+J), [Jana Straková](https://arxiv.org/search/cs?searchtype=author&query=Straková%2C+J)

> We propose a character-based nonautoregressive GEC approach, with automatically generated character transformations. Recently, per-word classification of correction edits has proven an efficient, parallelizable alternative to current encoder-decoder GEC systems. We show that word replacement edits may be suboptimal and lead to explosion of rules for spelling, diacritization and errors in morphologically rich languages, and propose a method for generating character transformations from GEC corpus. Finally, we train character transformation models for Czech, German and Russian, reaching solid results and dramatic speedup compared to autoregressive systems. The source code is released at [this https URL](https://github.com/ufal/wnut2021_character_transformations_gec).

| Comments: | Accepted to W-NUT 2021                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.09280](https://arxiv.org/abs/2111.09280) [cs.CL]** |
|           | (or **[arXiv:2111.09280v1](https://arxiv.org/abs/2111.09280v1) [cs.CL]** for this version) |





<h2 id="2021-11-18-3">3. XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale
</h2>

Title: [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296)
Authors: [Arun Babu](https://arxiv.org/search/cs?searchtype=author&query=Babu%2C+A), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Andros Tjandra](https://arxiv.org/search/cs?searchtype=author&query=Tjandra%2C+A), [Kushal Lakhotia](https://arxiv.org/search/cs?searchtype=author&query=Lakhotia%2C+K), [Qiantong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Q), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Kritika Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+K), [Patrick von Platen](https://arxiv.org/search/cs?searchtype=author&query=von+Platen%2C+P), [Yatharth Saraf](https://arxiv.org/search/cs?searchtype=author&query=Saraf%2C+Y), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Alexei Baevski](https://arxiv.org/search/cs?searchtype=author&query=Baevski%2C+A), [Alexis Conneau](https://arxiv.org/search/cs?searchtype=author&query=Conneau%2C+A), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M)

> This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets a new state of the art on VoxLingua107 language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can outperform English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.09296](https://arxiv.org/abs/2111.09296) [cs.CL]** |
|           | (or **[arXiv:2111.09296v1](https://arxiv.org/abs/2111.09296v1) [cs.CL]** for this version) |






# 2021-11-17

[Return to Index](#Index)



<h2 id="2021-11-17-1">1. Joint Unsupervised and Supervised Training for Multilingual ASR
</h2>

Title: [Joint Unsupervised and Supervised Training for Multilingual ASR](https://arxiv.org/abs/2111.08137)

Authors: [Junwen Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+J), [Bo Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Yu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Nikhil Siddhartha](https://arxiv.org/search/cs?searchtype=author&query=Siddhartha%2C+N), [Khe Chai Sim](https://arxiv.org/search/cs?searchtype=author&query=Sim%2C+K+C), [Tara N. Sainath](https://arxiv.org/search/cs?searchtype=author&query=Sainath%2C+T+N)

> Self-supervised training has shown promising gains in pretraining models and facilitating the downstream finetuning for speech recognition, like multilingual ASR. Most existing methods adopt a 2-stage scheme where the self-supervised loss is optimized in the first pretraining stage, and the standard supervised finetuning resumes in the second stage. In this paper, we propose an end-to-end (E2E) Joint Unsupervised and Supervised Training (JUST) method to combine the supervised RNN-T loss and the self-supervised contrastive and masked language modeling (MLM) losses. We validate its performance on the public dataset Multilingual LibriSpeech (MLS), which includes 8 languages and is extremely imbalanced. On MLS, we explore (1) JUST trained from scratch, and (2) JUST finetuned from a pretrained checkpoint. Experiments show that JUST can consistently outperform other existing state-of-the-art methods, and beat the monolingual baseline by a significant margin, demonstrating JUST's capability of handling low-resource languages in multilingual ASR. Our average WER of all languages outperforms average monolingual baseline by 33.3%, and the state-of-the-art 2-stage XLSR by 32%. On low-resource languages like Polish, our WER is less than half of the monolingual baseline and even beats the supervised transfer learning method which uses external supervision.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.08137](https://arxiv.org/abs/2111.08137) [cs.CL]** |
|           | (or **[arXiv:2111.08137v1](https://arxiv.org/abs/2111.08137v1) [cs.CL]** for this version) |



<h2 id="2021-11-17-2">2. Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts
</h2>

Title: [Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts](https://arxiv.org/abs/2111.08276)

Authors: [Yan Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+Y), [Xinsong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Hang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H)

> Most existing methods in vision language pre-training rely on object-centric features extracted through object detection, and make fine-grained alignments between the extracted features and texts. We argue that the use of object detection may not be suitable for vision language pre-training. Instead, we point out that the task should be performed so that the regions of `visual concepts' mentioned in the texts are located in the images, and in the meantime alignments between texts and visual concepts are identified, where the alignments are in multi-granularity. This paper proposes a new method called X-VLM to perform `multi-grained vision language pre-training'. Experimental results show that X-VLM consistently outperforms state-of-the-art methods in many downstream vision language tasks.

| Comments: | 13 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2111.08276](https://arxiv.org/abs/2111.08276) [cs.CL]** |
|           | (or **[arXiv:2111.08276v1](https://arxiv.org/abs/2111.08276v1) [cs.CL]** for this version) |



<h2 id="2021-11-17-3">3. Few-Shot Self-Rationalization with Natural Language Prompts
</h2>

Title: [Few-Shot Self-Rationalization with Natural Language Prompts](https://arxiv.org/abs/2111.08284)

Authors: [Ana Marasović](https://arxiv.org/search/cs?searchtype=author&query=Marasović%2C+A), [Iz Beltagy](https://arxiv.org/search/cs?searchtype=author&query=Beltagy%2C+I), [Doug Downey](https://arxiv.org/search/cs?searchtype=author&query=Downey%2C+D), [Matthew E. Peters](https://arxiv.org/search/cs?searchtype=author&query=Peters%2C+M+E)

> Self-rationalization models that predict task labels and generate free-text elaborations for their predictions could enable more intuitive interaction with NLP systems. These models are, however, currently trained with a large amount of human-written free-text explanations for each task which hinders their broader usage. We propose to study a more realistic setting of self-rationalization using few training examples. We present FEB -- a standardized collection of four existing English-language datasets and associated metrics. We identify the right prompting approach by extensively exploring natural language prompts on FEB. Then, by using this prompt and scaling the model size, we demonstrate that making progress on few-shot self-rationalization is possible. We show there is still ample room for improvement in this task: the average plausibility of generated explanations assessed by human annotators is at most 51%, while plausibility of human explanations is 76%. We hope that FEB together with our proposed approach will spur the community to take on the few-shot self-rationalization challenge.

| Comments: | First two authors contributed equally                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.08284](https://arxiv.org/abs/2111.08284) [cs.CL]** |
|           | (or **[arXiv:2111.08284v1](https://arxiv.org/abs/2111.08284v1) [cs.CL]** for this version) |



<h2 id="2021-11-17-4">4. Integrated Semantic and Phonetic Post-correction for Chinese Speech Recognition
</h2>

Title: [Integrated Semantic and Phonetic Post-correction for Chinese Speech Recognition](https://arxiv.org/abs/2111.08400)

Authors: [Yi-Chang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Chun-Yen Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+C), [Chien-An Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Ming-Chieh Sung](https://arxiv.org/search/cs?searchtype=author&query=Sung%2C+M), [Yi-Ren Yeh](https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+Y)

> Due to the recent advances of natural language processing, several works have applied the pre-trained masked language model (MLM) of BERT to the post-correction of speech recognition. However, existing pre-trained models only consider the semantic correction while the phonetic features of words is neglected. The semantic-only post-correction will consequently decrease the performance since homophonic errors are fairly common in Chinese ASR. In this paper, we proposed a novel approach to collectively exploit the contextualized representation and the phonetic information between the error and its replacing candidates to alleviate the error rate of Chinese ASR. Our experiment results on real world speech recognition datasets showed that our proposed method has evidently lower CER than the baseline model, which utilized a pre-trained BERT MLM as the corrector.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.08400](https://arxiv.org/abs/2111.08400) [cs.CL]** |
|           | (or **[arXiv:2111.08400v1](https://arxiv.org/abs/2111.08400v1) [cs.CL]** for this version) |





<h2 id="2021-11-17-5">5. Generative Pre-Trained Transformer for Design Concept Generation: An Exploration
</h2>

Title: [Generative Pre-Trained Transformer for Design Concept Generation: An Exploration](https://arxiv.org/abs/2111.08489)

Authors: [Qihao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Q), [Jianxi Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J)

> Novel concepts are essential for design innovation and can be generated with the aid of data stimuli and computers. However, current generative design algorithms focus on diagrammatic or spatial concepts that are either too abstract to understand or too detailed for early phase design exploration. This paper explores the uses of generative pre-trained transformers (GPT) for natural language design concept generation. Our experiments involve the use of GPT-2 and GPT-3 for different creative reasonings in design tasks. Both show reasonably good performance for verbal design concept generation.

| Comments: | Submitted to the DESIGN 2022 Conference                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.08489](https://arxiv.org/abs/2111.08489) [cs.CL]** |
|           | (or **[arXiv:2111.08489v1](https://arxiv.org/abs/2111.08489v1) [cs.CL]** for this version) |

<h2 id="2021-11-17-6">6. CVSS-BERT: Explainable Natural Language Processing to Determine the Severity of a Computer Security Vulnerability from its Description
</h2>

Title: [CVSS-BERT: Explainable Natural Language Processing to Determine the Severity of a Computer Security Vulnerability from its Description](https://arxiv.org/abs/2111.08510)

Authors: [Mustafizur Shahid](https://arxiv.org/search/cs?searchtype=author&query=Shahid%2C+M) (IP Paris), [Hervé Debar](https://arxiv.org/search/cs?searchtype=author&query=Debar%2C+H)

> When a new computer security vulnerability is publicly disclosed, only a textual description of it is available. Cybersecurity experts later provide an analysis of the severity of the vulnerability using the Common Vulnerability Scoring System (CVSS). Specifically, the different characteristics of the vulnerability are summarized into a vector (consisting of a set of metrics), from which a severity score is computed. However, because of the high number of vulnerabilities disclosed everyday this process requires lot of manpower, and several days may pass before a vulnerability is analyzed. We propose to leverage recent advances in the field of Natural Language Processing (NLP) to determine the CVSS vector and the associated severity score of a vulnerability from its textual description in an explainable manner. To this purpose, we trained multiple BERT classifiers, one for each metric composing the CVSS vector. Experimental results show that our trained classifiers are able to determine the value of the metrics of the CVSS vector with high accuracy. The severity score computed from the predicted CVSS vector is also very close to the real severity score attributed by a human expert. For explainability purpose, gradient-based input saliency method was used to determine the most relevant input words for a given prediction made by our classifiers. Often, the top relevant words include terms in agreement with the rationales of a human cybersecurity expert, making the explanation comprehensible for end-users.

| Comments: | 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA), Dec 2021, Pasadena, United States |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.08510](https://arxiv.org/abs/2111.08510) [cs.CL]** |
|           | (or **[arXiv:2111.08510v1](https://arxiv.org/abs/2111.08510v1) [cs.CL]** for this version) |



<h2 id="2021-11-17-7">7. NVIDIA NeMo Neural Machine Translation Systems for English-German and English-Russian News and Biomedical Tasks at WMT21
</h2>

Title: [NVIDIA NeMo Neural Machine Translation Systems for English-German and English-Russian News and Biomedical Tasks at WMT21](https://arxiv.org/abs/2111.08634)

Authors: [Sandeep Subramanian](https://arxiv.org/search/cs?searchtype=author&query=Subramanian%2C+S), [Oleksii Hrinchuk](https://arxiv.org/search/cs?searchtype=author&query=Hrinchuk%2C+O), [Virginia Adams](https://arxiv.org/search/cs?searchtype=author&query=Adams%2C+V), [Oleksii Kuchaiev](https://arxiv.org/search/cs?searchtype=author&query=Kuchaiev%2C+O)

> This paper provides an overview of NVIDIA NeMo's neural machine translation systems for the constrained data track of the WMT21 News and Biomedical Shared Translation Tasks. Our news task submissions for English-German (En-De) and English-Russian (En-Ru) are built on top of a baseline transformer-based sequence-to-sequence model. Specifically, we use a combination of 1) checkpoint averaging 2) model scaling 3) data augmentation with backtranslation and knowledge distillation from right-to-left factorized models 4) finetuning on test sets from previous years 5) model ensembling 6) shallow fusion decoding with transformer language models and 7) noisy channel re-ranking. Additionally, our biomedical task submission for English-Russian uses a biomedically biased vocabulary and is trained from scratch on news task data, medically relevant text curated from the news task dataset, and biomedical data provided by the shared task. Our news system achieves a sacreBLEU score of 39.5 on the WMT'20 En-De test set outperforming the best submission from last year's task of 38.8. Our biomedical task Ru-En and En-Ru systems reach BLEU scores of 43.8 and 40.3 respectively on the WMT'20 Biomedical Task Test set, outperforming the previous year's best submissions.

| Comments: | WMT'21 news and biomedical shared task submission            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.08634](https://arxiv.org/abs/2111.08634) [cs.CL]** |
|           | (or **[arXiv:2111.08634v1](https://arxiv.org/abs/2111.08634v1) [cs.CL]** for this version) |



<h2 id="2021-11-17-8">8. Document AI: Benchmarks, Models and Applications
</h2>

Title: [Document AI: Benchmarks, Models and Applications](https://arxiv.org/abs/2111.08609)

Authors: [Lei Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L), [Yiheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Tengchao Lv](https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+T), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Document AI, or Document Intelligence, is a relatively new research topic that refers to the techniques for automatically reading, understanding, and analyzing business documents. It is an important research direction for natural language processing and computer vision. In recent years, the popularity of deep learning technology has greatly advanced the development of Document AI, such as document layout analysis, visual information extraction, document visual question answering, document image classification, etc. This paper briefly reviews some of the representative models, tasks, and benchmark datasets. Furthermore, we also introduce early-stage heuristic rule-based document analysis, statistical machine learning algorithms, and deep learning approaches especially pre-training methods. Finally, we look into future directions for Document AI research.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.08609](https://arxiv.org/abs/2111.08609) [cs.CL]** |
|           | (or **[arXiv:2111.08609v1](https://arxiv.org/abs/2111.08609v1) [cs.CL]** for this version) |





# 2021-11-16

[Return to Index](#Index)



<h2 id="2021-11-16-1">1. Curriculum Learning for Vision-and-Language Navigation
</h2>

Title: [Curriculum Learning for Vision-and-Language Navigation](https://arxiv.org/abs/2111.07228)

Authors: [Jiwen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Zhongyu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Z), [Jianqing Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+J), [Jiajie Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+J)

> Vision-and-Language Navigation (VLN) is a task where an agent navigates in an embodied indoor environment under human instructions. Previous works ignore the distribution of sample difficulty and we argue that this potentially degrade their agent performance. To tackle this issue, we propose a novel curriculum-based training paradigm for VLN tasks that can balance human prior knowledge and agent learning progress about training samples. We develop the principle of curriculum design and re-arrange the benchmark Room-to-Room (R2R) dataset to make it suitable for curriculum training. Experiments show that our method is model-agnostic and can significantly improve the performance, the generalizability, and the training efficiency of current state-of-the-art navigation agents without increasing model complexity.

| Comments: | Accepted by NeurIPS 2021                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2111.07228](https://arxiv.org/abs/2111.07228) [cs.LG]** |
|           | (or **[arXiv:2111.07228v1](https://arxiv.org/abs/2111.07228v1) [cs.LG]** for this version) |





<h2 id="2021-11-16-2">2. LiT: Zero-Shot Transfer with Locked-image Text Tuning
</h2>

Title: [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991)

Authors: [Xiaohua Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+X), [Xiao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Basil Mustafa](https://arxiv.org/search/cs?searchtype=author&query=Mustafa%2C+B), [Andreas Steiner](https://arxiv.org/search/cs?searchtype=author&query=Steiner%2C+A), [Daniel Keysers](https://arxiv.org/search/cs?searchtype=author&query=Keysers%2C+D), [Alexander Kolesnikov](https://arxiv.org/search/cs?searchtype=author&query=Kolesnikov%2C+A), [Lucas Beyer](https://arxiv.org/search/cs?searchtype=author&query=Beyer%2C+L)

> This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning "Locked-image Text tuning" (LiT-tuning), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT-tuned model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT-tuning is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT-tuned model achieves 84.5% zero-shot transfer accuracy on the ImageNet test set, and 81.1% on the challenging out-of-distribution ObjectNet test set.

| Comments: | Xiaohua, Xiao, Basil, Andreas and Lucas contributed equally  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.07991](https://arxiv.org/abs/2111.07991) [cs.CV]** |
|           | (or **[arXiv:2111.07991v1](https://arxiv.org/abs/2111.07991v1) [cs.CV]** for this version) |





<h2 id="2021-11-16-3">3. Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning
</h2>

Title: [Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning](https://arxiv.org/abs/2111.07180)

Authors: [Yizhen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Minkyu Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+M), [Kuan Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+K), [Zhongming Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z)

> In natural language processing, most models try to learn semantic representations merely from texts. The learned representations encode the distributional semantics but fail to connect to any knowledge about the physical world. In contrast, humans learn language by grounding concepts in perception and action and the brain encodes grounded semantics for cognition. Inspired by this notion and recent work in vision-language learning, we design a two-stream model for grounding language learning in vision. The model includes a VGG-based visual stream and a Bert-based language stream. The two streams merge into a joint representational space. Through cross-modal contrastive learning, the model first learns to align visual and language representations with the MS COCO dataset. The model further learns to retrieve visual objects with language queries through a cross-modal attention module and to infer the visual relations between the retrieved objects through a bilinear operator with the Visual Genome dataset. After training, the language stream of this model is a stand-alone language model capable of embedding concepts in a visually grounded semantic space. This semantic space manifests principal dimensions explainable with human intuition and neurobiological knowledge. Word embeddings in this semantic space are predictive of human-defined norms of semantic features and are segregated into perceptually distinctive clusters. Furthermore, the visually grounded language model also enables compositional language understanding based on visual knowledge and multimodal image search with queries based on images, texts, or their combinations.

| Comments: | 10 pages, 7 figures, 1 appendix, to be published in Neurips 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.07180](https://arxiv.org/abs/2111.07180) [cs.CL]** |
|           | (or **[arXiv:2111.07180v1](https://arxiv.org/abs/2111.07180v1) [cs.CL]** for this version) |





<h2 id="2021-11-16-4">4. DEEP: DEnoising Entity Pre-training for Neural Machine Translation
</h2>

Title: [DEEP: DEnoising Entity Pre-training for Neural Machine Translation](https://arxiv.org/abs/2111.07393)

Authors: [Junjie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J), [Hiroaki Hayashi](https://arxiv.org/search/cs?searchtype=author&query=Hayashi%2C+H), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> It has been shown that machine translation models usually generate poor translations for named entities that are infrequent in the training corpus. Earlier named entity translation methods mainly focus on phonetic transliteration, which ignores the sentence context for translation and is limited in domain and language coverage. To address this limitation, we propose DEEP, a DEnoising Entity Pre-training method that leverages large amounts of monolingual data and a knowledge base to improve named entity translation accuracy within sentences. Besides, we investigate a multi-task learning strategy that finetunes a pre-trained neural machine translation model on both entity-augmented monolingual data and parallel data to further improve entity translation. Experimental results on three language pairs demonstrate that \method results in significant improvements over strong denoising auto-encoding baselines, with a gain of up to 1.3 BLEU and up to 9.2 entity accuracy points for English-Russian translation.

| Comments: | 13 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2111.07393](https://arxiv.org/abs/2111.07393) [cs.CL]** |
|           | (or **[arXiv:2111.07393v1](https://arxiv.org/abs/2111.07393v1) [cs.CL]** for this version) |





<h2 id="2021-11-16-5">5. Time Waits for No One! Analysis and Challenges of Temporal Misalignment
</h2>

Title: [Time Waits for No One! Analysis and Challenges of Temporal Misalignment](https://arxiv.org/abs/2111.07408)

Authors: [Kelvin Luu](https://arxiv.org/search/cs?searchtype=author&query=Luu%2C+K), [Daniel Khashabi](https://arxiv.org/search/cs?searchtype=author&query=Khashabi%2C+D), [Suchin Gururangan](https://arxiv.org/search/cs?searchtype=author&query=Gururangan%2C+S), [Karishma Mandyam](https://arxiv.org/search/cs?searchtype=author&query=Mandyam%2C+K), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting temporal misalignment can degrade end-task performance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and reviews) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain-specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal misalignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously reported. We also find that, while temporal adaptation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models.

| Comments: | 9 pages, 6 figures, 3 tables                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.07408](https://arxiv.org/abs/2111.07408) [cs.CL]** |
|           | (or **[arXiv:2111.07408v1](https://arxiv.org/abs/2111.07408v1) [cs.CL]** for this version) |





<h2 id="2021-11-16-6">6. Measuring Uncertainty in Translation Quality Evaluation (TQE)
</h2>

Title: [Measuring Uncertainty in Translation Quality Evaluation (TQE)](https://arxiv.org/abs/2111.07699)

Authors: [Serge Gladkoff](https://arxiv.org/search/cs?searchtype=author&query=Gladkoff%2C+S), [Irina Sorokina](https://arxiv.org/search/cs?searchtype=author&query=Sorokina%2C+I), [Lifeng Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+L), [Alexandra Alekseeva](https://arxiv.org/search/cs?searchtype=author&query=Alekseeva%2C+A)

> From both human translators (HT) and machine translation (MT) researchers' point of view, translation quality evaluation (TQE) is an essential task. Translation service providers (TSPs) have to deliver large volumes of translations which meet customer specifications with harsh constraints of required quality level in tight time-frames and costs. MT researchers strive to make their models better, which also requires reliable quality evaluation. While automatic machine translation evaluation (MTE) metrics and quality estimation (QE) tools are widely available and easy to access, existing automated tools are not good enough, and human assessment from professional translators (HAP) are often chosen as the golden standard \cite{han-etal-2021-TQA}. Human evaluations, however, are often accused of having low reliability and agreement. Is this caused by subjectivity or statistics is at play? How to avoid the entire text to be checked and be more efficient with TQE from cost and efficiency perspectives, and what is the optimal sample size of the translated text, so as to reliably estimate the translation quality of the entire material? This work carries out such motivated research to correctly estimate the confidence intervals \cite{Brown_etal2001Interval} depending on the sample size of the translated text, e.g. the amount of words or sentences, that needs to be processed on TQE workflow step for confident and reliable evaluation of overall translation quality. The methodology we applied for this work is from Bernoulli Statistical Distribution Modelling (BSDM) and Monte Carlo Sampling Analysis (MCSA).

| Comments: | 13 pages, 9 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Numerical Analysis (math.NA); Applications (stat.AP) |
| Cite as:  | **[arXiv:2111.07699](https://arxiv.org/abs/2111.07699) [cs.CL]** |
|           | (or **[arXiv:2111.07699v1](https://arxiv.org/abs/2111.07699v1) [cs.CL]** for this version) |





<h2 id="2021-11-16-7">7. Data Augmentation for Speech Recognition in Maltese: A Low-Resource Perspective
</h2>

Title: [Data Augmentation for Speech Recognition in Maltese: A Low-Resource Perspective](https://arxiv.org/abs/2111.07793)

Authors: [Carlos Mena](https://arxiv.org/search/cs?searchtype=author&query=Mena%2C+C), [Andrea DeMarco](https://arxiv.org/search/cs?searchtype=author&query=DeMarco%2C+A), [Claudia Borg](https://arxiv.org/search/cs?searchtype=author&query=Borg%2C+C), [Lonneke van der Plas](https://arxiv.org/search/cs?searchtype=author&query=van+der+Plas%2C+L), [Albert Gatt](https://arxiv.org/search/cs?searchtype=author&query=Gatt%2C+A)

> Developing speech technologies is a challenge for low-resource languages for which both annotated and raw speech data is sparse. Maltese is one such language. Recent years have seen an increased interest in the computational processing of Maltese, including speech technologies, but resources for the latter remain sparse. In this paper, we consider data augmentation techniques for improving speech recognition for such languages, focusing on Maltese as a test case. We consider three different types of data augmentation: unsupervised training, multilingual training and the use of synthesized speech as training data. The goal is to determine which of these techniques, or combination of them, is the most effective to improve speech recognition for languages where the starting point is a small corpus of approximately 7 hours of transcribed speech. Our results show that combining the three data augmentation techniques studied here lead us to an absolute WER improvement of 15% without the use of a language model.

| Comments: | 30 pages; 9 tables                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.07793](https://arxiv.org/abs/2111.07793) [cs.CL]** |
|           | (or **[arXiv:2111.07793v1](https://arxiv.org/abs/2111.07793v1) [cs.CL]** for this version) |





<h2 id="2021-11-16-8">8. Evaluating Metrics for Bias in Word Embeddings
</h2>

Title: [Evaluating Metrics for Bias in Word Embeddings](https://arxiv.org/abs/2111.07864)

Authors: [Sarah Schröder](https://arxiv.org/search/cs?searchtype=author&query=Schröder%2C+S), [Alexander Schulz](https://arxiv.org/search/cs?searchtype=author&query=Schulz%2C+A), [Philip Kenneweg](https://arxiv.org/search/cs?searchtype=author&query=Kenneweg%2C+P), [Robert Feldhans](https://arxiv.org/search/cs?searchtype=author&query=Feldhans%2C+R), [Fabian Hinder](https://arxiv.org/search/cs?searchtype=author&query=Hinder%2C+F), [Barbara Hammer](https://arxiv.org/search/cs?searchtype=author&query=Hammer%2C+B)

> Over the last years, word and sentence embeddings have established as text preprocessing for all kinds of NLP tasks and improved the performances significantly. Unfortunately, it has also been shown that these embeddings inherit various kinds of biases from the training data and thereby pass on biases present in society to NLP solutions. Many papers attempted to quantify bias in word or sentence embeddings to evaluate debiasing methods or compare different embedding models, usually with cosine-based metrics. However, lately some works have raised doubts about these metrics showing that even though such metrics report low biases, other tests still show biases. In fact, there is a great variety of bias metrics or tests proposed in the literature without any consensus on the optimal solutions. Yet we lack works that evaluate bias metrics on a theoretical level or elaborate the advantages and disadvantages of different bias metrics. In this work, we will explore different cosine based bias metrics. We formalize a bias definition based on the ideas from previous works and derive conditions for bias metrics. Furthermore, we thoroughly investigate the existing cosine-based metrics and their limitations to show why these metrics can fail to report biases in some cases. Finally, we propose a new metric, SAME, to address the shortcomings of existing metrics and mathematically prove that SAME behaves appropriately.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.07864](https://arxiv.org/abs/2111.07864) [cs.CL]** |
|           | (or **[arXiv:2111.07864v1](https://arxiv.org/abs/2111.07864v1) [cs.CL]** for this version) |





<h2 id="2021-11-16-9">9. CoLLIE: Continual Learning of Language Grounding from Language-Image Embeddings
</h2>

Title: [CoLLIE: Continual Learning of Language Grounding from Language-Image Embeddings](https://arxiv.org/abs/2111.07993)

Authors: [Gabriel Skantze](https://arxiv.org/search/cs?searchtype=author&query=Skantze%2C+G), [Bram Willemsen](https://arxiv.org/search/cs?searchtype=author&query=Willemsen%2C+B)

> This paper presents CoLLIE: a simple, yet effective model for continual learning of how language is grounded in vision. Given a pre-trained multimodal embedding model, where language and images are projected in the same semantic space (in this case CLIP by OpenAI), CoLLIE learns a transformation function that adjusts the language embeddings when needed to accommodate new language use. Unlike traditional few-shot learning, the model does not just learn new classes and labels, but can also generalize to similar language use. We verify the model's performance on two different tasks of continual learning and show that it can efficiently learn and generalize from only a few examples, with little interference with the model's original zero-shot performance.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.07993](https://arxiv.org/abs/2111.07993) [cs.CL]** |
|           | (or **[arXiv:2111.07993v1](https://arxiv.org/abs/2111.07993v1) [cs.CL]** for this version) |







# 2021-11-15

[Return to Index](#Index)



<h2 id="2021-11-15-1">1. On Transferability of Prompt Tuning for Natural Language Understanding
</h2>

Title: [On Transferability of Prompt Tuning for Natural Language Understanding](https://arxiv.org/abs/2111.06719)

Authors: [Yusheng Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y), [Xiaozhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yujia Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y), [Chi-Min Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+C), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Juanzi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Lei Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+L), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Prompt tuning (PT) is a promising parameter-efficient method to utilize extremely large pre-trained language models (PLMs), which could achieve comparable performance to full-parameter fine-tuning by only tuning a few soft prompts. However, compared to fine-tuning, PT empirically requires much more training steps. To explore whether we can improve the efficiency of PT by reusing trained soft prompts and sharing learned knowledge, we empirically investigate the transferability of soft prompts across different tasks and models. In cross-task transfer, we find that trained soft prompts can well transfer to similar tasks and initialize PT for them to accelerate training and improve performance. Moreover, to explore what factors influence prompts' transferability across tasks, we investigate how to measure the prompt similarity and find that the overlapping rate of activated neurons highly correlates to the transferability. In cross-model transfer, we explore how to project the prompts of a PLM to another PLM and successfully train a kind of projector which can achieve non-trivial transfer performance on similar tasks. However, initializing PT with the projected prompts does not work well, which may be caused by optimization preferences and PLMs' high redundancy. Our findings show that improving PT with knowledge transfer is possible and promising, while prompts' cross-task transferability is generally better than the cross-model transferability.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.06719](https://arxiv.org/abs/2111.06719) [cs.CL]** |
|           | (or **[arXiv:2111.06719v1](https://arxiv.org/abs/2111.06719v1) [cs.CL]** for this version) |





<h2 id="2021-11-15-2">2. BitextEdit: Automatic Bitext Editing for Improved Low-Resource Machine Translation
</h2>

Title: [BitextEdit: Automatic Bitext Editing for Improved Low-Resource Machine Translation](https://arxiv.org/abs/2111.06787)

Authors: [Eleftheria Briakou](https://arxiv.org/search/cs?searchtype=author&query=Briakou%2C+E), [Sida I. Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S+I), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Marjan Ghazvininejad](https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M)

> Mined bitexts can contain imperfect translations that yield unreliable training signals for Neural Machine Translation (NMT). While filtering such pairs out is known to improve final model quality, we argue that it is suboptimal in low-resource conditions where even mined data can be limited. In our work, we propose instead, to refine the mined bitexts via automatic editing: given a sentence in a language xf, and a possibly imperfect translation of it xe, our model generates a revised version xf' or xe' that yields a more equivalent translation pair (i.e., <xf, xe'> or <xf', xe>). We use a simple editing strategy by (1) mining potentially imperfect translations for each sentence in a given bitext, (2) learning a model to reconstruct the original translations and translate, in a multi-task fashion. Experiments demonstrate that our approach successfully improves the quality of CCMatrix mined bitext for 5 low-resource language-pairs and 10 translation directions by up to ~ 8 BLEU points, in most cases improving upon a competitive back-translation baseline.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.06787](https://arxiv.org/abs/2111.06787) [cs.CL]** |
|           | (or **[arXiv:2111.06787v1](https://arxiv.org/abs/2111.06787v1) [cs.CL]** for this version) |





<h2 id="2021-11-15-3">3. Deciphering Speech: a Zero-Resource Approach to Cross-Lingual Transfer in ASR
</h2>

Title: [Deciphering Speech: a Zero-Resource Approach to Cross-Lingual Transfer in ASR](https://arxiv.org/abs/2111.06799)

Authors: [Ondrej Klejch](https://arxiv.org/search/cs?searchtype=author&query=Klejch%2C+O), [Electra Wallington](https://arxiv.org/search/cs?searchtype=author&query=Wallington%2C+E), [Peter Bell](https://arxiv.org/search/cs?searchtype=author&query=Bell%2C+P)

> We present a method for cross-lingual training an ASR system using absolutely no transcribed training data from the target language, and with no phonetic knowledge of the language in question. Our approach uses a novel application of a decipherment algorithm, which operates given only unpaired speech and text data from the target language. We apply this decipherment to phone sequences generated by a universal phone recogniser trained on out-of-language speech corpora, which we follow with flat-start semi-supervised training to obtain an acoustic model for the new language. To the best of our knowledge, this is the first practical approach to zero-resource cross-lingual ASR which does not rely on any hand-crafted phonetic information. We carry out experiments on read speech from the GlobalPhone corpus, and show that it is possible to learn a decipherment model on just 20 minutes of data from the target language. When used to generate pseudo-labels for semi-supervised training, we obtain WERs that range from 25% to just 5% absolute worse than the equivalent fully supervised models trained on the same data.

| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.06799](https://arxiv.org/abs/2111.06799) [cs.CL]** |
|           | (or **[arXiv:2111.06799v1](https://arxiv.org/abs/2111.06799v1) [cs.CL]** for this version) |






# 2021-11-12

[Return to Index](#Index)



<h2 id="2021-11-12-1">1. Self-Normalized Importance Sampling for Neural Language Modeling
</h2>

Title: [Self-Normalized Importance Sampling for Neural Language Modeling](https://arxiv.org/abs/2111.06310)

Authors: [Zijian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Yingbo Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y), [Alexander Gerstenberger](https://arxiv.org/search/cs?searchtype=author&query=Gerstenberger%2C+A), [Jintao Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+J), [Ralf Schlüter](https://arxiv.org/search/cs?searchtype=author&query=Schlüter%2C+R), [Hermann Ney](https://arxiv.org/search/cs?searchtype=author&query=Ney%2C+H)

> To mitigate the problem of having to traverse over the full vocabulary in the softmax normalization of a neural language model, sampling-based training criteria are proposed and investigated in the context of large vocabulary word-based neural language models. These training criteria typically enjoy the benefit of faster training and testing, at a cost of slightly degraded performance in terms of perplexity and almost no visible drop in word error rate. While noise contrastive estimation is one of the most popular choices, recently we show that other sampling-based criteria can also perform well, as long as an extra correction step is done, where the intended class posterior probability is recovered from the raw model outputs. In this work, we propose self-normalized importance sampling. Compared to our previous work, the criteria considered in this work are self-normalized and there is no need to further conduct a correction step. Compared to noise contrastive estimation, our method is directly comparable in terms of complexity in application. Through self-normalized language model training as well as lattice rescoring experiments, we show that our proposed self-normalized importance sampling is competitive in both research-oriented and production-oriented automatic speech recognition tasks.

| Comments: | submitted to ICASSP 2022                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2111.06310](https://arxiv.org/abs/2111.06310) [cs.CL]** |
|           | (or **[arXiv:2111.06310v1](https://arxiv.org/abs/2111.06310v1) [cs.CL]** for this version) |





# 2021-11-11

[Return to Index](#Index)



<h2 id="2021-11-11-1">1. MNet-Sim: A Multi-layered Semantic Similarity Network to Evaluate Sentence Similarity
</h2>

Title: [MNet-Sim: A Multi-layered Semantic Similarity Network to Evaluate Sentence Similarity](https://arxiv.org/abs/2111.05412)

Authors: [Manuela Nayantara Jeyaraj](https://arxiv.org/search/cs?searchtype=author&query=Jeyaraj%2C+M+N), [Dharshana Kasthurirathna](https://arxiv.org/search/cs?searchtype=author&query=Kasthurirathna%2C+D)

> Similarity is a comparative-subjective measure that varies with the domain within which it is considered. In several NLP applications such as document classification, pattern recognition, chatbot question-answering, sentiment analysis, etc., identifying an accurate similarity score for sentence pairs has become a crucial area of research. In the existing models that assess similarity, the limitation of effectively computing this similarity based on contextual comparisons, the localization due to the centering theory, and the lack of non-semantic textual comparisons have proven to be drawbacks. Hence, this paper presents a multi-layered semantic similarity network model built upon multiple similarity measures that render an overall sentence similarity score based on the principles of Network Science, neighboring weighted relational edges, and a proposed extended node similarity computation formula. The proposed multi-layered network model was evaluated and tested against established state-of-the-art models and is shown to have demonstrated better performance scores in assessing sentence similarity.

| Subjects:          | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | International Journal of Engineering Trends and Technology 69.7(2021):181-189 |
| DOI:               | [10.14445/22315381/IJETT-V69I7P225](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.14445%2F22315381%2FIJETT-V69I7P225&v=1c1e38c6) |
| Cite as:           | **[arXiv:2111.05412](https://arxiv.org/abs/2111.05412) [cs.LG]** |
|                    | (or **[arXiv:2111.05412v1](https://arxiv.org/abs/2111.05412v1) [cs.LG]** for this version) |





<h2 id="2021-11-11-2">2. Prune Once for All: Sparse Pre-Trained Language Models
</h2>

Title: [Prune Once for All: Sparse Pre-Trained Language Models](https://arxiv.org/abs/2111.05754)

Authors: [Ofir Zafrir](https://arxiv.org/search/cs?searchtype=author&query=Zafrir%2C+O), [Ariel Larey](https://arxiv.org/search/cs?searchtype=author&query=Larey%2C+A), [Guy Boudoukh](https://arxiv.org/search/cs?searchtype=author&query=Boudoukh%2C+G), [Haihao Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+H), [Moshe Wasserblat](https://arxiv.org/search/cs?searchtype=author&query=Wasserblat%2C+M)

> Transformer-based language models are applied to a wide range of applications in natural language processing. However, they are inefficient and difficult to deploy. In recent years, many compression algorithms have been proposed to increase the implementation efficiency of large Transformer-based models on target hardware. In this work we present a new method for training sparse pre-trained Transformer language models by integrating weight pruning and model distillation. These sparse pre-trained models can be used to transfer learning for a wide range of tasks while maintaining their sparsity pattern. We demonstrate our method with three known architectures to create sparse pre-trained BERT-Base, BERT-Large and DistilBERT. We show how the compressed sparse pre-trained models we trained transfer their knowledge to five different downstream natural language tasks with minimal accuracy loss. Moreover, we show how to further compress the sparse models' weights to 8bit precision using quantization-aware training. For example, with our sparse pre-trained BERT-Large fine-tuned on SQuADv1.1 and quantized to 8bit we achieve a compression ratio of 40X for the encoder with less than 1% accuracy loss. To the best of our knowledge, our results show the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT.

| Comments: | ENLSP NeurIPS Workshop 2021, 12 pages                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.05754](https://arxiv.org/abs/2111.05754) [cs.CL]** |
|           | (or **[arXiv:2111.05754v1](https://arxiv.org/abs/2111.05754v1) [cs.CL]** for this version) |





<h2 id="2021-11-11-3">3. Cross-lingual Adaption Model-Agnostic Meta-Learning for Natural Language Understanding 
</h2>

Title: [Cross-lingual Adaption Model-Agnostic Meta-Learning for Natural Language Understanding](https://arxiv.org/abs/2111.05805)

Authors: [Qianying Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q), [Fei Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+F), [Sadao Kurohashi](https://arxiv.org/search/cs?searchtype=author&query=Kurohashi%2C+S)

> Meta learning with auxiliary languages has demonstrated promising improvements for cross-lingual natural language processing. However, previous studies sample the meta-training and meta-testing data from the same language, which limits the ability of the model for cross-lingual transfer. In this paper, we propose XLA-MAML, which performs direct cross-lingual adaption in the meta-learning stage. We conduct zero-shot and few-shot experiments on Natural Language Inference and Question Answering. The experimental results demonstrate the effectiveness of our method across different languages, tasks, and pretrained models. We also give analysis on various cross-lingual specific settings for meta-learning including sampling strategy and parallelism.

| Comments: | 11 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.05805](https://arxiv.org/abs/2111.05805) [cs.CL]** |
|           | (or **[arXiv:2111.05805v1](https://arxiv.org/abs/2111.05805v1) [cs.CL]** for this version) |






# 2021-11-10

[Return to Index](#Index)



<h2 id="2021-11-10-1">1. Speaker Generation
</h2>

Authors: [Speaker Generation](https://arxiv.org/abs/2111.05095)

Title: [Daisy Stanton](https://arxiv.org/search/cs?searchtype=author&query=Stanton%2C+D), [Matt Shannon](https://arxiv.org/search/cs?searchtype=author&query=Shannon%2C+M), [Soroosh Mariooryad](https://arxiv.org/search/cs?searchtype=author&query=Mariooryad%2C+S), [RJ Skerry-Ryan](https://arxiv.org/search/cs?searchtype=author&query=Skerry-Ryan%2C+R), [Eric Battenberg](https://arxiv.org/search/cs?searchtype=author&query=Battenberg%2C+E), [Tom Bagby](https://arxiv.org/search/cs?searchtype=author&query=Bagby%2C+T), [David Kao](https://arxiv.org/search/cs?searchtype=author&query=Kao%2C+D)

> This work explores the task of synthesizing speech in nonexistent human-sounding voices. We call this task "speaker generation", and present TacoSpawn, a system that performs competitively at this task. TacoSpawn is a recurrent attention-based text-to-speech model that learns a distribution over a speaker embedding space, which enables sampling of novel and diverse speakers. Our method is easy to implement, and does not require transfer learning from speaker ID systems. We present objective and subjective metrics for evaluating performance on this task, and demonstrate that our proposed objective metrics correlate with human perception of speaker similarity. Audio samples are available on our demo page.

| Comments:    | 12 pages, 3 figures, 4 tables, appendix with 2 tables        |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Sound (cs.SD)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS) |
| ACM classes: | I.2.7; G.3                                                   |
| Cite as:     | **[arXiv:2111.05095](https://arxiv.org/abs/2111.05095) [cs.SD]** |
|              | (or **[arXiv:2111.05095v1](https://arxiv.org/abs/2111.05095v1) [cs.SD]** for this version) |





<h2 id="2021-11-10-2">2. A Survey on Green Deep Learning
</h2>

Authors: [A Survey on Green Deep Learning](https://arxiv.org/abs/2111.05193)

Title: [Jingjing Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Wangchunshu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Zhiyi Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Z), [Hao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H), [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L)

> In recent years, larger and deeper models are springing up and continuously pushing state-of-the-art (SOTA) results across various fields like natural language processing (NLP) and computer vision (CV). However, despite promising results, it needs to be noted that the computations required by SOTA models have been increased at an exponential rate. Massive computations not only have a surprisingly large carbon footprint but also have negative effects on research inclusiveness and deployment on real-world applications. 
> Green deep learning is an increasingly hot research field that appeals to researchers to pay attention to energy usage and carbon emission during model training and inference. The target is to yield novel results with lightweight and efficient technologies. Many technologies can be used to achieve this goal, like model compression and knowledge distillation. This paper focuses on presenting a systematic review of the development of Green deep learning technologies. We classify these approaches into four categories: (1) compact networks, (2) energy-efficient training strategies, (3) energy-efficient inference approaches, and (4) efficient data usage. For each category, we discuss the progress that has been achieved and the unresolved challenges.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.05193](https://arxiv.org/abs/2111.05193) [cs.LG]** |
|           | (or **[arXiv:2111.05193v1](https://arxiv.org/abs/2111.05193v1) [cs.LG]** for this version) |





<h2 id="2021-11-10-3">3. FPM: A Collection of Large-scale Foundation Pre-trained Language Models
</h2>

Authors: [FPM: A Collection of Large-scale Foundation Pre-trained Language Models](https://arxiv.org/abs/2111.04909)

Title: [Dezhou Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+D)

> Recent work in language modeling has shown that training large-scale Transformer models has promoted the latest developments in natural language processing applications. However, there is very little work to unify the current effective models. In this work, we use the current effective model structure to launch a model set through the current most mainstream technology. We think this will become the basic model in the future. For Chinese, using the GPT-2[9] model, a 10.3 billion parameter language model was trained on the Chinese dataset, and, in particular, a 2.9 billion parameter language model based on dialogue data was trained; the BERT model was trained on the Chinese dataset with 495 million parameters; the Transformer model has trained a language model with 5.6 billion parameters on the Chinese dataset. In English, corresponding training work has also been done. Using the GPT-2 model, a language model with 6.4 billion parameters was trained on the English dataset; the BERT[3] model trained a language model with 1.24 billion parameters on the English dataset, and in particular, it trained a 688 million parameter based on single card training technology Language model; Transformer model trained a language model with 5.6 billion parameters on the English dataset. In the TNEWS classification task evaluated by CLUE[13], the BERT-C model exceeded the 59.46% accuracy of ALBERT-xxlarge with an accuracy rate of 59.99%, an increase of 0.53%. In the QQP classification task evaluated by GLUE[11], the accuracy rate of 78.95% surpassed the accuracy rate of BERT-Large of 72.1%, an increase of 6.85%. Compared with the current accuracy rate of ERNIE, the first place in the GLUE evaluation of 75.2%, an increase of 3.75%.

| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2111.04909](https://arxiv.org/abs/2111.04909) [cs.CL]** |
|              | (or **[arXiv:2111.04909v1](https://arxiv.org/abs/2111.04909v1) [cs.CL]** for this version) |









# 2021-11-09

[Return to Index](#Index)



<h2 id="2021-11-09-1">1. Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling
</h2>

Title: [Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling](https://arxiv.org/abs/2111.03930)

Authors: [Renrui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Rongyao Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+R), [Peng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+P), [Wei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Kunchang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+K), [Jifeng Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+J), [Yu Qiao](https://arxiv.org/search/cs?searchtype=author&query=Qiao%2C+Y), [Hongsheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H)

> Contrastive Vision-Language Pre-training, known as CLIP, has provided a new paradigm for learning visual representations by using large-scale contrastive image-text pairs. It shows impressive performance on zero-shot knowledge transfer to downstream tasks. To further enhance CLIP's few-shot capability, CLIP-Adapter proposed to fine-tune a lightweight residual feature adapter and significantly improves the performance for few-shot classification. However, such a process still needs extra training and computational resources. In this paper, we propose \textbf{T}raining-Free CL\textbf{IP}-\textbf{Adapter} (\textbf{Tip-Adapter}), which not only inherits CLIP's training-free advantage but also performs comparably or even better than CLIP-Adapter. Tip-Adapter does not require any back propagation for training the adapter, but creates the weights by a key-value cache model constructed from the few-shot training set. In this non-parametric manner, Tip-Adapter acquires well-performed adapter weights without any training, which is both efficient and effective. Moreover, the performance of Tip-Adapter can be further boosted by fine-tuning such properly initialized adapter for only a few epochs with super-fast convergence speed. We conduct extensive experiments of few-shot classification on ImageNet and other 10 datasets to demonstrate the superiority of proposed Tip-Adapter. The code will be released at \url{[this https URL](https://github.com/gaopengcuhk/Tip-Adapter)}.

| Comments: | preprints                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2111.03930](https://arxiv.org/abs/2111.03930) [cs.CV]** |
|           | (or **[arXiv:2111.03930v1](https://arxiv.org/abs/2111.03930v1) [cs.CV]** for this version) |





<h2 id="2021-11-09-2">2. Analyzing Architectures for Neural Machine Translation Using Low Computational Resources
</h2>

Title: [Analyzing Architectures for Neural Machine Translation Using Low Computational Resources](https://arxiv.org/abs/2111.03813)

Authors: [Aditya Mandke](https://arxiv.org/search/cs?searchtype=author&query=Mandke%2C+A), [Onkar Litake](https://arxiv.org/search/cs?searchtype=author&query=Litake%2C+O), [Dipali Kadam](https://arxiv.org/search/cs?searchtype=author&query=Kadam%2C+D)

> With the recent developments in the field of Natural Language Processing, there has been a rise in the use of different architectures for Neural Machine Translation. Transformer architectures are used to achieve state-of-the-art accuracy, but they are very computationally expensive to train. Everyone cannot have such setups consisting of high-end GPUs and other resources. We train our models on low computational resources and investigate the results. As expected, transformers outperformed other architectures, but there were some surprising results. Transformers consisting of more encoders and decoders took more time to train but had fewer BLEU scores. LSTM performed well in the experiment and took comparatively less time to train than transformers, making it suitable to use in situations having time constraints.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.03813](https://arxiv.org/abs/2111.03813) [cs.CL]** |
|           | (or **[arXiv:2111.03813v1](https://arxiv.org/abs/2111.03813v1) [cs.CL]** for this version) |





<h2 id="2021-11-09-3">3. Information Extraction from Visually Rich Documents with Font Style Embeddings
</h2>

Title: [Information Extraction from Visually Rich Documents with Font Style Embeddings](https://arxiv.org/abs/2111.04045)

Authors: [Ismail Oussaid](https://arxiv.org/search/cs?searchtype=author&query=Oussaid%2C+I), [William Vanhuffel](https://arxiv.org/search/cs?searchtype=author&query=Vanhuffel%2C+W), [Pirashanth Ratnamogan](https://arxiv.org/search/cs?searchtype=author&query=Ratnamogan%2C+P), [Mhamed Hajaiej](https://arxiv.org/search/cs?searchtype=author&query=Hajaiej%2C+M), [Alexis Mathey](https://arxiv.org/search/cs?searchtype=author&query=Mathey%2C+A), [Thomas Gilles](https://arxiv.org/search/cs?searchtype=author&query=Gilles%2C+T)

> Information extraction (IE) from documents is an intensive area of research with a large set of industrial applications. Current state-of-the-art methods focus on scanned documents with approaches combining computer vision, natural language processing and layout representation. We propose to challenge the usage of computer vision in the case where both token style and visual representation are available (i.e native PDF documents). Our experiments on three real-world complex datasets demonstrate that using token style attributes based embedding instead of a raw visual embedding in LayoutLM model is beneficial. Depending on the dataset, such an embedding yields an improvement of 0.18% to 2.29% in the weighted F1-score with a decrease of 30.7% in the final number of trainable parameters of the model, leading to an improvement in both efficiency and effectiveness.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.04045](https://arxiv.org/abs/2111.04045) [cs.CL]** |
|           | (or **[arXiv:2111.04045v1](https://arxiv.org/abs/2111.04045v1) [cs.CL]** for this version) |





<h2 id="2021-11-09-4">4. Variance-Aware Machine Translation Test Sets
</h2>

Title: [Variance-Aware Machine Translation Test Sets](https://arxiv.org/abs/2111.04079)

Authors: [Runzhe Zhan](https://arxiv.org/search/cs?searchtype=author&query=Zhan%2C+R), [Xuebo Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X), [Derek F. Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+D+F), [Lidia S. Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+L+S)

> We release 70 small and discriminative test sets for machine translation (MT) evaluation called variance-aware test sets (VAT), covering 35 translation directions from WMT16 to WMT20 competitions. VAT is automatically created by a novel variance-aware filtering method that filters the indiscriminative test instances of the current MT test sets without any human labor. Experimental results show that VAT outperforms the original WMT test sets in terms of the correlation with human judgement across mainstream language pairs and test sets. Further analysis on the properties of VAT reveals the challenging linguistic features (e.g., translation of low-frequency words and proper nouns) for competitive MT systems, providing guidance for constructing future MT test sets. The test sets and the code for preparing variance-aware MT test sets are freely available at [this https URL](https://github.com/NLP2CT/Variance-Aware-MT-Test-Sets) .

| Comments: | Accepted to NeurIPS 2021 Datasets and Benchmarks Track       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.04079](https://arxiv.org/abs/2111.04079) [cs.CL]** |
|           | (or **[arXiv:2111.04079v1](https://arxiv.org/abs/2111.04079v1) [cs.CL]** for this version) |





<h2 id="2021-11-09-5">5. Developing neural machine translation models for Hungarian-English
</h2>

Title: [Developing neural machine translation models for Hungarian-English](https://arxiv.org/abs/2111.04099)

Authors: [Attila Nagy](https://arxiv.org/search/cs?searchtype=author&query=Nagy%2C+A)

> I train models for the task of neural machine translation for English-Hungarian and Hungarian-English, using the Hunglish2 corpus. The main contribution of this work is evaluating different data augmentation methods during the training of NMT models. I propose 5 different augmentation methods that are structure-aware, meaning that instead of randomly selecting words for blanking or replacement, the dependency tree of sentences is used as a basis for augmentation. I start my thesis with a detailed literature review on neural networks, sequential modeling, neural machine translation, dependency parsing and data augmentation. After a detailed exploratory data analysis and preprocessing of the Hunglish2 corpus, I perform experiments with the proposed data augmentation techniques. The best model for Hungarian-English achieves a BLEU score of 33.9, while the best model for English-Hungarian achieves a BLEU score of 28.6.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.04099](https://arxiv.org/abs/2111.04099) [cs.CL]** |
|           | (or **[arXiv:2111.04099v1](https://arxiv.org/abs/2111.04099v1) [cs.CL]** for this version) |





<h2 id="2021-11-09-6">6. NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework
</h2>

Title: [NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework](https://arxiv.org/abs/2111.04130)

Authors: [Xingcheng Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+X), [Yanan Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y), [Xiaocong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+X), [Zhilin Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z)

> Pretrained language models have become the standard approach for many NLP tasks due to strong performance, but they are very expensive to train. We propose a simple and efficient learning framework, TLM, that does not rely on large-scale pretraining. Given some labeled task data and a large general corpus, TLM uses task data as queries to retrieve a tiny subset of the general corpus and jointly optimizes the task objective and the language modeling objective from scratch. On eight classification datasets in four domains, TLM achieves results better than or similar to pretrained language models (e.g., RoBERTa-Large) while reducing the training FLOPs by two orders of magnitude. With high accuracy and efficiency, we hope TLM will contribute to democratizing NLP and expediting its development.

| Comments: | 13 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.04130](https://arxiv.org/abs/2111.04130) [cs.CL]** |
|           | (or **[arXiv:2111.04130v1](https://arxiv.org/abs/2111.04130v1) [cs.CL]** for this version) |





<h2 id="2021-11-09-7">7. Machine-in-the-Loop Rewriting for Creative Image Captioning
</h2>

Title: [Machine-in-the-Loop Rewriting for Creative Image Captioning](https://arxiv.org/abs/2111.04193)

Authors: [Vishakh Padmakumar](https://arxiv.org/search/cs?searchtype=author&query=Padmakumar%2C+V), [He He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+H)

> Machine-in-the-loop writing aims to enable humans to collaborate with models to complete their writing tasks more effectively. Prior work has found that providing humans a machine-written draft or sentence-level continuations has limited success since the generated text tends to deviate from humans' intention. To allow the user to retain control over the content, we train a rewriting model that, when prompted, modifies specified spans of text within the user's original draft to introduce descriptive and figurative elements locally in the text. We evaluate the model on its ability to collaborate with humans on the task of creative image captioning. On a user study through Amazon Mechanical Turk, our model is rated to be more helpful than a baseline infilling language model. In addition, third-party evaluation shows that users write more descriptive and figurative captions when collaborating with our model compared to completing the task alone.

| Comments: | Novel Ideas in Learning-to-Learn through Interaction - Workshop @ EMNLP 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.04193](https://arxiv.org/abs/2111.04193) [cs.CL]** |
|           | (or **[arXiv:2111.04193v1](https://arxiv.org/abs/2111.04193v1) [cs.CL]** for this version) |





<h2 id="2021-11-09-8">8. TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning
</h2>

Title: [TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning](https://arxiv.org/abs/2111.04198)

Authors: [Yixuan Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y), [Fangyu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Zaiqiao Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Z), [Lei Shu](https://arxiv.org/search/cs?searchtype=author&query=Shu%2C+L), [Ehsan Shareghi](https://arxiv.org/search/cs?searchtype=author&query=Shareghi%2C+E), [Nigel Collier](https://arxiv.org/search/cs?searchtype=author&query=Collier%2C+N)

> Masked language models (MLMs) such as BERT and RoBERTa have revolutionized the field of Natural Language Understanding in the past few years. However, existing pre-trained MLMs often output an anisotropic distribution of token representations that occupies a narrow subset of the entire representation space. Such token representations are not ideal, especially for tasks that demand discriminative semantic meanings of distinct tokens. In this work, we propose TaCL (Token-aware Contrastive Learning), a novel continual pre-training approach that encourages BERT to learn an isotropic and discriminative distribution of token representations. TaCL is fully unsupervised and requires no additional data. We extensively test our approach on a wide range of English and Chinese benchmarks. The results show that TaCL brings consistent and notable improvements over the original BERT model. Furthermore, we conduct detailed ablation study and careful analysis to reveal the merits and inner-workings of our approach.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.04198](https://arxiv.org/abs/2111.04198) [cs.CL]** |
|           | (or **[arXiv:2111.04198v1](https://arxiv.org/abs/2111.04198v1) [cs.CL]** for this version) |





# 2021-11-08

[Return to Index](#Index)



<h2 id="2021-11-08-1">1. StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis
</h2>

Title: [StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis](https://arxiv.org/abs/2111.03133)

Authors: [Peter Schaldenbrand](https://arxiv.org/search/cs?searchtype=author&query=Schaldenbrand%2C+P), [Zhixuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Jean Oh](https://arxiv.org/search/cs?searchtype=author&query=Oh%2C+J)

> Generating images that fit a given text description using machine learning has improved greatly with the release of technologies such as the CLIP image-text encoder model; however, current methods lack artistic control of the style of image to be generated. We introduce StyleCLIPDraw which adds a style loss to the CLIPDraw text-to-drawing synthesis model to allow artistic control of the synthesized drawings in addition to control of the content via text. Whereas performing decoupled style transfer on a generated image only affects the texture, our proposed coupled approach is able to capture a style in both texture and shape, suggesting that the style of the drawing is coupled with the drawing process itself. More results and our code are available at [this https URL](https://github.com/pschaldenbrand/StyleCLIPDraw)

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.03133](https://arxiv.org/abs/2111.03133) [cs.CV]** |
|           | (or **[arXiv:2111.03133v1](https://arxiv.org/abs/2111.03133v1) [cs.CV]** for this version) |





<h2 id="2021-11-08-2">2. The Curious Layperson: Fine-Grained Image Recognition without Expert Labels
</h2>

Title: [The Curious Layperson: Fine-Grained Image Recognition without Expert Labels](https://arxiv.org/abs/2111.03651)

Authors: [Subhabrata Choudhury](https://arxiv.org/search/cs?searchtype=author&query=Choudhury%2C+S), [Iro Laina](https://arxiv.org/search/cs?searchtype=author&query=Laina%2C+I), [Christian Rupprecht](https://arxiv.org/search/cs?searchtype=author&query=Rupprecht%2C+C), [Andrea Vedaldi](https://arxiv.org/search/cs?searchtype=author&query=Vedaldi%2C+A)

> Most of us are not experts in specific fields, such as ornithology. Nonetheless, we do have general image and language understanding capabilities that we use to match what we see to expert resources. This allows us to expand our knowledge and perform novel tasks without ad-hoc external supervision. On the contrary, machines have a much harder time consulting expert-curated knowledge bases unless trained specifically with that knowledge in mind. Thus, in this paper we consider a new problem: fine-grained image recognition without expert annotations, which we address by leveraging the vast knowledge available in web encyclopedias. First, we learn a model to describe the visual appearance of objects using non-expert image descriptions. We then train a fine-grained textual similarity model that matches image descriptions with documents on a sentence-level basis. We evaluate the method on two datasets and compare with several strong baselines and the state of the art in cross-modal retrieval. Code is available at: [this https URL](https://github.com/subhc/clever)

| Comments: | To appear in BMVC 2021 (Oral). Project page: [this https URL](https://www.robots.ox.ac.uk/~vgg/research/clever/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2111.03651](https://arxiv.org/abs/2111.03651) [cs.CV]** |
|           | (or **[arXiv:2111.03651v1](https://arxiv.org/abs/2111.03651v1) [cs.CV]** for this version) |





<h2 id="2021-11-08-3">3. How Do Neural Sequence Models Generalize? Local and Global Context Cues for Out-of-Distribution Prediction
</h2>

Title: [How Do Neural Sequence Models Generalize? Local and Global Context Cues for Out-of-Distribution Prediction](https://arxiv.org/abs/2111.03108)

Authors: [Anthony Bau](https://arxiv.org/search/cs?searchtype=author&query=Bau%2C+A), [Jacob Andreas](https://arxiv.org/search/cs?searchtype=author&query=Andreas%2C+J)

> After a neural sequence model encounters an unexpected token, can its behavior be predicted? We show that RNN and transformer language models exhibit structured, consistent generalization in out-of-distribution contexts. We begin by introducing two idealized models of generalization in next-word prediction: a local context model in which generalization is consistent with the last word observed, and a global context model in which generalization is consistent with the global structure of the input. In experiments in English, Finnish, Mandarin, and random regular languages, we demonstrate that neural language models interpolate between these two forms of generalization: their predictions are well-approximated by a log-linear combination of local and global predictive distributions. We then show that, in some languages, noise mediates the two forms of generalization: noise applied to input tokens encourages global generalization, while noise in history representations encourages local generalization. Finally, we offer a preliminary theoretical explanation of these results by proving that the observed interpolation behavior is expected in log-linear models with a particular feature correlation structure. These results help explain the effectiveness of two popular regularization schemes and show that aspects of sequence model generalization can be understood and controlled.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.03108](https://arxiv.org/abs/2111.03108) [cs.CL]** |
|           | (or **[arXiv:2111.03108v1](https://arxiv.org/abs/2111.03108v1) [cs.CL]** for this version) |





<h2 id="2021-11-08-4">4. A Syntax-Guided Grammatical Error Correction Model with Dependency Tree Correction
</h2>

Title: [A Syntax-Guided Grammatical Error Correction Model with Dependency Tree Correction](https://arxiv.org/abs/2111.03294)

Authors: [Zhaohong Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+Z), [Xiaojun Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+X)

> Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sentences. Recently, neural machine translation systems have become popular approaches for this task. However, these methods lack the use of syntactic knowledge which plays an important role in the correction of grammatical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) which adopts the graph attention mechanism to utilize the syntactic knowledge of dependency trees. Considering the dependency trees of the grammatically incorrect source sentences might provide incorrect syntactic knowledge, we propose a dependency tree correction task to deal with it. Combining with data augmentation method, our model achieves strong performances without using any large pre-trained models. We evaluate our model on public benchmarks of GEC task and it achieves competitive results.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.03294](https://arxiv.org/abs/2111.03294) [cs.CL]** |
|           | (or **[arXiv:2111.03294v1](https://arxiv.org/abs/2111.03294v1) [cs.CL]** for this version) |






# 2021-11-05

[Return to Index](#Index)



<h2 id="2021-11-05-1">1. Benchmarking Multimodal AutoML for Tabular Data with Text Fields
</h2>

Title: [Benchmarking Multimodal AutoML for Tabular Data with Text Fields](https://arxiv.org/abs/2111.02705)

Authors: [Xingjian Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+X), [Jonas Mueller](https://arxiv.org/search/cs?searchtype=author&query=Mueller%2C+J), [Nick Erickson](https://arxiv.org/search/cs?searchtype=author&query=Erickson%2C+N), [Mu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M), [Alexander J. Smola](https://arxiv.org/search/cs?searchtype=author&query=Smola%2C+A+J)

> We consider the use of automated supervised learning systems for data tables that not only contain numeric/categorical columns, but one or more text fields as well. Here we assemble 18 multimodal data tables that each contain some text fields and stem from a real business application. Our publicly-available benchmark enables researchers to comprehensively evaluate their own methods for supervised learning with numeric, categorical, and text features. To ensure that any single modeling strategy which performs well over all 18 datasets will serve as a practical foundation for multimodal text/tabular AutoML, the diverse datasets in our benchmark vary greatly in: sample size, problem types (a mix of classification and regression tasks), number of features (with the number of text columns ranging from 1 to 28 between datasets), as well as how the predictive signal is decomposed between text vs. numeric/categorical features (and predictive interactions thereof). Over this benchmark, we evaluate various straightforward pipelines to model such data, including standard two-stage approaches where NLP is used to featurize the text such that AutoML for tabular data can then be applied. Compared with human data science teams, the fully automated methodology that performed best on our benchmark (stack ensembling a multimodal Transformer with various tree models) also manages to rank 1st place when fit to the raw text/tabular data in two MachineHack prediction competitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price Suggestion Challenge.

| Comments: | Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2111.02705](https://arxiv.org/abs/2111.02705) [cs.LG]** |
|           | (or **[arXiv:2111.02705v1](https://arxiv.org/abs/2111.02705v1) [cs.LG]** for this version) |





<h2 id="2021-11-05-2">2. Lexically Aware Semi-Supervised Learning for OCR Post-Correction
</h2>

Title: [Lexically Aware Semi-Supervised Learning for OCR Post-Correction](https://arxiv.org/abs/2111.02622)

Authors: [Shruti Rijhwani](https://arxiv.org/search/cs?searchtype=author&query=Rijhwani%2C+S), [Daisy Rosenblum](https://arxiv.org/search/cs?searchtype=author&query=Rosenblum%2C+D), [Antonios Anastasopoulos](https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Much of the existing linguistic data in many languages of the world is locked away in non-digitized books and documents. Optical character recognition (OCR) can be used to produce digitized text, and previous work has demonstrated the utility of neural post-correction methods that improve the results of general-purpose OCR systems on recognition of less-well-resourced languages. However, these methods rely on manually curated post-correction data, which are relatively scarce compared to the non-annotated raw images that need to be digitized. 
> In this paper, we present a semi-supervised learning method that makes it possible to utilize these raw images to improve performance, specifically through the use of self-training, a technique where a model is iteratively trained on its own outputs. In addition, to enforce consistency in the recognized vocabulary, we introduce a lexically-aware decoding method that augments the neural post-correction model with a count-based language model constructed from the recognized texts, implemented using weighted finite-state automata (WFSA) for efficient and effective decoding. 
> Results on four endangered languages demonstrate the utility of the proposed method, with relative error reductions of 15-29%, where we find the combination of self-training and lexically-aware decoding essential for achieving consistent improvements. Data and code are available at [this https URL](https://shrutirij.github.io/ocr-el/).

| Comments: | Accepted to the Transactions of the Association for Computational Linguistics (TACL) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.02622](https://arxiv.org/abs/2111.02622) [cs.CL]** |
|           | (or **[arXiv:2111.02622v1](https://arxiv.org/abs/2111.02622v1) [cs.CL]** for this version) |





<h2 id="2021-11-05-3">3. Response Generation with Context-Aware Prompt Learning
</h2>

Title: [Response Generation with Context-Aware Prompt Learning](https://arxiv.org/abs/2111.02643)

Authors: [Xiaodong Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+X), [Kang Min Yoo](https://arxiv.org/search/cs?searchtype=author&query=Yoo%2C+K+M), [Sang-Woo Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S)

> Pre-trained language models (PLM) have marked a huge leap in neural dialogue modeling. While PLMs are pre-trained on large-scale text corpora, they are usually fine-tuned on scarce dialogue data with specific domain knowledge and dialogue styles. However, tailoring the language models while fully utilizing prior knowledge in large pre-trained models remains a challenge. In this paper, we present a novel approach for pre-trained dialogue modeling that casts the dialogue generation problem as a prompt-learning task. Instead of fine-tuning on limited dialogue data, our approach, DialogPrompt, learns continuous prompt embeddings optimized for dialogue contexts, which appropriately elicit knowledge from the large pre-trained model. To encourage the model to better utilize the prompt embeddings, the prompt encoders are designed to be conditioned on the input dialogue context. Experiments on popular conversation datasets show that our approach significantly outperforms the fine-tuning baseline and the generic prompt-learning methods. Furthermore, human evaluations strongly support the superiority of DialogPrompt in regard to response generation quality.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.02643](https://arxiv.org/abs/2111.02643) [cs.CL]** |
|           | (or **[arXiv:2111.02643v1](https://arxiv.org/abs/2111.02643v1) [cs.CL]** for this version) |





<h2 id="2021-11-05-4">4. A text autoencoder from transformer for fast encoding language representation
</h2>

Title: [A text autoencoder from transformer for fast encoding language representation](https://arxiv.org/abs/2111.02844)

Authors: [Tan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+T)

> In recent years BERT shows apparent advantages and great potential in natural language processing tasks. However, both training and applying BERT requires intensive time and resources for computing contextual language representations, which hinders its universality and applicability. To overcome this bottleneck, we propose a deep bidirectional language model by using window masking mechanism at attention layer. This work computes contextual language representations without random masking as does in BERT and maintains the deep bidirectional architecture like BERT. To compute the same sentence representation, our method shows O(n) complexity less compared to other transformer-based models with O(n2). To further demonstrate its superiority, computing context language representations on CPU environments is conducted, by using the embeddings from the proposed method, logistic regression shows much higher accuracy in terms of SMS classification. Moverover, the proposed method also achieves significant higher performance in semantic similarity tasks.

| Comments: | 8 pages, 8 figures. arXiv admin note: text overlap with [arXiv:2004.08097](https://arxiv.org/abs/2004.08097) by other authors |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2111.02844](https://arxiv.org/abs/2111.02844) [cs.CL]** |
|           | (or **[arXiv:2111.02844v1](https://arxiv.org/abs/2111.02844v1) [cs.CL]** for this version) |





<h2 id="2021-11-05-5">5. CoreLM: Coreference-aware Language Model Fine-Tuning
</h2>

Title: [CoreLM: Coreference-aware Language Model Fine-Tuning](https://arxiv.org/abs/2111.02687)

Authors: [Nikolaos Stylianou](https://arxiv.org/search/cs?searchtype=author&query=Stylianou%2C+N), [Ioannis Vlahavas](https://arxiv.org/search/cs?searchtype=author&query=Vlahavas%2C+I)

> Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the fine-tuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models' performance in terms of Accuracy in LAMBADA and Children's Book Test, with and without the use of model-created coreference annotations.

| Comments: | 12 pages, 2 figures, Accepted at Fourth Workshop on Computational Models of Reference, Anaphora and Coreference |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.02687](https://arxiv.org/abs/2111.02687) [cs.CL]** |
|           | (or **[arXiv:2111.02687v1](https://arxiv.org/abs/2111.02687v1) [cs.CL]** for this version) |





# 2021-11-04

[Return to Index](#Index)



<h2 id="2021-11-04-1">1. LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs
</h2>

Title: [LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs](https://arxiv.org/abs/2111.02114)

Authors: [Christoph Schuhmann](https://arxiv.org/search/cs?searchtype=author&query=Schuhmann%2C+C), [Richard Vencu](https://arxiv.org/search/cs?searchtype=author&query=Vencu%2C+R), [Romain Beaumont](https://arxiv.org/search/cs?searchtype=author&query=Beaumont%2C+R), [Robert Kaczmarczyk](https://arxiv.org/search/cs?searchtype=author&query=Kaczmarczyk%2C+R), [Clayton Mullis](https://arxiv.org/search/cs?searchtype=author&query=Mullis%2C+C), [Aarush Katta](https://arxiv.org/search/cs?searchtype=author&query=Katta%2C+A), [Theo Coombes](https://arxiv.org/search/cs?searchtype=author&query=Coombes%2C+T), [Jenia Jitsev](https://arxiv.org/search/cs?searchtype=author&query=Jitsev%2C+J), [Aran Komatsuzaki](https://arxiv.org/search/cs?searchtype=author&query=Komatsuzaki%2C+A)

> Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratch. To address this issue, in a community effort we build and release for public LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.

| Comments: | Short version. Accepted at Data Centric AI NeurIPS Workshop 2021 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.02114](https://arxiv.org/abs/2111.02114) [cs.CV]** |
|           | (or **[arXiv:2111.02114v1](https://arxiv.org/abs/2111.02114v1) [cs.CV]** for this version) |





<h2 id="2021-11-04-2">2. VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts
</h2>

Title: [VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts](https://arxiv.org/abs/2111.02358)

Authors: [Wenhui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Hangbo Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+H), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer, where each block contains a pool of modality-specific experts and a shared self-attention layer. Because of the modeling flexibility of MoME, pretrained VLMo can be fine-tuned as a fusion encoder for vision-language classification tasks, or used as a dual encoder for efficient image-text retrieval. Moreover, we propose a stagewise pre-training strategy, which effectively leverages large-scale image-only and text-only data besides image-text pairs. Experimental results show that VLMo achieves state-of-the-art results on various vision-language tasks, including VQA and NLVR2. The code and pretrained models are available at [this https URL](https://aka.ms/vlmo).

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.02358](https://arxiv.org/abs/2111.02358) [cs.CV]** |
|           | (or **[arXiv:2111.02358v1](https://arxiv.org/abs/2111.02358v1) [cs.CV]** for this version) |



<h2 id="2021-11-04-3">3. An Empirical Study of Training End-to-End Vision-and-Language Transformers
</h2>

Title: [An Empirical Study of Training End-to-End Vision-and-Language Transformers](https://arxiv.org/abs/2111.02387)

Authors: [Zi-Yi Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Z), [Yichong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Jianfeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Nanyun](https://arxiv.org/search/cs?searchtype=author&query=Nanyun) (Violet)[Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng), [Zicheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M)

> Vision-and-language (VL) pre-training has proven to be highly effective on various VL downstream tasks. While recent work has shown that fully transformer-based VL models can be more efficient than previous region-feature-based methods, their performance on downstream tasks are often degraded significantly. In this paper, we present METER~(\textbf{M}ultimodal \textbf{E}nd-to-end \textbf{T}ransform\textbf{ER}), through which we systematically investigate how to design and pre-train a fully transformer-based VL model in an end-to-end manner. Specifically, we dissect the model designs along multiple dimensions: vision encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa, DeBERTa), multimodal fusion (e.g., merged attention vs. co-attention), architecture design (e.g., encoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked image modeling). We conduct comprehensive experiments on a wide range of VL tasks, and provide insights on how to train a performant VL transformer while maintaining fast inference speed. Notably, METER~achieves an accuracy of 77.64\% on the VQAv2 test-std set using only 4M images for pre-training, surpassing the state-of-the-art region-feature-based VinVL model by +1.04\%, and outperforming the previous best fully transformer-based ALBEF model by +1.6\%.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.02387](https://arxiv.org/abs/2111.02387) [cs.CV]** |
|           | (or **[arXiv:2111.02387v1](https://arxiv.org/abs/2111.02387v1) [cs.CV]** for this version) |





<h2 id="2021-11-04-4">4. OpenPrompt: An Open-source Framework for Prompt-learning
</h2>

Title: [OpenPrompt: An Open-source Framework for Prompt-learning](https://arxiv.org/abs/2111.01998)

Authors: [Ning Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N), [Shengding Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S), [Weilin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W), [Yulin Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Hai-Tao Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M)

> Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing prompt-learning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, and verbalizing strategy, etc. need to be considered in prompt-learning, practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper, we present {OpenPrompt}, a unified easy-to-use toolkit to conduct prompt-learning over PLMs. OpenPrompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task formats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. OpenPrompt is publicly released at {\url{ [this https URL](https://github.com/thunlp/OpenPrompt)}}.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.01998](https://arxiv.org/abs/2111.01998) [cs.CL]** |
|           | (or **[arXiv:2111.01998v1](https://arxiv.org/abs/2111.01998v1) [cs.CL]** for this version) |





<h2 id="2021-11-04-5">5. Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task
</h2>

Title: [Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task](https://arxiv.org/abs/2111.02086)

Authors: [Jian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Haoyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Alexandre Muzio](https://arxiv.org/search/cs?searchtype=author&query=Muzio%2C+A), [Saksham Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal%2C+S), [Hany Hassan Awadalla](https://arxiv.org/search/cs?searchtype=author&query=Awadalla%2C+H+H), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> This report describes Microsoft's machine translation systems for the WMT21 shared task on large-scale multilingual machine translation. We participated in all three evaluation tracks including Large Track and two Small Tracks where the former one is unconstrained and the latter two are fully constrained. Our model submissions to the shared task were initialized with DeltaLM\footnote{\url{[this https URL](https://aka.ms/deltalm)}}, a generic pre-trained multilingual encoder-decoder model, and fine-tuned correspondingly with the vast collected parallel data and allowed data sources according to track settings, together with applying progressive learning and iterative back-translation approaches to further improve the performance. Our final submissions ranked first on three tracks in terms of the automatic evaluation metric.

| Comments: | WMT21                                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.02086](https://arxiv.org/abs/2111.02086) [cs.CL]** |
|           | (or **[arXiv:2111.02086v1](https://arxiv.org/abs/2111.02086v1) [cs.CL]** for this version) |





<h2 id="2021-11-04-6">6. Lingua Custodia's participation at the WMT 2021 Machine Translation using Terminologies shared task
</h2>

Title: [Lingua Custodia's participation at the WMT 2021 Machine Translation using Terminologies shared task](https://arxiv.org/abs/2111.02120)

Authors: [Melissa Ailem](https://arxiv.org/search/cs?searchtype=author&query=Ailem%2C+M), [Jinghsu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Raheel Qader](https://arxiv.org/search/cs?searchtype=author&query=Qader%2C+R)

> This paper describes Lingua Custodia's submission to the WMT21 shared task on machine translation using terminologies. We consider three directions, namely English to French, Russian, and Chinese. We rely on a Transformer-based architecture as a building block, and we explore a method which introduces two main changes to the standard procedure to handle terminologies. The first one consists in augmenting the training data in such a way as to encourage the model to learn a copy behavior when it encounters terminology constraint terms. The second change is constraint token masking, whose purpose is to ease copy behavior learning and to improve model generalization. Empirical results show that our method satisfies most terminology constraints while maintaining high translation quality.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.02120](https://arxiv.org/abs/2111.02120) [cs.CL]** |
|           | (or **[arXiv:2111.02120v1](https://arxiv.org/abs/2111.02120v1) [cs.CL]** for this version) |





<h2 id="2021-11-04-7">7. BERT-DRE: BERT with Deep Recursive Encoder for Natural Language Sentence Matching
</h2>

Title: [BERT-DRE: BERT with Deep Recursive Encoder for Natural Language Sentence Matching](https://arxiv.org/abs/2111.02188)

Authors: [Ehsan Tavan](https://arxiv.org/search/cs?searchtype=author&query=Tavan%2C+E), [Ali Rahmati](https://arxiv.org/search/cs?searchtype=author&query=Rahmati%2C+A), [Maryam Najafi](https://arxiv.org/search/cs?searchtype=author&query=Najafi%2C+M), [Saeed Bibak](https://arxiv.org/search/cs?searchtype=author&query=Bibak%2C+S)

> This paper presents a deep neural architecture, for Natural Language Sentence Matching (NLSM) by adding a deep recursive encoder to BERT so called BERT with Deep Recursive Encoder (BERT-DRE). Our analysis of model behavior shows that BERT still does not capture the full complexity of text, so a deep recursive encoder is applied on top of BERT. Three Bi-LSTM layers with residual connection are used to design a recursive encoder and an attention module is used on top of this encoder. To obtain the final vector, a pooling layer consisting of average and maximum pooling is used. We experiment our model on four benchmarks, SNLI, FarsTail, MultiNLI, SciTail, and a novel Persian religious questions dataset. This paper focuses on improving the BERT results in the NLSM task. In this regard, comparisons between BERT-DRE and BERT are conducted, and it is shown that in all cases, BERT-DRE outperforms only BERT. The BERT algorithm on the religious dataset achieved an accuracy of 89.70%, and BERT-DRE architectures improved to 90.29% using the same dataset.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.02188](https://arxiv.org/abs/2111.02188) [cs.CL]** |
|           | (or **[arXiv:2111.02188v1](https://arxiv.org/abs/2111.02188v1) [cs.CL]** for this version) |







# 2021-11-03

[Return to Index](#Index)



<h2 id="2021-11-03-1">1. Recent Advances in End-to-End Automatic Speech Recognition
</h2>

Title: [Recent Advances in End-to-End Automatic Speech Recognition](https://arxiv.org/abs/2111.01690)

Authors: [Jinyu Li](https://arxiv.org/search/eess?searchtype=author&query=Li%2C+J)

> Recently, the speech community is seeing a significant trend of moving from deep neural network based hybrid modeling to end-to-end (E2E) modeling for automatic speech recognition (ASR). While E2E models achieve the state-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid models are still used in a large proportion of commercial ASR systems at the current time. There are lots of practical factors that affect the production model deployment decision. Traditional hybrid models, being optimized for production for decades, are usually good at these factors. Without providing excellent solutions to all these factors, it is hard for E2E models to be widely commercialized. In this paper, we will overview the recent advances in E2E models, focusing on technologies addressing those challenges from the industry's perspective.

| Comments: | invited paper submitted to APSIPA Transactions on Signal and Information Processing |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2111.01690](https://arxiv.org/abs/2111.01690) [eess.AS]** |
|           | (or **[arXiv:2111.01690v1](https://arxiv.org/abs/2111.01690v1) [eess.AS]** for this version) |





<h2 id="2021-11-03-2">2. Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey
</h2>

Title: [Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey](https://arxiv.org/abs/2111.01243)

Authors: [Bonan Min](https://arxiv.org/search/cs?searchtype=author&query=Min%2C+B), [Hayley Ross](https://arxiv.org/search/cs?searchtype=author&query=Ross%2C+H), [Elior Sulem](https://arxiv.org/search/cs?searchtype=author&query=Sulem%2C+E), [Amir Pouran Ben Veyseh](https://arxiv.org/search/cs?searchtype=author&query=Veyseh%2C+A+P+B), [Thien Huu Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+H), [Oscar Sainz](https://arxiv.org/search/cs?searchtype=author&query=Sainz%2C+O), [Eneko Agirre](https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E), [Ilana Heinz](https://arxiv.org/search/cs?searchtype=author&query=Heinz%2C+I), [Dan Roth](https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+D)

> Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.01243](https://arxiv.org/abs/2111.01243) [cs.CL]** |
|           | (or **[arXiv:2111.01243v1](https://arxiv.org/abs/2111.01243v1) [cs.CL]** for this version) |





<h2 id="2021-11-03-3">3. Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP
</h2>

Title: [Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP](https://arxiv.org/abs/2111.01322)

Authors: Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP

[Trapit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+T), [Karthick Gunasekaran](https://arxiv.org/search/cs?searchtype=author&query=Gunasekaran%2C+K), [Tong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Tsendsuren Munkhdalai](https://arxiv.org/search/cs?searchtype=author&query=Munkhdalai%2C+T), [Andrew McCallum](https://arxiv.org/search/cs?searchtype=author&query=McCallum%2C+A)

> Meta-learning considers the problem of learning an efficient learning process that can leverage its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from limited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automatically proposed from unlabeled text, to enable large-scale meta-learning in NLP. We design multiple distributions of self-supervised tasks by considering important aspects of task diversity, difficulty, type, domain, and curriculum, and investigate how they affect meta-learning performance. Our analysis shows that all these factors meaningfully alter the task distribution, some inducing significant improvements in downstream few-shot accuracy of the meta-learned models. Empirically, results on 20 downstream tasks show significant improvements in few-shot learning -- adding up to +4.2% absolute accuracy (on average) to the previous unsupervised meta-learning method, and perform comparably to supervised methods on the FewRel 2.0 benchmark.

| Comments: | To appear at EMNLP 2021                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.01322](https://arxiv.org/abs/2111.01322) [cs.CL]** |
|           | (or **[arXiv:2111.01322v1](https://arxiv.org/abs/2111.01322v1) [cs.CL]** for this version) |





<h2 id="2021-11-03-4">4. Adapting to the Long Tail: A Meta-Analysis of Transfer Learning Research for Language Understanding Tasks
</h2>

Title: [Adapting to the Long Tail: A Meta-Analysis of Transfer Learning Research for Language Understanding Tasks](https://arxiv.org/abs/2111.01340)

Authors: [Aakanksha Naik](https://arxiv.org/search/cs?searchtype=author&query=Naik%2C+A), [Jill Lehman](https://arxiv.org/search/cs?searchtype=author&query=Lehman%2C+J), [Carolyn Rose](https://arxiv.org/search/cs?searchtype=author&query=Rose%2C+C)

> Natural language understanding (NLU) has made massive progress driven by large benchmarks, paired with research on transfer learning to broaden its impact. Benchmarks are dominated by a small set of frequent phenomena, leaving a long tail of infrequent phenomena underrepresented. In this work, we reflect on the question: have transfer learning methods sufficiently addressed performance of benchmark-trained models on the long tail? Since benchmarks do not list included/excluded phenomena, we conceptualize the long tail using macro-level dimensions such as underrepresented genres, topics, etc. We assess trends in transfer learning research through a qualitative meta-analysis of 100 representative papers on transfer learning for NLU. Our analysis asks three questions: (i) Which long tail dimensions do transfer learning studies target? (ii) Which properties help adaptation methods improve performance on the long tail? (iii) Which methodological gaps have greatest negative impact on long tail performance? Our answers to these questions highlight major avenues for future research in transfer learning for the long tail. Lastly, we present a case study comparing the performance of various adaptation methods on clinical narratives to show how systematically conducted meta-experiments can provide insights that enable us to make progress along these future avenues.

| Comments: | 14 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.01340](https://arxiv.org/abs/2111.01340) [cs.CL]** |
|           | (or **[arXiv:2111.01340v1](https://arxiv.org/abs/2111.01340v1) [cs.CL]** for this version) |





<h2 id="2021-11-03-5">5. System Combination for Grammatical Error Correction Based on Integer Programming
</h2>

Title: [System Combination for Grammatical Error Correction Based on Integer Programming](https://arxiv.org/abs/2111.01465)

Authors: [Ruixi Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+R), [Hwee Tou Ng](https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+H+T)

> In this paper, we propose a system combination method for grammatical error correction (GEC), based on nonlinear integer programming (IP). Our method optimizes a novel F score objective based on error types, and combines multiple end-to-end GEC systems. The proposed IP approach optimizes the selection of a single best system for each grammatical error type present in the data. Experiments of the IP approach on combining state-of-the-art standalone GEC systems show that the combined system outperforms all standalone systems. It improves F0.5 score by 3.61% when combining the two best participating systems in the BEA 2019 shared task, and achieves F0.5 score of 73.08%. We also perform experiments to compare our IP approach with another state-of-the-art system combination method for GEC, demonstrating IP's competitive combination capability.

| Comments:          | Accepted for RANLP 2021                                      |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | RANLP (RECENT ADVANCES IN NATURAL LANGUAGE PROCESSING) (2021) |
| Cite as:           | **[arXiv:2111.01465](https://arxiv.org/abs/2111.01465) [cs.CL]** |
|                    | (or **[arXiv:2111.01465v1](https://arxiv.org/abs/2111.01465v1) [cs.CL]** for this version) |





<h2 id="2021-11-03-6">6. Zero-Shot Translation using Diffusion Models
</h2>

Title: [Zero-Shot Translation using Diffusion Models](https://arxiv.org/abs/2111.01471)

Authors: [Eliya Nachmani](https://arxiv.org/search/cs?searchtype=author&query=Nachmani%2C+E), [Shaked Dovrat](https://arxiv.org/search/cs?searchtype=author&query=Dovrat%2C+S)

> In this work, we show a novel method for neural machine translation (NMT), using a denoising diffusion probabilistic model (DDPM), adjusted for textual data, following recent advances in the field. We show that it's possible to translate sentences non-autoregressively using a diffusion model conditioned on the source sentence. We also show that our model is able to translate between pairs of languages unseen during training (zero-shot learning).

| Comments: | preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.01471](https://arxiv.org/abs/2111.01471) [cs.CL]** |
|           | (or **[arXiv:2111.01471v1](https://arxiv.org/abs/2111.01471v1) [cs.CL]** for this version) |





<h2 id="2021-11-03-7">7. HydraText: Multi-objective Optimization for Adversarial Textual Attack
</h2>

Title: [HydraText: Multi-objective Optimization for Adversarial Textual Attack](https://arxiv.org/abs/2111.01528)

Authors: [Shengcai Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Ning Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+N), [Cheng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Chao Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+C), [Ke Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+K)

> The field of adversarial textual attack has significantly grown over the last years, where the commonly considered objective is to craft adversarial examples that can successfully fool the target models. However, the imperceptibility of attacks, which is also an essential objective, is often left out by previous studies. In this work, we advocate considering both objectives at the same time, and propose a novel multi-optimization approach (dubbed HydraText) with provable performance guarantee to achieve successful attacks with high imperceptibility. We demonstrate the efficacy of HydraText through extensive experiments under both score-based and decision-based settings, involving five modern NLP models across five benchmark datasets. In comparison to existing state-of-the-art attacks, HydraText consistently achieves simultaneously higher success rates, lower modification rates, and higher semantic similarity to the original texts. A human evaluation study shows that the adversarial examples crafted by HydraText maintain validity and naturality well. Finally, these examples also exhibit good transferability and can bring notable robustness improvement to the target models by adversarial training.

| Subjects: | **Computation and Language (cs.CL)**; Neural and Evolutionary Computing (cs.NE) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.01528](https://arxiv.org/abs/2111.01528) [cs.CL]** |
|           | (or **[arXiv:2111.01528v1](https://arxiv.org/abs/2111.01528v1) [cs.CL]** for this version) |





<h2 id="2021-11-03-8">8. LMdiff: A Visual Diff Tool to Compare Language Models
</h2>

Title: [LMdiff: A Visual Diff Tool to Compare Language Models](https://arxiv.org/abs/2111.01582)

Authors: [Hendrik Strobelt](https://arxiv.org/search/cs?searchtype=author&query=Strobelt%2C+H), [Benjamin Hoover](https://arxiv.org/search/cs?searchtype=author&query=Hoover%2C+B), [Arvind Satyanarayan](https://arxiv.org/search/cs?searchtype=author&query=Satyanarayan%2C+A), [Sebastian Gehrmann](https://arxiv.org/search/cs?searchtype=author&query=Gehrmann%2C+S)

> While different language models are ubiquitous in NLP, it is hard to contrast their outputs and identify which contexts one can handle better than the other. To address this question, we introduce LMdiff, a tool that visually compares probability distributions of two models that differ, e.g., through finetuning, distillation, or simply training with different parameter sizes. LMdiff allows the generation of hypotheses about model behavior by investigating text instances token by token and further assists in choosing these interesting text instances by identifying the most interesting phrases from large corpora. We showcase the applicability of LMdiff for hypothesis generation across multiple case studies. A demo is available at [this http URL](http://lmdiff.net/) .

| Comments: | EMNLP 2021 Demo Paper                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2111.01582](https://arxiv.org/abs/2111.01582) [cs.CL]** |
|           | (or **[arXiv:2111.01582v1](https://arxiv.org/abs/2111.01582v1) [cs.CL]** for this version) |








# 2021-11-02

[Return to Index](#Index)



<h2 id="2021-11-02-1">1. Introspective Distillation for Robust Question Answering
</h2>

Title: [Introspective Distillation for Robust Question Answering](https://arxiv.org/abs/2111.01026)

Authors: [Yulei Niu](https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+Y), [Hanwang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H)

> Question answering (QA) models are well-known to exploit data bias, e.g., the language prior in visual QA and the position bias in reading comprehension. Recent debiasing methods achieve good out-of-distribution (OOD) generalizability with a considerable sacrifice of the in-distribution (ID) performance. Therefore, they are only applicable in domains where the test distribution is known in advance. In this paper, we present a novel debiasing method called Introspective Distillation (IntroD) to make the best of both worlds for QA. Our key technical contribution is to blend the inductive bias of OOD and ID by introspecting whether a training sample fits in the factual ID world or the counterfactual OOD one. Experiments on visual QA datasets VQA v2, VQA-CP, and reading comprehension dataset SQuAD demonstrate that our proposed IntroD maintains the competitive OOD performance compared to other debiasing methods, while sacrificing little or even achieving better ID performance compared to the non-debiasing ones.

| Comments: | Accepted by NeurIPS 2021                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2111.01026](https://arxiv.org/abs/2111.01026) [cs.CV]** |
|           | (or **[arXiv:2111.01026v1](https://arxiv.org/abs/2111.01026v1) [cs.CV]** for this version) |





<h2 id="2021-11-02-2">2. TransAug: Translate as Augmentation for Sentence Embeddings
</h2>

Title: [TransAug: Translate as Augmentation for Sentence Embeddings](https://arxiv.org/abs/2111.00157)

Authors: [Jue Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Haofan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Xing Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Chaochen Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C), [Debing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D)

> While contrastive learning greatly advances the representation of sentence embeddings, it is still limited by the size of the existing sentence datasets. In this paper, we present TransAug (Translate as Augmentation), which provide the first exploration of utilizing translated sentence pairs as data augmentation for text, and introduce a two-stage paradigm to advances the state-of-the-art sentence embeddings. Instead of adopting an encoder trained in other languages setting, we first distill a Chinese encoder from a SimCSE encoder (pretrained in English), so that their embeddings are close in semantic space, which can be regraded as implicit data augmentation. Then, we only update the English encoder via cross-lingual contrastive learning and frozen the distilled Chinese encoder. Our approach achieves a new state-of-art on standard semantic textual similarity (STS), outperforming both SimCSE and Sentence-T5, and the best performance in corresponding tracks on transfer tasks evaluated by SentEval.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.00157](https://arxiv.org/abs/2111.00157) [cs.CL]** |
|           | (or **[arXiv:2111.00157v1](https://arxiv.org/abs/2111.00157v1) [cs.CL]** for this version) |





<h2 id="2021-11-02-3">3. How should human translation coexist with NMT? Efficient tool for building high quality parallel corpus
</h2>

Title: [How should human translation coexist with NMT? Efficient tool for building high quality parallel corpus](https://arxiv.org/abs/2111.00191)

Authors: [Chanjun Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+C), [Seolhwa Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S), [Hyeonseok Moon](https://arxiv.org/search/cs?searchtype=author&query=Moon%2C+H), [Sugyeong Eo](https://arxiv.org/search/cs?searchtype=author&query=Eo%2C+S), [Jaehyung Seo](https://arxiv.org/search/cs?searchtype=author&query=Seo%2C+J), [Heuiseok Lim](https://arxiv.org/search/cs?searchtype=author&query=Lim%2C+H)

> This paper proposes a tool for efficiently constructing high-quality parallel corpora with minimizing human labor and making this tool publicly available. Our proposed construction process is based on neural machine translation (NMT) to allow for it to not only coexist with human translation, but also improve its efficiency by combining data quality control with human translation in a data-centric approach.

| Comments: | Accepted for Data-centric AI workshop at NeurIPS 2021        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2111.00191](https://arxiv.org/abs/2111.00191) [cs.CL]** |
|           | (or **[arXiv:2111.00191v1](https://arxiv.org/abs/2111.00191v1) [cs.CL]** for this version) |





<h2 id="2021-11-02-4">4. Visualization: the missing factor in Simultaneous Speech Translation
</h2>

Title: [Visualization: the missing factor in Simultaneous Speech Translation](https://arxiv.org/abs/2111.00514)

Authors: [Sara Papi](https://arxiv.org/search/cs?searchtype=author&query=Papi%2C+S), [Matteo Negri](https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M), [Marco Turchi](https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M)

> Simultaneous speech translation (SimulST) is the task in which output generation has to be performed on partial, incremental speech input. In recent years, SimulST has become popular due to the spread of cross-lingual application scenarios, like international live conferences and streaming lectures, in which on-the-fly speech translation can facilitate users' access to audio-visual content. In this paper, we analyze the characteristics of the SimulST systems developed so far, discussing their strengths and weaknesses. We then concentrate on the evaluation framework required to properly assess systems' effectiveness. To this end, we raise the need for a broader performance analysis, also including the user experience standpoint. SimulST systems, indeed, should be evaluated not only in terms of quality/latency measures, but also via task-oriented metrics accounting, for instance, for the visualization strategy adopted. In light of this, we highlight which are the goals achieved by the community and what is still missing.

| Comments: | Accepted at CLIC-it 2021                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.00514](https://arxiv.org/abs/2111.00514) [cs.CL]** |
|           | (or **[arXiv:2111.00514v1](https://arxiv.org/abs/2111.00514v1) [cs.CL]** for this version) |





<h2 id="2021-11-02-5">5. Quality Estimation Using Round-trip Translation with Sentence Embeddings
</h2>

Title: [Quality Estimation Using Round-trip Translation with Sentence Embeddings](https://arxiv.org/abs/2111.00554)

Authors: [Nathan Crone](https://arxiv.org/search/cs?searchtype=author&query=Crone%2C+N), [Adam Power](https://arxiv.org/search/cs?searchtype=author&query=Power%2C+A), [John Weldon](https://arxiv.org/search/cs?searchtype=author&query=Weldon%2C+J)

> Estimating the quality of machine translation systems has been an ongoing challenge for researchers in this field. Many previous attempts at using round-trip translation as a measure of quality have failed, and there is much disagreement as to whether it can be a viable method of quality estimation. In this paper, we revisit round-trip translation, proposing a system which aims to solve the previous pitfalls found with the approach. Our method makes use of recent advances in language representation learning to more accurately gauge the similarity between the original and round-trip translated sentences. Experiments show that while our approach does not reach the performance of current state of the art methods, it may still be an effective approach for some language pairs.

| Comments: | 10 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.00554](https://arxiv.org/abs/2111.00554) [cs.CL]** |
|           | (or **[arXiv:2111.00554v1](https://arxiv.org/abs/2111.00554v1) [cs.CL]** for this version) |





<h2 id="2021-11-02-6">6. Unsupervised Domain Adaptation with Adapter
</h2>

Title: [Unsupervised Domain Adaptation with Adapter](https://arxiv.org/abs/2111.00667)

Authors: [Rongsheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Yinhe Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y), [Xiaoxi Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+X), [Minlie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M)

> Unsupervised domain adaptation (UDA) with pre-trained language models (PrLM) has achieved promising results since these pre-trained models embed generic knowledge learned from various domains. However, fine-tuning all the parameters of the PrLM on a small domain-specific corpus distort the learned generic knowledge, and it is also expensive to deployment a whole fine-tuned PrLM for each domain. This paper explores an adapter-based fine-tuning approach for unsupervised domain adaptation. Specifically, several trainable adapter modules are inserted in a PrLM, and the embedded generic knowledge is preserved by fixing the parameters of the original PrLM at fine-tuning. A domain-fusion scheme is introduced to train these adapters using a mix-domain corpus to better capture transferable features. Elaborated experiments on two benchmark datasets are carried out, and the results demonstrate that our approach is effective with different tasks, dataset sizes, and domain similarities.

| Comments: | Accepted by NeurIPS2021 workshop                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2111.00667](https://arxiv.org/abs/2111.00667) [cs.CL]** |
|           | (or **[arXiv:2111.00667v1](https://arxiv.org/abs/2111.00667v1) [cs.CL]** for this version) |





<h2 id="2021-11-02-7">7. Interpretable contrastive word mover's embedding
</h2>

Title: [Interpretable contrastive word mover's embedding](https://arxiv.org/abs/2111.01023)

Authors: [Ruijie Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+R), [Julia Gouvea](https://arxiv.org/search/cs?searchtype=author&query=Gouvea%2C+J), [Eric Miller](https://arxiv.org/search/cs?searchtype=author&query=Miller%2C+E), [David Hammer](https://arxiv.org/search/cs?searchtype=author&query=Hammer%2C+D), [Shuchin Aeron](https://arxiv.org/search/cs?searchtype=author&query=Aeron%2C+S)

> This paper shows that a popular approach to the supervised embedding of documents for classification, namely, contrastive Word Mover's Embedding, can be significantly enhanced by adding interpretability. This interpretability is achieved by incorporating a clustering promoting mechanism into the contrastive loss. On several public datasets, we show that our method improves significantly upon existing baselines while providing interpretation to the clusters via identifying a set of keywords that are the most representative of a particular class. Our approach was motivated in part by the need to develop Natural Language Processing (NLP) methods for the \textit{novel problem of assessing student work for scientific writing and thinking} - a problem that is central to the area of (educational) Learning Sciences (LS). In this context, we show that our approach leads to a meaningful assessment of the student work related to lab reports from a biology class and can help LS researchers gain insights into student understanding and assess evidence of scientific thought processes.

| Comments: | 8 pages, 4 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.01023](https://arxiv.org/abs/2111.01023) [cs.CL]** |
|           | (or **[arXiv:2111.01023v1](https://arxiv.org/abs/2111.01023v1) [cs.CL]** for this version) |





# 2021-11-01

[Return to Index](#Index)



<h2 id="2021-11-01-1">1. Decision Attentive Regularization to Improve Simultaneous Speech Translation Systems
</h2>

Title: [Decision Attentive Regularization to Improve Simultaneous Speech Translation Systems](https://arxiv.org/abs/2110.15729)

Authors: [Mohd Abbas Zaidi](https://arxiv.org/search/cs?searchtype=author&query=Zaidi%2C+M+A), [Beomseok Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+B), [Nikhil Kumar Lakumarapu](https://arxiv.org/search/cs?searchtype=author&query=Lakumarapu%2C+N+K), [Sangha Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S), [Chanwoo Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+C)

> Simultaneous Speech-to-text Translation (SimulST) systems translate source speech in tandem with the speaker using partial input. Recent works have tried to leverage the text translation task to improve the performance of Speech Translation (ST) in the offline domain. Motivated by these improvements, we propose to add Decision Attentive Regularization (DAR) to Monotonic Multihead Attention (MMA) based SimulST systems. DAR improves the read/write decisions for speech using the Simultaneous text Translation (SimulMT) task. We also extend several techniques from the offline domain to the SimulST task. Our proposed system achieves significant performance improvements for the MuST-C English-German (EnDe) SimulST task, where we provide an average BLUE score improvement of around 4.57 points or 34.17% across different latencies. Further, the latency-quality tradeoffs establish that the proposed model achieves better results compared to the baseline.

| Comments: | 5 pages, 3 figures, 1 table                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2110.15729](https://arxiv.org/abs/2110.15729) [cs.SD]** |
|           | (or **[arXiv:2110.15729v1](https://arxiv.org/abs/2110.15729v1) [cs.SD]** for this version) |





<h2 id="2021-11-01-2">2. Analysing the Effect of Masking Length Distribution of MLM: An Evaluation Framework and Case Study on Chinese MRC Datasets
</h2>

Title: [Analysing the Effect of Masking Length Distribution of MLM: An Evaluation Framework and Case Study on Chinese MRC Datasets](https://arxiv.org/abs/2110.15712)

Authors: [Changchang. Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+C), [Shaobo. Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S)

> Machine reading comprehension (MRC) is a challenging natural language processing (NLP) task. Recently, the emergence of pre-trained models (PTM) has brought this research field into a new era, in which the training objective plays a key role. The masked language model (MLM) is a self-supervised training objective that widely used in various PTMs. With the development of training objectives, many variants of MLM have been proposed, such as whole word masking, entity masking, phrase masking, span masking, and so on. In different MLM, the length of the masked tokens is different. Similarly, in different machine reading comprehension tasks, the length of the answer is also different, and the answer is often a word, phrase, or sentence. Thus, in MRC tasks with different answer lengths, whether the length of MLM is related to performance is a question worth studying. If this hypothesis is true, it can guide us how to pre-train the MLM model with a relatively suitable mask length distribution for MRC task. In this paper, we try to uncover how much of MLM's success in the machine reading comprehension tasks comes from the correlation between masking length distribution and answer length in MRC dataset. In order to address this issue, herein, (1) we propose four MRC tasks with different answer length distributions, namely short span extraction task, long span extraction task, short multiple-choice cloze task, long multiple-choice cloze task; (2) four Chinese MRC datasets are created for these tasks; (3) we also have pre-trained four masked language models according to the answer length distributions of these datasets; (4) ablation experiments are conducted on the datasets to verify our hypothesis. The experimental results demonstrate that our hypothesis is true.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.15712](https://arxiv.org/abs/2110.15712) [cs.CL]** |
|           | (or **[arXiv:2110.15712v1](https://arxiv.org/abs/2110.15712v1) [cs.CL]** for this version) |





<h2 id="2021-11-01-3">3. Building the Language Resource for a Cebuano-Filipino Neural Machine Translation System
</h2>

Title: [Building the Language Resource for a Cebuano-Filipino Neural Machine Translation System](https://arxiv.org/abs/2110.15716)

Authors: [Kristine Mae Adlaon](https://arxiv.org/search/cs?searchtype=author&query=Adlaon%2C+K+M), [Nelson Marcos](https://arxiv.org/search/cs?searchtype=author&query=Marcos%2C+N)

> Parallel corpus is a critical resource in machine learning-based translation. The task of collecting, extracting, and aligning texts in order to build an acceptable corpus for doing the translation is very tedious most especially for low-resource languages. In this paper, we present the efforts made to build a parallel corpus for Cebuano and Filipino from two different domains: biblical texts and the web. For the biblical resource, subword unit translation for verbs and copy-able approach for nouns were applied to correct inconsistencies in the translation. This correction mechanism was applied as a preprocessing technique. On the other hand, for Wikipedia being the main web resource, commonly occurring topic segments were extracted from both the source and the target languages. These observed topic segments are unique in 4 different categories. The identification of these topic segments may be used for the automatic extraction of sentences. A Recurrent Neural Network was used to implement the translation using OpenNMT sequence modeling tool in TensorFlow. The two different corpora were then evaluated by using them as two separate inputs in the neural network. Results have shown a difference in BLEU scores in both corpora.

| Comments:    | Published in the Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval. arXiv admin note: substantial text overlap with [arXiv:1902.07250](https://arxiv.org/abs/1902.07250) |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | A.2                                                          |
| DOI:         | [10.1145/3342827.3342833](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3342827.3342833&v=b6a91262) |
| Cite as:     | **[arXiv:2110.15716](https://arxiv.org/abs/2110.15716) [cs.CL]** |
|              | (or **[arXiv:2110.15716v1](https://arxiv.org/abs/2110.15716v1) [cs.CL]** for this version) |





<h2 id="2021-11-01-4">4. Batch-Softmax Contrastive Loss for Pairwise Sentence Scoring Tasks
</h2>

Title: [Batch-Softmax Contrastive Loss for Pairwise Sentence Scoring Tasks](https://arxiv.org/abs/2110.15725)

Authors: [Anton Chernyavskiy](https://arxiv.org/search/cs?searchtype=author&query=Chernyavskiy%2C+A), [Dmitry Ilvovsky](https://arxiv.org/search/cs?searchtype=author&query=Ilvovsky%2C+D), [Pavel Kalinin](https://arxiv.org/search/cs?searchtype=author&query=Kalinin%2C+P), [Preslav Nakov](https://arxiv.org/search/cs?searchtype=author&query=Nakov%2C+P)

> The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP). Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss as well as in the overall training procedure; in particular, we find that data shuffling can be quite important. Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression. Finally, we offer detailed analysis and discussion, which should be useful for researchers aiming to explore the utility of contrastive loss in NLP.

| Comments:    | batch-softmax contrastive loss, pairwise sentence scoring, classification, ranking, and regression |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |
| MSC classes: | 68T50                                                        |
| ACM classes: | F.2.2; I.2.7                                                 |
| Cite as:     | **[arXiv:2110.15725](https://arxiv.org/abs/2110.15725) [cs.CL]** |
|              | (or **[arXiv:2110.15725v1](https://arxiv.org/abs/2110.15725v1) [cs.CL]** for this version) |





<h2 id="2021-11-01-5">5. BERMo: What can BERT learn from ELMo?
</h2>

Title: [BERMo: What can BERT learn from ELMo?](https://arxiv.org/abs/2110.15802)

Authors: [Sangamesh Kodge](https://arxiv.org/search/cs?searchtype=author&query=Kodge%2C+S), [Kaushik Roy](https://arxiv.org/search/cs?searchtype=author&query=Roy%2C+K)

> We propose BERMo, an architectural modification to BERT, which makes predictions based on a hierarchy of surface, syntactic and semantic language features. We use linear combination scheme proposed in Embeddings from Language Models (ELMo) to combine the scaled internal representations from different network depths. Our approach has two-fold benefits: (1) improved gradient flow for the downstream task as every layer has a direct connection to the gradients of the loss function and (2) increased representative power as the model no longer needs to copy the features learned in the shallower layer which are necessary for the downstream task. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network. Experiments on the probing task from SentEval dataset show that our model performs up to 4.65% better in accuracy than the baseline with an average improvement of 2.67% on the semantic tasks. When subject to compression techniques, we find that our model enables stable pruning for compressing small datasets like SST-2, where the BERT model commonly diverges. We observe that our approach converges 1.67× and 1.15× faster than the baseline on MNLI and QQP tasks from GLUE dataset. Moreover, our results show that our approach can obtain better parameter efficiency for penalty based pruning approaches on QQP task.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2110.15802](https://arxiv.org/abs/2110.15802) [cs.CL]** |
|           | (or **[arXiv:2110.15802v1](https://arxiv.org/abs/2110.15802v1) [cs.CL]** for this version) |





<h2 id="2021-11-01-6">6. MetaICL: Learning to Learn In Context
</h2>

Title: [MetaICL: Learning to Learn In Context](https://arxiv.org/abs/2110.15943)

Authors: [Sewon Min](https://arxiv.org/search/cs?searchtype=author&query=Min%2C+S), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Hannaneh Hajishirzi](https://arxiv.org/search/cs?searchtype=author&query=Hajishirzi%2C+H)

> We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learn-ing on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task training data, and outperforms much bigger models with nearly 8x parameters.

| Comments: | 18 pages (9 pages for the main paper, 9 pages for references and appendices). 1 figure. Code available at [this https URL](https://github.com/facebookresearch/MetaICL) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2110.15943](https://arxiv.org/abs/2110.15943) [cs.CL]** |
|           | (or **[arXiv:2110.15943v1](https://arxiv.org/abs/2110.15943v1) [cs.CL]** for this version) |




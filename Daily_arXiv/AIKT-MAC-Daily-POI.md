# MA C.'s Daily Paper Of Interest - January, 2022

# Index


- [2022-01-05](#2022-01-05)

  - [1. Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety](#2022-01-05-1)
  - [2. StyleM: Stylized Metrics for Image Captioning Built with Contrastive N-grams](#2022-01-05-2)

- [2022-01-04](#2022-01-04)

  - [1. How do lexical semantics affect translation? An empirical study](#2022-01-04-1)
  - [2. Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models](#2022-01-04-2)
  - [3. Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions](#2022-01-04-3)

- [2022-01-03](#2022-01-03)

  - [1. ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation](#2022-01-03-1)
  - [2. Deconfounded Visual Grounding](#2022-01-03-2)
  - [3. Materialized Knowledge Bases from Commonsense Transformers](#2022-01-03-3)
  - [4. ViNMT: Neural Machine Translation Tookit](#2022-01-03-4)

- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MAC-Daily-POI-index.md)



# 2022-01-05

[Return to Index](#Index)



<h2 id="2022-01-05-1">1. Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety
</h2>

Title: [Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety](https://arxiv.org/abs/2201.00969)

Authors: [Rajagopal A](https://arxiv.org/search/cs?searchtype=author&query=A%2C+R), [Nirmala V](https://arxiv.org/search/cs?searchtype=author&query=V%2C+N), [Arun Muthuraj Vedamanickam](https://arxiv.org/search/cs?searchtype=author&query=Vedamanickam%2C+A+M)

> There is amazing progress in Deep Learning based models for Image captioning and Low Light image enhancement. For the first time in literature, this paper develops a Deep Learning model that translates night scenes to sentences, opening new possibilities for AI applications in the safety of visually impaired women. Inspired by Image Captioning and Visual Question Answering, a novel Interactive Image Captioning is developed. A user can make the AI focus on any chosen person of interest by influencing the attention scoring. Attention context vectors are computed from CNN feature vectors and user-provided start word. The Encoder-Attention-Decoder neural network learns to produce captions from low brightness images. This paper demonstrates how women safety can be enabled by researching a novel AI capability in the Interactive Vision-Language model for perception of the environment in the night.

| Comments:    | In Springer Proceedings. International Conference On Big Data, Machine Learning and Applications 2021. [this http URL](http://bigdml.nits.ac.in/) |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| ACM classes: | I.2.0                                                        |
| Cite as:     | **[arXiv:2201.00969](https://arxiv.org/abs/2201.00969) [cs.CV]** |
|              | (or **[arXiv:2201.00969v1](https://arxiv.org/abs/2201.00969v1) [cs.CV]** for this version) |





<h2 id="2022-01-05-2">2. StyleM: Stylized Metrics for Image Captioning Built with Contrastive N-grams
</h2>

Title: [StyleM: Stylized Metrics for Image Captioning Built with Contrastive N-grams](https://arxiv.org/abs/2201.00975)

Authors: [Chengxi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Brent Harrison](https://arxiv.org/search/cs?searchtype=author&query=Harrison%2C+B)

> In this paper, we build two automatic evaluation metrics for evaluating the association between a machine-generated caption and a ground truth stylized caption: OnlyStyle and StyleCIDEr.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.00975](https://arxiv.org/abs/2201.00975) [cs.CV]** |
|           | (or **[arXiv:2201.00975v1](https://arxiv.org/abs/2201.00975v1) [cs.CV]** for this version) |







# 2022-01-04

[Return to Index](#Index)



<h2 id="2022-01-04-1">1. How do lexical semantics affect translation? An empirical study
</h2>

Title: [How do lexical semantics affect translation? An empirical study](https://arxiv.org/abs/2201.00075)

Authors:[Vivek Subramanian](https://arxiv.org/search/cs?searchtype=author&query=Subramanian%2C+V), [Dhanasekar Sundararaman](https://arxiv.org/search/cs?searchtype=author&query=Sundararaman%2C+D)

> Neural machine translation (NMT) systems aim to map text from one language into another. While there are a wide variety of applications of NMT, one of the most important is translation of natural language. A distinguishing factor of natural language is that words are typically ordered according to the rules of the grammar of a given language. Although many advances have been made in developing NMT systems for translating natural language, little research has been done on understanding how the word ordering of and lexical similarity between the source and target language affect translation performance. Here, we investigate these relationships on a variety of low-resource language pairs from the OpenSubtitles2016 database, where the source language is English, and find that the more similar the target language is to English, the greater the translation performance. In addition, we study the impact of providing NMT models with part of speech of words (POS) in the English sequence and find that, for Transformer-based models, the more dissimilar the target language is from English, the greater the benefit provided by POS.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.00075](https://arxiv.org/abs/2201.00075) [cs.CL]** |
|           | (or **[arXiv:2201.00075v1](https://arxiv.org/abs/2201.00075v1) [cs.CL]** for this version) |





<h2 id="2022-01-04-2">2. Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models
</h2>

Title: [Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models](https://arxiv.org/abs/2201.00558)

Authors:[Made Nindyatama Nityasya](https://arxiv.org/search/cs?searchtype=author&query=Nityasya%2C+M+N), [Haryo Akbarianto Wibowo](https://arxiv.org/search/cs?searchtype=author&query=Wibowo%2C+H+A), [Rendi Chevi](https://arxiv.org/search/cs?searchtype=author&query=Chevi%2C+R), [Radityo Eko Prasojo](https://arxiv.org/search/cs?searchtype=author&query=Prasojo%2C+R+E), [Alham Fikri Aji](https://arxiv.org/search/cs?searchtype=author&query=Aji%2C+A+F)

> We perform knowledge distillation (KD) benchmark from task-specific BERT-base teacher models to various student models: BiLSTM, CNN, BERT-Tiny, BERT-Mini, and BERT-Small. Our experiment involves 12 datasets grouped in two tasks: text classification and sequence labeling in the Indonesian language. We also compare various aspects of distillations including the usage of word embeddings and unlabeled data augmentation. Our experiments show that, despite the rising popularity of Transformer-based models, using BiLSTM and CNN student models provide the best trade-off between performance and computational resource (CPU, RAM, and storage) compared to pruned BERT models. We further propose some quick wins on performing KD to produce small NLP models via efficient KD training mechanisms involving simple choices of loss functions, word embeddings, and unlabeled data preparation.

| Comments:    | 14 pages, 3 figures, submitted to Elsevier                   |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7; I.2.6                                                 |
| Cite as:     | **[arXiv:2201.00558](https://arxiv.org/abs/2201.00558) [cs.CL]** |
|              | (or **[arXiv:2201.00558v1](https://arxiv.org/abs/2201.00558v1) [cs.CL]** for this version) |





<h2 id="2022-01-04-3">3. Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions
</h2>

Title: [Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions](https://arxiv.org/abs/2201.00768)

Authors:[Marwan Omar](https://arxiv.org/search/cs?searchtype=author&query=Omar%2C+M), [Soohyeon Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+S), [DaeHun Nyang](https://arxiv.org/search/cs?searchtype=author&query=Nyang%2C+D), [David Mohaisen](https://arxiv.org/search/cs?searchtype=author&query=Mohaisen%2C+D)

> Recent natural language processing (NLP) techniques have accomplished high performance on benchmark datasets, primarily due to the significant improvement in the performance of deep learning. The advances in the research community have led to great enhancements in state-of-the-art production systems for NLP tasks, such as virtual assistants, speech recognition, and sentiment analysis. However, such NLP systems still often fail when tested with adversarial attacks. The initial lack of robustness exposed troubling gaps in current models' language understanding capabilities, creating problems when NLP systems are deployed in real life. In this paper, we present a structured overview of NLP robustness research by summarizing the literature in a systemic way across various dimensions. We then take a deep-dive into the various dimensions of robustness, across techniques, metrics, embeddings, and benchmarks. Finally, we argue that robustness should be multi-dimensional, provide insights into current research, identify gaps in the literature to suggest directions worth pursuing to address these gaps.

| Comments: | Survey; 2 figures, 4 tables                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2201.00768](https://arxiv.org/abs/2201.00768) [cs.CL]** |
|           | (or **[arXiv:2201.00768v1](https://arxiv.org/abs/2201.00768v1) [cs.CL]** for this version) |



# 2022-01-03

[Return to Index](#Index)



<h2 id="2022-01-03-1">1. ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation
</h2>

Title: [ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation](https://arxiv.org/abs/2112.15283)

Authors: [Han Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Weichong Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+W), [Yewei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Lanxin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Boqiang Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+B), [Zhihua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Hao Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+H), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. Based on the image quantization models, we formulate both image generation and text generation as autoregressive generative tasks conditioned on the text/image input. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. For the text-to-image generation process, we further propose an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning.

| Comments: | 15 pages, 7 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2112.15283](https://arxiv.org/abs/2112.15283) [cs.CV]** |
|           | (or **[arXiv:2112.15283v1](https://arxiv.org/abs/2112.15283v1) [cs.CV]** for this version) |





<h2 id="2022-01-03-2">2. Deconfounded Visual Grounding
</h2>

Title: [Deconfounded Visual Grounding](https://arxiv.org/abs/2112.15324)

Authors: [Jianqiang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Yu Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y), [Jiaxin Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+J), [Qianru Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Q), [Hanwang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H)

> We focus on the confounding bias between language and location in the visual grounding pipeline, where we find that the bias is the major visual reasoning bottleneck. For example, the grounding process is usually a trivial language-location association without visual reasoning, e.g., grounding any language query containing sheep to the nearly central regions, due to that most queries about sheep have ground-truth locations at the image center. First, we frame the visual grounding pipeline into a causal graph, which shows the causalities among image, query, target location and underlying confounder. Through the causal graph, we know how to break the grounding bottleneck: deconfounded visual grounding. Second, to tackle the challenge that the confounder is unobserved in general, we propose a confounder-agnostic approach called: Referring Expression Deconfounder (RED), to remove the confounding bias. Third, we implement RED as a simple language attention, which can be applied in any grounding method. On popular benchmarks, RED improves various state-of-the-art grounding methods by a significant margin. Code will soon be available at: [this https URL](https://github.com/JianqiangH/Deconfounded_VG).

| Comments: | AAAI 2022 Accepted                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2112.15324](https://arxiv.org/abs/2112.15324) [cs.CV]** |
|           | (or **[arXiv:2112.15324v1](https://arxiv.org/abs/2112.15324v1) [cs.CV]** for this version) |





<h2 id="2022-01-03-3">3. Materialized Knowledge Bases from Commonsense Transformers
</h2>

Title: [Materialized Knowledge Bases from Commonsense Transformers](https://arxiv.org/abs/2112.14815)

Authors: [Tuan-Phong Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T), [Simon Razniewski](https://arxiv.org/search/cs?searchtype=author&query=Razniewski%2C+S)

> Starting from the COMET methodology by Bosselut et al. (2019), generating commonsense knowledge directly from pre-trained language models has recently received significant attention. Surprisingly, up to now no materialized resource of commonsense knowledge generated this way is publicly available. This paper fills this gap, and uses the materialized resources to perform a detailed analysis of the potential of this approach in terms of precision and recall. Furthermore, we identify common problem cases, and outline use cases enabled by materialized resources. We posit that the availability of these resources is important for the advancement of the field, as it enables an off-the-shelf-use of the resulting knowledge, as well as further analyses on its strengths and weaknesses.

| Comments: | 7 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2112.14815](https://arxiv.org/abs/2112.14815) [cs.CL]** |
|           | (or **[arXiv:2112.14815v1](https://arxiv.org/abs/2112.14815v1) [cs.CL]** for this version) |





<h2 id="2022-01-03-4">4. ViNMT: Neural Machine Translation Tookit
</h2>

Title: [ViNMT: Neural Machine Translation Tookit](https://arxiv.org/abs/2112.15272)

Authors: [Nguyen Hoang Quan](https://arxiv.org/search/cs?searchtype=author&query=Quan%2C+N+H), [Nguyen Thanh Dat](https://arxiv.org/search/cs?searchtype=author&query=Dat%2C+N+T), [Nguyen Hoang Minh Cong](https://arxiv.org/search/cs?searchtype=author&query=Cong%2C+N+H+M), [Nguyen Van Vinh](https://arxiv.org/search/cs?searchtype=author&query=Van+Vinh%2C+N), [Ngo Thi Vinh](https://arxiv.org/search/cs?searchtype=author&query=Vinh%2C+N+T), [Nguyen Phuong Thai](https://arxiv.org/search/cs?searchtype=author&query=Thai%2C+N+P), [Tran Hong Viet](https://arxiv.org/search/cs?searchtype=author&query=Viet%2C+T+H)

> We present an open-source toolkit for neural machine translation (NMT). The new toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along with many other improvements detailed below, in order to create a self-contained, simple to use, consistent and comprehensive framework for Machine Translation tasks of various domains. It is tooled to support both bilingual and multilingual translation tasks, starting from building the model from respective corpora, to inferring new predictions or packaging the model to serving-capable JIT format.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.15272](https://arxiv.org/abs/2112.15272) [cs.CL]** |
|           | (or **[arXiv:2112.15272v1](https://arxiv.org/abs/2112.15272v1) [cs.CL]** for this version) |

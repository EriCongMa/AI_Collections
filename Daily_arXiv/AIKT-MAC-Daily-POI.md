# MA C.'s Daily Paper Of Interest - January, 2022

# Index


- [2022-01-24](#2022-01-24)

  - [1. Black-box Prompt Learning for Pre-trained Language Models](#2022-01-24-1)
  - [2. Context-Tuning: Learning Contextualized Prompts for Natural Language Generation](#2022-01-24-2)
  
- [2022-01-21](#2022-01-21)
  - [1. Construction of a Quality Estimation Dataset for Automatic Evaluation of Japanese Grammatical Error Correction](#2022-01-21-1)
  - [2. VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine Translation](#2022-01-21-2)
  - [3. Linguistically-driven Multi-task Pre-training for Low-resource Neural Machine Translation](#2022-01-21-3)
- [2022-01-20](#2022-01-20)

  - [1. Improving Neural Machine Translation by Denoising Training](#2022-01-20-1)
  - [2. Neural Language Models are Effective Plagiarists](#2022-01-20-2)
  - [3. Interpreting Arabic Transformer Models](#2022-01-20-3)
  - [4. CM3: A Causal Masked Multimodal Model of the Internet](#2022-01-20-4)
- [2022-01-19](#2022-01-19)

  - [1. Multi-Staged Cross-Lingual Acoustic Model Adaption for Robust Speech Recognition in Real-World Applications -- A Case Study on German Oral History Interviews](2022-01-19-1)
  - [2. CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks](2022-01-19-2)
  - [3. The Dark Side of the Language: Pre-trained Transformers in the DarkNet](2022-01-19-3)
  - [4. Cost-Effective Training in Low-Resource Neural Machine Translation](2022-01-19-4)
  - [5. WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation](2022-01-19-5)
  - [6. SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples](2022-01-19-6)
  - [7. Memory-assisted prompt editing to improve GPT-3 after deployment](2022-01-19-7)
  - [8. Syntax-based data augmentation for Hungarian-English machine translation](2022-01-19-8)
  - [9. Instance-aware Prompt Learning for Language Understanding and Generation](2022-01-19-9)
- [2022-01-17](#2022-01-17)
  - [1. A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering](#2022-01-17-1)
  - [2. A Survey of Pretrained Language Models Based Text Generation](#2022-01-17-2)
  - [3. CommonsenseQA 2.0: Exposing the Limits of AI through Gamification](#2022-01-17-3)
  - [4. A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models](#2022-01-17-4)
  - [5. Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer](#2022-01-17-5)
  - [6. Czech Grammar Error Correction with a Large and Diverse Corpus](#2022-01-17-6)
  - [7. Multilingual Open Text 1.0: Public Domain News in 44 Languages](#2022-01-17-7)
- [2022-01-14](#2022-01-14)
  - [1. Towards Automated Error Analysis: Learning to Characterize Errors](#2022-01-14)
- [2022-01-13](#2022-01-13)

  - [1. PromptBERT: Improving BERT Sentence Embeddings with Prompts](#2022-01-13-1)
  - [2. How Does Data Corruption Affect Natural Language Understanding Models? A Study on GLUE datasets](#2022-01-13-2)
- [2022-01-12](#2022-01-12)

  - [1. Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training](#2022-01-12-1)
  - [2. CVSS Corpus and Massively Multilingual Speech-to-Speech Translation](#2022-01-12-2)
  - [3. Quantifying Robustness to Adversarial Word Substitutions](#2022-01-12-3)
- [2022-01-11](#2022-01-11)

  - [1. Towards the Next 1000 Languages in Multilingual Machine Translation: Exploring the Synergy Between Supervised and Self-Supervised Learning](#2022-01-11-1)
  - [2. Black-Box Tuning for Language-Model-as-a-Service](#2022-01-11-2)
  - [3. SCROLLS: Standardized CompaRison Over Long Language Sequences](#2022-01-11-3)
- [2022-01-10](#2022-01-10)

  - [1. Automatic Speech Recognition Datasets in Cantonese Language: A Survey and a New Dataset](#2022-01-10-1)
  - [2. Semantic-based Data Augmentation for Math Word Problems](#2022-01-10-2)
  - [3. Repairing Adversarial Texts through Perturbation](#2022-01-10-3)
  - [4. Code-Switching Text Augmentation for Multilingual Speech Processing](#2022-01-10-4)
- [2022-01-07](#2022-01-07)
  - [1. Compact Bidirectional Transformer for Image Captioning](#2022-01-07-1)
  - [2. Self-Training Vision Language BERTs with a Unified Conditional Model](#2022-01-07-2)
  - [3. Phrase-level Adversarial Example Generation for Neural Machine Translation](#2022-01-07-3)
- [2022-01-06](#2022-01-06)

  - [1. All You Need In Sign Language Production](#2022-01-06-1)
  - [2. SMDT: Selective Memory-Augmented Neural Document Translation](#2022-01-06-2)
- [2022-01-05](#2022-01-05)

  - [1. Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety](#2022-01-05-1)
  - [2. StyleM: Stylized Metrics for Image Captioning Built with Contrastive N-grams](#2022-01-05-2)
- [2022-01-04](#2022-01-04)

  - [1. How do lexical semantics affect translation? An empirical study](#2022-01-04-1)
  - [2. Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models](#2022-01-04-2)
  - [3. Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions](#2022-01-04-3)
- [2022-01-03](#2022-01-03)

  - [1. ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation](#2022-01-03-1)
  - [2. Deconfounded Visual Grounding](#2022-01-03-2)
  - [3. Materialized Knowledge Bases from Commonsense Transformers](#2022-01-03-3)
  - [4. ViNMT: Neural Machine Translation Tookit](#2022-01-03-4)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MAC-Daily-POI-index.md)





# 2022-01-24

[Return to Index](#Index)



<h2 id="2022-01-24-1">1. Black-box Prompt Learning for Pre-trained Language Models
</h2>

Title: [Black-box Prompt Learning for Pre-trained Language Models](https://arxiv.org/abs/2201.08531)

Authors: [Shizhe Diao](https://arxiv.org/search/cs?searchtype=author&query=Diao%2C+S), [Xuechun Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Yong Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Zhichao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z), [Tong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T)

> Domain-specific fine-tuning strategies for large pre-trained models received vast attention in recent years. In previously studied settings, the model architectures and parameters are tunable or at least visible, which we refer to as white-box settings. This work considers a new scenario, where we do not have access to a pre-trained model, except for its outputs given inputs, and we call this problem black-box fine-tuning. To illustrate our approach, we first introduce the black-box setting formally on text classification, where the pre-trained model is not only frozen but also invisible. We then propose our solution black-box prompt, a new technique in the prompt-learning family, which can leverage the knowledge learned by pre-trained models from the pre-training corpus. Our experiments demonstrate that the proposed method achieved the state-of-the-art performance on eight datasets. Further analyses on different human-designed objectives, prompt lengths, and intuitive explanations demonstrate the robustness and flexibility of our method.

| Comments: | 10 pages, 5 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.08531](https://arxiv.org/abs/2201.08531) [cs.CL]** |
|           | (or **[arXiv:2201.08531v1](https://arxiv.org/abs/2201.08531v1) [cs.CL]** for this version) |





<h2 id="2022-01-24-2">2. Context-Tuning: Learning Contextualized Prompts for Natural Language Generation
</h2>

Title: [Context-Tuning: Learning Contextualized Prompts for Natural Language Generation](https://arxiv.org/abs/2201.08670)

Authors: [Tianyi Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+T), [Junyi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Wayne Xin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W+X)

> Recently, pretrained language models (PLMs) have made exceptional success in language generation. To leverage the rich knowledge encoded by PLMs, a simple yet powerful mechanism is to use prompts, in the form of either discrete tokens or continuous embeddings. In existing studies, manual prompts are time-consuming and require domain expertise, while continuous prompts are typically independent of the inputs. To address this issue, we propose a novel continuous prompting approach, called Context-Tuning, to fine-tuning PLMs for natural language generation. Firstly, the prompts are derived based on the input text, so that they can elicit useful knowledge from PLMs for generation. We refer to such prompts as contextualized prompts. Secondly, to further enhance the relevance of the generated text to the inputs, we utilize continuous inverse prompting to refine the process of natural language generation by modeling an inverse generation process from output to input. Moreover, we propose a lightweight contexttuning, fine-tuning only 0.4% of parameters while retaining well performance.

| Comments: | 13 pages, 6 figures, 6 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.08670](https://arxiv.org/abs/2201.08670) [cs.CL]** |
|           | (or **[arXiv:2201.08670v1](https://arxiv.org/abs/2201.08670v1) [cs.CL]** for this version) |






# 2022-01-21

[Return to Index](#Index)



<h2 id="2022-01-21-1">1. Construction of a Quality Estimation Dataset for Automatic Evaluation of Japanese Grammatical Error Correction
</h2>

Title: [Construction of a Quality Estimation Dataset for Automatic Evaluation of Japanese Grammatical Error Correction](https://arxiv.org/abs/2201.08038)

Authors: [Daisuke Suzuki](https://arxiv.org/search/cs?searchtype=author&query=Suzuki%2C+D), [Yujin Takahashi](https://arxiv.org/search/cs?searchtype=author&query=Takahashi%2C+Y), [Ikumi Yamashita](https://arxiv.org/search/cs?searchtype=author&query=Yamashita%2C+I), [Taichi Aida](https://arxiv.org/search/cs?searchtype=author&query=Aida%2C+T), [Tosho Hirasawa](https://arxiv.org/search/cs?searchtype=author&query=Hirasawa%2C+T), [Michitaka Nakatsuji](https://arxiv.org/search/cs?searchtype=author&query=Nakatsuji%2C+M), [Masato Mita](https://arxiv.org/search/cs?searchtype=author&query=Mita%2C+M), [Mamoru Komachi](https://arxiv.org/search/cs?searchtype=author&query=Komachi%2C+M)

> In grammatical error correction (GEC), automatic evaluation is an important factor for research and development of GEC systems. Previous studies on automatic evaluation have demonstrated that quality estimation models built from datasets with manual evaluation can achieve high performance in automatic evaluation of English GEC without using reference sentences.. However, quality estimation models have not yet been studied in Japanese, because there are no datasets for constructing quality estimation models. Therefore, in this study, we created a quality estimation dataset with manual evaluation to build an automatic evaluation model for Japanese GEC. Moreover, we conducted a meta-evaluation to verify the dataset's usefulness in building the Japanese quality estimation model.

| Comments: | 8 pages (6pages + references)                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.08038](https://arxiv.org/abs/2201.08038) [cs.CL]** |
|           | (or **[arXiv:2201.08038v1](https://arxiv.org/abs/2201.08038v1) [cs.CL]** for this version) |





<h2 id="2022-01-21-2">2. VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine Translation
</h2>

Title: [VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine Translation](https://arxiv.org/abs/2201.08054)

Authors: [Yihang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Shuichiro Shimizu](https://arxiv.org/search/cs?searchtype=author&query=Shimizu%2C+S), [Weiqi Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+W), [Chenhui Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+C), [Sadao Kurohashi](https://arxiv.org/search/cs?searchtype=author&query=Kurohashi%2C+S)

> Existing multimodal machine translation (MMT) datasets consist of images and video captions or general subtitles, which rarely contain linguistic ambiguity, making visual information not so effective to generate appropriate translations. We introduce VISA, a new dataset that consists of 40k Japanese-English parallel sentence pairs and corresponding video clips with the following key features: (1) the parallel sentences are subtitles from movies and TV episodes; (2) the source subtitles are ambiguous, which means they have multiple possible translations with different meanings; (3) we divide the dataset into Polysemy and Omission according to the cause of ambiguity. We show that VISA is challenging for the latest MMT system, and we hope that the dataset can facilitate MMT research.

| Comments: | 9 pages, 6 figures, submitted to LREC2022                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.08054](https://arxiv.org/abs/2201.08054) [cs.CL]** |
|           | (or **[arXiv:2201.08054v1](https://arxiv.org/abs/2201.08054v1) [cs.CL]** for this version) |





<h2 id="2022-01-21-3">3. Linguistically-driven Multi-task Pre-training for Low-resource Neural Machine Translation
</h2>

Title: [Linguistically-driven Multi-task Pre-training for Low-resource Neural Machine Translation](https://arxiv.org/abs/2201.08070)

Authors: [Zhuoyuan Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+Z), [Chenhui Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+C), [Sadao Kurohashi](https://arxiv.org/search/cs?searchtype=author&query=Kurohashi%2C+S)

> In the present study, we propose novel sequence-to-sequence pre-training objectives for low-resource machine translation (NMT): Japanese-specific sequence to sequence (JASS) for language pairs involving Japanese as the source or target language, and English-specific sequence to sequence (ENSS) for language pairs involving English. JASS focuses on masking and reordering Japanese linguistic units known as bunsetsu, whereas ENSS is proposed based on phrase structure masking and reordering tasks. Experiments on ASPEC Japanese--English & Japanese--Chinese, Wikipedia Japanese--Chinese, News English--Korean corpora demonstrate that JASS and ENSS outperform MASS and other existing language-agnostic pre-training methods by up to +2.9 BLEU points for the Japanese--English tasks, up to +7.0 BLEU points for the Japanese--Chinese tasks and up to +1.3 BLEU points for English--Korean tasks. Empirical analysis, which focuses on the relationship between individual parts in JASS and ENSS, reveals the complementary nature of the subtasks of JASS and ENSS. Adequacy evaluation using LASER, human evaluation, and case studies reveals that our proposed methods significantly outperform pre-training methods without injected linguistic knowledge and they have a larger positive impact on the adequacy as compared to the fluency. We release codes here: [this https URL](https://github.com/Mao-KU/JASS/tree/master/linguistically-driven-pretraining).

| Comments:          | An extension of work [arXiv:2005.03361](https://arxiv.org/abs/2005.03361) |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**                         |
| Journal reference: | TALLIP Volume 21, Issue 4, July 2022                         |
| DOI:               | [10.1145/3491065](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1145%2F3491065&v=ede59a3a) |
| Cite as:           | **[arXiv:2201.08070](https://arxiv.org/abs/2201.08070) [cs.CL]** |
|                    | (or **[arXiv:2201.08070v1](https://arxiv.org/abs/2201.08070v1) [cs.CL]** for this version) |





# 2022-01-20

[Return to Index](#Index)



<h2 id="2022-01-20-1">1. Improving Neural Machine Translation by Denoising Training
</h2>

Title: [Improving Neural Machine Translation by Denoising Training](https://arxiv.org/abs/2201.07365)

Authors: [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Keqin Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+K), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D)

> We present a simple and effective pretraining strategy {D}en{o}ising {T}raining DoT for neural machine translation. Specifically, we update the model parameters with source- and target-side denoising tasks at the early stage and then tune the model normally. Notably, our approach does not increase any parameters or training steps, requiring the parallel data merely. Experiments show that DoT consistently improves the neural machine translation performance across 12 bilingual and 16 multilingual directions (data size ranges from 80K to 20M). In addition, we show that DoT can complement existing data manipulation strategies, i.e. curriculum learning, knowledge distillation, data diversification, bidirectional training, and back-translation. Encouragingly, we found that DoT outperforms costly pretrained model mBART in high-resource settings. Analyses show DoT is a novel in-domain cross-lingual pretraining strategy and could offer further improvements with task-relevant self-supervisions.

| Comments: | arXiv admin note: text overlap with [arXiv:2109.07780](https://arxiv.org/abs/2109.07780) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.07365](https://arxiv.org/abs/2201.07365) [cs.CL]** |
|           | (or **[arXiv:2201.07365v1](https://arxiv.org/abs/2201.07365v1) [cs.CL]** for this version) |





<h2 id="2022-01-20-2">2. Neural Language Models are Effective Plagiarists
</h2>

Title: [Neural Language Models are Effective Plagiarists](https://arxiv.org/abs/2201.07406)

Authors: [Stella Biderman](https://arxiv.org/search/cs?searchtype=author&query=Biderman%2C+S), [Edward Raff](https://arxiv.org/search/cs?searchtype=author&query=Raff%2C+E)

> As artificial intelligence (AI) technologies become increasingly powerful and prominent in society, their misuse is a growing concern. In educational settings, AI technologies could be used by students to cheat on assignments and exams. In this paper we explore whether transformers can be used to solve introductory level programming assignments while bypassing commonly used AI tools to detect plagiarism. We find that a student using GPT-J [Wang and Komatsuzaki, 2021] can complete introductory level programming assignments without triggering suspicion from MOSS [Aiken, 2000], a widely used plagiarism detection tool. This holds despite the fact that GPT-J was not trained on the problems in question and is not provided with any examples to work from. We further find that the code written by GPT-J is diverse in structure, lacking any particular tells that future plagiarism detection techniques may use to try to identify algorithmically generated code. We conclude with a discussion of the ethical and educational implications of large language models and directions for future research.

| Comments: | 6 pages of main text, 2 pages of references, 86 pages of appendices |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2201.07406](https://arxiv.org/abs/2201.07406) [cs.CL]** |
|           | (or **[arXiv:2201.07406v1](https://arxiv.org/abs/2201.07406v1) [cs.CL]** for this version) |





<h2 id="2022-01-20-3">3. Interpreting Arabic Transformer Models
</h2>

Title: [Interpreting Arabic Transformer Models](https://arxiv.org/abs/2201.07434)

Authors: [Ahmed Abdelali](https://arxiv.org/search/cs?searchtype=author&query=Abdelali%2C+A), [Nadir Durrani](https://arxiv.org/search/cs?searchtype=author&query=Durrani%2C+N), [Fahim Dalvi](https://arxiv.org/search/cs?searchtype=author&query=Dalvi%2C+F), [Hassan Sajjad](https://arxiv.org/search/cs?searchtype=author&query=Sajjad%2C+H)

> Arabic is a Semitic language which is widely spoken with many dialects. Given the success of pre-trained language models, many transformer models trained on Arabic and its dialects have surfaced. While these models have been compared with respect to downstream NLP tasks, no evaluation has been carried out to directly compare the internal representations. We probe how linguistic information is encoded in Arabic pretrained models, trained on different varieties of Arabic language. We perform a layer and neuron analysis on the models using three intrinsic tasks: two morphological tagging tasks based on MSA (modern standard Arabic) and dialectal POS-tagging and a dialectal identification task. Our analysis enlightens interesting findings such as: i) word morphology is learned at the lower and middle layers ii) dialectal identification necessitate more knowledge and hence preserved even in the final layers, iii) despite a large overlap in their vocabulary, the MSA-based models fail to capture the nuances of Arabic dialects, iv) we found that neurons in embedding layers are polysemous in nature, while the neurons in middle layers are exclusive to specific properties.

| Comments: | 11 pages, 6 figures, 4 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.07434](https://arxiv.org/abs/2201.07434) [cs.CL]** |
|           | (or **[arXiv:2201.07434v1](https://arxiv.org/abs/2201.07434v1) [cs.CL]** for this version) |





<h2 id="2022-01-20-4">4. CM3: A Causal Masked Multimodal Model of the Internet
</h2>

Title: [CM3: A Causal Masked Multimodal Model of the Internet](https://arxiv.org/abs/2201.07520)

Authors: [Armen Aghajanyan](https://arxiv.org/search/cs?searchtype=author&query=Aghajanyan%2C+A), [Bernie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+B), [Candace Ross](https://arxiv.org/search/cs?searchtype=author&query=Ross%2C+C), [Vladimir Karpukhin](https://arxiv.org/search/cs?searchtype=author&query=Karpukhin%2C+V), [Hu Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Dmytro Okhonko](https://arxiv.org/search/cs?searchtype=author&query=Okhonko%2C+D), [Mandar Joshi](https://arxiv.org/search/cs?searchtype=author&query=Joshi%2C+M), [Gargi Ghosh](https://arxiv.org/search/cs?searchtype=author&query=Ghosh%2C+G), [Mike Lewis](https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+M), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L)

> We introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.07520](https://arxiv.org/abs/2201.07520) [cs.CL]** |
|           | (or **[arXiv:2201.07520v1](https://arxiv.org/abs/2201.07520v1) [cs.CL]** for this version) |





# 2022-01-19

[Return to Index](#Index)



<h2 id="2022-01-19-1">1. Multi-Staged Cross-Lingual Acoustic Model Adaption for Robust Speech Recognition in Real-World Applications -- A Case Study on German Oral History Interviews
</h2>

Title: [Multi-Staged Cross-Lingual Acoustic Model Adaption for Robust Speech Recognition in Real-World Applications -- A Case Study on German Oral History Interviews](https://arxiv.org/abs/2005.12562)

Authors: [Michael Gref](https://arxiv.org/search/eess?searchtype=author&query=Gref%2C+M), [Oliver Walter](https://arxiv.org/search/eess?searchtype=author&query=Walter%2C+O), [Christoph Schmidt](https://arxiv.org/search/eess?searchtype=author&query=Schmidt%2C+C), [Sven Behnke](https://arxiv.org/search/eess?searchtype=author&query=Behnke%2C+S), [Joachim Köhler](https://arxiv.org/search/eess?searchtype=author&query=Köhler%2C+J)

> While recent automatic speech recognition systems achieve remarkable performance when large amounts of adequate, high quality annotated speech data is used for training, the same systems often only achieve an unsatisfactory result for tasks in domains that greatly deviate from the conditions represented by the training data. For many real-world applications, there is a lack of sufficient data that can be directly used for training robust speech recognition systems. To address this issue, we propose and investigate an approach that performs a robust acoustic model adaption to a target domain in a cross-lingual, multi-staged manner. Our approach enables the exploitation of large-scale training data from other domains in both the same and other languages. We evaluate our approach using the challenging task of German oral history interviews, where we achieve a relative reduction of the word error rate by more than 30% compared to a model trained from scratch only on the target domain, and 6-7% relative compared to a model trained robustly on 1000 hours of same-language out-of-domain training data.

| Comments:          | Published version of the paper can be accessed via [this https URL](https://www.aclweb.org/anthology/2020.lrec-1.780) |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL) |
| Journal reference: | 12th International Conference on Language Resources and Evaluation (LREC 2020), pages 6354-6362 |
| Cite as:           | **[arXiv:2005.12562](https://arxiv.org/abs/2005.12562) [eess.AS]** |
|                    | (or **[arXiv:2005.12562v1](https://arxiv.org/abs/2005.12562v1) [eess.AS]** for this version) |





<h2 id="2022-01-19-2">2. CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks
</h2>

Title: [CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks](https://arxiv.org/abs/2201.05729)

Authors: [Zhecan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Noel Codella](https://arxiv.org/search/cs?searchtype=author&query=Codella%2C+N), [Yen-Chun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Luowei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Jianwei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Xiyang Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+X), [Bin Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+B), [Haoxuan You](https://arxiv.org/search/cs?searchtype=author&query=You%2C+H), [Shih-Fu Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+S), [Lu Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+L)

> Contrastive language-image pretraining (CLIP) links vision and language modalities into a unified embedding space, yielding the tremendous potential for vision-language (VL) tasks. While early concurrent works have begun to study this potential on a subset of tasks, important questions remain: 1) What is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches without impacting inference or pretraining complexity? In this work, we seek to answer these questions through two key contributions. First, we introduce an evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of data availability constraints and conditions of domain shift. Second, we propose an approach, named CLIP Targeted Distillation (CLIP-TD), to intelligently distill knowledge from CLIP into existing architectures using a dynamically weighted objective applied to adaptively selected tokens per instance. Experiments demonstrate that our proposed CLIP-TD leads to exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to 71.3%) conditions of VCR, while simultaneously improving performance under standard fully-supervised conditions (up to 2%), achieving state-of-art performance on VCR compared to other single models that are pretrained with image-text data only. On SNLI-VE, CLIP-TD produces significant gains in low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works utilizing CLIP for finetuning, as well as baseline naive distillation approaches. Code will be made available.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.05729](https://arxiv.org/abs/2201.05729) [cs.CV]** |
|           | (or **[arXiv:2201.05729v1](https://arxiv.org/abs/2201.05729v1) [cs.CV]** for this version) |





<h2 id="2022-01-19-3">3. The Dark Side of the Language: Pre-trained Transformers in the DarkNet
</h2>

Title: [The Dark Side of the Language: Pre-trained Transformers in the DarkNet](https://arxiv.org/abs/2201.05613)

Authors: [Leonardo Ranaldi](https://arxiv.org/search/cs?searchtype=author&query=Ranaldi%2C+L), [Aria Nourbakhsh](https://arxiv.org/search/cs?searchtype=author&query=Nourbakhsh%2C+A), [Arianna Patrizi](https://arxiv.org/search/cs?searchtype=author&query=Patrizi%2C+A), [Elena Sofia Ruzzetti](https://arxiv.org/search/cs?searchtype=author&query=Ruzzetti%2C+E+S), [Dario Onorati](https://arxiv.org/search/cs?searchtype=author&query=Onorati%2C+D), [Francesca Fallucchi Fabio Massimo Zanzotto](https://arxiv.org/search/cs?searchtype=author&query=Zanzotto%2C+F+F+F+M)

> Pre-trained Transformers are challenging human performances in many natural language processing tasks. The gigantic datasets used for pre-training seem to be the key for their success on existing tasks. In this paper, we explore how a range of pre-trained natural language understanding models perform on truly novel and unexplored data, provided by classification tasks over a DarkNet corpus. Surprisingly, results show that syntactic and lexical neural networks largely outperform pre-trained Transformers. This seems to suggest that pre-trained Transformers have serious difficulties in adapting to radically novel texts.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.05613](https://arxiv.org/abs/2201.05613) [cs.CL]** |
|           | (or **[arXiv:2201.05613v1](https://arxiv.org/abs/2201.05613v1) [cs.CL]** for this version) |





<h2 id="2022-01-19-4">4. Cost-Effective Training in Low-Resource Neural Machine Translation
</h2>

Title: [Cost-Effective Training in Low-Resource Neural Machine Translation](https://arxiv.org/abs/2201.05700)

Authors: [Sai Koneru](https://arxiv.org/search/cs?searchtype=author&query=Koneru%2C+S), [Danni Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Jan Niehues](https://arxiv.org/search/cs?searchtype=author&query=Niehues%2C+J)

> While Active Learning (AL) techniques are explored in Neural Machine Translation (NMT), only a few works focus on tackling low annotation budgets where a limited number of sentences can get translated. Such situations are especially challenging and can occur for endangered languages with few human annotators or having cost constraints to label large amounts of data. Although AL is shown to be helpful with large budgets, it is not enough to build high-quality translation systems in these low-resource conditions. In this work, we propose a cost-effective training procedure to increase the performance of NMT models utilizing a small number of annotated sentences and dictionary entries. Our method leverages monolingual data with self-supervised objectives and a small-scale, inexpensive dictionary for additional supervision to initialize the NMT model before applying AL. We show that improving the model using a combination of these knowledge sources is essential to exploit AL strategies and increase gains in low-resource conditions. We also present a novel AL strategy inspired by domain adaptation for NMT and show that it is effective for low budgets. We propose a new hybrid data-driven approach, which samples sentences that are diverse from the labelled data and also most similar to unlabelled data. Finally, we show that initializing the NMT model and further using our AL strategy can achieve gains of up to 13 BLEU compared to conventional AL methods.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.05700](https://arxiv.org/abs/2201.05700) [cs.CL]** |
|           | (or **[arXiv:2201.05700v1](https://arxiv.org/abs/2201.05700v1) [cs.CL]** for this version) |





<h2 id="2022-01-19-5">5. WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation
</h2>

Title: [WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation](https://arxiv.org/abs/2201.05955)

Authors: [Alisa Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+A), [Swabha Swayamdipta](https://arxiv.org/search/cs?searchtype=author&query=Swayamdipta%2C+S), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y)

> A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel paradigm for dataset creation based on human and machine collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI, our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers to ensure quality. The resulting dataset, WANLI, consists of 108,357 natural language inference (NLI) examples that present unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI instead of MNLI (which is 4 times larger) improves performance on seven out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI. Moreover, combining MNLI with WANLI is more effective than combining with other augmentation sets that have been introduced. Our results demonstrate the potential of natural language generation techniques to curate NLP datasets of enhanced quality and diversity.

| Comments: | preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.05955](https://arxiv.org/abs/2201.05955) [cs.CL]** |
|           | (or **[arXiv:2201.05955v1](https://arxiv.org/abs/2201.05955v1) [cs.CL]** for this version) |





<h2 id="2022-01-19-6">6. SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples
</h2>

Title: [SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples](https://arxiv.org/abs/2201.05979)

Authors: [Hao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Yangguang Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Zhen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z), [Yong Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Y), [Lingpeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L), [Jing Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+J)

> Unsupervised sentence embedding aims to obtain the most appropriate embedding for a sentence to reflect its semantic. Contrastive learning has been attracting developing attention. For a sentence, current models utilize diverse data augmentation methods to generate positive samples, while consider other independent sentences as negative samples. Then they adopt InfoNCE loss to pull the embeddings of positive pairs gathered, and push those of negative pairs scattered. Although these models have made great progress on sentence embedding, we argue that they may suffer from feature suppression. The models fail to distinguish and decouple textual similarity and semantic similarity. And they may overestimate the semantic similarity of any pairs with similar textual regardless of the actual semantic difference between them. This is because positive pairs in unsupervised contrastive learning come with similar and even the same textual through data augmentation. To alleviate feature suppression, we propose contrastive learning for unsupervised sentence embedding with soft negative samples (SNCSE). Soft negative samples share highly similar textual but have surely and apparently different semantic with the original samples. Specifically, we take the negation of original sentences as soft negative samples, and propose Bidirectional Margin Loss (BML) to introduce them into traditional contrastive learning framework, which merely involves positive and negative samples. Our experimental results show that SNCSE can obtain state-of-the-art performance on semantic textual similarity (STS) task with average Spearman's correlation coefficient of 78.97% on BERTbase and 79.23% on RoBERTabase. Besides, we adopt rank-based error analysis method to detect the weakness of SNCSE for future study.

| Comments: | 7 pages, 4 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.05979](https://arxiv.org/abs/2201.05979) [cs.CL]** |
|           | (or **[arXiv:2201.05979v2](https://arxiv.org/abs/2201.05979v2) [cs.CL]** for this version) |





<h2 id="2022-01-19-7">7. Memory-assisted prompt editing to improve GPT-3 after deployment
</h2>

Title: [Memory-assisted prompt editing to improve GPT-3 after deployment](https://arxiv.org/abs/2201.06009)

Authors: [Aman Madaan](https://arxiv.org/search/cs?searchtype=author&query=Madaan%2C+A), [Niket Tandon](https://arxiv.org/search/cs?searchtype=author&query=Tandon%2C+N), [Peter Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+P), [Yiming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y)

> Large LMs such as GPT-3, while powerful, are not immune to mistakes, but are prohibitively costly to retrain. One failure mode is misinterpreting a user's instruction (e.g., GPT-3 interpreting "What word is similar to good?" to mean a homonym, while the user intended a synonym). Our goal is to allow users to correct such errors directly through interaction -- without retraining. Our approach pairs GPT-3 with a growing memory of cases where the model misunderstood the user's intent and was provided with feedback, clarifying the instruction. Given a new query, our memory-enhanced GPT-3 uses feedback from similar, prior queries to enrich the prompt. Through simple proof-of-concept experiments, we show how a (simulated) user can interactively teach a deployed GPT-3, doubling its accuracy on basic lexical tasks (e.g., generate a synonym) where users query in different, novel (often misunderstood) ways. In such scenarios, memory helps avoid repeating similar past mistakes. Our simple idea is a first step towards strengthening deployed models, potentially broadening their utility. All the code and data is available at [this https URL](https://github.com/madaan/memprompt).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.06009](https://arxiv.org/abs/2201.06009) [cs.CL]** |
|           | (or **[arXiv:2201.06009v1](https://arxiv.org/abs/2201.06009v1) [cs.CL]** for this version) |





<h2 id="2022-01-19-8">8. Syntax-based data augmentation for Hungarian-English machine translation
</h2>

Title: [Syntax-based data augmentation for Hungarian-English machine translation](https://arxiv.org/abs/2201.06876)

Authors: [Attila Nagy](https://arxiv.org/search/cs?searchtype=author&query=Nagy%2C+A), [Patrick Nanys](https://arxiv.org/search/cs?searchtype=author&query=Nanys%2C+P), [Balázs Frey Konrád](https://arxiv.org/search/cs?searchtype=author&query=Konrád%2C+B+F), [Bence Bial](https://arxiv.org/search/cs?searchtype=author&query=Bial%2C+B), [Judit Ács](https://arxiv.org/search/cs?searchtype=author&query=Ács%2C+J)

> We train Transformer-based neural machine translation models for Hungarian-English and English-Hungarian using the Hunglish2 corpus. Our best models achieve a BLEU score of 40.0 on HungarianEnglish and 33.4 on English-Hungarian. Furthermore, we present results on an ongoing work about syntax-based augmentation for neural machine translation. Both our code and models are publicly available.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.06876](https://arxiv.org/abs/2201.06876) [cs.CL]** |
|           | (or **[arXiv:2201.06876v1](https://arxiv.org/abs/2201.06876v1) [cs.CL]** for this version) |





<h2 id="2022-01-19-9">9. Instance-aware Prompt Learning for Language Understanding and Generation
</h2>

Title: [Instance-aware Prompt Learning for Language Understanding and Generation](https://arxiv.org/abs/2201.07126)

Authors: [Feihu Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+F), [Jinliang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+J), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Chengqing Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+C)

> Recently, prompt learning has become a new paradigm to utilize pre-trained language models (PLMs) and achieves promising results in downstream tasks with a negligible increase of parameters. The current usage of discrete and continuous prompts assumes that the prompt is fixed for a specific task and all samples in the task share the same prompt. However, a task may contain quite diverse samples in which some are easy and others are difficult, and diverse prompts are desirable. In this paper, we propose an instance-aware prompt learning method that learns a different prompt for each instance. Specifically, we suppose that each learnable prompt token has a different contribution to different instances, and we learn the contribution by calculating the relevance score between an instance and each prompt token. The contribution weighted prompt would be instance aware. We apply our method to both unidirectional and bidirectional PLMs on both language understanding and generation tasks. Extensive experiments demonstrate that our method obtains considerable improvements compared to strong baselines. Especially, our method achieves the state-of-the-art on the SuperGLUE few-shot learning benchmark.

| Comments: | 7 pages, 5 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.07126](https://arxiv.org/abs/2201.07126) [cs.CL]** |
|           | (or **[arXiv:2201.07126v1](https://arxiv.org/abs/2201.07126v1) [cs.CL]** for this version) |






# 2022-01-17

[Return to Index](#Index)



<h2 id="2022-01-17-1">1. A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering
</h2>

Title: [A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering](https://arxiv.org/abs/2201.05299)

Authors: [Feng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+F), [Qing Ping](https://arxiv.org/search/cs?searchtype=author&query=Ping%2C+Q), [Govind Thattai](https://arxiv.org/search/cs?searchtype=author&query=Thattai%2C+G), [Aishwarya Reganti](https://arxiv.org/search/cs?searchtype=author&query=Reganti%2C+A), [Ying Nian Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y+N), [Prem Natarajan](https://arxiv.org/search/cs?searchtype=author&query=Natarajan%2C+P)

> Outside-knowledge visual question answering (OK-VQA) requires the agent to comprehend the image, make use of relevant knowledge from the entire web, and digest all the information to answer the question. Most previous works address the problem by first fusing the image and question in the multi-modal space, which is inflexible for further fusion with a vast amount of external knowledge. In this paper, we call for a paradigm shift for the OK-VQA task, which transforms the image into plain text, so that we can enable knowledge passage retrieval, and generative question-answering in the natural language space. This paradigm takes advantage of the sheer volume of gigantic knowledge bases and the richness of pre-trained language models. A Transform-Retrieve-Generate framework (TRiG) framework is proposed, which can be plug-and-played with alternative image-to-text models and textual knowledge bases. Experimental results show that our TRiG framework outperforms all state-of-the-art supervised methods by at least 11.1% absolute margin.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Information Retrieval (cs.IR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.05299](https://arxiv.org/abs/2201.05299) [cs.CV]** |
|           | (or **[arXiv:2201.05299v1](https://arxiv.org/abs/2201.05299v1) [cs.CV]** for this version) |





<h2 id="2022-01-17-2">2. A Survey of Pretrained Language Models Based Text Generation
</h2>

Title: [A Survey of Pretrained Language Models Based Text Generation](https://arxiv.org/abs/2201.05273)

Authors: [Junyi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Tianyi Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+T), [Wayne Xin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W+X), [Jian-Yun Nie](https://arxiv.org/search/cs?searchtype=author&query=Nie%2C+J), [Ji-Rong Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+J)

> Text Generation aims to produce plausible and readable text in human language from input data. The resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). Grounding text generation on PLMs is seen as a promising direction in both academia and industry. In this survey, we present the recent advances achieved in the topic of PLMs for text generation. In detail, we begin with introducing three key points of applying PLMs to text generation: 1) how to encode the input data as representations preserving input semantics which can be fused into PLMs; 2) how to design a universal and performant architecture of PLMs served as generation models; and 3) how to optimize PLMs given the reference text and ensure the generated text satisfying special text properties. Then, we figure out several challenges and future directions within each key point. Next, we present a summary of various useful resources and typical text generation applications to work with PLMs. Finally, we conclude and summarize the contribution of this survey.

| Comments: | 37 pages, 2 figures, 2 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.05273](https://arxiv.org/abs/2201.05273) [cs.CL]** |
|           | (or **[arXiv:2201.05273v1](https://arxiv.org/abs/2201.05273v1) [cs.CL]** for this version) |





<h2 id="2022-01-17-3">3. CommonsenseQA 2.0: Exposing the Limits of AI through Gamification
</h2>

Title: [CommonsenseQA 2.0: Exposing the Limits of AI through Gamification](https://arxiv.org/abs/2201.05320)

Authors: [Alon Talmor](https://arxiv.org/search/cs?searchtype=author&query=Talmor%2C+A), [Ori Yoran](https://arxiv.org/search/cs?searchtype=author&query=Yoran%2C+O), [Ronan Le Bras](https://arxiv.org/search/cs?searchtype=author&query=Bras%2C+R+L), [Chandra Bhagavatula](https://arxiv.org/search/cs?searchtype=author&query=Bhagavatula%2C+C), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J)

> Constructing benchmarks that test the abilities of modern natural language understanding models is difficult - pre-trained language models exploit artifacts in benchmarks to achieve human parity, but still fail on adversarial examples and make errors that demonstrate a lack of common sense. In this work, we propose gamification as a framework for data construction. The goal of players in the game is to compose questions that mislead a rival AI while using specific phrases for extra points. The game environment leads to enhanced user engagement and simultaneously gives the game designer control over the collected data, allowing us to collect high-quality data at scale. Using our method we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and demonstrate its difficulty for models that are orders-of-magnitude larger than the AI used in the game itself. Our best baseline, the T5-based Unicorn with 11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3 (52.9%) in a few-shot inference setup. Both score well below human performance which is at 94.1%.

| Comments: | Presented as Oral at NeurIPS 2021                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2201.05320](https://arxiv.org/abs/2201.05320) [cs.CL]** |
|           | (or **[arXiv:2201.05320v1](https://arxiv.org/abs/2201.05320v1) [cs.CL]** for this version) |





<h2 id="2022-01-17-4">4. A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models
</h2>

Title: [A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models](https://arxiv.org/abs/2201.05337)

Authors: [Hanqing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Haolin Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+H), [Shaoyu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Ming Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M), [Dawei Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D)

> Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that are more natural and better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the lower level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks which may require different types of controlled constraints. In this paper, we present a systematic critical review on the common tasks, main approaches and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey paper to summarize CTG techniques from the perspective of PLMs. We hope it can help researchers in related fields to quickly track the academic frontier, providing them with a landscape of the area and a roadmap for future research.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.05337](https://arxiv.org/abs/2201.05337) [cs.CL]** |
|           | (or **[arXiv:2201.05337v1](https://arxiv.org/abs/2201.05337v1) [cs.CL]** for this version) |





<h2 id="2022-01-17-5">5. Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer
</h2>

Title: [Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer](https://arxiv.org/abs/2201.05411)

Authors: [Yinyi Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Y), [Tong Mo](https://arxiv.org/search/cs?searchtype=author&query=Mo%2C+T), [Yongtao Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y), [Weiping Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W), [Wen Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W)

> Recent advances on prompt-tuning cast few-shot classification tasks as a masked language modeling problem. By wrapping input into a template and using a verbalizer which constructs a mapping between label space and label word space, prompt-tuning can achieve excellent results in zero-shot and few-shot scenarios. However, typical prompt-tuning needs a manually designed verbalizer which requires domain expertise and human efforts. And the insufficient label space may introduce considerable bias into the results. In this paper, we focus on eliciting knowledge from pretrained language models and propose a prototypical prompt verbalizer for prompt-tuning. Labels are represented by prototypical embeddings in the feature space rather than by discrete words. The distances between the embedding at the masked position of input and prototypical embeddings are used as classification criterion. For zero-shot settings, knowledge is elicited from pretrained language models by a manually designed template to form initial prototypical embeddings. For few-shot settings, models are tuned to learn meaningful and interpretable prototypical embeddings. Our method optimizes models by contrastive learning. Extensive experimental results on several many-class text classification datasets with low-resource settings demonstrate the effectiveness of our approach compared with other verbalizer construction methods. Our implementation is available at [this https URL](https://github.com/Ydongd/prototypical-prompt-verbalizer).

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.05411](https://arxiv.org/abs/2201.05411) [cs.CL]** |
|           | (or **[arXiv:2201.05411v1](https://arxiv.org/abs/2201.05411v1) [cs.CL]** for this version) |





<h2 id="2022-01-17-6">6. Czech Grammar Error Correction with a Large and Diverse Corpus
</h2>

Title: [Czech Grammar Error Correction with a Large and Diverse Corpus](https://arxiv.org/abs/2201.05590)

Authors: [Jakub Náplava](https://arxiv.org/search/cs?searchtype=author&query=Náplava%2C+J), [Milan Straka](https://arxiv.org/search/cs?searchtype=author&query=Straka%2C+M), [Jana Straková](https://arxiv.org/search/cs?searchtype=author&query=Straková%2C+J), [Alexandr Rosen](https://arxiv.org/search/cs?searchtype=author&query=Rosen%2C+A)

> We introduce a large and diverse Czech corpus annotated for grammatical error correction (GEC) with the aim to contribute to the still scarce data resources in this domain for languages other than English. The Grammar Error Correction Corpus for Czech (GECCC) offers a variety of four domains, covering error distributions ranging from high error density essays written by non-native speakers, to website texts, where errors are expected to be much less common. We compare several Czech GEC systems, including several Transformer-based ones, setting a strong baseline to future research. Finally, we meta-evaluate common GEC metrics against human judgements on our data. We make the new Czech GEC corpus publicly available under the CC BY-SA 4.0 license at [this http URL](http://hdl.handle.net/11234/1-4639) .

| Comments: | Accepted to TACL, MIT Press                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.05590](https://arxiv.org/abs/2201.05590) [cs.CL]** |
|           | (or **[arXiv:2201.05590v1](https://arxiv.org/abs/2201.05590v1) [cs.CL]** for this version) |





<h2 id="2022-01-17-7">7. Multilingual Open Text 1.0: Public Domain News in 44 Languages
</h2>

Title: [Multilingual Open Text 1.0: Public Domain News in 44 Languages](https://arxiv.org/abs/2201.05609)

Authors: [Chester Palen-Michel](https://arxiv.org/search/cs?searchtype=author&query=Palen-Michel%2C+C), [June Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J), [Constantine Lignos](https://arxiv.org/search/cs?searchtype=author&query=Lignos%2C+C)

> We present a new multilingual corpus containing text in 44 languages, many of which have relatively few existing resources for natural language processing. The first release of the corpus contains over 2.7 million news articles and 1 million shorter passages published between 2001--2021, collected from Voice of America news websites. We describe our process for collecting, filtering, and processing the data. The source material is in the public domain, our collection is licensed using a creative commons license (CC BY 4.0), and all software used to create the corpus is released under the MIT License. The corpus will be regularly updated as additional documents are published.

| Comments: | Submitted to LREC 2022                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2201.05609](https://arxiv.org/abs/2201.05609) [cs.CL]** |
|           | (or **[arXiv:2201.05609v1](https://arxiv.org/abs/2201.05609v1) [cs.CL]** for this version) |






# 2022-01-14

[Return to Index](#Index)



<h2 id="2022-01-14-1">1. Towards Automated Error Analysis: Learning to Characterize Errors
</h2>

Title: [Towards Automated Error Analysis: Learning to Characterize Errors](https://arxiv.org/abs/2201.05017)

Authors: [Tong Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+T), [Shivang Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+S), [Raymond J. Mooney](https://arxiv.org/search/cs?searchtype=author&query=Mooney%2C+R+J)

> Characterizing the patterns of errors that a system makes helps researchers focus future development on increasing its accuracy and robustness. We propose a novel form of "meta learning" that automatically learns interpretable rules that characterize the types of errors that a system makes, and demonstrate these rules' ability to help understand and improve two NLP systems. Our approach works by collecting error cases on validation data, extracting meta-features describing these samples, and finally learning rules that characterize errors using these features. We apply our approach to VilBERT, for Visual Question Answering, and RoBERTa, for Common Sense Question Answering. Our system learns interpretable rules that provide insights into systemic errors these systems make on the given tasks. Using these insights, we are also able to "close the loop" and modestly improve performance of these systems.

| Comments: | 12 pages, 11 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2201.05017](https://arxiv.org/abs/2201.05017) [cs.CL]** |
|           | (or **[arXiv:2201.05017v1](https://arxiv.org/abs/2201.05017v1) [cs.CL]** for this version) |









# 2022-01-13

[Return to Index](#Index)



<h2 id="2022-01-13-1">1. PromptBERT: Improving BERT Sentence Embeddings with Prompts
</h2>

Title: [PromptBERT: Improving BERT Sentence Embeddings with Prompts](https://arxiv.org/abs/2201.04337)

Authors: [Ting Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+T), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Zihan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Deqing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Fuzhen Zhuang](https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+F), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Haizhen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Liangjie Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Qi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q)

> The poor performance of the original BERT for sentence semantic similarity has been widely discussed in previous works. We find that unsatisfactory performance is mainly due to the static token embeddings biases and the ineffective BERT layers, rather than the high cosine similarity of the sentence embeddings. To this end, we propose a prompt based sentence embeddings method which can reduce token embeddings biases and make the original BERT layers more effective. By reformulating the sentence embeddings task as the fillin-the-blanks problem, our method significantly improves the performance of original BERT. We discuss two prompt representing methods and three prompt searching methods for prompt based sentence embeddings. Moreover, we propose a novel unsupervised training objective by the technology of template denoising, which substantially shortens the performance gap between the supervised and unsupervised setting. For experiments, we evaluate our method on both non fine-tuned and fine-tuned settings. Even a non fine-tuned method can outperform the fine-tuned methods like unsupervised ConSERT on STS tasks. Our fine-tuned method outperforms the state-of-the-art method SimCSE in both unsupervised and supervised settings. Compared to SimCSE, we achieve 2.29 and 2.58 points improvements on BERT and RoBERTa respectively under the unsupervised setting.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.04337](https://arxiv.org/abs/2201.04337) [cs.CL]** |
|           | (or **[arXiv:2201.04337v1](https://arxiv.org/abs/2201.04337v1) [cs.CL]** for this version) |





<h2 id="2022-01-13-2">2. How Does Data Corruption Affect Natural Language Understanding Models? A Study on GLUE datasets
</h2>

Title: [How Does Data Corruption Affect Natural Language Understanding Models? A Study on GLUE datasets](https://arxiv.org/abs/2201.04467)

Authors: [Aarne Talman](https://arxiv.org/search/cs?searchtype=author&query=Talman%2C+A), [Marianna Apidianaki](https://arxiv.org/search/cs?searchtype=author&query=Apidianaki%2C+M), [Stergios Chatzikyriakidis](https://arxiv.org/search/cs?searchtype=author&query=Chatzikyriakidis%2C+S), [Jörg Tiedemann](https://arxiv.org/search/cs?searchtype=author&query=Tiedemann%2C+J)

> A central question in natural language understanding (NLU) research is whether high performance demonstrates the models' strong reasoning capabilities. We present an extensive series of controlled experiments where pre-trained language models are exposed to data that have undergone specific corruption transformations. The transformations involve removing instances of specific word classes and often lead to non-sensical sentences. Our results show that performance remains high for most GLUE tasks when the models are fine-tuned or tested on corrupted data, suggesting that the models leverage other cues for prediction even in non-sensical contexts. Our proposed data transformations can be used as a diagnostic tool for assessing the extent to which a specific dataset constitutes a proper testbed for evaluating models' language understanding capabilities.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.04467](https://arxiv.org/abs/2201.04467) [cs.CL]** |
|           | (or **[arXiv:2201.04467v1](https://arxiv.org/abs/2201.04467v1) [cs.CL]** for this version) |





# 2022-01-12

[Return to Index](#Index)



<h2 id="2022-01-12-1">1. Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training
</h2>

Title:  [Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training](https://arxiv.org/abs/2201.04026)

Authors: [Yehao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Jiahao Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+J), [Yingwei Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+Y), [Ting Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+T), [Weiyao Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+W), [Tao Mei](https://arxiv.org/search/cs?searchtype=author&query=Mei%2C+T)

> Vision-language pre-training has been an emerging and fast-developing research topic, which transfers multi-modal knowledge from rich-resource pre-training task to limited-resource downstream tasks. Unlike existing works that predominantly learn a single generic encoder, we present a pre-trainable Universal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language perception (e.g., visual question answering) and generation (e.g., image captioning). Uni-EDEN is a two-stream Transformer based structure, consisting of three modules: object and sentence encoders that separately learns the representations of each modality, and sentence decoder that enables both multi-modal reasoning and sentence generation via inter-modal interaction. Considering that the linguistic representations of each image can span different granularities in this hierarchy including, from simple to comprehensive, individual label, a phrase, and a natural sentence, we pre-train Uni-EDEN through multi-granular vision-language proxy tasks: Masked Object Classification (MOC), Masked Region Phrase Generation (MRPG), Image-Sentence Matching (ISM), and Masked Sentence Generation (MSG). In this way, Uni-EDEN is endowed with the power of both multi-modal representation extraction and language modeling. Extensive experiments demonstrate the compelling generalizability of Uni-EDEN by fine-tuning it to four vision-language perception and generation downstream tasks.

| Comments: | ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Multimedia (cs.MM) |
| Cite as:  | **[arXiv:2201.04026](https://arxiv.org/abs/2201.04026) [cs.CV]** |
|           | (or **[arXiv:2201.04026v1](https://arxiv.org/abs/2201.04026v1) [cs.CV]** for this version) |





<h2 id="2022-01-12-2">2. CVSS Corpus and Massively Multilingual Speech-to-Speech Translation
</h2>

Title:  [CVSS Corpus and Massively Multilingual Speech-to-Speech Translation](https://arxiv.org/abs/2201.03713)

Authors: [Ye Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+Y), [Michelle Tadmor Ramanovich](https://arxiv.org/search/cs?searchtype=author&query=Ramanovich%2C+M+T), [Quan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Heiga Zen](https://arxiv.org/search/cs?searchtype=author&query=Zen%2C+H)

> We introduce CVSS, a massively multilingual-to-English speech-to-speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems. Two versions of translation speeches are provided: 1) CVSS-C: All the translation speeches are in a single high-quality canonical voice; 2) CVSS-T: The translation speeches are in voices transferred from the corresponding source speeches. In addition, CVSS provides normalized translation text which matches the pronunciation in the translation speech. On each version of CVSS, we built baseline multilingual direct S2ST models and cascade S2ST models, verifying the effectiveness of the corpus. To build strong cascade S2ST baselines, we trained an ST model on CoVoST 2, which outperforms the previous state-of-the-art trained on the corpus without extra data by 5.8 BLEU. Nevertheless, the performance of the direct S2ST models approaches the strong cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU difference on ASR transcribed translation when initialized from matching ST models.

| Comments: | Submitted to LREC 2022                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2201.03713](https://arxiv.org/abs/2201.03713) [cs.CL]** |
|           | (or **[arXiv:2201.03713v1](https://arxiv.org/abs/2201.03713v1) [cs.CL]** for this version) |





<h2 id="2022-01-12-3">3. Quantifying Robustness to Adversarial Word Substitutions
</h2>

Title:  [Quantifying Robustness to Adversarial Word Substitutions](https://arxiv.org/abs/2201.03829)

Authors: [Yuting Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Pei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+P), [FeiFei Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+F), [Juan Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+J), [Meishan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Jian Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Jintao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J)

> Deep-learning-based NLP models are found to be vulnerable to word substitution perturbations. Before they are widely adopted, the fundamental issues of robustness need to be addressed. Along this line, we propose a formal framework to evaluate word-level robustness. First, to study safe regions for a model, we introduce robustness radius which is the boundary where the model can resist any perturbation. As calculating the maximum robustness radius is computationally hard, we estimate its upper and lower bound. We repurpose attack methods as ways of seeking upper bound and design a pseudo-dynamic programming algorithm for a tighter upper bound. Then verification method is utilized for a lower bound. Further, for evaluating the robustness of regions outside a safe radius, we reexamine robustness from another view: quantification. A robustness metric with a rigorous statistical guarantee is introduced to measure the quantification of adversarial examples, which indicates the model's susceptibility to perturbations outside the safe radius. The metric helps us figure out why state-of-the-art models like BERT can be easily fooled by a few word substitutions, but generalize well in the presence of real-world noises.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.03829](https://arxiv.org/abs/2201.03829) [cs.CL]** |
|           | (or **[arXiv:2201.03829v1](https://arxiv.org/abs/2201.03829v1) [cs.CL]** for this version) |





# 2022-01-11

[Return to Index](#Index)



<h2 id="2022-01-11-1">1. Towards the Next 1000 Languages in Multilingual Machine Translation: Exploring the Synergy Between Supervised and Self-Supervised Learning
</h2>

Title: [Towards the Next 1000 Languages in Multilingual Machine Translation: Exploring the Synergy Between Supervised and Self-Supervised Learning](https://arxiv.org/abs/2201.03110)

Authors: [Aditya Siddhant](https://arxiv.org/search/cs?searchtype=author&query=Siddhant%2C+A), [Ankur Bapna](https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Yuan Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y), [Mia Xu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M+X), [Isaac Caswell](https://arxiv.org/search/cs?searchtype=author&query=Caswell%2C+I), [Xavier Garcia](https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+X)

> Achieving universal translation between all human language pairs is the holy-grail of machine translation (MT) research. While recent progress in massively multilingual MT is one step closer to reaching this goal, it is becoming evident that extending a multilingual MT system simply by training on more parallel data is unscalable, since the availability of labeled data for low-resource and non-English-centric language pairs is forbiddingly limited. To this end, we present a pragmatic approach towards building a multilingual MT model that covers hundreds of languages, using a mixture of supervised and self-supervised objectives, depending on the data availability for different language pairs. We demonstrate that the synergy between these two training paradigms enables the model to produce high-quality translations in the zero-resource setting, even surpassing supervised translation quality for low- and mid-resource languages. We conduct a wide array of experiments to understand the effect of the degree of multilingual supervision, domain mismatches and amounts of parallel and monolingual data on the quality of our self-supervised multilingual models. To demonstrate the scalability of the approach, we train models with over 200 languages and demonstrate high performance on zero-resource translation on several previously under-studied languages. We hope our findings will serve as a stepping stone towards enabling translation for the next thousand languages.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.03110](https://arxiv.org/abs/2201.03110) [cs.CL]** |
|           | (or **[arXiv:2201.03110v1](https://arxiv.org/abs/2201.03110v1) [cs.CL]** for this version) |





<h2 id="2022-01-11-2">2. Black-Box Tuning for Language-Model-as-a-Service
</h2>

Title: [Black-Box Tuning for Language-Model-as-a-Service](https://arxiv.org/abs/2201.03514)

Authors: [Tianxiang Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+T), [Yunfan Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+Y), [Hong Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+H), [Xuanjing Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X)

> Extremely large pre-trained language models (PTMs) such as GPT-3 are usually released as a service, allowing users to design task-specific prompts to query the PTMs through some black-box APIs. In such a scenario, which we call Language-Model-as-a-Service (LMaaS), gradients of the PTMs are usually not available. Can we optimize the task prompts by only accessing the model inference APIs? Based on recent observations that large PTMs have a very low intrinsic dimensionality, this work proposes the Black-Box Tuning to optimize PTMs through derivative-free algorithms. In particular, we invoke the CMA-ES to optimize the continuous prompt prepended to the input text by iteratively calling PTM inference APIs. Our experimental results demonstrate that, black-box tuning with RoBERTa on a few labeled samples not only significantly outperforms manual prompt and GPT-3's in-context learning, but also surpasses the gradient-based counterparts, namely prompt tuning and full model tuning.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2201.03514](https://arxiv.org/abs/2201.03514) [cs.CL]** |
|           | (or **[arXiv:2201.03514v1](https://arxiv.org/abs/2201.03514v1) [cs.CL]** for this version) |





<h2 id="2022-01-11-3">3. SCROLLS: Standardized CompaRison Over Long Language Sequences
</h2>

Title: [SCROLLS: Standardized CompaRison Over Long Language Sequences](https://arxiv.org/abs/2201.03533)

Authors: [Uri Shaham](https://arxiv.org/search/cs?searchtype=author&query=Shaham%2C+U), [Elad Segal](https://arxiv.org/search/cs?searchtype=author&query=Segal%2C+E), [Maor Ivgi](https://arxiv.org/search/cs?searchtype=author&query=Ivgi%2C+M), [Avia Efrat](https://arxiv.org/search/cs?searchtype=author&query=Efrat%2C+A), [Ori Yoran](https://arxiv.org/search/cs?searchtype=author&query=Yoran%2C+O), [Adi Haviv](https://arxiv.org/search/cs?searchtype=author&query=Haviv%2C+A), [Ankit Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A), [Wenhan Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+W), [Mor Geva](https://arxiv.org/search/cs?searchtype=author&query=Geva%2C+M), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J), [Omer Levy](https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+O)

> NLP benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a considerable amount of natural language in the wild. We introduce SCROLLS, a suite of tasks that require reasoning over long texts. We examine existing long-text datasets, and handpick ones where the text is naturally long, while prioritizing tasks that involve synthesizing information across the input. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. Initial baselines, including Longformer Encoder-Decoder, indicate that there is ample room for improvement on SCROLLS. We make all datasets available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.03533](https://arxiv.org/abs/2201.03533) [cs.CL]** |
|           | (or **[arXiv:2201.03533v1](https://arxiv.org/abs/2201.03533v1) [cs.CL]** for this version) |







# 2022-01-10

[Return to Index](#Index)



<h2 id="2022-01-10-1">1. Automatic Speech Recognition Datasets in Cantonese Language: A Survey and a New Dataset
</h2>

Title: [Automatic Speech Recognition Datasets in Cantonese Language: A Survey and a New Dataset](https://arxiv.org/abs/2201.02419)

Authors: [Tiezheng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+T), [Rita Frieske](https://arxiv.org/search/cs?searchtype=author&query=Frieske%2C+R), [Peng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P), [Samuel Cahyawijaya](https://arxiv.org/search/cs?searchtype=author&query=Cahyawijaya%2C+S), [Cheuk Tung Shadow Yiu](https://arxiv.org/search/cs?searchtype=author&query=Yiu%2C+C+T+S), [Holy Lovenia](https://arxiv.org/search/cs?searchtype=author&query=Lovenia%2C+H), [Wenliang Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+W), [Elham J. Barezi](https://arxiv.org/search/cs?searchtype=author&query=Barezi%2C+E+J), [Qifeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Xiaojuan Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X), [Bertram E. Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+B+E), [Pascale Fung](https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P)

> Automatic speech recognition (ASR) on low resource languages improves access of linguistic minorities to technological advantages provided by Artificial Intelligence (AI). In this paper, we address a problem of data scarcity of Hong Kong Cantonese language by creating a new Cantonese dataset. Our dataset, Multi-Domain Cantonese Corpus (MDCC), consists of 73.6 hours of clean read speech paired with transcripts, collected from Cantonese audiobooks from Hong Kong. It combines philosophy, politics, education, culture, lifestyle and family domains, covering a wide range of topics. We also review all existing Cantonese datasets and perform experiments on the two biggest datasets (MDCC and Common Voice zh-HK). We analyze the existing datasets according to their speech type, data source, total size and availability. The results of experiments conducted with Fairseq S2T Transformer, a state-of-the-art ASR model, show the effectiveness of our dataset. In addition, we create a powerful and robust Cantonese ASR model by applying multi-dataset learning on MDCC and Common Voice zh-HK.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.02419](https://arxiv.org/abs/2201.02419) [cs.CL]** |
|           | (or **[arXiv:2201.02419v1](https://arxiv.org/abs/2201.02419v1) [cs.CL]** for this version) |





<h2 id="2022-01-10-2">2. Semantic-based Data Augmentation for Math Word Problems
</h2>

Title: [Semantic-based Data Augmentation for Math Word Problems](https://arxiv.org/abs/2201.02489)

Authors: [Ailisi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+A), [Jiaqing Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+J), [Yanghua Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+Y)

> It's hard for neural MWP solvers to deal with tiny local variances. In MWP task, some local changes conserve the original semantic while the others may totally change the underlying logic. Currently, existing datasets for MWP task contain limited samples which are key for neural models to learn to disambiguate different kinds of local variances in questions and solve the questions correctly. In this paper, we propose a set of novel data augmentation approaches to supplement existing datasets with such data that are augmented with different kinds of local variances, and help to improve the generalization ability of current neural models. New samples are generated by knowledge guided entity replacement, and logic guided problem reorganization. The augmentation approaches are ensured to keep the consistency between the new data and their labels. Experimental results have shown the necessity and the effectiveness of our methods.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.02489](https://arxiv.org/abs/2201.02489) [cs.CL]** |
|           | (or **[arXiv:2201.02489v1](https://arxiv.org/abs/2201.02489v1) [cs.CL]** for this version) |





<h2 id="2022-01-10-3">3. Repairing Adversarial Texts through Perturbation
</h2>

Title: [Repairing Adversarial Texts through Perturbation](https://arxiv.org/abs/2201.02504)

Authors: [Guoliang Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+G), [Jingyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Jun Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J), [Sudipta Chattopadhyay](https://arxiv.org/search/cs?searchtype=author&query=Chattopadhyay%2C+S), [Xinyu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Ting Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+T), [Jie Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+J), [Jin Song Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+J+S)

> It is known that neural networks are subject to attacks through adversarial perturbations, i.e., inputs which are maliciously crafted through perturbations to induce wrong predictions. Furthermore, such attacks are impossible to eliminate, i.e., the adversarial perturbation is still possible after applying mitigation methods such as adversarial training. Multiple approaches have been developed to detect and reject such adversarial inputs, mostly in the image domain. Rejecting suspicious inputs however may not be always feasible or ideal. First, normal inputs may be rejected due to false alarms generated by the detection algorithm. Second, denial-of-service attacks may be conducted by feeding such systems with adversarial inputs. To address the gap, in this work, we propose an approach to automatically repair adversarial texts at runtime. Given a text which is suspected to be adversarial, we novelly apply multiple adversarial perturbation methods in a positive way to identify a repair, i.e., a slightly mutated but semantically equivalent text that the neural network correctly classifies. Our approach has been experimented with multiple models trained for natural language processing tasks and the results show that our approach is effective, i.e., it successfully repairs about 80\% of the adversarial texts. Furthermore, depending on the applied perturbation method, an adversarial text could be repaired in as short as one second on average.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.02504](https://arxiv.org/abs/2201.02504) [cs.CL]** |
|           | (or **[arXiv:2201.02504v1](https://arxiv.org/abs/2201.02504v1) [cs.CL]** for this version) |





<h2 id="2022-01-10-4">4. Code-Switching Text Augmentation for Multilingual Speech Processing
</h2>

Title: [Code-Switching Text Augmentation for Multilingual Speech Processing](https://arxiv.org/abs/2201.02550)

Authors: [Amir Hussein](https://arxiv.org/search/cs?searchtype=author&query=Hussein%2C+A), [Shammur Absar Chowdhury](https://arxiv.org/search/cs?searchtype=author&query=Chowdhury%2C+S+A), [Ahmed Abdelali](https://arxiv.org/search/cs?searchtype=author&query=Abdelali%2C+A), [Najim Dehak](https://arxiv.org/search/cs?searchtype=author&query=Dehak%2C+N), [Ahmed Ali](https://arxiv.org/search/cs?searchtype=author&query=Ali%2C+A)

> The pervasiveness of intra-utterance Code-switching (CS) in spoken content has enforced ASR systems to handle mixed input. Yet, designing a CS-ASR has many challenges, mainly due to the data scarcity, grammatical structure complexity, and mismatch along with unbalanced language usage distribution. Recent ASR studies showed the predominance of E2E-ASR using multilingual data to handle CS phenomena with little CS data. However, the dependency on the CS data still remains. In this work, we propose a methodology to augment the monolingual data for artificially generating spoken CS text to improve different speech modules. We based our approach on Equivalence Constraint theory while exploiting aligned translation pairs, to generate grammatically valid CS content. Our empirical results show a relative gain of 29-34 % in perplexity and around 2% in WER for two ecological and noisy CS test sets. Finally, the human evaluation suggests that 83.8% of the generated data is acceptable to humans.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.02550](https://arxiv.org/abs/2201.02550) [cs.CL]** |
|           | (or **[arXiv:2201.02550v1](https://arxiv.org/abs/2201.02550v1) [cs.CL]** for this version) |






# 2022-01-07

[Return to Index](#Index)



<h2 id="2022-01-07-1">1. Compact Bidirectional Transformer for Image Captioning
</h2>

Title: [Compact Bidirectional Transformer for Image Captioning](https://arxiv.org/abs/2201.01984)

Authors: [Yuanen Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y), [Zhenzhen Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Z), [Daqing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D), [Huixia Ben](https://arxiv.org/search/cs?searchtype=author&query=Ben%2C+H), [Meng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M)

> Most current image captioning models typically generate captions from left to right. This unidirectional property makes them can only leverage past context but not future context. Though recent refinement-based models can exploit both past and future context by generating a new caption in the second stage based on pre-retrieved or pre-generated captions in the first stage, the decoder of these models generally consists of two networks~(i.e. a retriever or captioner in the first stage and a refiner in the second stage), which can only be executed sequentially. In this paper, we introduce a Compact Bidirectional Transformer model for image captioning that can leverage bidirectional context implicitly and explicitly while the decoder can be executed parallelly. Specifically, it is implemented by tightly coupling left-to-right(L2R) and right-to-left(R2L) flows into a single compact model~(i.e. implicitly) and optionally allowing interaction of the two flows(i.e. explicitly), while the final caption is chosen from either L2R or R2L flow in a sentence-level ensemble manner. We conduct extensive ablation studies on the MSCOCO benchmark and find that the compact architecture, which serves as a regularization for implicitly exploiting bidirectional context, and the sentence-level ensemble play more important roles than the explicit interaction mechanism. By combining with word-level ensemble seamlessly, the effect of the sentence-level ensemble is further enlarged. We further extend the conventional one-flow self-critical training to the two-flows version under this architecture and achieve new state-of-the-art results in comparison with non-vision-language-pretraining models. Source code is available at {\color{magenta}\url{[this https URL](https://github.com/YuanEZhou/CBTrans)}}.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.01984](https://arxiv.org/abs/2201.01984) [cs.CV]** |
|           | (or **[arXiv:2201.01984v1](https://arxiv.org/abs/2201.01984v1) [cs.CV]** for this version) |





<h2 id="2022-01-07-2">2. Self-Training Vision Language BERTs with a Unified Conditional Model
</h2>

Title: [Self-Training Vision Language BERTs with a Unified Conditional Model](https://arxiv.org/abs/2201.02010)

Authors: [Xiaofeng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+X), [Fengmao Lv](https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+F), [Fayao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Guosheng Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+G)

> Natural language BERTs are trained with language corpus in a self-supervised manner. Unlike natural language BERTs, vision language BERTs need paired data to train, which restricts the scale of VL-BERT pretraining. We propose a self-training approach that allows training VL-BERTs from unlabeled image data. The proposed method starts with our unified conditional model -- a vision language BERT model that can perform zero-shot conditional generation. Given different conditions, the unified conditional model can generate captions, dense captions, and even questions. We use the labeled image data to train a teacher model and use the trained model to generate pseudo captions on unlabeled image data. We then combine the labeled data and pseudo labeled data to train a student model. The process is iterated by putting the student model as a new teacher. By using the proposed self-training approach and only 300k unlabeled extra data, we are able to get competitive or even better performances compared to the models of similar model size trained with 3 million extra image data.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.02010](https://arxiv.org/abs/2201.02010) [cs.CV]** |
|           | (or **[arXiv:2201.02010v1](https://arxiv.org/abs/2201.02010v1) [cs.CV]** for this version) |





<h2 id="2022-01-07-3">3. Phrase-level Adversarial Example Generation for Neural Machine Translation
</h2>

Title: [Phrase-level Adversarial Example Generation for Neural Machine Translation](https://arxiv.org/abs/2201.02009)

Authors: [Juncheng Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+J), [Jian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Weinan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Yong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> While end-to-end neural machine translation (NMT) has achieved impressive progress, noisy input usually leads models to become fragile and unstable. Generating adversarial examples as the augmented data is proved to be useful to alleviate this problem. Existing methods for adversarial example generation (AEG) are word-level or character-level. In this paper, we propose a phrase-level adversarial example generation (PAEG) method to enhance the robustness of the model. Our method leverages a gradient-based strategy to substitute phrases of vulnerable positions in the source input. We verify our method on three benchmarks, including LDC Chinese-English, IWSLT14 German-English, and WMT14 English-German tasks. Experimental results demonstrate that our approach significantly improves performance compared to previous methods.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.02009](https://arxiv.org/abs/2201.02009) [cs.CL]** |
|           | (or **[arXiv:2201.02009v1](https://arxiv.org/abs/2201.02009v1) [cs.CL]** for this version) |





# 2022-01-06

[Return to Index](#Index)



<h2 id="2022-01-06-1">1. All You Need In Sign Language Production
</h2>

Title: [All You Need In Sign Language Production](https://arxiv.org/abs/2201.01609)

Authors: [Razieh Rastgoo](https://arxiv.org/search/cs?searchtype=author&query=Rastgoo%2C+R), [Kourosh Kiani](https://arxiv.org/search/cs?searchtype=author&query=Kiani%2C+K), [Sergio Escalera](https://arxiv.org/search/cs?searchtype=author&query=Escalera%2C+S), [Vassilis Athitsos](https://arxiv.org/search/cs?searchtype=author&query=Athitsos%2C+V), [Mohammad Sabokrou](https://arxiv.org/search/cs?searchtype=author&query=Sabokrou%2C+M)

> Sign Language is the dominant form of communication language used in the deaf and hearing-impaired community. To make an easy and mutual communication between the hearing-impaired and the hearing communities, building a robust system capable of translating the spoken language into sign language and vice versa is fundamental. To this end, sign language recognition and production are two necessary parts for making such a two-way system. Sign language recognition and production need to cope with some critical challenges. In this survey, we review recent advances in Sign Language Production (SLP) and related areas using deep learning. To have more realistic perspectives to sign language, we present an introduction to the Deaf culture, Deaf centers, psychological perspective of sign language, the main differences between spoken language and sign language. Furthermore, we present the fundamental components of a bi-directional sign language translation system, discussing the main challenges in this area. Also, the backbone architectures and methods in SLP are briefly introduced and the proposed taxonomy on SLP is presented. Finally, a general framework for SLP and performance evaluation, and also a discussion on the recent developments, advantages, and limitations in SLP, commenting on possible lines for future research are presented.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:2103.15910](https://arxiv.org/abs/2103.15910) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2201.01609](https://arxiv.org/abs/2201.01609) [cs.CV]** |
|           | (or **[arXiv:2201.01609v1](https://arxiv.org/abs/2201.01609v1) [cs.CV]** for this version) |





<h2 id="2022-01-06-2">2. SMDT: Selective Memory-Augmented Neural Document Translation
</h2>

Title: [SMDT: Selective Memory-Augmented Neural Document Translation](https://arxiv.org/abs/2201.01631)

Authors: [Xu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Jian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Haoyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Jinlong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> Existing document-level neural machine translation (NMT) models have sufficiently explored different context settings to provide guidance for target generation. However, little attention is paid to inaugurate more diverse context for abundant context information. In this paper, we propose a Selective Memory-augmented Neural Document Translation model to deal with documents containing large hypothesis space of the context. Specifically, we retrieve similar bilingual sentence pairs from the training corpus to augment global context and then extend the two-stream attention model with selective mechanism to capture local context and diverse global contexts. This unified approach allows our model to be trained elegantly on three publicly document-level machine translation datasets and significantly outperforms previous document-level NMT models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.01631](https://arxiv.org/abs/2201.01631) [cs.CL]** |
|           | (or **[arXiv:2201.01631v1](https://arxiv.org/abs/2201.01631v1) [cs.CL]** for this version) |





# 2022-01-05

[Return to Index](#Index)



<h2 id="2022-01-05-1">1. Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety
</h2>

Title: [Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety](https://arxiv.org/abs/2201.00969)

Authors: [Rajagopal A](https://arxiv.org/search/cs?searchtype=author&query=A%2C+R), [Nirmala V](https://arxiv.org/search/cs?searchtype=author&query=V%2C+N), [Arun Muthuraj Vedamanickam](https://arxiv.org/search/cs?searchtype=author&query=Vedamanickam%2C+A+M)

> There is amazing progress in Deep Learning based models for Image captioning and Low Light image enhancement. For the first time in literature, this paper develops a Deep Learning model that translates night scenes to sentences, opening new possibilities for AI applications in the safety of visually impaired women. Inspired by Image Captioning and Visual Question Answering, a novel Interactive Image Captioning is developed. A user can make the AI focus on any chosen person of interest by influencing the attention scoring. Attention context vectors are computed from CNN feature vectors and user-provided start word. The Encoder-Attention-Decoder neural network learns to produce captions from low brightness images. This paper demonstrates how women safety can be enabled by researching a novel AI capability in the Interactive Vision-Language model for perception of the environment in the night.

| Comments:    | In Springer Proceedings. International Conference On Big Data, Machine Learning and Applications 2021. [this http URL](http://bigdml.nits.ac.in/) |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| ACM classes: | I.2.0                                                        |
| Cite as:     | **[arXiv:2201.00969](https://arxiv.org/abs/2201.00969) [cs.CV]** |
|              | (or **[arXiv:2201.00969v1](https://arxiv.org/abs/2201.00969v1) [cs.CV]** for this version) |





<h2 id="2022-01-05-2">2. StyleM: Stylized Metrics for Image Captioning Built with Contrastive N-grams
</h2>

Title: [StyleM: Stylized Metrics for Image Captioning Built with Contrastive N-grams](https://arxiv.org/abs/2201.00975)

Authors: [Chengxi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Brent Harrison](https://arxiv.org/search/cs?searchtype=author&query=Harrison%2C+B)

> In this paper, we build two automatic evaluation metrics for evaluating the association between a machine-generated caption and a ground truth stylized caption: OnlyStyle and StyleCIDEr.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.00975](https://arxiv.org/abs/2201.00975) [cs.CV]** |
|           | (or **[arXiv:2201.00975v1](https://arxiv.org/abs/2201.00975v1) [cs.CV]** for this version) |







# 2022-01-04

[Return to Index](#Index)



<h2 id="2022-01-04-1">1. How do lexical semantics affect translation? An empirical study
</h2>

Title: [How do lexical semantics affect translation? An empirical study](https://arxiv.org/abs/2201.00075)

Authors:[Vivek Subramanian](https://arxiv.org/search/cs?searchtype=author&query=Subramanian%2C+V), [Dhanasekar Sundararaman](https://arxiv.org/search/cs?searchtype=author&query=Sundararaman%2C+D)

> Neural machine translation (NMT) systems aim to map text from one language into another. While there are a wide variety of applications of NMT, one of the most important is translation of natural language. A distinguishing factor of natural language is that words are typically ordered according to the rules of the grammar of a given language. Although many advances have been made in developing NMT systems for translating natural language, little research has been done on understanding how the word ordering of and lexical similarity between the source and target language affect translation performance. Here, we investigate these relationships on a variety of low-resource language pairs from the OpenSubtitles2016 database, where the source language is English, and find that the more similar the target language is to English, the greater the translation performance. In addition, we study the impact of providing NMT models with part of speech of words (POS) in the English sequence and find that, for Transformer-based models, the more dissimilar the target language is from English, the greater the benefit provided by POS.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2201.00075](https://arxiv.org/abs/2201.00075) [cs.CL]** |
|           | (or **[arXiv:2201.00075v1](https://arxiv.org/abs/2201.00075v1) [cs.CL]** for this version) |





<h2 id="2022-01-04-2">2. Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models
</h2>

Title: [Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models](https://arxiv.org/abs/2201.00558)

Authors:[Made Nindyatama Nityasya](https://arxiv.org/search/cs?searchtype=author&query=Nityasya%2C+M+N), [Haryo Akbarianto Wibowo](https://arxiv.org/search/cs?searchtype=author&query=Wibowo%2C+H+A), [Rendi Chevi](https://arxiv.org/search/cs?searchtype=author&query=Chevi%2C+R), [Radityo Eko Prasojo](https://arxiv.org/search/cs?searchtype=author&query=Prasojo%2C+R+E), [Alham Fikri Aji](https://arxiv.org/search/cs?searchtype=author&query=Aji%2C+A+F)

> We perform knowledge distillation (KD) benchmark from task-specific BERT-base teacher models to various student models: BiLSTM, CNN, BERT-Tiny, BERT-Mini, and BERT-Small. Our experiment involves 12 datasets grouped in two tasks: text classification and sequence labeling in the Indonesian language. We also compare various aspects of distillations including the usage of word embeddings and unlabeled data augmentation. Our experiments show that, despite the rising popularity of Transformer-based models, using BiLSTM and CNN student models provide the best trade-off between performance and computational resource (CPU, RAM, and storage) compared to pruned BERT models. We further propose some quick wins on performing KD to produce small NLP models via efficient KD training mechanisms involving simple choices of loss functions, word embeddings, and unlabeled data preparation.

| Comments:    | 14 pages, 3 figures, submitted to Elsevier                   |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| MSC classes: | 68T50                                                        |
| ACM classes: | I.2.7; I.2.6                                                 |
| Cite as:     | **[arXiv:2201.00558](https://arxiv.org/abs/2201.00558) [cs.CL]** |
|              | (or **[arXiv:2201.00558v1](https://arxiv.org/abs/2201.00558v1) [cs.CL]** for this version) |





<h2 id="2022-01-04-3">3. Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions
</h2>

Title: [Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions](https://arxiv.org/abs/2201.00768)

Authors:[Marwan Omar](https://arxiv.org/search/cs?searchtype=author&query=Omar%2C+M), [Soohyeon Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+S), [DaeHun Nyang](https://arxiv.org/search/cs?searchtype=author&query=Nyang%2C+D), [David Mohaisen](https://arxiv.org/search/cs?searchtype=author&query=Mohaisen%2C+D)

> Recent natural language processing (NLP) techniques have accomplished high performance on benchmark datasets, primarily due to the significant improvement in the performance of deep learning. The advances in the research community have led to great enhancements in state-of-the-art production systems for NLP tasks, such as virtual assistants, speech recognition, and sentiment analysis. However, such NLP systems still often fail when tested with adversarial attacks. The initial lack of robustness exposed troubling gaps in current models' language understanding capabilities, creating problems when NLP systems are deployed in real life. In this paper, we present a structured overview of NLP robustness research by summarizing the literature in a systemic way across various dimensions. We then take a deep-dive into the various dimensions of robustness, across techniques, metrics, embeddings, and benchmarks. Finally, we argue that robustness should be multi-dimensional, provide insights into current research, identify gaps in the literature to suggest directions worth pursuing to address these gaps.

| Comments: | Survey; 2 figures, 4 tables                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2201.00768](https://arxiv.org/abs/2201.00768) [cs.CL]** |
|           | (or **[arXiv:2201.00768v1](https://arxiv.org/abs/2201.00768v1) [cs.CL]** for this version) |



# 2022-01-03

[Return to Index](#Index)



<h2 id="2022-01-03-1">1. ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation
</h2>

Title: [ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation](https://arxiv.org/abs/2112.15283)

Authors: [Han Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Weichong Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+W), [Yewei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Lanxin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Boqiang Duan](https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+B), [Zhihua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Hao Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+H), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. Based on the image quantization models, we formulate both image generation and text generation as autoregressive generative tasks conditioned on the text/image input. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. For the text-to-image generation process, we further propose an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning.

| Comments: | 15 pages, 7 figures                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2112.15283](https://arxiv.org/abs/2112.15283) [cs.CV]** |
|           | (or **[arXiv:2112.15283v1](https://arxiv.org/abs/2112.15283v1) [cs.CV]** for this version) |





<h2 id="2022-01-03-2">2. Deconfounded Visual Grounding
</h2>

Title: [Deconfounded Visual Grounding](https://arxiv.org/abs/2112.15324)

Authors: [Jianqiang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Yu Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y), [Jiaxin Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+J), [Qianru Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Q), [Hanwang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H)

> We focus on the confounding bias between language and location in the visual grounding pipeline, where we find that the bias is the major visual reasoning bottleneck. For example, the grounding process is usually a trivial language-location association without visual reasoning, e.g., grounding any language query containing sheep to the nearly central regions, due to that most queries about sheep have ground-truth locations at the image center. First, we frame the visual grounding pipeline into a causal graph, which shows the causalities among image, query, target location and underlying confounder. Through the causal graph, we know how to break the grounding bottleneck: deconfounded visual grounding. Second, to tackle the challenge that the confounder is unobserved in general, we propose a confounder-agnostic approach called: Referring Expression Deconfounder (RED), to remove the confounding bias. Third, we implement RED as a simple language attention, which can be applied in any grounding method. On popular benchmarks, RED improves various state-of-the-art grounding methods by a significant margin. Code will soon be available at: [this https URL](https://github.com/JianqiangH/Deconfounded_VG).

| Comments: | AAAI 2022 Accepted                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2112.15324](https://arxiv.org/abs/2112.15324) [cs.CV]** |
|           | (or **[arXiv:2112.15324v1](https://arxiv.org/abs/2112.15324v1) [cs.CV]** for this version) |





<h2 id="2022-01-03-3">3. Materialized Knowledge Bases from Commonsense Transformers
</h2>

Title: [Materialized Knowledge Bases from Commonsense Transformers](https://arxiv.org/abs/2112.14815)

Authors: [Tuan-Phong Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T), [Simon Razniewski](https://arxiv.org/search/cs?searchtype=author&query=Razniewski%2C+S)

> Starting from the COMET methodology by Bosselut et al. (2019), generating commonsense knowledge directly from pre-trained language models has recently received significant attention. Surprisingly, up to now no materialized resource of commonsense knowledge generated this way is publicly available. This paper fills this gap, and uses the materialized resources to perform a detailed analysis of the potential of this approach in terms of precision and recall. Furthermore, we identify common problem cases, and outline use cases enabled by materialized resources. We posit that the availability of these resources is important for the advancement of the field, as it enables an off-the-shelf-use of the resulting knowledge, as well as further analyses on its strengths and weaknesses.

| Comments: | 7 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2112.14815](https://arxiv.org/abs/2112.14815) [cs.CL]** |
|           | (or **[arXiv:2112.14815v1](https://arxiv.org/abs/2112.14815v1) [cs.CL]** for this version) |





<h2 id="2022-01-03-4">4. ViNMT: Neural Machine Translation Tookit
</h2>

Title: [ViNMT: Neural Machine Translation Tookit](https://arxiv.org/abs/2112.15272)

Authors: [Nguyen Hoang Quan](https://arxiv.org/search/cs?searchtype=author&query=Quan%2C+N+H), [Nguyen Thanh Dat](https://arxiv.org/search/cs?searchtype=author&query=Dat%2C+N+T), [Nguyen Hoang Minh Cong](https://arxiv.org/search/cs?searchtype=author&query=Cong%2C+N+H+M), [Nguyen Van Vinh](https://arxiv.org/search/cs?searchtype=author&query=Van+Vinh%2C+N), [Ngo Thi Vinh](https://arxiv.org/search/cs?searchtype=author&query=Vinh%2C+N+T), [Nguyen Phuong Thai](https://arxiv.org/search/cs?searchtype=author&query=Thai%2C+N+P), [Tran Hong Viet](https://arxiv.org/search/cs?searchtype=author&query=Viet%2C+T+H)

> We present an open-source toolkit for neural machine translation (NMT). The new toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along with many other improvements detailed below, in order to create a self-contained, simple to use, consistent and comprehensive framework for Machine Translation tasks of various domains. It is tooled to support both bilingual and multilingual translation tasks, starting from building the model from respective corpora, to inferring new predictions or packaging the model to serving-capable JIT format.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.15272](https://arxiv.org/abs/2112.15272) [cs.CL]** |
|           | (or **[arXiv:2112.15272v1](https://arxiv.org/abs/2112.15272v1) [cs.CL]** for this version) |

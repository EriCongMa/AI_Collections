# MA C.'s Daily Paper Of Interest - December, 2021

# Index


- [2021-12-24](#2021-12-24)

  - [1. Are E2E ASR models ready for an industrial usage?](#2021-12-24-1)
  - [2. Do Multi-Lingual Pre-trained Language Models Reveal Consistent Token Attributions in Different Languages?](#2021-12-24-2)
  - [3. Distilling the Knowledge of Romanian BERTs Using Multiple Teachers](#2021-12-24-3)
  - [4. ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](#2021-12-24-4)
  
- [2021-12-23](#2021-12-23)

  - [1. On the Compression of Natural Language Models](#2021-12-23-1)
  - [2. English2Gbe: A multilingual machine translation model for {Fon/Ewe}Gbe](#2021-12-23-2)
  - [3. Diformer: Directional Transformer for Neural Machine Translation](#2021-12-23-3)
  - [4. Self-Distillation Mixup Training for Non-autoregressive Neural Machine Translation](#2021-12-23-4)
  - [5. Joint-training on Symbiosis Networks for Deep Nueral Machine Translation models](#2021-12-23-5)
  - [6. How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?](#2021-12-23-6)
  - [7. A Survey of Natural Language Generation](#2021-12-23-7)
  - [8. Towards Interactive Language Modeling](#2021-12-23-8)
  - [9. Text is no more Enough! A Benchmark for Profile-based Spoken Language Understanding](#2021-12-23-9)
  - [10. Toward Educator-focused Automated Scoring Systems for Reading and Writing](#2021-12-23-10)

- [2021-12-22](#2021-12-22)
  - [1. Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement](#2021-12-22-1)
  - [2. Scaling Language Models: Methods, Analysis & Insights from Training Gopher](#2021-12-22-2)
- [2021-12-21](#2021-12-21)

  - [1. English-to-Chinese Transliteration with Phonetic Back-transliteration](#2021-12-21-1)
  - [2. Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP](#2021-12-21-2)
  - [3. Few-shot Learning with Multilingual Language Models](#2021-12-21-3)
  - [4. Efficient Large Scale Language Modeling with Mixtures of Experts](#2021-12-21-4)
- [2021-12-20](#2021-12-20)
  - [1. An Empirical Investigation of the Role of Pre-training in Lifelong Learning](#2021-12-20-1)
  - [2. Continual Learning for Monolingual End-to-End Automatic Speech Recognition](#2021-12-20-2)
  - [3. Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations](#2021-12-20-3)
- [2021-12-17](#2021-12-17)
  - [1. Lacuna Reconstruction: Self-supervised Pre-training for Low-Resource Historical Document Transcription](#2021-12-17-1)
  - [2. Prosody-Aware Neural Machine Translation for Dubbing](#2021-12-17-2)
  - [3. Neural Content Extraction for Poster Generation of Scientific Papers](#2021-12-17-3)
  - [4. Can Multilinguality benefit Non-autoregressive Machine Translation?](#2021-12-17-4)
  - [5. KAT: A Knowledge Augmented Transformer for Vision-and-Language](#2021-12-17-5)
  - [6. Amortized Noisy Channel Neural Machine Translation](#2021-12-17-6)
  - [7. IsometricMT: Neural Machine Translation for Automatic Dubbing](#2021-12-17-7)
  - [8. DOCmT5: Document-Level Pretraining of Multilingual Language Models](#2021-12-17-8)
  - [9. Distilled Dual-Encoder Model for Vision-Language Understanding](#2021-12-17-9)
  - [10. NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics](#2021-12-17-10)
- [2021-12-16](#2021-12-16)

  - [1. Est-ce que vous compute? Code-switching, cultural identity, and AI](#2021-12-16-1)
  - [2. LongT5: Efficient Text-To-Text Transformer for Long Sequences](#2021-12-16-2)
  - [3. Faster Nearest Neighbor Machine Translation](#2021-12-16-3)
  - [4. Lesan -- Machine Translation for Low Resource Languages](#2021-12-16-4)
  - [5. Improving both domain robustness and domain adaptability in machine translation](#2021-12-16-5)
  - [6. Measure and Improve Robustness in NLP Models: A Survey](#2021-12-16-6)
  - [7. Textless Speech-to-Speech Translation on Real Data](#2021-12-16-7)
- [2021-12-15](#2021-12-15)

  - [1. Improving Hybrid CTC/Attention End-to-end Speech Recognition with Pretrained Acoustic and Language Model](#2021-12-15-1)
  - [2. CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising](#2021-12-15-2)
  - [3. Language Models are not Models of Language](#2021-12-15-3)
  - [4. From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression](#2021-12-15-4)
  - [5. Model Uncertainty-Aware Knowledge Amalgamation for Pre-Trained Language Models](#2021-12-15-5)
  - [6. VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena](#2021-12-15-6)
  - [7. Massive-scale Decoding for Text Generation using Lattices](#2021-12-15-7)
- [2021-12-14](#2021-12-14)

  - [1. VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks](#2021-12-14-1)
  - [2. Sequence-level self-learning with multiple hypotheses](#2021-12-14-2)
  - [3. Selecting Parallel In-domain Sentences for Neural Machine Translation Using Monolingual Texts](#2021-12-14-3)
  - [4. Communication-Efficient Federated Learning for Neural Machine Translation](#2021-12-14-4)
  - [5. Do Data-based Curricula Work?](#2021-12-14-5)
  - [6. WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models](#2021-12-14-6)
  - [7. GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](#2021-12-14-7)
- [2021-12-13](#2021-12-13)

  - [1. Injecting Semantic Concepts into End-to-End Image Captioning](#2021-12-13-1)
  - [2. MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning](#2021-12-13-2)
  - [3. Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation](#2021-12-13-3)
  - [4. Word Embeddings via Causal Inference: Gender Bias Reducing and Semantic Information Preserving](#2021-12-13-4)
  - [5. Shennong: a Python toolbox for audio speech features extraction](#2021-12-13-5)
  - [6. Analysis and Prediction of NLP Models Via Task Embeddings](#2021-12-13-6)
  - [7. Pruning Pretrained Encoders with a Multitask Objective](#2021-12-13-7)
- [2021-12-10](#2021-12-10)
  - [1. Self-Supervised Image-to-Text and Text-to-Image Synthesis](#2021-12-10-1)
- [2021-12-9](#2021-12-9)

  - [1. Transformer-Based Approach for Joint Handwriting and Named Entity Recognition in Historical documents](#2021-12-9-1)
  - [2. MLP Architectures for Vision-and-Language Modeling: An Empirical Study](#2021-12-9-2)
  - [3. Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand](#2021-12-9-3)
  - [4. Improving language models by retrieving from trillions of tokens](#2021-12-9-4)
- [2021-12-8](#2021-12-8)

  - [1. CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification](#2021-12-8-1)
  - [2. Grounded Language-Image Pre-training](#2021-12-8-2)
  - [3. Parsing with Pretrained Language Models, Multiple Datasets, and Dataset Embeddings](#2021-12-8-3)
  - [4. Natural Answer Generation: From Factoid Answer to Full-length Answer using Grammar Correction](#2021-12-8-4)
- [2021-12-7](#2021-12-7)

  - [1. Legal Document Retrieval using Document Vector Embeddings and Deep Learning](#2021-12-7-1)
  - [2. VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts](#2021-12-7-2)
  - [3. VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning](#2021-12-7-3)
  - [4. Embedding Arithmetic for Text-driven Image Transformation](#2021-12-7-4)
  - [5. Text2Mesh: Text-Driven Neural Stylization for Meshes](#2021-12-7-5)
  - [6. CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks](#2021-12-7-6)
  - [7. Towards More Robust Natural Language Understanding](#2021-12-7-7)
  - [8. Quantifying Adaptability in Pre-trained Language Models with 500 Tasks](#2021-12-7-8)
- [2021-12-6](#2021-12-6)

  - [1. Linear algebra with transformers](#2021-12-6-1)
  - [2. Multitask Finetuning for Improving Neural Machine Translation in Indian Languages](#2021-12-6-2)
  - [3. Translating Politeness Across Cultures: Case of Hindi and English](#2021-12-6-3)
  - [4. Semantic Segmentation of Legal Documents via Rhetorical Roles](#2021-12-6-4)
  - [5. A Proposal of Automatic Error Correction in Text](#2021-12-6-5)
- [2021-12-3](#2021-12-3)
  - [1. Consensus Graph Representation Learning for Better Grounded Image Captioning](#2021-12-3-1)
  - [2. A Mixture of Expert Based Deep Neural Network for Improved ASR](#2021-12-3-2)
  - [3. DenseCLIP: Extract Free Dense Labels from CLIP](#2021-12-3-3)
- [2021-12-2](#2021-12-2)
  - [1. CLIPstyler: Image Style Transfer with a Single Text Condition](#2021-12-2-1)
  - [2. Translation-equivariant Image Quantizer for Bi-directional Image-Text Generation](#2021-12-2-2)
  - [3. DPRK-BERT: The Supreme Language Model](#2021-12-2-3)
- [2021-12-1](#2021-12-1)

  - [1. Do We Still Need Automatic Speech Recognition for Spoken Language Understanding?](#2021-12-1-1)
  - [2. Improvement in Machine Translation with Generative Adversarial Networks](#2021-12-1-2)
  - [3. Pureformer: Do We Even Need Attention?](#2021-12-1-3)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MAC-Daily-POI-index.md)



# 2021-12-24

[Return to Index](#Index)



<h2 id="2021-12-24-1">1. Are E2E ASR models ready for an industrial usage?
</h2>

Title: [Are E2E ASR models ready for an industrial usage?](https://arxiv.org/abs/2112.12572)

Authors: [Valentin Vielzeuf](https://arxiv.org/search/eess?searchtype=author&query=Vielzeuf%2C+V), [Grigory Antipov](https://arxiv.org/search/eess?searchtype=author&query=Antipov%2C+G)

> The Automated Speech Recognition (ASR) community experiences a major turning point with the rise of the fully-neural (End-to-End, E2E) approaches. At the same time, the conventional hybrid model remains the standard choice for the practical usage of ASR. According to previous studies, the adoption of E2E ASR in real-world applications was hindered by two main limitations: their ability to generalize on unseen domains and their high operational cost. In this paper, we investigate both above-mentioned drawbacks by performing a comprehensive multi-domain benchmark of several contemporary E2E models and a hybrid baseline. Our experiments demonstrate that E2E models are viable alternatives for the hybrid approach, and even outperform the baseline both in accuracy and in operational efficiency. As a result, our study shows that the generalization and complexity issues are no longer the major obstacle for industrial integration, and draws the community's attention to other potential limitations of the E2E approaches in some specific use-cases.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.12572](https://arxiv.org/abs/2112.12572) [eess.AS]** |
|           | (or **[arXiv:2112.12572v1](https://arxiv.org/abs/2112.12572v1) [eess.AS]** for this version) |





<h2 id="2021-12-24-2">2. Do Multi-Lingual Pre-trained Language Models Reveal Consistent Token Attributions in Different Languages?
</h2>

Title: [Do Multi-Lingual Pre-trained Language Models Reveal Consistent Token Attributions in Different Languages?](https://arxiv.org/abs/2112.12356)

Authors: [Junxiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Xuchao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Bo Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+B), [Yanchi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Wei Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+W), [Jingchao Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+J), [Haifeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H), [Liang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+L)

> During the past several years, a surge of multi-lingual Pre-trained Language Models (PLMs) has been proposed to achieve state-of-the-art performance in many cross-lingual downstream tasks. However, the understanding of why multi-lingual PLMs perform well is still an open domain. For example, it is unclear whether multi-Lingual PLMs reveal consistent token attributions in different languages. To address this, in this paper, we propose a Cross-lingual Consistency of Token Attributions (CCTA) evaluation framework. Extensive experiments in three downstream tasks demonstrate that multi-lingual PLMs assign significantly different attributions to multi-lingual synonyms. Moreover, we have the following observations: 1) the Spanish achieves the most consistent token attributions in different languages when it is used for training PLMs; 2) the consistency of token attributions strongly correlates with performance in downstream tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.12356](https://arxiv.org/abs/2112.12356) [cs.CL]** |
|           | (or **[arXiv:2112.12356v1](https://arxiv.org/abs/2112.12356v1) [cs.CL]** for this version) |





<h2 id="2021-12-24-3">3. Distilling the Knowledge of Romanian BERTs Using Multiple Teachers
</h2>

Title: [Distilling the Knowledge of Romanian BERTs Using Multiple Teachers](https://arxiv.org/abs/2112.12650)

Authors: [Andrei-Marius Avram](https://arxiv.org/search/cs?searchtype=author&query=Avram%2C+A), [Darius Catrina](https://arxiv.org/search/cs?searchtype=author&query=Catrina%2C+D), [Dumitru-Clementin Cercel](https://arxiv.org/search/cs?searchtype=author&query=Cercel%2C+D), [Mihai Dascălu](https://arxiv.org/search/cs?searchtype=author&query=Dascălu%2C+M), [Traian Rebedea](https://arxiv.org/search/cs?searchtype=author&query=Rebedea%2C+T), [Vasile Păiş](https://arxiv.org/search/cs?searchtype=author&query=Păiş%2C+V), [Dan Tufiş](https://arxiv.org/search/cs?searchtype=author&query=Tufiş%2C+D)

> As transfer learning from large-scale pre-trained language models has become prevalent in Natural Language Processing, running these models in computationally constrained environments remains a challenging problem yet to address. Several solutions including knowledge distillation, network quantization or network pruning have been proposed; however, these approaches focus mostly on the English language, thus widening the gap when considering low-resource languages. In this work, we introduce three light and fast versions of distilled BERT models for the Romanian language: Distil-BERT-base-ro, Distil-RoBERT-base and DistilMulti-BERT-base-ro. The first two models resulted from individually distilling the knowledge of the two base versions of Romanian BERTs available in literature, while the last one was obtained by distilling their ensemble. To our knowledge, this is the first attempt to create publicly available Romanian distilled BERT models, which were thoroughly evaluated on five tasks: part-of-speech tagging, named entity recognition, sentiment analysis, semantic textual similarity and dialect identification. The experimental results on these benchmarks proved that our three distilled models maintain most performance in terms of accuracy with their teachers, while being twice as fast on a GPU and ~35\% smaller. In addition, we further test the similarity between our students and their teachers prediction by measuring their label and probability loyalty, together with regression loyalty - a new metric introduced in this work.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.12650](https://arxiv.org/abs/2112.12650) [cs.CL]** |
|           | (or **[arXiv:2112.12650v1](https://arxiv.org/abs/2112.12650v1) [cs.CL]** for this version) |





<h2 id="2021-12-24-4">4. ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
</h2>

Title: [ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2112.12731)

Authors: [Shuohuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Yang Xiang](https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+Y), [Zhihua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Siyu Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+S), [Weibao Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+W), [Shikun Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S), [Junyuan Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+J), [Yanbin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Chao Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+C), [Jiaxiang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J), [Xuyi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Yuxiang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Weixin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W), [Xi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Yangfan Bai](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+Y), [Qiuliang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Li Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+L), [Shiyong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S), [Peng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+P), [Dianhai Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D), [Yanjun Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Y), [Hao Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+H), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Tian Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+T), [Wei Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+W), [Ge Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G), [Wen Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+W), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.

| Comments: | arXiv admin note: text overlap with [arXiv:2107.02137](https://arxiv.org/abs/2107.02137) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2112.12731](https://arxiv.org/abs/2112.12731) [cs.CL]** |
|           | (or **[arXiv:2112.12731v1](https://arxiv.org/abs/2112.12731v1) [cs.CL]** for this version) |





# 2021-12-23

[Return to Index](#Index)



<h2 id="2021-12-23-1">1. On the Compression of Natural Language Models
</h2>

Title: [On the Compression of Natural Language Models](https://arxiv.org/abs/2112.11480)

Authors: [Saeed Damadi](https://arxiv.org/search/cs?searchtype=author&query=Damadi%2C+S)

> Deep neural networks are effective feature extractors but they are prohibitively large for deployment scenarios. Due to the huge number of parameters, interpretability of parameters in different layers is not straight-forward. This is why neural networks are sometimes considered black boxes. Although simpler models are easier to explain, finding them is not easy. If found, a sparse network that can fit to a data from scratch would help to interpret parameters of a neural network. To this end, lottery ticket hypothesis states that typical dense neural networks contain a small sparse sub-network that can be trained to a reach similar test accuracy in an equal number of steps. The goal of this work is to assess whether such a trainable subnetwork exists for natural language models (NLM)s. To achieve this goal we will review state-of-the-art compression techniques such as quantization, knowledge distillation, and pruning.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.11480](https://arxiv.org/abs/2112.11480) [cs.CL]** |
|           | (or **[arXiv:2112.11480v1](https://arxiv.org/abs/2112.11480v1) [cs.CL]** for this version) |





<h2 id="2021-12-23-2">2. English2Gbe: A multilingual machine translation model for {Fon/Ewe}Gbe
</h2>

Title: [English2Gbe: A multilingual machine translation model for {Fon/Ewe}Gbe](https://arxiv.org/abs/2112.11482)

Authors: [Gilles Hacheme](https://arxiv.org/search/cs?searchtype=author&query=Hacheme%2C+G)

> Language is an essential factor of emancipation. Unfortunately, most of the more than 2,000 African languages are low-resourced. The community has recently used machine translation to revive and strengthen several African languages. However, the trained models are often bilingual, resulting in a potentially exponential number of models to train and maintain to cover all possible translation directions. Additionally, bilingual models do not leverage the similarity between some of the languages. Consequently, multilingual neural machine translation (NMT) is gaining considerable interest, especially for low-resourced languages. Nevertheless, its adoption by the community is still limited. This paper introduces English2Gbe, a multilingual NMT model capable of translating from English to Ewe or Fon. Using the BLEU, CHRF, and TER scores computed with the Sacrebleu (Post, 2018) package for reproducibility, we show that English2Gbe outperforms bilingual models (English to Ewe and English to Fon) and gives state-of-the-art results on the JW300 benchmark for Fon established by Nekoto et al. (2020). We hope this work will contribute to the massive adoption of Multilingual models inside the community. Our code is made accessible from Github.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| ACM classes:       | I.2.7                                                        |
| Journal reference: | ML4D, 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia |
| Cite as:           | **[arXiv:2112.11482](https://arxiv.org/abs/2112.11482) [cs.CL]** |
|                    | (or **[arXiv:2112.11482v1](https://arxiv.org/abs/2112.11482v1) [cs.CL]** for this version) |





<h2 id="2021-12-23-3">3. Diformer: Directional Transformer for Neural Machine Translation
</h2>

Title: [Diformer: Directional Transformer for Neural Machine Translation](Diformer: Directional Transformer for Neural Machine Translation)

Authors: [Minghan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Jiaxin Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Yuxia Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Daimeng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+D), [Hengchao Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+H), [Chang Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+C), [Yimeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yinglu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Shimin Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+S), [Hao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H)

> Autoregressive (AR) and Non-autoregressive (NAR) models have their own superiority on the performance and latency, combining them into one model may take advantage of both. Current combination frameworks focus more on the integration of multiple decoding paradigms with a unified generative model, e.g. Masked Language Model. However, the generalization can be harmful to the performance due to the gap between training objective and inference. In this paper, we aim to close the gap by preserving the original objective of AR and NAR under a unified framework. Specifically, we propose the Directional Transformer (Diformer) by jointly modelling AR and NAR into three generation directions (left-to-right, right-to-left and straight) with a newly introduced direction variable, which works by controlling the prediction of each token to have specific dependencies under that direction. The unification achieved by direction successfully preserves the original dependency assumption used in AR and NAR, retaining both generalization and performance. Experiments on 4 WMT benchmarks demonstrate that Diformer outperforms current united-modelling works with more than 1.5 BLEU points for both AR and NAR decoding, and is also competitive to the state-of-the-art independent AR and NAR models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.11632](https://arxiv.org/abs/2112.11632) [cs.CL]** |
|           | (or **[arXiv:2112.11632v1](https://arxiv.org/abs/2112.11632v1) [cs.CL]** for this version) |





<h2 id="2021-12-23-4">4. Self-Distillation Mixup Training for Non-autoregressive Neural Machine Translation
</h2>

Title: [Self-Distillation Mixup Training for Non-autoregressive Neural Machine Translation](https://arxiv.org/abs/2112.11640)

Authors: [Jiaxin Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Minghan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Daimeng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+D), [Hengchao Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+H), [Yuxia Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Zongyao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Zhengzhe Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z), [Zhanglin Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Yimeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Chang Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+C), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Lizhi Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+L), [shimin tao](https://arxiv.org/search/cs?searchtype=author&query=tao%2C+s), [Hao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H)

> Recently, non-autoregressive (NAT) models predict outputs in parallel, achieving substantial improvements in generation speed compared to autoregressive (AT) models. While performing worse on raw data, most NAT models are trained as student models on distilled data generated by AT teacher models, which is known as sequence-level Knowledge Distillation. An effective training strategy to improve the performance of AT models is Self-Distillation Mixup (SDM) Training, which pre-trains a model on raw data, generates distilled data by the pre-trained model itself and finally re-trains a model on the combination of raw data and distilled data. In this work, we aim to view SDM for NAT models, but find directly adopting SDM to NAT models gains no improvements in terms of translation quality. Through careful analysis, we observe the invalidation is correlated to Modeling Diversity and Confirmation Bias between the AT teacher model and the NAT student models. Based on these findings, we propose an enhanced strategy named SDMRT by adding two stages to classic SDM: one is Pre-Rerank on self-distilled data, the other is Fine-Tune on Filtered teacher-distilled data. Our results outperform baselines by 0.6 to 1.2 BLEU on multiple NAT models. As another bonus, for Iterative Refinement NAT models, our methods can outperform baselines within half iteration number, which means 2X acceleration.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.11640](https://arxiv.org/abs/2112.11640) [cs.CL]** |
|           | (or **[arXiv:2112.11640v1](https://arxiv.org/abs/2112.11640v1) [cs.CL]** for this version) |







<h2 id="2021-12-23-5">5. Joint-training on Symbiosis Networks for Deep Nueral Machine Translation models
</h2>

Title: [Joint-training on Symbiosis Networks for Deep Nueral Machine Translation models](https://arxiv.org/abs/2112.11642)

Authors: [Zhengzhe Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z), [Jiaxin Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J), [Minghan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M), [Daimeng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+D), [Hengchao Shang](https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+H), [Zongyao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Zhanglin Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Yuxia Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Yimeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Chang Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+C), [Min Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Lizhi Lei](https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+L), [shimin tao](https://arxiv.org/search/cs?searchtype=author&query=tao%2C+s), [Hao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H)

> Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but it reaches the upper bound of translation quality when the number of encoder layers exceeds 18. Worse still, deeper networks consume a lot of memory, making it impossible to train efficiently. In this paper, we present Symbiosis Networks, which include a full network as the Symbiosis Main Network (M-Net) and another shared sub-network with the same structure but less layers as the Symbiotic Sub Network (S-Net). We adopt Symbiosis Networks on Transformer-deep (m-n) architecture and define a particular regularization loss τbetween the M-Net and S-Net in NMT. We apply joint-training on the Symbiosis Networks and aim to improve the M-Net performance. Our proposed training strategy improves Transformer-deep (12-6) by 0.61, 0.49 and 0.69 BLEU over the baselines under classic training on WMT'14 EN->DE, DE->EN and EN->FR tasks. Furthermore, our Transformer-deep (12-6) even outperforms classic Transformer-deep (18-6).

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.11642](https://arxiv.org/abs/2112.11642) [cs.CL]** |
|           | (or **[arXiv:2112.11642v1](https://arxiv.org/abs/2112.11642v1) [cs.CL]** for this version) |





<h2 id="2021-12-23-6">6. How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?
</h2>

Title: [How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?](https://arxiv.org/abs/2112.11668)

Authors: [Xinhsuai Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+X), [Luu Anh Tuan](https://arxiv.org/search/cs?searchtype=author&query=Tuan%2C+L+A), [Min Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+M), [Shuicheng Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+S), [Hanwang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H)

> The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.

| Comments: | Accepted by NeurIPS-2021                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2112.11668](https://arxiv.org/abs/2112.11668) [cs.CL]** |
|           | (or **[arXiv:2112.11668v1](https://arxiv.org/abs/2112.11668v1) [cs.CL]** for this version) |





<h2 id="2021-12-23-7">7. A Survey of Natural Language Generation
</h2>

Title: [A Survey of Natural Language Generation](https://arxiv.org/abs/2112.11739)

Authors: [Chenhe Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+C), [Yinghui Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Haifan Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H), [Miaoxin Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M), [Junxin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Ying Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Min Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+M)

> This paper offers a comprehensive review of the research on Natural Language Generation (NLG) over the past two decades, especially in relation to data-to-text generation and text-to-text generation deep learning methods, as well as new applications of NLG technology. This survey aims to (a) give the latest synthesis of deep learning research on the NLG core tasks, as well as the architectures adopted in the field; (b) detail meticulously and comprehensively various NLG tasks and datasets, and draw attention to the challenges in NLG evaluation, focusing on different evaluation methods and their relationships; (c) highlight some future emphasis and relatively recent research issues that arise due to the increasing synergy between NLG and other artificial intelligence areas, such as computer vision, text and computational creativity.

| Comments: | 36 pages, 4 tables; Under review                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2112.11739](https://arxiv.org/abs/2112.11739) [cs.CL]** |
|           | (or **[arXiv:2112.11739v1](https://arxiv.org/abs/2112.11739v1) [cs.CL]** for this version) |





<h2 id="2021-12-23-8">8. Towards Interactive Language Modeling
</h2>

Title: [Towards Interactive Language Modeling](https://arxiv.org/abs/2112.11911)

Authors: [Maartje ter Hoeve](https://arxiv.org/search/cs?searchtype=author&query=ter+Hoeve%2C+M), [Evgeny Kharitonov](https://arxiv.org/search/cs?searchtype=author&query=Kharitonov%2C+E), [Dieuwke Hupkes](https://arxiv.org/search/cs?searchtype=author&query=Hupkes%2C+D), [Emmanuel Dupoux](https://arxiv.org/search/cs?searchtype=author&query=Dupoux%2C+E)

> Interaction between caregivers and children plays a critical role in human language acquisition and development. Given this observation, it is remarkable that explicit interaction plays little to no role in artificial language modeling -- which also targets the acquisition of human language, yet by artificial models. Moreover, an interactive approach to language modeling has the potential to make language models substantially more versatile and to considerably impact downstream applications. Motivated by these considerations, we pioneer the space of interactive language modeling. As a first contribution we present a road map in which we detail the steps that need to be taken towards interactive language modeling. We then lead by example and take the first steps on this road map, showing the initial feasibility of our approach. As such, this work aims to be the start of a larger research agenda on interactive language modeling.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.11911](https://arxiv.org/abs/2112.11911) [cs.CL]** |
|           | (or **[arXiv:2112.11911v1](https://arxiv.org/abs/2112.11911v1) [cs.CL]** for this version) |





<h2 id="2021-12-23-9">9. Text is no more Enough! A Benchmark for Profile-based Spoken Language Understanding
</h2>

Title: [Text is no more Enough! A Benchmark for Profile-based Spoken Language Understanding](https://arxiv.org/abs/2112.11953)

Authors: [Xiao Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+X), [Libo Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+L), [Kaiji Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Guoxing Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+G), [Linlin Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Wanxiang Che](https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W)

> Current researches on spoken language understanding (SLU) heavily are limited to a simple setting: the plain text-based SLU that takes the user utterance as input and generates its corresponding semantic frames (e.g., intent and slots). Unfortunately, such a simple setting may fail to work in complex real-world scenarios when an utterance is semantically ambiguous, which cannot be achieved by the text-based SLU models. In this paper, we first introduce a new and important task, Profile-based Spoken Language Understanding (ProSLU), which requires the model that not only relies on the plain text but also the supporting profile information to predict the correct intents and slots. To this end, we further introduce a large-scale human-annotated Chinese dataset with over 5K utterances and their corresponding supporting profile information (Knowledge Graph (KG), User Profile (UP), Context Awareness (CA)). In addition, we evaluate several state-of-the-art baseline models and explore a multi-level knowledge adapter to effectively incorporate profile information. Experimental results reveal that all existing text-based SLU models fail to work when the utterances are semantically ambiguous and our proposed framework can effectively fuse the supporting information for sentence-level intent detection and token-level slot filling. Finally, we summarize key challenges and provide new points for future directions, which hopes to facilitate the research.

| Comments: | Accepted by AAAI 2022                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2112.11953](https://arxiv.org/abs/2112.11953) [cs.CL]** |
|           | (or **[arXiv:2112.11953v1](https://arxiv.org/abs/2112.11953v1) [cs.CL]** for this version) |



<h2 id="2021-12-23-10">10. Toward Educator-focused Automated Scoring Systems for Reading and Writing
</h2>

Title: [Toward Educator-focused Automated Scoring Systems for Reading and Writing](https://arxiv.org/abs/2112.11973)

Authors: [Mike Hardy](https://arxiv.org/search/cs?searchtype=author&query=Hardy%2C+M)

> This paper presents methods for improving automated essay scoring with techniques that address the computational trade-offs of self-attention and document length. To make Automated Essay Scoring (AES) more useful to practitioners, researchers must overcome the challenges of data and label availability, authentic and extended writing, domain scoring, prompt and source variety, and transfer learning. This paper addresses these challenges using neural network models by employing techniques that preserve essay length as an important feature without increasing model training costs. It introduces techniques for minimizing classification loss on ordinal labels using multi-objective learning, capturing semantic information across the entire essay using sentence embeddings to use transformer architecture across arbitrarily long documents, the use of such models for transfer learning, automated hyperparameter generation based on prompt-corpus metadata, and, most importantly, the use of semantic information to provide meaningful insights into student reading through analysis of passage-dependent writing resulting in state-of-the-art results for various essay tasks.

| Comments:    | 10 pages, 8 figures                                          |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7; K.3.1                                                 |
| Cite as:     | **[arXiv:2112.11973](https://arxiv.org/abs/2112.11973) [cs.CL]** |
|              | (or **[arXiv:2112.11973v1](https://arxiv.org/abs/2112.11973v1) [cs.CL]** for this version) |






# 2021-12-22

[Return to Index](#Index)



<h2 id="2021-12-22-1">1. Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement
</h2>

Title: [Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement](https://arxiv.org/abs/2112.10991)

Authors: [Yichao Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y), [Zhirui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Weizhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Jun Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+J), [Tong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+T)

> End-to-end speech-to-text translation~(E2E-ST) is becoming increasingly popular due to the potential of its less error propagation, lower latency, and fewer parameters. Given the triplet training corpus ⟨speech,transcription,translation⟩, the conventional high-quality E2E-ST system leverages the ⟨speech,transcription⟩ pair to pre-train the model and then utilizes the ⟨speech,translation⟩ pair to optimize it further. However, this process only involves two-tuple data at each stage, and this loose coupling fails to fully exploit the association between triplet data. In this paper, we attempt to model the joint probability of transcription and translation based on the speech input to directly leverage such triplet data. Based on that, we propose a novel regularization method for model training to improve the agreement of dual-path decomposition within triplet data, which should be equal in theory. To achieve this goal, we introduce two Kullback-Leibler divergence regularization terms into the model training objective to reduce the mismatch between output probabilities of dual-path. Then the well-trained model can be naturally transformed as the E2E-ST models by the pre-defined early stop tag. Experiments on the MuST-C benchmark demonstrate that our proposed approach significantly outperforms state-of-the-art E2E-ST baselines on all 8 language pairs, while achieving better performance in the automatic speech recognition task. Our code is open-sourced at [this https URL](https://github.com/duyichao/E2E-ST-TDA).

| Comments: | AAAI 2022                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2112.10991](https://arxiv.org/abs/2112.10991) [cs.CL]** |
|           | (or **[arXiv:2112.10991v1](https://arxiv.org/abs/2112.10991v1) [cs.CL]** for this version) |





<h2 id="2021-12-22-2">2. Scaling Language Models: Methods, Analysis & Insights from Training Gopher
</h2>

Title: [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)

Authors: [Jack W. Rae](https://arxiv.org/search/cs?searchtype=author&query=Rae%2C+J+W), [Sebastian Borgeaud](https://arxiv.org/search/cs?searchtype=author&query=Borgeaud%2C+S), [Trevor Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+T), [Katie Millican](https://arxiv.org/search/cs?searchtype=author&query=Millican%2C+K), [Jordan Hoffmann](https://arxiv.org/search/cs?searchtype=author&query=Hoffmann%2C+J), [Francis Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+F), [John Aslanides](https://arxiv.org/search/cs?searchtype=author&query=Aslanides%2C+J), [Sarah Henderson](https://arxiv.org/search/cs?searchtype=author&query=Henderson%2C+S), [Roman Ring](https://arxiv.org/search/cs?searchtype=author&query=Ring%2C+R), [Susannah Young](https://arxiv.org/search/cs?searchtype=author&query=Young%2C+S), [Eliza Rutherford](https://arxiv.org/search/cs?searchtype=author&query=Rutherford%2C+E), [Tom Hennigan](https://arxiv.org/search/cs?searchtype=author&query=Hennigan%2C+T), [Jacob Menick](https://arxiv.org/search/cs?searchtype=author&query=Menick%2C+J), [Albin Cassirer](https://arxiv.org/search/cs?searchtype=author&query=Cassirer%2C+A), [Richard Powell](https://arxiv.org/search/cs?searchtype=author&query=Powell%2C+R), [George van den Driessche](https://arxiv.org/search/cs?searchtype=author&query=van+den+Driessche%2C+G), [Lisa Anne Hendricks](https://arxiv.org/search/cs?searchtype=author&query=Hendricks%2C+L+A), [Maribeth Rauh](https://arxiv.org/search/cs?searchtype=author&query=Rauh%2C+M), [Po-Sen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+P), [Amelia Glaese](https://arxiv.org/search/cs?searchtype=author&query=Glaese%2C+A), [Johannes Welbl](https://arxiv.org/search/cs?searchtype=author&query=Welbl%2C+J), [Sumanth Dathathri](https://arxiv.org/search/cs?searchtype=author&query=Dathathri%2C+S), [Saffron Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Jonathan Uesato](https://arxiv.org/search/cs?searchtype=author&query=Uesato%2C+J), [John Mellor](https://arxiv.org/search/cs?searchtype=author&query=Mellor%2C+J), [Irina Higgins](https://arxiv.org/search/cs?searchtype=author&query=Higgins%2C+I), [Antonia Creswell](https://arxiv.org/search/cs?searchtype=author&query=Creswell%2C+A), [Nat McAleese](https://arxiv.org/search/cs?searchtype=author&query=McAleese%2C+N), [Amy Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+A), [Erich Elsen](https://arxiv.org/search/cs?searchtype=author&query=Elsen%2C+E), [Siddhant Jayakumar](https://arxiv.org/search/cs?searchtype=author&query=Jayakumar%2C+S), [Elena Buchatskaya](https://arxiv.org/search/cs?searchtype=author&query=Buchatskaya%2C+E), [David Budden](https://arxiv.org/search/cs?searchtype=author&query=Budden%2C+D), [Esme Sutherland](https://arxiv.org/search/cs?searchtype=author&query=Sutherland%2C+E), [Karen Simonyan](https://arxiv.org/search/cs?searchtype=author&query=Simonyan%2C+K), [Michela Paganini](https://arxiv.org/search/cs?searchtype=author&query=Paganini%2C+M), [Laurent Sifre](https://arxiv.org/search/cs?searchtype=author&query=Sifre%2C+L), [Lena Martens](https://arxiv.org/search/cs?searchtype=author&query=Martens%2C+L), [Xiang Lorraine Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X+L), [Adhiguna Kuncoro](https://arxiv.org/search/cs?searchtype=author&query=Kuncoro%2C+A), [Aida Nematzadeh](https://arxiv.org/search/cs?searchtype=author&query=Nematzadeh%2C+A), [Elena Gribovskaya](https://arxiv.org/search/cs?searchtype=author&query=Gribovskaya%2C+E), [Domenic Donato](https://arxiv.org/search/cs?searchtype=author&query=Donato%2C+D), [Angeliki Lazaridou](https://arxiv.org/search/cs?searchtype=author&query=Lazaridou%2C+A), [Arthur Mensch](https://arxiv.org/search/cs?searchtype=author&query=Mensch%2C+A), [Jean-Baptiste Lespiau](https://arxiv.org/search/cs?searchtype=author&query=Lespiau%2C+J), [Maria Tsimpoukelli](https://arxiv.org/search/cs?searchtype=author&query=Tsimpoukelli%2C+M), [Nikolai Grigorev](https://arxiv.org/search/cs?searchtype=author&query=Grigorev%2C+N), [Doug Fritz](https://arxiv.org/search/cs?searchtype=author&query=Fritz%2C+D), [Thibault Sottiaux](https://arxiv.org/search/cs?searchtype=author&query=Sottiaux%2C+T), [Mantas Pajarskas](https://arxiv.org/search/cs?searchtype=author&query=Pajarskas%2C+M), [Toby Pohlen](https://arxiv.org/search/cs?searchtype=author&query=Pohlen%2C+T), [Zhitao Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Z), [Daniel Toyama](https://arxiv.org/search/cs?searchtype=author&query=Toyama%2C+D), [Cyprien de Masson d'Autume](https://arxiv.org/search/cs?searchtype=author&query=de+Masson+d'Autume%2C+C), [Yujia Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Tayfun Terzi](https://arxiv.org/search/cs?searchtype=author&query=Terzi%2C+T), [Vladimir Mikulik](https://arxiv.org/search/cs?searchtype=author&query=Mikulik%2C+V), [Igor Babuschkin](https://arxiv.org/search/cs?searchtype=author&query=Babuschkin%2C+I), [Aidan Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+A), [Diego de Las Casas](https://arxiv.org/search/cs?searchtype=author&query=de+Las+Casas%2C+D), [Aurelia Guy](https://arxiv.org/search/cs?searchtype=author&query=Guy%2C+A), [Chris Jones](https://arxiv.org/search/cs?searchtype=author&query=Jones%2C+C), [James Bradbury](https://arxiv.org/search/cs?searchtype=author&query=Bradbury%2C+J), [Matthew Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M), [Blake Hechtman](https://arxiv.org/search/cs?searchtype=author&query=Hechtman%2C+B), [Laura Weidinger](https://arxiv.org/search/cs?searchtype=author&query=Weidinger%2C+L), [Iason Gabriel](https://arxiv.org/search/cs?searchtype=author&query=Gabriel%2C+I), [William Isaac](https://arxiv.org/search/cs?searchtype=author&query=Isaac%2C+W), [Ed Lockhart](https://arxiv.org/search/cs?searchtype=author&query=Lockhart%2C+E), [Simon Osindero](https://arxiv.org/search/cs?searchtype=author&query=Osindero%2C+S), [Laura Rimell](https://arxiv.org/search/cs?searchtype=author&query=Rimell%2C+L), [Chris Dyer](https://arxiv.org/search/cs?searchtype=author&query=Dyer%2C+C), [Oriol Vinyals](https://arxiv.org/search/cs?searchtype=author&query=Vinyals%2C+O), [Kareem Ayoub](https://arxiv.org/search/cs?searchtype=author&query=Ayoub%2C+K), [Jeff Stanway](https://arxiv.org/search/cs?searchtype=author&query=Stanway%2C+J), [Lorrayne Bennett](https://arxiv.org/search/cs?searchtype=author&query=Bennett%2C+L), [Demis Hassabis](https://arxiv.org/search/cs?searchtype=author&query=Hassabis%2C+D), [Koray Kavukcuoglu](https://arxiv.org/search/cs?searchtype=author&query=Kavukcuoglu%2C+K), [Geoffrey Irving](https://arxiv.org/search/cs?searchtype=author&query=Irving%2C+G)

> Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.

| Comments: | 118 pages                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2112.11446](https://arxiv.org/abs/2112.11446) [cs.CL]** |
|           | (or **[arXiv:2112.11446v1](https://arxiv.org/abs/2112.11446v1) [cs.CL]** for this version) |





# 2021-12-21

[Return to Index](#Index)



<h2 id="2021-12-21-1">1. English-to-Chinese Transliteration with Phonetic Back-transliteration
</h2>

Title: [English-to-Chinese Transliteration with Phonetic Back-transliteration](https://arxiv.org/abs/2112.10321)

Authors: [Shi Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+S), [Zhuofei Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Z), [Songpeng Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+S)

> Transliteration is a task of translating named entities from a language to another, based on phonetic similarity. The task has embraced deep learning approaches in recent years, yet, most ignore the phonetic features of the involved languages. In this work, we incorporate phonetic information into neural networks in two ways: we synthesize extra data using forward and back-translation but in a phonetic manner; and we pre-train models on a phonetic task before learning transliteration. Our experiments include three language pairs and six directions, namely English to and from Chinese, Hebrew and Thai. Results indicate that our proposed approach brings benefits to the model and achieves better or similar performance when compared to state of the art.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.10321](https://arxiv.org/abs/2112.10321) [cs.CL]** |
|           | (or **[arXiv:2112.10321v1](https://arxiv.org/abs/2112.10321v1) [cs.CL]** for this version) |





<h2 id="2021-12-21-2">2. Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP
</h2>

Title: [Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP](https://arxiv.org/abs/2112.10508)

Authors: [Sabrina J. Mielke](https://arxiv.org/search/cs?searchtype=author&query=Mielke%2C+S+J), [Zaid Alyafeai](https://arxiv.org/search/cs?searchtype=author&query=Alyafeai%2C+Z), [Elizabeth Salesky](https://arxiv.org/search/cs?searchtype=author&query=Salesky%2C+E), [Colin Raffel](https://arxiv.org/search/cs?searchtype=author&query=Raffel%2C+C), [Manan Dey](https://arxiv.org/search/cs?searchtype=author&query=Dey%2C+M), [Matthias Gallé](https://arxiv.org/search/cs?searchtype=author&query=Gallé%2C+M), [Arun Raja](https://arxiv.org/search/cs?searchtype=author&query=Raja%2C+A), [Chenglei Si](https://arxiv.org/search/cs?searchtype=author&query=Si%2C+C), [Wilson Y. Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+W+Y), [Benoît Sagot](https://arxiv.org/search/cs?searchtype=author&query=Sagot%2C+B), [Samson Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+S)

> What are the units of text that we want to model? From bytes to multi-word expressions, text can be analyzed and generated at many granularities. Until recently, most natural language processing (NLP) models operated over words, treating those as discrete and atomic tokens, but starting with byte-pair encoding (BPE), subword-based approaches have become dominant in many areas, enabling small vocabularies while still allowing for fast inference. Is the end of the road character-level model or byte-level processing? In this survey, we connect several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subword-based approaches based on learned segmentation have been proposed and evaluated. We conclude that there is and likely will never be a silver bullet singular solution for all applications and that thinking seriously about tokenization remains important for many applications.

| Comments: | 15 page preprint                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2112.10508](https://arxiv.org/abs/2112.10508) [cs.CL]** |
|           | (or **[arXiv:2112.10508v1](https://arxiv.org/abs/2112.10508v1) [cs.CL]** for this version) |





<h2 id="2021-12-21-3">3. Few-shot Learning with Multilingual Language Models
</h2>

Title: [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668)

Authors: [Xi Victoria Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+X+V), [Todor Mihaylov](https://arxiv.org/search/cs?searchtype=author&query=Mihaylov%2C+T), [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Tianlu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Shuohui Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Daniel Simig](https://arxiv.org/search/cs?searchtype=author&query=Simig%2C+D), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Shruti Bhosale](https://arxiv.org/search/cs?searchtype=author&query=Bhosale%2C+S), [Jingfei Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+J), [Ramakanth Pasunuru](https://arxiv.org/search/cs?searchtype=author&query=Pasunuru%2C+R), [Sam Shleifer](https://arxiv.org/search/cs?searchtype=author&query=Shleifer%2C+S), [Punit Singh Koura](https://arxiv.org/search/cs?searchtype=author&query=Koura%2C+P+S), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Brian O'Horo](https://arxiv.org/search/cs?searchtype=author&query=O'Horo%2C+B), [Jeff Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Zornitsa Kozareva](https://arxiv.org/search/cs?searchtype=author&query=Kozareva%2C+Z), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M), [Veselin Stoyanov](https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+V), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X)

> Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We present a detailed analysis of where the model succeeds and fails, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models.

| Comments: | 36 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2112.10668](https://arxiv.org/abs/2112.10668) [cs.CL]** |
|           | (or **[arXiv:2112.10668v1](https://arxiv.org/abs/2112.10668v1) [cs.CL]** for this version) |





<h2 id="2021-12-21-4">4. Efficient Large Scale Language Modeling with Mixtures of Experts
</h2>

Title: [Efficient Large Scale Language Modeling with Mixtures of Experts](https://arxiv.org/abs/2112.10684)

Authors: [Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M), [Shruti Bhosale](https://arxiv.org/search/cs?searchtype=author&query=Bhosale%2C+S), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+N), [Todor Mihaylov](https://arxiv.org/search/cs?searchtype=author&query=Mihaylov%2C+T), [Myle Ott](https://arxiv.org/search/cs?searchtype=author&query=Ott%2C+M), [Sam Shleifer](https://arxiv.org/search/cs?searchtype=author&query=Shleifer%2C+S), [Xi Victoria Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+X+V), [Jingfei Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+J), [Srinivasan Iyer](https://arxiv.org/search/cs?searchtype=author&query=Iyer%2C+S), [Ramakanth Pasunuru](https://arxiv.org/search/cs?searchtype=author&query=Pasunuru%2C+R), [Giri Anantharaman](https://arxiv.org/search/cs?searchtype=author&query=Anantharaman%2C+G), [Xian Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Shuohui Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Halil Akin](https://arxiv.org/search/cs?searchtype=author&query=Akin%2C+H), [Mandeep Baines](https://arxiv.org/search/cs?searchtype=author&query=Baines%2C+M), [Louis Martin](https://arxiv.org/search/cs?searchtype=author&query=Martin%2C+L), [Xing Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X), [Punit Singh Koura](https://arxiv.org/search/cs?searchtype=author&query=Koura%2C+P+S), [Brian O'Horo](https://arxiv.org/search/cs?searchtype=author&query=O'Horo%2C+B), [Jeff Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M), [Zornitsa Kozareva](https://arxiv.org/search/cs?searchtype=author&query=Kozareva%2C+Z), [Ves Stoyanov](https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+V)

> Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using ∼4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.10684](https://arxiv.org/abs/2112.10684) [cs.CL]** |
|           | (or **[arXiv:2112.10684v1](https://arxiv.org/abs/2112.10684v1) [cs.CL]** for this version) |






# 2021-12-20

[Return to Index](#Index)



<h2 id="2021-12-20-1">1. An Empirical Investigation of the Role of Pre-training in Lifelong Learning
</h2>

Title: [An Empirical Investigation of the Role of Pre-training in Lifelong Learning](https://arxiv.org/abs/2112.09153)

Authors: [Sanket Vaibhav Mehta](https://arxiv.org/search/cs?searchtype=author&query=Mehta%2C+S+V), [Darshan Patil](https://arxiv.org/search/cs?searchtype=author&query=Patil%2C+D), [Sarath Chandar](https://arxiv.org/search/cs?searchtype=author&query=Chandar%2C+S), [Emma Strubell](https://arxiv.org/search/cs?searchtype=author&query=Strubell%2C+E)

> The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning, but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel dataset of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized models. We then further investigate why pre-training alleviates forgetting in this setting. We study this phenomenon by analyzing the loss landscape, finding that pre-trained weights appear to ease forgetting by leading to wider minima. Based on this insight, we propose jointly optimizing for current task loss and loss basin sharpness in order to explicitly encourage wider basins during sequential fine-tuning. We show that this optimization approach leads to performance comparable to the state-of-the-art in task-sequential continual learning across multiple settings, without retaining a memory that scales in size with the number of tasks.

| Comments: | 30 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2112.09153](https://arxiv.org/abs/2112.09153) [cs.LG]** |
|           | (or **[arXiv:2112.09153v1](https://arxiv.org/abs/2112.09153v1) [cs.LG]** for this version) |





<h2 id="2021-12-20-2">2. Continual Learning for Monolingual End-to-End Automatic Speech Recognition
</h2>

Title: [Continual Learning for Monolingual End-to-End Automatic Speech Recognition](https://arxiv.org/abs/2112.09427)

Authors: [Steven Vander Eeckt](https://arxiv.org/search/eess?searchtype=author&query=Eeckt%2C+S+V), [Hugo Van hamme](https://arxiv.org/search/eess?searchtype=author&query=Van+hamme%2C+H)

> Adapting Automatic Speech Recognition (ASR) models to new domains leads to a deterioration of performance on the original domain(s), a phenomenon called Catastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to new accents, dialects, topics, etc. without suffering from CF, making them unable to be continually enhanced without storing all past data. Fortunately, Continual Learning (CL) methods, which aim to enable continual adaptation while overcoming CF, can be used. In this paper, we implement an extensive number of CL methods for End-to-End ASR and test and compare their ability to extend a monolingual Hybrid CTC-Transformer model across four new tasks. We find that the best performing CL method closes the gap between the fine-tuned model (lower bound) and the model trained jointly on all tasks (upper bound) by more than 40%, while requiring access to only 0.6% of the original data.

| Comments: | Submitted to ICASSP 2021. 5 pages, 1 figure                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML) |
| Cite as:  | **[arXiv:2112.09427](https://arxiv.org/abs/2112.09427) [eess.AS]** |
|           | (or **[arXiv:2112.09427v1](https://arxiv.org/abs/2112.09427v1) [eess.AS]** for this version) |





<h2 id="2021-12-20-3">3. Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations
</h2>

Title: [Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations](https://arxiv.org/abs/2112.09669)

Authors: [Siddhant Arora](https://arxiv.org/search/cs?searchtype=author&query=Arora%2C+S), [Danish Pruthi](https://arxiv.org/search/cs?searchtype=author&query=Pruthi%2C+D), [Norman Sadeh](https://arxiv.org/search/cs?searchtype=author&query=Sadeh%2C+N), [William W. Cohen](https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+W+W), [Zachary C. Lipton](https://arxiv.org/search/cs?searchtype=author&query=Lipton%2C+Z+C), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> In attempts to "explain" predictions of machine learning models, researchers have proposed hundreds of techniques for attributing predictions to features that are deemed important. While these attributions are often claimed to hold the potential to improve human "understanding" of the models, surprisingly little work explicitly evaluates progress towards this aspiration. In this paper, we conduct a crowdsourcing study, where participants interact with deception detection models that have been trained to distinguish between genuine and fake hotel reviews. They are challenged both to simulate the model on fresh reviews, and to edit reviews with the goal of lowering the probability of the originally predicted class. Successful manipulations would lead to an adversarial example. During the training (but not the test) phase, input spans are highlighted to communicate salience. Through our evaluation, we observe that for a linear bag-of-words model, participants with access to the feature coefficients during training are able to cause a larger reduction in model confidence in the testing phase when compared to the no-explanation control. For the BERT-based classifier, popular local explanations do not improve their ability to reduce the model confidence over the no-explanation case. Remarkably, when the explanation for the BERT model is given by the (global) attributions of a linear model trained to imitate the BERT model, people can effectively manipulate the model.

| Comments: | AAAI 2022                                                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2112.09669](https://arxiv.org/abs/2112.09669) [cs.CL]** |
|           | (or **[arXiv:2112.09669v1](https://arxiv.org/abs/2112.09669v1) [cs.CL]** for this version) |








# 2021-12-17

[Return to Index](#Index)



<h2 id="2021-12-17-1">1. Lacuna Reconstruction: Self-supervised Pre-training for Low-Resource Historical Document Transcription
</h2>

Title: [Lacuna Reconstruction: Self-supervised Pre-training for Low-Resource Historical Document Transcription](https://arxiv.org/abs/2112.08692)

Authors: [Nikolai Vogler](https://arxiv.org/search/cs?searchtype=author&query=Vogler%2C+N), [Jonathan Parkes Allen](https://arxiv.org/search/cs?searchtype=author&query=Allen%2C+J+P), [Matthew Thomas Miller](https://arxiv.org/search/cs?searchtype=author&query=Miller%2C+M+T), [Taylor Berg-Kirkpatrick](https://arxiv.org/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T)

> We present a self-supervised pre-training approach for learning rich visual language representations for both handwritten and printed historical document transcription. After supervised fine-tuning of our pre-trained encoder representations for low-resource document transcription on two languages, (1) a heterogeneous set of handwritten Islamicate manuscript images and (2) early modern English printed documents, we show a meaningful improvement in recognition accuracy over the same supervised model trained from scratch with as few as 30 line image transcriptions for training. Our masked language model-style pre-training strategy, where the model is trained to be able to identify the true masked visual representation from distractors sampled from within the same line, encourages learning robust contextualized language representations invariant to scribal writing style and printing noise present across documents.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.08692](https://arxiv.org/abs/2112.08692) [cs.CV]** |
|           | (or **[arXiv:2112.08692v1](https://arxiv.org/abs/2112.08692v1) [cs.CV]** for this version) |





<h2 id="2021-12-17-2">2. Prosody-Aware Neural Machine Translation for Dubbing
</h2>

Title: [Prosody-Aware Neural Machine Translation for Dubbing](https://arxiv.org/abs/2112.08548)

Authors: [Derek Tam](https://arxiv.org/search/cs?searchtype=author&query=Tam%2C+D), [Surafel M. Lakew](https://arxiv.org/search/cs?searchtype=author&query=Lakew%2C+S+M), [Yogesh Virkar](https://arxiv.org/search/cs?searchtype=author&query=Virkar%2C+Y), [Prashant Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+P), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M)

> We introduce the task of prosody-aware machine translation which aims at generating translations suitable for dubbing. Dubbing of a spoken sentence requires transferring the content as well as the prosodic structure of the source into the target language to preserve timing information. Practically, this implies correctly projecting pauses from the source to the target and ensuring that target speech segments have roughly the same duration of the corresponding source segments. In this work, we propose an implicit and explicit modeling approaches to integrate prosody information into neural machine translation. Experiments on English-German/French with automatic metrics show that the simplest of the considered approaches works best. Results are confirmed by human evaluations of translations and dubbed videos.

| Comments: | Submitted to IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2022 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2112.08548](https://arxiv.org/abs/2112.08548) [cs.CL]** |
|           | (or **[arXiv:2112.08548v1](https://arxiv.org/abs/2112.08548v1) [cs.CL]** for this version) |





<h2 id="2021-12-17-3">3. Neural Content Extraction for Poster Generation of Scientific Papers
</h2>

Title: [Neural Content Extraction for Poster Generation of Scientific Papers](https://arxiv.org/abs/2112.08550)

Authors: [Sheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S), [Xiaojun Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+X)

> The problem of poster generation for scientific papers is under-investigated. Posters often present the most important information of papers, and the task can be considered as a special form of document summarization. Previous studies focus mainly on poster layout and panel composition, while neglecting the importance of content extraction. Besides, their datasets are not publicly available, which hinders further research. In this paper, we construct a benchmark dataset from scratch for this task. Then we propose a three-step framework to tackle this task and focus on the content extraction step in this study. To get both textual and visual elements of a poster panel, a neural extractive model is proposed to extract text, figures and tables of a paper section simultaneously. We conduct experiments on the dataset and also perform ablation study. Results demonstrate the efficacy of our proposed model. The dataset and code will be released.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.08550](https://arxiv.org/abs/2112.08550) [cs.CL]** |
|           | (or **[arXiv:2112.08550v1](https://arxiv.org/abs/2112.08550v1) [cs.CL]** for this version) |





<h2 id="2021-12-17-4">4. Can Multilinguality benefit Non-autoregressive Machine Translation?
</h2>

Title: [Can Multilinguality benefit Non-autoregressive Machine Translation?](https://arxiv.org/abs/2112.08570)

Authors: [Sweta Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal%2C+S), [Julia Kreutzer](https://arxiv.org/search/cs?searchtype=author&query=Kreutzer%2C+J), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C)

> Non-autoregressive (NAR) machine translation has recently achieved significant improvements, and now outperforms autoregressive (AR) models on some benchmarks, providing an efficient alternative to AR inference. However, while AR translation is often implemented using multilingual models that benefit from transfer between languages and from improved serving efficiency, multilingual NAR models remain relatively unexplored. Taking Connectionist Temporal Classification (CTC) as an example NAR model and Imputer as a semi-NAR model, we present a comprehensive empirical study of multilingual NAR. We test its capabilities with respect to positive transfer between related languages and negative transfer under capacity constraints. As NAR models require distilled training sets, we carefully study the impact of bilingual versus multilingual teachers. Finally, we fit a scaling law for multilingual NAR, which quantifies its performance relative to the AR model as model scale increases.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.08570](https://arxiv.org/abs/2112.08570) [cs.CL]** |
|           | (or **[arXiv:2112.08570v1](https://arxiv.org/abs/2112.08570v1) [cs.CL]** for this version) |





<h2 id="2021-12-17-5">5. KAT: A Knowledge Augmented Transformer for Vision-and-Language
</h2>

Title: [KAT: A Knowledge Augmented Transformer for Vision-and-Language](https://arxiv.org/abs/2112.08614)

Authors: [Liangke Gui](https://arxiv.org/search/cs?searchtype=author&query=Gui%2C+L), [Borui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Qiuyuan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Q), [Alex Hauptmann](https://arxiv.org/search/cs?searchtype=author&query=Hauptmann%2C+A), [Yonatan Bisk](https://arxiv.org/search/cs?searchtype=author&query=Bisk%2C+Y), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J)

> The primary focus of recent work with largescale transformers has been on optimizing the amount of information packed into the model's parameters. In this work, we ask a different question: Can multimodal transformers leverage explicit knowledge in their reasoning? Existing, primarily unimodal, methods have explored approaches under the paradigm of knowledge retrieval followed by answer prediction, but leave open questions about the quality and relevance of the retrieved knowledge used, and how the reasoning processes over implicit and explicit knowledge should be integrated. To address these challenges, we propose a novel model - Knowledge Augmented Transformer (KAT) - which achieves a strong state-of-the-art result (+6 points absolute) on the open-domain multimodal task of OK-VQA. Our approach integrates implicit and explicit knowledge in an end to end encoder-decoder architecture, while still jointly reasoning over both knowledge sources during answer generation. An additional benefit of explicit knowledge integration is seen in improved interpretability of model predictions in our analysis.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.08614](https://arxiv.org/abs/2112.08614) [cs.CL]** |
|           | (or **[arXiv:2112.08614v1](https://arxiv.org/abs/2112.08614v1) [cs.CL]** for this version) |





<h2 id="2021-12-17-6">6. Amortized Noisy Channel Neural Machine Translation
</h2>

Title: [Amortized Noisy Channel Neural Machine Translation](https://arxiv.org/abs/2112.08670)

Authors: [Richard Yuanzhe Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+R+Y), [He He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+H), [Kyunghyun Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K)

> Noisy channel models have been especially effective in neural machine translation (NMT). However, recent approaches like "beam search and rerank" (BSR) incur significant computation overhead during inference, making real-world application infeasible. We aim to build an amortized noisy channel NMT model such that greedily decoding from it would generate translations that maximize the same reward as translations generated using BSR. We attempt three approaches: knowledge distillation, 1-step-deviation imitation learning, and Q learning. The first approach obtains the noisy channel signal from a pseudo-corpus, and the latter two approaches aim to optimize toward a noisy-channel MT reward directly. All three approaches speed up inference by 1-2 orders of magnitude. For all three approaches, the generated translations fail to achieve rewards comparable to BSR, but the translation quality approximated by BLEU is similar to the quality of BSR-produced translations.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.08670](https://arxiv.org/abs/2112.08670) [cs.CL]** |
|           | (or **[arXiv:2112.08670v1](https://arxiv.org/abs/2112.08670v1) [cs.CL]** for this version) |





<h2 id="2021-12-17-7">7. IsometricMT: Neural Machine Translation for Automatic Dubbing
</h2>

Title: [IsometricMT: Neural Machine Translation for Automatic Dubbing](https://arxiv.org/abs/2112.08682)

Authors: [Surafel M. Lakew](https://arxiv.org/search/cs?searchtype=author&query=Lakew%2C+S+M), [Yogesh Virkar](https://arxiv.org/search/cs?searchtype=author&query=Virkar%2C+Y), [Prashant Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+P), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M)

> Automatic dubbing (AD) is among the use cases where translations should fit a given length template in order to achieve synchronicity between source and target speech. For neural machine translation (MT), generating translations of length close to the source length (e.g. within +-10% in character count), while preserving quality is a challenging task. Controlling NMT output length comes at a cost to translation quality which is usually mitigated with a two step approach of generation of n-best hypotheses and then re-ranking them based on length and quality. This work, introduces a self-learning approach that allows a transformer model to directly learn to generate outputs that closely match the source length, in short isometric MT. In particular, our approach for isometric MT does not require to generate multiple hypotheses nor any auxiliary scoring function. We report results on four language pairs (English - French, Italian, German, Spanish) with a publicly available benchmark based on TED Talk data. Both automatic and manual evaluations show that our self-learning approach to performs on par with more complex isometric MT approaches.

| Comments: | Submitted to IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2022 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2112.08682](https://arxiv.org/abs/2112.08682) [cs.CL]** |
|           | (or **[arXiv:2112.08682v1](https://arxiv.org/abs/2112.08682v1) [cs.CL]** for this version) |





<h2 id="2021-12-17-8">8. DOCmT5: Document-Level Pretraining of Multilingual Language Models
</h2>

Title: [DOCmT5: Document-Level Pretraining of Multilingual Language Models](https://arxiv.org/abs/2112.08709)

Authors: [Chia-Hsuan Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+C), [Aditya Siddhant](https://arxiv.org/search/cs?searchtype=author&query=Siddhant%2C+A), [Viresh Ratnakar](https://arxiv.org/search/cs?searchtype=author&query=Ratnakar%2C+V), [Melvin Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+M)

> In this paper, we introduce DOCmT5, a multilingual sequence-to-sequence language model pre-trained with large scale parallel documents. While previous approaches have focused on leveraging sentence-level parallel data, we try to build a general-purpose pre-trained model that can understand and generate long documents. We propose a simple and effective pre-training objective - Document Reordering Machine Translation (DrMT), in which the input documents that are shuffled and masked need to be translated. DrMT brings consistent improvements over strong baselines on a variety of document-level generation tasks, including over 12 BLEU points for seen-language-pair document-level MT, over 7 BLEU points for unseen-language-pair document-level MT and over 3 ROUGE-1 points for seen-language-pair cross-lingual summarization. We achieve state-of-the-art (SOTA) on WMT20 De-En and IWSLT15 Zh-En document translation tasks. We also conduct extensive analysis on various factors for document pre-training, including (1) the effects of pre-training data quality and (2) The effects of combining mono-lingual and cross-lingual pre-training. We plan to make our model checkpoints publicly available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.08709](https://arxiv.org/abs/2112.08709) [cs.CL]** |
|           | (or **[arXiv:2112.08709v1](https://arxiv.org/abs/2112.08709v1) [cs.CL]** for this version) |





<h2 id="2021-12-17-9">9. Distilled Dual-Encoder Model for Vision-Language Understanding
</h2>

Title: [Distilled Dual-Encoder Model for Vision-Language Understanding](https://arxiv.org/abs/2112.08723)

Authors: [Zekun Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Wenhui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Haichao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+H), [Ming Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+M), [Bing Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+B), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> We propose a cross-modal attention distillation framework to train a dual-encoder model for vision-language understanding tasks, such as visual reasoning and visual question answering. Dual-encoder models have a faster inference speed than fusion-encoder models and enable the pre-computation of images and text during inference. However, the shallow interaction module used in dual-encoder models is insufficient to handle complex vision-language understanding tasks. In order to learn deep interactions of images and text, we introduce cross-modal attention distillation, which uses the image-to-text and text-to-image attention distributions of a fusion-encoder model to guide the training of our dual-encoder model. In addition, we show that applying the cross-modal attention distillation for both pre-training and fine-tuning stages achieves further improvements. Experimental results demonstrate that the distilled dual-encoder model achieves competitive performance for visual reasoning, visual entailment and visual question answering tasks while enjoying a much faster inference speed than fusion-encoder models. Our code and models will be publicly available at [this https URL](https://github.com/kugwzk/Distilled-DualEncoder).

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2112.08723](https://arxiv.org/abs/2112.08723) [cs.CL]** |
|           | (or **[arXiv:2112.08723v1](https://arxiv.org/abs/2112.08723v1) [cs.CL]** for this version) |





<h2 id="2021-12-17-10">10. NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics
</h2>

Title: [NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics](https://arxiv.org/abs/2112.08726)

Authors: [Ximing Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+X), [Sean Welleck](https://arxiv.org/search/cs?searchtype=author&query=Welleck%2C+S), [Peter West](https://arxiv.org/search/cs?searchtype=author&query=West%2C+P), [Liwei Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+L), [Jungo Kasai](https://arxiv.org/search/cs?searchtype=author&query=Kasai%2C+J), [Daniel Khashabi](https://arxiv.org/search/cs?searchtype=author&query=Khashabi%2C+D), [Ronan Le Bras](https://arxiv.org/search/cs?searchtype=author&query=Bras%2C+R+L), [Lianhui Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+L), [Youngjae Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y), [Rowan Zellers](https://arxiv.org/search/cs?searchtype=author&query=Zellers%2C+R), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y)

> The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. 
> Drawing inspiration from the A* search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop efficient lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. 
> Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.08726](https://arxiv.org/abs/2112.08726) [cs.CL]** |
|           | (or **[arXiv:2112.08726v1](https://arxiv.org/abs/2112.08726v1) [cs.CL]** for this version) |







# 2021-12-16

[Return to Index](#Index)



<h2 id="2021-12-16-1">1. Est-ce que vous compute? Code-switching, cultural identity, and AI
</h2>

Title: [Est-ce que vous compute? Code-switching, cultural identity, and AI](https://arxiv.org/abs/2112.08256)

Authors: [Arianna Falbo](https://arxiv.org/search/cs?searchtype=author&query=Falbo%2C+A), [Travis LaCroix](https://arxiv.org/search/cs?searchtype=author&query=LaCroix%2C+T)

> Cultural code-switching concerns how we adjust our overall behaviours, manners of speaking, and appearance in response to a perceived change in our social environment. We defend the need to investigate cultural code-switching capacities in artificial intelligence systems. We explore a series of ethical and epistemic issues that arise when bringing cultural code-switching to bear on artificial intelligence. Building upon Dotson's (2014) analysis of testimonial smothering, we discuss how emerging technologies in AI can give rise to epistemic oppression, and specifically, a form of self-silencing that we call 'cultural smothering'. By leaving the socio-dynamic features of cultural code-switching unaddressed, AI systems risk negatively impacting already-marginalised social groups by widening opportunity gaps and further entrenching social inequalities.

| Comments: | 19 pages. Under Review. Please cite published version, if available |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computers and Society (cs.CY)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2112.08256](https://arxiv.org/abs/2112.08256) [cs.CY]** |
|           | (or **[arXiv:2112.08256v1](https://arxiv.org/abs/2112.08256v1) [cs.CY]** for this version) |





<h2 id="2021-12-16-2">2. LongT5: Efficient Text-To-Text Transformer for Long Sequences
</h2>

Title: [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)

Authors: [Mandy Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+M), [Joshua Ainslie](https://arxiv.org/search/cs?searchtype=author&query=Ainslie%2C+J), [David Uthus](https://arxiv.org/search/cs?searchtype=author&query=Uthus%2C+D), [Santiago Ontanon](https://arxiv.org/search/cs?searchtype=author&query=Ontanon%2C+S), [Jianmo Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+J), [Yun-Hsuan Sung](https://arxiv.org/search/cs?searchtype=author&query=Sung%2C+Y), [Yinfei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y)

> Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.

| Comments: | preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2112.07916](https://arxiv.org/abs/2112.07916) [cs.CL]** |
|           | (or **[arXiv:2112.07916v1](https://arxiv.org/abs/2112.07916v1) [cs.CL]** for this version) |





<h2 id="2021-12-16-3">3. Faster Nearest Neighbor Machine Translation
</h2>

Title: [Faster Nearest Neighbor Machine Translation](https://arxiv.org/abs/2112.08152)

Authors: [Shuhe Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Jiwei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Yuxian Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Rongbin Ouyang](https://arxiv.org/search/cs?searchtype=author&query=Ouyang%2C+R), [Guoyin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G), [Xiaoya Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X), [Tianwei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T), [Shi Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+S)

> kNN based neural machine translation (kNN-MT) has achieved state-of-the-art results in a variety of MT tasks. One significant shortcoming of kNN-MT lies in its inefficiency in identifying the k nearest neighbors of the query representation from the entire datastore, which is prohibitively time-intensive when the datastore size is large. In this work, we propose \textbf{Faster kNN-MT} to address this issue. The core idea of Faster kNN-MT is to use a hierarchical clustering strategy to approximate the distance between the query and a data point in the datastore, which is decomposed into two parts: the distance between the query and the center of the cluster that the data point belongs to, and the distance between the data point and the cluster center. We propose practical ways to compute these two parts in a significantly faster manner. Through extensive experiments on different MT benchmarks, we show that \textbf{Faster kNN-MT} is faster than Fast kNN-MT \citep{meng2021fast} and only slightly (1.2 times) slower than its vanilla counterpart while preserving model performance as kNN-MT. Faster kNN-MT enables the deployment of kNN-MT models on real-world MT services.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.08152](https://arxiv.org/abs/2112.08152) [cs.CL]** |
|           | (or **[arXiv:2112.08152v1](https://arxiv.org/abs/2112.08152v1) [cs.CL]** for this version) |





<h2 id="2021-12-16-4">4. Lesan -- Machine Translation for Low Resource Languages
</h2>

Title: [Lesan -- Machine Translation for Low Resource Languages](https://arxiv.org/abs/2112.08191)

Authors: [Asmelash Teka Hadgu](https://arxiv.org/search/cs?searchtype=author&query=Hadgu%2C+A+T), [Abel Aregawi](https://arxiv.org/search/cs?searchtype=author&query=Aregawi%2C+A), [Adam Beaudoin](https://arxiv.org/search/cs?searchtype=author&query=Beaudoin%2C+A)

> Millions of people around the world can not access content on the Web because most of the content is not readily available in their language. Machine translation (MT) systems have the potential to change this for many languages. Current MT systems provide very accurate results for high resource language pairs, e.g., German and English. However, for many low resource languages, MT is still under active research. The key challenge is lack of datasets to build these systems. We present Lesan, an MT system for low resource languages. Our pipeline solves the key bottleneck to low resource MT by leveraging online and offline sources, a custom OCR system for Ethiopic and an automatic alignment module. The final step in the pipeline is a sequence to sequence model that takes parallel corpus as input and gives us a translation model. Lesan's translation model is based on the Transformer architecture. After constructing a base model, back translation, is used to leverage monolingual corpora. Currently Lesan supports translation to and from Tigrinya, Amharic and English. We perform extensive human evaluation and show that Lesan outperforms state-of-the-art systems such as Google Translate and Microsoft Translator across all six pairs. Lesan is freely available and has served more than 10 million translations so far. At the moment, there are only 217 Tigrinya and 15,009 Amharic Wikipedia articles. We believe that Lesan will contribute towards democratizing access to the Web through MT for millions of people.

| Comments:    | 4 pages, 2 figures, 35th Conference on Neural Information Processing Systems (NeurIPS 2021) demonstrations track |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | I.2.7; I.2.1                                                 |
| Cite as:     | **[arXiv:2112.08191](https://arxiv.org/abs/2112.08191) [cs.CL]** |
|              | (or **[arXiv:2112.08191v1](https://arxiv.org/abs/2112.08191v1) [cs.CL]** for this version) |





<h2 id="2021-12-16-5">5. Improving both domain robustness and domain adaptability in machine translation
</h2>

Title: [Improving both domain robustness and domain adaptability in machine translation](https://arxiv.org/abs/2112.08288)

Authors: [Wen Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+W), [Jindřich Libovický](https://arxiv.org/search/cs?searchtype=author&query=Libovický%2C+J), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A)

> We address two problems of domain adaptation in neural machine translation. First, we want to reach domain robustness, i.e., good quality of both domains from the training data, and domains unseen in the training data. Second, we want our systems to be adaptive, i.e., making it possible to finetune systems with just hundreds of in-domain parallel sentences. In this paper, we introduce a novel combination of two previous approaches, word adaptive modelling, which addresses domain robustness, and meta-learning, which addresses domain adaptability, and we present empirical results showing that our new combination improves both of these properties.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.08288](https://arxiv.org/abs/2112.08288) [cs.CL]** |
|           | (or **[arXiv:2112.08288v1](https://arxiv.org/abs/2112.08288v1) [cs.CL]** for this version) |





<h2 id="2021-12-16-6">6. Measure and Improve Robustness in NLP Models: A Survey
</h2>

Title: [Measure and Improve Robustness in NLP Models: A Survey](https://arxiv.org/abs/2112.08313)

Authors: [Xuezhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Haohan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Diyi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+D)

> As NLP models achieved state-of-the-art performances over benchmarks and gained wide applications, it has been increasingly important to ensure the safe deployment of these models in the real world, e.g., making sure the models are robust against unseen or challenging scenarios. Despite robustness being an increasingly studied topic, it has been separately explored in applications like vision and NLP, with various definitions, evaluation and mitigation strategies in multiple lines of research. In this paper, we aim to provide a unifying survey of how to define, measure and improve robustness in NLP. We first connect multiple definitions of robustness, then unify various lines of work on identifying robustness failures and evaluating models' robustness. Correspondingly, we present mitigation strategies that are data-driven, model-driven, and inductive-prior-based, with a more systematic view of how to effectively improve robustness in NLP models. Finally, we conclude by outlining open challenges and future directions to motivate further research in this area.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.08313](https://arxiv.org/abs/2112.08313) [cs.CL]** |
|           | (or **[arXiv:2112.08313v1](https://arxiv.org/abs/2112.08313v1) [cs.CL]** for this version) |





<h2 id="2021-12-16-7">7. Textless Speech-to-Speech Translation on Real Data
</h2>

Title: [Textless Speech-to-Speech Translation on Real Data](https://arxiv.org/abs/2112.08352)

Authors: [Ann Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+A), [Hongyu Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H), [Paul-Ambroise Duquenne](https://arxiv.org/search/cs?searchtype=author&query=Duquenne%2C+P), [Holger Schwenk](https://arxiv.org/search/cs?searchtype=author&query=Schwenk%2C+H), [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Sravya Popuri](https://arxiv.org/search/cs?searchtype=author&query=Popuri%2C+S), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J), [Jiatao Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J), [Wei-Ning Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+W)

> We present a textless speech-to-speech translation (S2ST) system that can translate speech from one language into another language and can be built without the need of any text data. Different from existing work in the literature, we tackle the challenge in modeling multi-speaker target speech and train the systems with real-world S2ST data. The key to our approach is a self-supervised unit-based speech normalization technique, which finetunes a pre-trained speech encoder with paired audios from multiple speakers and a single reference speaker to reduce the variations due to accents, while preserving the lexical content. With only 10 minutes of paired data for speech normalization, we obtain on average 3.2 BLEU gain when training the S2ST model on the \vp~S2ST dataset, compared to a baseline trained on un-normalized speech target. We also incorporate automatically mined S2ST data and show an additional 2.0 BLEU gain. To our knowledge, we are the first to establish a textless S2ST technique that can be trained with real-world data and works for multiple language pairs.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.08352](https://arxiv.org/abs/2112.08352) [cs.CL]** |
|           | (or **[arXiv:2112.08352v1](https://arxiv.org/abs/2112.08352v1) [cs.CL]** for this version) |





# 2021-12-15

[Return to Index](#Index)



<h2 id="2021-12-15-1">1. Improving Hybrid CTC/Attention End-to-end Speech Recognition with Pretrained Acoustic and Language Model
</h2>

Title: [Improving Hybrid CTC/Attention End-to-end Speech Recognition with Pretrained Acoustic and Language Model](https://arxiv.org/abs/2112.07254)

Authors: [Keqi Deng](https://arxiv.org/search/eess?searchtype=author&query=Deng%2C+K), [Songjun Cao](https://arxiv.org/search/eess?searchtype=author&query=Cao%2C+S), [Yike Zhang](https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+Y), [Long Ma](https://arxiv.org/search/eess?searchtype=author&query=Ma%2C+L)

> Recently, self-supervised pretraining has achieved impressive results in end-to-end (E2E) automatic speech recognition (ASR). However, the dominant sequence-to-sequence (S2S) E2E model is still hard to fully utilize the self-supervised pre-training methods because its decoder is conditioned on acoustic representation thus cannot be pretrained separately. In this paper, we propose a pretrained Transformer (Preformer) S2S ASR architecture based on hybrid CTC/attention E2E models to fully utilize the pretrained acoustic models (AMs) and language models (LMs). In our framework, the encoder is initialized with a pretrained AM (wav2vec2.0). The Preformer leverages CTC as an auxiliary task during training and inference. Furthermore, we design a one-cross decoder (OCD), which relaxes the dependence on acoustic representations so that it can be initialized with pretrained LM (DistilGPT2). Experiments are conducted on the AISHELL-1 corpus and achieve a 4.6% character error rate (CER) on the test set. Compared with our vanilla hybrid CTC/attention Transformer baseline, our proposed CTC/attention-based Preformer yields 27% relative CER reduction. To the best of our knowledge, this is the first work to utilize both pretrained AM and LM in a S2S ASR system.

| Comments: | ASRU2021                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| Cite as:  | **[arXiv:2112.07254](https://arxiv.org/abs/2112.07254) [eess.AS]** |
|           | (or **[arXiv:2112.07254v1](https://arxiv.org/abs/2112.07254v1) [eess.AS]** for this version) |





<h2 id="2021-12-15-2">2. CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising
</h2>

Title: [CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising](https://arxiv.org/abs/2112.07515)

Authors: [Jianjie Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J), [Yehao Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Yingwei Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+Y), [Ting Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+T), [Hongyang Chao](https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+H), [Tao Mei](https://arxiv.org/search/cs?searchtype=author&query=Mei%2C+T)

> BERT-type structure has led to the revolution of vision-language pre-training and the achievement of state-of-the-art results on numerous vision-language downstream tasks. Existing solutions dominantly capitalize on the multi-modal inputs with mask tokens to trigger mask-based proxy pre-training tasks (e.g., masked language modeling and masked object/frame prediction). In this work, we argue that such masked inputs would inevitably introduce noise for cross-modal matching proxy task, and thus leave the inherent vision-language association under-explored. As an alternative, we derive a particular form of cross-modal proxy objective for video-language pre-training, i.e., Contrastive Cross-modal matching and denoising (CoCo). By viewing the masked frame/word sequences as the noisy augmentation of primary unmasked ones, CoCo strengthens video-language association by simultaneously pursuing inter-modal matching and intra-modal denoising between masked and unmasked inputs in a contrastive manner. Our CoCo proxy objective can be further integrated into any BERT-type encoder-decoder structure for video-language pre-training, named as Contrastive Cross-modal BERT (CoCo-BERT). We pre-train CoCo-BERT on TV dataset and a newly collected large-scale GIF video dataset (ACTION). Through extensive experiments over a wide range of downstream tasks (e.g., cross-modal retrieval, video question answering, and video captioning), we demonstrate the superiority of CoCo-BERT as a pre-trained structure.

| Comments: | ACM Multimedia 2021                                          |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multimedia (cs.MM) |
| Cite as:  | **[arXiv:2112.07515](https://arxiv.org/abs/2112.07515) [cs.CV]** |
|           | (or **[arXiv:2112.07515v1](https://arxiv.org/abs/2112.07515v1) [cs.CV]** for this version) |





<h2 id="2021-12-15-3">3. Language Models are not Models of Language
</h2>

Title: [Language Models are not Models of Language](https://arxiv.org/abs/2112.07055)

Authors: [Csaba Veres](https://arxiv.org/search/cs?searchtype=author&query=Veres%2C+C)

> Natural Language Processing (NLP) has become one of the leading application areas in the current Artificial Intelligence boom. Transfer learning has enabled large deep learning neural networks trained on the language modeling task to vastly improve performance in almost all language tasks. Interestingly, when the models are trained with data that includes software code, they demonstrate remarkable abilities in generating functioning computer code from natural language specifications. We argue that this creates a conundrum for claims that neural models provide an alternative theory to generative phrase structure grammars in explaining how language works. Since the syntax of programming languages is determined by phrase structure grammars, successful neural models are apparently uninformative about the theoretical foundations of programming languages, and by extension, natural languages. We argue that the term language model is misleading because deep learning models are not theoretical models of language and propose the adoption of corpus model instead, which better reflects the genesis and contents of the model.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.07055](https://arxiv.org/abs/2112.07055) [cs.CL]** |
|           | (or **[arXiv:2112.07055v1](https://arxiv.org/abs/2112.07055v1) [cs.CL]** for this version) |





<h2 id="2021-12-15-4">4. From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression
</h2>
Title: [From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression](https://arxiv.org/abs/2112.07198)

Authors: [Runxin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+R), [Fuli Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+F), [Chengyu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Baobao Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+B), [Jun Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Songfang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F)

> Pre-trained Language Models (PLMs) have achieved great success in various Natural Language Processing (NLP) tasks under the pre-training and fine-tuning paradigm. With large quantities of parameters, PLMs are computation-intensive and resource-hungry. Hence, model pruning has been introduced to compress large-scale PLMs. However, most prior approaches only consider task-specific knowledge towards downstream tasks, but ignore the essential task-agnostic knowledge during pruning, which may cause catastrophic forgetting problem and lead to poor generalization ability. To maintain both task-agnostic and task-specific knowledge in our pruned model, we propose ContrAstive Pruning (CAP) under the paradigm of pre-training and fine-tuning. It is designed as a general framework, compatible with both structured and unstructured pruning. Unified in contrastive learning, CAP enables the pruned model to learn from the pre-trained model for task-agnostic knowledge, and fine-tuned model for task-specific knowledge. Besides, to better retain the performance of the pruned model, the snapshots (i.e., the intermediate models at each pruning iteration) also serve as effective supervisions for pruning. Our extensive experiments show that adopting CAP consistently yields significant improvements, especially in extremely high sparsity scenarios. With only 3% model parameters reserved (i.e., 97% sparsity), CAP successfully achieves 99.2% and 96.3% of the original BERT performance in QQP and MNLI tasks. In addition, our probing experiments demonstrate that the model pruned by CAP tends to achieve better generalization ability.

| Comments: | Accepted to AAAI 2022                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2112.07198](https://arxiv.org/abs/2112.07198) [cs.CL]** |
|           | (or **[arXiv:2112.07198v1](https://arxiv.org/abs/2112.07198v1) [cs.CL]** for this version) |

<h2 id="2021-12-15-5">5. Model Uncertainty-Aware Knowledge Amalgamation for Pre-Trained Language Models
</h2>

Title: [Model Uncertainty-Aware Knowledge Amalgamation for Pre-Trained Language Models](https://arxiv.org/abs/2112.07327)

Authors: [Lei Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y), [Xuancheng Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X), [Guangxiang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+G), [Peng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Xu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X)

> As many fine-tuned pre-trained language models~(PLMs) with promising performance are generously released, investigating better ways to reuse these models is vital as it can greatly reduce the retraining computational cost and the potential environmental side-effects. In this paper, we explore a novel model reuse paradigm, Knowledge Amalgamation~(KA) for PLMs. Without human annotations available, KA aims to merge the knowledge from different teacher-PLMs, each of which specializes in a different classification problem, into a versatile student model. The achieve this, we design a Model Uncertainty--aware Knowledge Amalgamation~(MUKA) framework, which identifies the potential adequate teacher using Monte-Carlo Dropout for approximating the golden supervision to guide the student. Experimental results demonstrate that MUKA achieves substantial improvements over baselines on benchmark datasets. Further analysis shows that MUKA can generalize well under several complicate settings with multiple teacher models, heterogeneous teachers, and even cross-dataset teachers.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.07327](https://arxiv.org/abs/2112.07327) [cs.CL]** |
|           | (or **[arXiv:2112.07327v1](https://arxiv.org/abs/2112.07327v1) [cs.CL]** for this version) |





<h2 id="2021-12-15-6">6. VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena
</h2>

Title: [VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena](https://arxiv.org/abs/2112.07566)

Authors: [Letitia Parcalabescu](https://arxiv.org/search/cs?searchtype=author&query=Parcalabescu%2C+L), [Michele Cafagna](https://arxiv.org/search/cs?searchtype=author&query=Cafagna%2C+M), [Lilitta Muradjan](https://arxiv.org/search/cs?searchtype=author&query=Muradjan%2C+L), [Anette Frank](https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+A), [Iacer Calixto](https://arxiv.org/search/cs?searchtype=author&query=Calixto%2C+I), [Albert Gatt](https://arxiv.org/search/cs?searchtype=author&query=Gatt%2C+A)

> We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective, complementing the canonical task-centred V&L evaluations.

| Comments:    | 28 pages, 4 figures, 11 tables                               |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| MSC classes: | 68Txx                                                        |
| ACM classes: | I.2.7; I.2.10                                                |
| Cite as:     | **[arXiv:2112.07566](https://arxiv.org/abs/2112.07566) [cs.CL]** |
|              | (or **[arXiv:2112.07566v1](https://arxiv.org/abs/2112.07566v1) [cs.CL]** for this version) |





<h2 id="2021-12-15-7">7. Massive-scale Decoding for Text Generation using Lattices
</h2>

Title: [Massive-scale Decoding for Text Generation using Lattices](https://arxiv.org/abs/2112.07660)

Authors: [Jiacheng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Greg Durrett](https://arxiv.org/search/cs?searchtype=author&query=Durrett%2C+G)

> Neural text generation models like those used for summarization and translation generate high-quality outputs, but often concentrate around a mode when what we really want is a diverse set of options. We present a search algorithm to construct lattices encoding a massive number of generation options. First, we restructure decoding as a best-first search, which explores the space differently than beam search and improves efficiency by avoiding pruning paths. Second, we revisit the idea of hypothesis recombination: we can identify pairs of similar generation candidates during search and merge them as an approximation. On both document summarization and machine translation, we show that our algorithm encodes hundreds to thousands of diverse options that remain grammatical and high-quality into one linear-sized lattice. This algorithm provides a foundation for building downstream generation applications on top of massive-scale diverse outputs.

| Comments: | 19 pages, 13 figures, see [this https URL](https://github.com/jiacheng-xu/lattice-generation) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2112.07660](https://arxiv.org/abs/2112.07660) [cs.CL]** |
|           | (or **[arXiv:2112.07660v1](https://arxiv.org/abs/2112.07660v1) [cs.CL]** for this version) |







# 2021-12-14

[Return to Index](#Index)



<h2 id="2021-12-14-1">1. VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks
</h2>

Title: [VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks](https://arxiv.org/abs/2112.06825)
Authors: [Yi-Lin Sung](https://arxiv.org/search/cs?searchtype=author&query=Sung%2C+Y), [Jaemin Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+J), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Recently, fine-tuning language models pre-trained on large text corpora have provided huge improvements on vision-and-language (V&L) tasks as well as on pure language tasks. However, fine-tuning the entire parameter set of pre-trained models becomes impractical since the model size is growing rapidly. Hence, in this paper, we introduce adapter-based parameter-efficient transfer learning techniques to V&L models such as VL-BART and VL-T5. We evaluate our methods in a unified multi-task setup on four diverse V&L tasks: VQAv2, GQA, NLVR2 , and MSCOCO image captioning. With careful training and thorough experiments, we benchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter) against the standard full fine-tuning and the recently proposed prompt-tuning approach. We also enhance the efficiency and performance of adapters by sharing their weights to attain knowledge across tasks. Our results demonstrate that training the adapter with the weight-sharing technique (4.4% of total parameters) can match the performance of fine-tuning the entire model. Lastly, we present a comprehensive analysis including the combination of adapter and task-specific prompts and the impact of V&L pre-training on adapters. Our code is available at: [this https URL](https://github.com/ylsung/VL_adapter).

| Comments: | 13 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2112.06825](https://arxiv.org/abs/2112.06825) [cs.CV]** |
|           | (or **[arXiv:2112.06825v1](https://arxiv.org/abs/2112.06825v1) [cs.CV]** for this version) |





<h2 id="2021-12-14-2">2. Sequence-level self-learning with multiple hypotheses
</h2>

Title: [Sequence-level self-learning with multiple hypotheses](https://arxiv.org/abs/2112.05826)
Authors: [Kenichi Kumatani](https://arxiv.org/search/cs?searchtype=author&query=Kumatani%2C+K), [Dimitrios Dimitriadis](https://arxiv.org/search/cs?searchtype=author&query=Dimitriadis%2C+D), [Yashesh Gaur](https://arxiv.org/search/cs?searchtype=author&query=Gaur%2C+Y), [Robert Gmyr](https://arxiv.org/search/cs?searchtype=author&query=Gmyr%2C+R), [Sefik Emre Eskimez](https://arxiv.org/search/cs?searchtype=author&query=Eskimez%2C+S+E), [Jinyu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M)

> In this work, we develop new self-learning techniques with an attention-based sequence-to-sequence (seq2seq) model for automatic speech recognition (ASR). For untranscribed speech data, the hypothesis from an ASR system must be used as a label. However, the imperfect ASR result makes unsupervised learning difficult to consistently improve recognition performance especially in the case that multiple powerful teacher models are unavailable. In contrast to conventional unsupervised learning approaches, we adopt the \emph{multi-task learning} (MTL) framework where the n-th best ASR hypothesis is used as the label of each task. The seq2seq network is updated through the MTL framework so as to find the common representation that can cover multiple hypotheses. By doing so, the effect of the \emph{hard-decision} errors can be alleviated. 
> We first demonstrate the effectiveness of our self-learning methods through ASR experiments in an accent adaptation task between the US and British English speech. Our experiment results show that our method can reduce the WER on the British speech data from 14.55\% to 10.36\% compared to the baseline model trained with the US English data only. Moreover, we investigate the effect of our proposed methods in a federated learning scenario.

| Comments: | Published in Interspeech 2020: [this https URL](https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/2020.pdf) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2112.05826](https://arxiv.org/abs/2112.05826) [cs.CL]** |
|           | (or **[arXiv:2112.05826v1](https://arxiv.org/abs/2112.05826v1) [cs.CL]** for this version) |





<h2 id="2021-12-14-3">3. Selecting Parallel In-domain Sentences for Neural Machine Translation Using Monolingual Texts
</h2>

Title: [Selecting Parallel In-domain Sentences for Neural Machine Translation Using Monolingual Texts](https://arxiv.org/abs/2112.06096)
Authors: [Javad Pourmostafa Roshan Sharami](https://arxiv.org/search/cs?searchtype=author&query=Sharami%2C+J+P+R), [Dimitar Shterionov](https://arxiv.org/search/cs?searchtype=author&query=Shterionov%2C+D), [Pieter Spronck](https://arxiv.org/search/cs?searchtype=author&query=Spronck%2C+P)

> Continuously-growing data volumes lead to larger generic models. Specific use-cases are usually left out, since generic models tend to perform poorly in domain-specific cases. Our work addresses this gap with a method for selecting in-domain data from generic-domain (parallel text) corpora, for the task of machine translation. The proposed method ranks sentences in parallel general-domain data according to their cosine similarity with a monolingual domain-specific data set. We then select the top K sentences with the highest similarity score to train a new machine translation system tuned to the specific in-domain data. Our experimental results show that models trained on this in-domain data outperform models trained on generic or a mixture of generic and domain data. That is, our method selects high-quality domain-specific training instances at low computational cost and data size.

| Comments: | Accepted to the CLIN Journal on Dec 6, 2021                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2112.06096](https://arxiv.org/abs/2112.06096) [cs.CL]** |
|           | (or **[arXiv:2112.06096v1](https://arxiv.org/abs/2112.06096v1) [cs.CL]** for this version) |





<h2 id="2021-12-14-4">4. Communication-Efficient Federated Learning for Neural Machine Translation
</h2>

Title: [Communication-Efficient Federated Learning for Neural Machine Translation](https://arxiv.org/abs/2112.06135)
Authors: [Tanya Roosta](https://arxiv.org/search/cs?searchtype=author&query=Roosta%2C+T), [Peyman Passban](https://arxiv.org/search/cs?searchtype=author&query=Passban%2C+P), [Ankit Chadha](https://arxiv.org/search/cs?searchtype=author&query=Chadha%2C+A)

> Training neural machine translation (NMT) models in federated learning (FL) settings could be inefficient both computationally and communication-wise, due to the large size of translation engines as well as the multiple rounds of updates required to train clients and a central server. In this paper, we explore how to efficiently build NMT models in an FL setup by proposing a novel solution. In order to reduce the communication overhead, out of all neural layers we only exchange what we term "Controller" layers. Controllers are a small number of additional neural components connected to our pre-trained architectures. These new components are placed in between original layers. They act as liaisons to communicate with the central server and learn minimal information that is sufficient enough to update clients. 
> We evaluated the performance of our models on five datasets from different domains to translate from German into English. We noted that the models equipped with Controllers preform on par with those trained in a central and non-FL setting. In addition, we observed a substantial reduction in the communication traffic of the FL pipeline, which is a direct consequence of using Controllers. Based on our experiments, Controller-based models are ~6 times less expensive than their other peers. This reduction is significantly important when we consider the number of parameters in large models and it becomes even more critical when such parameters need to be exchanged for multiple rounds in FL settings.

| Comments: | The first two authors contributed equally                    |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2112.06135](https://arxiv.org/abs/2112.06135) [cs.CL]** |
|           | (or **[arXiv:2112.06135v1](https://arxiv.org/abs/2112.06135v1) [cs.CL]** for this version) |





<h2 id="2021-12-14-5">5. Do Data-based Curricula Work?
</h2>

Title: [Do Data-based Curricula Work?](https://arxiv.org/abs/2112.06510)
Authors: [Maxim K. Surkov](https://arxiv.org/search/cs?searchtype=author&query=Surkov%2C+M+K), [Vladislav D. Mosin](https://arxiv.org/search/cs?searchtype=author&query=Mosin%2C+V+D), [Ivan P. Yamshchikov](https://arxiv.org/search/cs?searchtype=author&query=Yamshchikov%2C+I+P)

> Current state-of-the-art NLP systems use large neural networks that require lots of computational resources for training. Inspired by human knowledge acquisition, researchers have proposed curriculum learning, - sequencing of tasks (task-based curricula) or ordering and sampling of the datasets (data-based curricula) that facilitate training. This work investigates the benefits of data-based curriculum learning for large modern language models such as BERT and T5. We experiment with various curricula based on a range of complexity measures and different sampling strategies. Extensive experiments on different NLP tasks show that curricula based on various complexity measures rarely has any benefits while random sampling performs either as well or better than curricula.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.06510](https://arxiv.org/abs/2112.06510) [cs.CL]** |
|           | (or **[arXiv:2112.06510v1](https://arxiv.org/abs/2112.06510v1) [cs.CL]** for this version) |





<h2 id="2021-12-14-6">6. WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models
</h2>

Title: [WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models](https://arxiv.org/abs/2112.06598)
Authors: [Benjamin Minixhofer](https://arxiv.org/search/cs?searchtype=author&query=Minixhofer%2C+B), [Fabian Paischer](https://arxiv.org/search/cs?searchtype=author&query=Paischer%2C+F), [Navid Rekabsaz](https://arxiv.org/search/cs?searchtype=author&query=Rekabsaz%2C+N)

> Recently, large pretrained language models (LMs) have gained popularity. Training these models requires ever more computational resources and most of the existing models are trained on English text only. It is exceedingly expensive to train these models in other languages. To alleviate this problem, we introduce a method -- called WECHSEL -- to transfer English models to new languages. We exchange the tokenizer of the English model with a tokenizer in the target language and initialize token embeddings such that they are close to semantically similar English tokens by utilizing multilingual static word embeddings covering English and the target language. We use WECHSEL to transfer GPT-2 and RoBERTa models to 4 other languages (French, German, Chinese and Swahili). WECHSEL improves over a previously proposed method for cross-lingual parameter transfer and outperforms models of comparable size trained from scratch in the target language with up to 64x less training effort. Our method makes training large language models for new languages more accessible and less damaging to the environment. We make our code and models publicly available.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.06598](https://arxiv.org/abs/2112.06598) [cs.CL]** |
|           | (or **[arXiv:2112.06598v1](https://arxiv.org/abs/2112.06598v1) [cs.CL]** for this version) |





<h2 id="2021-12-14-7">7. GLaM: Efficient Scaling of Language Models with Mixture-of-Experts
</h2>

Title: [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905)
Authors: [Nan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+N), [Yanping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y), [Andrew M. Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+A+M), [Simon Tong](https://arxiv.org/search/cs?searchtype=author&query=Tong%2C+S), [Dmitry Lepikhin](https://arxiv.org/search/cs?searchtype=author&query=Lepikhin%2C+D), [Yuanzhong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Maxim Krikun](https://arxiv.org/search/cs?searchtype=author&query=Krikun%2C+M), [Yanqi Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y), [Adams Wei Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+A+W), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Barret Zoph](https://arxiv.org/search/cs?searchtype=author&query=Zoph%2C+B), [Liam Fedus](https://arxiv.org/search/cs?searchtype=author&query=Fedus%2C+L), [Maarten Bosma](https://arxiv.org/search/cs?searchtype=author&query=Bosma%2C+M), [Zongwei Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Z), [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Yu Emma Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y+E), [Kellie Webster](https://arxiv.org/search/cs?searchtype=author&query=Webster%2C+K), [Marie Pellat](https://arxiv.org/search/cs?searchtype=author&query=Pellat%2C+M), [Kevin Robinson](https://arxiv.org/search/cs?searchtype=author&query=Robinson%2C+K), [Kathy Meier-Hellstern](https://arxiv.org/search/cs?searchtype=author&query=Meier-Hellstern%2C+K), [Toju Duke](https://arxiv.org/search/cs?searchtype=author&query=Duke%2C+T), [Lucas Dixon](https://arxiv.org/search/cs?searchtype=author&query=Dixon%2C+L), [Kun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+K), [Quoc V Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V), [Yonghui Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Zhifeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z), [Claire Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+C)

> Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.06905](https://arxiv.org/abs/2112.06905) [cs.CL]** |
|           | (or **[arXiv:2112.06905v1](https://arxiv.org/abs/2112.06905v1) [cs.CL]** for this version) |







# 2021-12-13

[Return to Index](#Index)



<h2 id="2021-12-13-1">1. Injecting Semantic Concepts into End-to-End Image Captioning
</h2>

Title: [Injecting Semantic Concepts into End-to-End Image Captioning](https://arxiv.org/abs/2112.05230)

Authors: [Zhiyuan Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Z), [Jianfeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Xiaowei Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X), [Lin Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+L), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Yezhou Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Zicheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z)

> Tremendous progress has been made in recent years in developing better image captioning models, yet most of them rely on a separate object detector to extract regional features. Recent vision-language studies are shifting towards the detector-free trend by leveraging grid representations for more flexible model training and faster inference speed. However, such development is primarily focused on image understanding tasks, and remains less investigated for the caption generation task. In this paper, we are concerned with a better-performing detector-free image captioning model, and propose a pure vision transformer-based image captioning model, dubbed as ViTCAP, in which grid representations are used without extracting the regional features. For improved performance, we introduce a novel Concept Token Network (CTN) to predict the semantic concepts and then incorporate them into the end-to-end captioning. In particular, the CTN is built on the basis of a vision transformer and is designed to predict the concept tokens through a classification task, from which the rich semantic information contained greatly benefits the captioning task. Compared with the previous detector-based models, ViTCAP drastically simplifies the architectures and at the same time achieves competitive performance on various challenging image captioning datasets. In particular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split, 93.8 and 108.6 CIDEr scores on nocaps, and Google-CC captioning datasets, respectively.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.05230](https://arxiv.org/abs/2112.05230) [cs.CV]** |
|           | (or **[arXiv:2112.05230v1](https://arxiv.org/abs/2112.05230v1) [cs.CV]** for this version) |





<h2 id="2021-12-13-2">2. MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning
</h2>

Title: [MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning](https://arxiv.org/abs/2112.05253)

Authors: [Constantin Eichenberg](https://arxiv.org/search/cs?searchtype=author&query=Eichenberg%2C+C), [Sidney Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+S), [Samuel Weinbach](https://arxiv.org/search/cs?searchtype=author&query=Weinbach%2C+S), [Letitia Parcalabescu](https://arxiv.org/search/cs?searchtype=author&query=Parcalabescu%2C+L), [Anette Frank](https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+A)

> Large-scale pretraining is fast becoming the norm in Vision-Language (VL) modeling. However, prevailing VL approaches are limited by the requirement for labeled data and the use of complex multi-step pretraining objectives. We present MAGMA - a simple method for augmenting generative language models with additional modalities using adapter-based finetuning. Building on Frozen, we train a series of VL models that autoregressively generate text from arbitrary combinations of visual and textual input. The pretraining is entirely end-to-end using a single language modeling objective, simplifying optimization compared to previous approaches. Importantly, the language model weights remain unchanged during training, allowing for transfer of encyclopedic knowledge and in-context learning abilities from language pretraining. MAGMA outperforms Frozen on open-ended generative tasks, achieving state of the art results on the OKVQA benchmark and competitive results on a range of other popular VL benchmarks, while pretraining on 0.2% of the number of samples used to train SimVLM.

| Comments:    | 11 pages, 6 figures, 2 tables                                |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| ACM classes: | I.2.7; I.4.8; I.5.1                                          |
| Cite as:     | **[arXiv:2112.05253](https://arxiv.org/abs/2112.05253) [cs.CV]** |
|              | (or **[arXiv:2112.05253v1](https://arxiv.org/abs/2112.05253v1) [cs.CV]** for this version) |





<h2 id="2021-12-13-3">3. Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation
</h2>

Title: [Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation](https://arxiv.org/abs/2112.05587)

Authors: [Tianyi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T), [Zuxuan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Wenhan Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+W), [Jingjing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J), [Yu-Gang Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y)

> Most existing vision-language pre-training methods focus on understanding tasks and use BERT-like objectives (masked language modeling and image-text matching) during pretraining. Although they perform well in many understanding downstream tasks, e.g., visual question answering, image-text retrieval and visual entailment, they do not possess the ability to generate. To tackle this problem, we propose Unified multimodal pre-training for both Vision-Language understanding and generation (UniVL). The proposed UniVL is capable of handling both understanding tasks and generative tasks. We augment existing pretraining paradigms that only use random masks with causal masks, i.e., triangular masks that mask out future tokens, such that the pre-trained models can have autoregressive generation abilities by design. We formulate several previous understanding tasks as a text generation task and propose to use prompt-based method for fine-tuning on different downstream tasks. Our experiments show that there is a trade-off between understanding tasks and generation tasks while using the same model, and a feasible way to improve both tasks is to use more data. Our UniVL framework attains comparable performance to recent vision-language pre-training methods on both understanding tasks and generation tasks. Moreover, we demostrate that prompt-based finetuning is more data-efficient - it outperforms discriminative methods in few-shot scenarios.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.05587](https://arxiv.org/abs/2112.05587) [cs.CV]** |
|           | (or **[arXiv:2112.05587v1](https://arxiv.org/abs/2112.05587v1) [cs.CV]** for this version) |





<h2 id="2021-12-13-4">4. Word Embeddings via Causal Inference: Gender Bias Reducing and Semantic Information Preserving
</h2>

Title: [Word Embeddings via Causal Inference: Gender Bias Reducing and Semantic Information Preserving](https://arxiv.org/abs/2112.05194)

Authors: [Lei Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Dengdeng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+D), [Jinhan Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+J), [Wenxing Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+W), [Shenggang Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S), [Meichen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+M), [Linglong Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L), [Hongsheng Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+H), [Yanchun Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+Y), [Bei Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+B)

> With widening deployments of natural language processing (NLP) in daily life, inherited social biases from NLP models have become more severe and problematic. Previous studies have shown that word embeddings trained on human-generated corpora have strong gender biases that can produce discriminative results in downstream tasks. Previous debiasing methods focus mainly on modeling bias and only implicitly consider semantic information while completely overlooking the complex underlying causal structure among bias and semantic components. To address these issues, we propose a novel methodology that leverages a causal inference framework to effectively remove gender bias. The proposed method allows us to construct and analyze the complex causal mechanisms facilitating gender information flow while retaining oracle semantic information within word embeddings. Our comprehensive experiments show that the proposed method achieves state-of-the-art results in gender-debiasing tasks. In addition, our methods yield better performance in word similarity evaluation and various extrinsic downstream NLP tasks.

| Comments: | Accepted by AAAI 2022                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computers and Society (cs.CY) |
| Cite as:  | **[arXiv:2112.05194](https://arxiv.org/abs/2112.05194) [cs.CL]** |
|           | (or **[arXiv:2112.05194v1](https://arxiv.org/abs/2112.05194v1) [cs.CL]** for this version) |





<h2 id="2021-12-13-5">5. Shennong: a Python toolbox for audio speech features extraction
</h2>

Title: [Shennong: a Python toolbox for audio speech features extraction](https://arxiv.org/abs/2112.05555)

Authors: [Mathieu Bernard](https://arxiv.org/search/cs?searchtype=author&query=Bernard%2C+M), [Maxime Poli](https://arxiv.org/search/cs?searchtype=author&query=Poli%2C+M), [Julien Karadayi](https://arxiv.org/search/cs?searchtype=author&query=Karadayi%2C+J), [Emmanuel Dupoux](https://arxiv.org/search/cs?searchtype=author&query=Dupoux%2C+E)

> We introduce Shennong, a Python toolbox and command-line utility for speech features extraction. It implements a wide range of well-established state of art algorithms including spectro-temporal filters such as Mel-Frequency Cepstral Filterbanks or Predictive Linear Filters, pre-trained neural networks, pitch estimators as well as speaker normalization methods and post-processing algorithms. Shennong is an open source, easy-to-use, reliable and extensible framework. The use of Python makes the integration to others speech modeling and machine learning tools easy. It aims to replace or complement several heterogeneous software, such as Kaldi or Praat. After describing the Shennong software architecture, its core components and implemented algorithms, this paper illustrates its use on three applications: a comparison of speech features performances on a phones discrimination task, an analysis of a Vocal Tract Length Normalization model as a function of the speech duration used for training and a comparison of pitch estimation algorithms under various noise conditions.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.05555](https://arxiv.org/abs/2112.05555) [cs.CL]** |
|           | (or **[arXiv:2112.05555v1](https://arxiv.org/abs/2112.05555v1) [cs.CL]** for this version) |





<h2 id="2021-12-13-6">6. Analysis and Prediction of NLP Models Via Task Embeddings
</h2>

Title: [Analysis and Prediction of NLP Models Via Task Embeddings](https://arxiv.org/abs/2112.05647)

Authors: [Damien Sileo](https://arxiv.org/search/cs?searchtype=author&query=Sileo%2C+D), [Marie-Francine Moens](https://arxiv.org/search/cs?searchtype=author&query=Moens%2C+M)

> Task embeddings are low-dimensional representations that are trained to capture task properties. In this paper, we propose MetaEval, a collection of 101 NLP tasks. We fit a single transformer to all MetaEval tasks jointly while conditioning it on learned embeddings. The resulting task embeddings enable a novel analysis of the space of tasks. We then show that task aspects can be mapped to task embeddings for new tasks without using any annotated examples. 
> Predicted embeddings can modulate the encoder for zero-shot inference and outperform a zero-shot baseline on GLUE tasks. The provided multitask setup can function as a benchmark for future transfer learning research.

| Subjects:    | **Computation and Language (cs.CL)**                         |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7; I.2.6                                                 |
| Cite as:     | **[arXiv:2112.05647](https://arxiv.org/abs/2112.05647) [cs.CL]** |
|              | (or **[arXiv:2112.05647v1](https://arxiv.org/abs/2112.05647v1) [cs.CL]** for this version) |





<h2 id="2021-12-13-7">7. Pruning Pretrained Encoders with a Multitask Objective
</h2>

Title: [Pruning Pretrained Encoders with a Multitask Objective](https://arxiv.org/abs/2112.05705)

Authors: [Patrick Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+P), [Richard Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+R)

> The sizes of pretrained language models make them challenging and expensive to use when there are multiple desired downstream tasks. In this work, we adopt recent strategies for model pruning during finetuning to explore the question of whether it is possible to prune a single encoder so that it can be used for multiple tasks. We allocate a fixed parameter budget and compare pruning a single model with a multitask objective against the best ensemble of single-task models. We find that under two pruning strategies (element-wise and rank pruning), the approach with the multitask objective outperforms training models separately when averaged across all tasks, and it is competitive on each individual one. Additional analysis finds that using a multitask objective during pruning can also be an effective method for reducing model sizes for low-resource tasks.

| Comments: | ENLSP NeurIPS 2021                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2112.05705](https://arxiv.org/abs/2112.05705) [cs.CL]** |
|           | (or **[arXiv:2112.05705v1](https://arxiv.org/abs/2112.05705v1) [cs.CL]** for this version) |






# 2021-12-10

[Return to Index](#Index)



<h2 id="2021-12-10-1">1. Self-Supervised Image-to-Text and Text-to-Image Synthesis
</h2>

Title: [Self-Supervised Image-to-Text and Text-to-Image Synthesis](https://arxiv.org/abs/2112.04928)

Authors: [Anindya Sundar Das](https://arxiv.org/search/cs?searchtype=author&query=Das%2C+A+S), [Sriparna Saha](https://arxiv.org/search/cs?searchtype=author&query=Saha%2C+S)

> A comprehensive understanding of vision and language and their interrelation are crucial to realize the underlying similarities and differences between these modalities and to learn more generalized, meaningful representations. In recent years, most of the works related to Text-to-Image synthesis and Image-to-Text generation, focused on supervised generative deep architectures to solve the problems, where very little interest was placed on learning the similarities between the embedding spaces across modalities. In this paper, we propose a novel self-supervised deep learning based approach towards learning the cross-modal embedding spaces; for both image to text and text to image generations. In our approach, we first obtain dense vector representations of images using StackGAN-based autoencoder model and also dense vector representations on sentence-level utilizing LSTM based text-autoencoder; then we study the mapping from embedding space of one modality to embedding space of the other modality utilizing GAN and maximum mean discrepancy based generative networks. We, also demonstrate that our model learns to generate textual description from image data as well as images from textual data both qualitatively and quantitatively.

| Comments:          | ICONIP 2021 : The 28th International Conference on Neural Information Processing |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Journal reference: | ICONIP 2021. Lecture Notes in Computer Science, vol 13111, pp 415-426. Springer, Cham |
| DOI:               | [10.1007/978-3-030-92273-3_34](https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1007%2F978-3-030-92273-3_34&v=e1cc4a4b) |
| Cite as:           | **[arXiv:2112.04928](https://arxiv.org/abs/2112.04928) [cs.CV]** |
|                    | (or **[arXiv:2112.04928v1](https://arxiv.org/abs/2112.04928v1) [cs.CV]** for this version) |







# 2021-12-9

[Return to Index](#Index)



<h2 id="2021-12-9-1">1. Transformer-Based Approach for Joint Handwriting and Named Entity Recognition in Historical documents
</h2>

Title: [Transformer-Based Approach for Joint Handwriting and Named Entity Recognition in Historical documents](https://arxiv.org/abs/2112.04189)

Authors: [Ahmed Cheikh Rouhoua](https://arxiv.org/search/cs?searchtype=author&query=Rouhoua%2C+A+C), [Marwa Dhiaf](https://arxiv.org/search/cs?searchtype=author&query=Dhiaf%2C+M), [Yousri Kessentini](https://arxiv.org/search/cs?searchtype=author&query=Kessentini%2C+Y), [Sinda Ben Salem](https://arxiv.org/search/cs?searchtype=author&query=Salem%2C+S+B)

> The extraction of relevant information carried out by named entities in handwriting documents is still a challenging task. Unlike traditional information extraction approaches that usually face text transcription and named entity recognition as separate subsequent tasks, we propose in this paper an end-to-end transformer-based approach to jointly perform these two tasks. The proposed approach operates at the paragraph level, which brings two main benefits. First, it allows the model to avoid unrecoverable early errors due to line segmentation. Second, it allows the model to exploit larger bi-dimensional context information to identify the semantic categories, reaching a higher final prediction accuracy. We also explore different training scenarios to show their effect on the performance and we demonstrate that a two-stage learning strategy can make the model reach a higher final prediction accuracy. As far as we know, this work presents the first approach that adopts the transformer networks for named entity recognition in handwritten documents. We achieve the new state-of-the-art performance in the ICDAR 2017 Information Extraction competition using the Esposalles database, for the complete task, even though the proposed technique does not use any dictionaries, language modeling, or post-processing.

| Subjects:          | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Pattern Recognition Letters, 2022                            |
| Cite as:           | **[arXiv:2112.04189](https://arxiv.org/abs/2112.04189) [cs.CV]** |
|                    | (or **[arXiv:2112.04189v1](https://arxiv.org/abs/2112.04189v1) [cs.CV]** for this version) |





<h2 id="2021-12-9-2">2. MLP Architectures for Vision-and-Language Modeling: An Empirical Study
</h2>

Title: [MLP Architectures for Vision-and-Language Modeling: An Empirical Study](https://arxiv.org/abs/2112.04453)

Authors: [Yixin Nie](https://arxiv.org/search/cs?searchtype=author&query=Nie%2C+Y), [Linjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M), [Zicheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L)

> We initiate the first empirical study on the use of MLP architectures for vision-and-language (VL) fusion. Through extensive experiments on 5 VL tasks and 5 robust VQA benchmarks, we find that: (i) Without pre-training, using MLPs for multimodal fusion has a noticeable performance gap compared to transformers; (ii) However, VL pre-training can help close the performance gap; (iii) Instead of heavy multi-head attention, adding tiny one-head attention to MLPs is sufficient to achieve comparable performance to transformers. Moreover, we also find that the performance gap between MLPs and transformers is not widened when being evaluated on the harder robust VQA benchmarks, suggesting using MLPs for VL fusion can generalize roughly to a similar degree as using transformers. These results hint that MLPs can effectively learn to align vision and text features extracted from lower-level encoders without heavy reliance on self-attention. Based on this, we ask an even bolder question: can we have an all-MLP architecture for VL modeling, where both VL fusion and the vision encoder are replaced with MLPs? Our result shows that an all-MLP VL model is sub-optimal compared to state-of-the-art full-featured VL models when both of them get pre-trained. However, pre-training an all-MLP can surprisingly achieve a better average score than full-featured transformer models without pre-training. This indicates the potential of large-scale pre-training of MLP-like architectures for VL modeling and inspires the future research direction on simplifying well-established VL modeling with less inductive design bias. Our code is publicly available at: [this https URL](https://github.com/easonnie/mlp-vil)

| Comments: | 15 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2112.04453](https://arxiv.org/abs/2112.04453) [cs.CV]** |
|           | (or **[arXiv:2112.04453v1](https://arxiv.org/abs/2112.04453v1) [cs.CV]** for this version) |





<h2 id="2021-12-9-3">3. Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand
</h2>

Title: [Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand](https://arxiv.org/abs/2112.04139)

Authors: [Jungo Kasai](https://arxiv.org/search/cs?searchtype=author&query=Kasai%2C+J), [Keisuke Sakaguchi](https://arxiv.org/search/cs?searchtype=author&query=Sakaguchi%2C+K), [Ronan Le Bras](https://arxiv.org/search/cs?searchtype=author&query=Bras%2C+R+L), [Lavinia Dunagan](https://arxiv.org/search/cs?searchtype=author&query=Dunagan%2C+L), [Jacob Morrison](https://arxiv.org/search/cs?searchtype=author&query=Morrison%2C+J), [Alexander R. Fabbri](https://arxiv.org/search/cs?searchtype=author&query=Fabbri%2C+A+R), [Yejin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> Natural language processing researchers have identified limitations of evaluation methodology for generation tasks, with new questions raised about the validity of automatic metrics and of crowdworker judgments. Meanwhile, efforts to improve generation models tend to focus on simple n-gram overlap metrics (e.g., BLEU, ROUGE). We argue that new advances on models and metrics should each more directly benefit and inform the other. We therefore propose a generalization of leaderboards, bidimensional leaderboards (Billboards), that simultaneously tracks progress in language generation tasks and metrics for their evaluation. Unlike conventional unidimensional leaderboards that sort submitted systems by predetermined metrics, a Billboard accepts both generators and evaluation metrics as competing entries. A Billboard automatically creates an ensemble metric that selects and linearly combines a few metrics based on a global analysis across generators. Further, metrics are ranked based on their correlations with human judgments. We release four Billboards for machine translation, summarization, and image captioning. We demonstrate that a linear ensemble of a few diverse metrics sometimes substantially outperforms existing metrics in isolation. Our mixed-effects model analysis shows that most automatic metrics, especially the reference-based ones, overrate machine over human generation, demonstrating the importance of updating metrics as generation models become stronger (and perhaps more similar to humans) in the future.

| Comments: | Project website: [this https URL](https://nlp.cs.washington.edu/billboard/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2112.04139](https://arxiv.org/abs/2112.04139) [cs.CL]** |
|           | (or **[arXiv:2112.04139v1](https://arxiv.org/abs/2112.04139v1) [cs.CL]** for this version) |





<h2 id="2021-12-9-4">4. Improving language models by retrieving from trillions of tokens
</h2>

Title: [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426)

Authors: [Sebastian Borgeaud](https://arxiv.org/search/cs?searchtype=author&query=Borgeaud%2C+S), [Arthur Mensch](https://arxiv.org/search/cs?searchtype=author&query=Mensch%2C+A), [Jordan Hoffmann](https://arxiv.org/search/cs?searchtype=author&query=Hoffmann%2C+J), [Trevor Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+T), [Eliza Rutherford](https://arxiv.org/search/cs?searchtype=author&query=Rutherford%2C+E), [Katie Millican](https://arxiv.org/search/cs?searchtype=author&query=Millican%2C+K), [George van den Driessche](https://arxiv.org/search/cs?searchtype=author&query=van+den+Driessche%2C+G), [Jean-Baptiste Lespiau](https://arxiv.org/search/cs?searchtype=author&query=Lespiau%2C+J), [Bogdan Damoc](https://arxiv.org/search/cs?searchtype=author&query=Damoc%2C+B), [Aidan Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+A), [Diego de Las Casas](https://arxiv.org/search/cs?searchtype=author&query=de+Las+Casas%2C+D), [Aurelia Guy](https://arxiv.org/search/cs?searchtype=author&query=Guy%2C+A), [Jacob Menick](https://arxiv.org/search/cs?searchtype=author&query=Menick%2C+J), [Roman Ring](https://arxiv.org/search/cs?searchtype=author&query=Ring%2C+R), [Tom Hennigan](https://arxiv.org/search/cs?searchtype=author&query=Hennigan%2C+T), [Saffron Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Loren Maggiore](https://arxiv.org/search/cs?searchtype=author&query=Maggiore%2C+L), [Chris Jones](https://arxiv.org/search/cs?searchtype=author&query=Jones%2C+C), [Albin Cassirer](https://arxiv.org/search/cs?searchtype=author&query=Cassirer%2C+A), [Andy Brock](https://arxiv.org/search/cs?searchtype=author&query=Brock%2C+A), [Michela Paganini](https://arxiv.org/search/cs?searchtype=author&query=Paganini%2C+M), [Geoffrey Irving](https://arxiv.org/search/cs?searchtype=author&query=Irving%2C+G), [Oriol Vinyals](https://arxiv.org/search/cs?searchtype=author&query=Vinyals%2C+O), [Simon Osindero](https://arxiv.org/search/cs?searchtype=author&query=Osindero%2C+S), [Karen Simonyan](https://arxiv.org/search/cs?searchtype=author&query=Simonyan%2C+K), [Jack W. Rae](https://arxiv.org/search/cs?searchtype=author&query=Rae%2C+J+W), [Erich Elsen](https://arxiv.org/search/cs?searchtype=author&query=Elsen%2C+E), [Laurent Sifre](https://arxiv.org/search/cs?searchtype=author&query=Sifre%2C+L)

> We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.04426](https://arxiv.org/abs/2112.04426) [cs.CL]** |
|           | (or **[arXiv:2112.04426v1](https://arxiv.org/abs/2112.04426v1) [cs.CL]** for this version) |





# 2021-12-8

[Return to Index](#Index)



<h2 id="2021-12-8-1">1. CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification
</h2>

Title: [CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification](https://arxiv.org/abs/2112.03562)

Authors: [Huidong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H) (1), [Shaoyuan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S) (2), [Jinmiao Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J) (2), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y) (2), [Ning Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+N) (2), [Chien-chih Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C) (2), [Bryan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B) (2), [Yi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y) (2) ((1) Stony Brook University, (2) Amazon Inc.)

> Modern Web systems such as social media and e-commerce contain rich contents expressed in images and text. Leveraging information from multi-modalities can improve the performance of machine learning tasks such as classification and recommendation. In this paper, we propose the Cross-Modality Attention Contrastive Language-Image Pre-training (CMA-CLIP), a new framework which unifies two types of cross-modality attentions, sequence-wise attention and modality-wise attention, to effectively fuse information from image and text pairs. The sequence-wise attention enables the framework to capture the fine-grained relationship between image patches and text tokens, while the modality-wise attention weighs each modality by its relevance to the downstream tasks. In addition, by adding task specific modality-wise attentions and multilayer perceptrons, our proposed framework is capable of performing multi-task classification with multi-modalities. 
> We conduct experiments on a Major Retail Website Product Attribute (MRWPA) dataset and two public datasets, Food101 and Fashion-Gen. The results show that CMA-CLIP outperforms the pre-trained and fine-tuned CLIP by an average of 11.9% in recall at the same level of precision on the MRWPA dataset for multi-task classification. It also surpasses the state-of-the-art method on Fashion-Gen Dataset by 5.5% in accuracy and achieves competitive performance on Food101 Dataset. Through detailed ablation studies, we further demonstrate the effectiveness of both cross-modality attention modules and our method's robustness against noise in image and text inputs, which is a common challenge in practice.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.03562](https://arxiv.org/abs/2112.03562) [cs.CV]** |
|           | (or **[arXiv:2112.03562v1](https://arxiv.org/abs/2112.03562v1) [cs.CV]** for this version) |





<h2 id="2021-12-8-2">2. Grounded Language-Image Pre-training
</h2>

Title: [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)

Authors: [Liunian Harold Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L+H), [Pengchuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Haotian Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Jianwei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J), [Chunyuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Yiwu Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+Y), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Lu Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+L), [Lei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Jenq-Neng Hwang](https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+J), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J)

> This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code will be released at [this https URL](https://github.com/microsoft/GLIP).

| Comments: | Code will be released at [this https URL](https://github.com/microsoft/GLIP) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM) |
| Cite as:  | **[arXiv:2112.03857](https://arxiv.org/abs/2112.03857) [cs.CV]** |
|           | (or **[arXiv:2112.03857v1](https://arxiv.org/abs/2112.03857v1) [cs.CV]** for this version) |





<h2 id="2021-12-8-3">3. Parsing with Pretrained Language Models, Multiple Datasets, and Dataset Embeddings
</h2>

Title: [Parsing with Pretrained Language Models, Multiple Datasets, and Dataset Embeddings](https://arxiv.org/abs/2112.03625)

Authors: [Rob van der Goot](https://arxiv.org/search/cs?searchtype=author&query=van+der+Goot%2C+R), [Miryam de Lhoneux](https://arxiv.org/search/cs?searchtype=author&query=de+Lhoneux%2C+M)

> With an increase of dataset availability, the potential for learning from a variety of data sources has increased. One particular method to improve learning from multiple data sources is to embed the data source during training. This allows the model to learn generalizable features as well as distinguishing features between datasets. However, these dataset embeddings have mostly been used before contextualized transformer-based embeddings were introduced in the field of Natural Language Processing. In this work, we compare two methods to embed datasets in a transformer-based multilingual dependency parser, and perform an extensive evaluation. We show that: 1) embedding the dataset is still beneficial with these models 2) performance increases are highest when embedding the dataset at the encoder level 3) unsurprisingly, we confirm that performance increases are highest for small datasets and datasets with a low baseline score. 4) we show that training on the combination of all datasets performs similarly to designing smaller clusters based on language-relatedness.

| Comments: | Accepted to TLT at SyntaxFest 2021                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2112.03625](https://arxiv.org/abs/2112.03625) [cs.CL]** |
|           | (or **[arXiv:2112.03625v1](https://arxiv.org/abs/2112.03625v1) [cs.CL]** for this version) |





<h2 id="2021-12-8-4">4. Natural Answer Generation: From Factoid Answer to Full-length Answer using Grammar Correction
</h2>

Title: [Natural Answer Generation: From Factoid Answer to Full-length Answer using Grammar Correction](https://arxiv.org/abs/2112.03849)

Authors: [Manas Jain](https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+M), [Sriparna Saha](https://arxiv.org/search/cs?searchtype=author&query=Saha%2C+S), [Pushpak Bhattacharyya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharyya%2C+P), [Gladvin Chinnadurai](https://arxiv.org/search/cs?searchtype=author&query=Chinnadurai%2C+G), [Manish Kumar Vatsa](https://arxiv.org/search/cs?searchtype=author&query=Vatsa%2C+M+K)

> Question Answering systems these days typically use template-based language generation. Though adequate for a domain-specific task, these systems are too restrictive and predefined for domain-independent systems. This paper proposes a system that outputs a full-length answer given a question and the extracted factoid answer (short spans such as named entities) as the input. Our system uses constituency and dependency parse trees of questions. A transformer-based Grammar Error Correction model GECToR (2020), is used as a post-processing step for better fluency. We compare our system with (i) Modified Pointer Generator (SOTA) and (ii) Fine-tuned DialoGPT for factoid questions. We also test our approach on existential (yes-no) questions with better results. Our model generates accurate and fluent answers than the state-of-the-art (SOTA) approaches. The evaluation is done on NewsQA and SqUAD datasets with an increment of 0.4 and 0.9 percentage points in ROUGE-1 score respectively. Also the inference time is reduced by 85\% as compared to the SOTA. The improved datasets used for our evaluation will be released as part of the research contribution.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.03849](https://arxiv.org/abs/2112.03849) [cs.CL]** |
|           | (or **[arXiv:2112.03849v1](https://arxiv.org/abs/2112.03849v1) [cs.CL]** for this version) |





# 2021-12-7

[Return to Index](#Index)



<h2 id="2021-12-7-1">1. Legal Document Retrieval using Document Vector Embeddings and Deep Learning
</h2>

Title: [Legal Document Retrieval using Document Vector Embeddings and Deep Learning](https://arxiv.org/abs/1805.10685)

Authors: [Keet Sugathadasa](https://arxiv.org/search/cs?searchtype=author&query=Sugathadasa%2C+K), [Buddhi Ayesha](https://arxiv.org/search/cs?searchtype=author&query=Ayesha%2C+B), [Nisansa de Silva](https://arxiv.org/search/cs?searchtype=author&query=de+Silva%2C+N), [Amal Shehan Perera](https://arxiv.org/search/cs?searchtype=author&query=Perera%2C+A+S), [Vindula Jayawardana](https://arxiv.org/search/cs?searchtype=author&query=Jayawardana%2C+V), [Dimuthu Lakmal](https://arxiv.org/search/cs?searchtype=author&query=Lakmal%2C+D), [Madhavi Perera](https://arxiv.org/search/cs?searchtype=author&query=Perera%2C+M)

> Domain specific information retrieval process has been a prominent and ongoing research in the field of natural language processing. Many researchers have incorporated different techniques to overcome the technical and domain specificity and provide a mature model for various domains of interest. The main bottleneck in these studies is the heavy coupling of domain experts, that makes the entire process to be time consuming and cumbersome. In this study, we have developed three novel models which are compared against a golden standard generated via the on line repositories provided, specifically for the legal domain. The three different models incorporated vector space representations of the legal domain, where document vector generation was done in two different mechanisms and as an ensemble of the above two. This study contains the research being carried out in the process of representing legal case documents into different vector spaces, whilst incorporating semantic word measures and natural language processing techniques. The ensemble model built in this study, shows a significantly higher accuracy level, which indeed proves the need for incorporation of domain specific semantic similarity measures into the information retrieval process. This study also shows, the impact of varying distribution of the word similarity measures, against varying document vector dimensions, which can lead to improvements in the process of legal information retrieval.

| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:1805.10685](https://arxiv.org/abs/1805.10685) [cs.IR]** |
|           | (or **[arXiv:1805.10685v1](https://arxiv.org/abs/1805.10685v1) [cs.IR]** for this version) |





<h2 id="2021-12-7-2">2. VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts
</h2>

Title: [VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts](https://arxiv.org/abs/2112.02399)

Authors: [Renrui Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Longtian Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+L), [Wei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Ziyao Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+Z)

> Contrastive Vision-Language Pre-training (CLIP) has drown increasing attention recently for its transferable visual representation learning. Supervised by large-scale image-text pairs, CLIP is able to align paired images and texts and thus conduct zero-shot recognition in open-vocabulary scenarios. However, there exists semantic gap between the specific application and generally pre-trained knowledge, which makes the matching sub-optimal on downstream tasks. In this paper, we propose VT-CLIP to enhance vision-language modeling via visual-guided texts. Specifically, we guide the text feature to adaptively explore informative regions on the image and aggregate the visual feature by cross-attention machanism. In this way, the visual-guided text become more semantically correlated with the image, which greatly benefits the matching process. In few-shot settings, we evaluate our VT-CLIP on 11 well-known classification datasets and experiment extensive ablation studies to demonstrate the effectiveness of VT-CLIP. The code will be released soon.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.02399](https://arxiv.org/abs/2112.02399) [cs.CV]** |
|           | (or **[arXiv:2112.02399v1](https://arxiv.org/abs/2112.02399v1) [cs.CV]** for this version) |





<h2 id="2021-12-7-3">3. VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning
</h2>

Title: [VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning](https://arxiv.org/abs/2112.02650)

Authors: [Qibin Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q), [Jeremy Lacomis](https://arxiv.org/search/cs?searchtype=author&query=Lacomis%2C+J), [Edward J. Schwartz](https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+E+J), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Bogdan Vasilescu](https://arxiv.org/search/cs?searchtype=author&query=Vasilescu%2C+B), [Claire Le Goues](https://arxiv.org/search/cs?searchtype=author&query=Goues%2C+C+L)

> Variable names are critical for conveying intended program behavior. Machine learning-based program analysis methods use variable name representations for a wide range of tasks, such as suggesting new variable names and bug detection. Ideally, such methods could capture semantic relationships between names beyond syntactic similarity, e.g., the fact that the names average and mean are similar. Unfortunately, previous work has found that even the best of previous representation approaches primarily capture relatedness (whether two variables are linked at all), rather than similarity (whether they actually have the same meaning). 
> We propose VarCLR, a new approach for learning semantic representations of variable names that effectively captures variable similarity in this stricter sense. We observe that this problem is an excellent fit for contrastive learning, which aims to minimize the distance between explicitly similar inputs, while maximizing the distance between dissimilar inputs. This requires labeled training data, and thus we construct a novel, weakly-supervised variable renaming dataset mined from GitHub edits. We show that VarCLR enables the effective application of sophisticated, general-purpose language models like BERT, to variable name representation and thus also to related downstream tasks like variable name similarity search or spelling correction. VarCLR produces models that significantly outperform the state-of-the-art on IdBench, an existing benchmark that explicitly captures variable similarity (as distinct from relatedness). Finally, we contribute a release of all data, code, and pre-trained models, aiming to provide a drop-in replacement for variable representations used in either existing or future program analyses that rely on variable names.

| Comments: | Accepted by ICSE 2022                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Software Engineering (cs.SE)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Programming Languages (cs.PL) |
| Cite as:  | **[arXiv:2112.02650](https://arxiv.org/abs/2112.02650) [cs.SE]** |
|           | (or **[arXiv:2112.02650v1](https://arxiv.org/abs/2112.02650v1) [cs.SE]** for this version) |





<h2 id="2021-12-7-4">4. Embedding Arithmetic for Text-driven Image Transformation
</h2>

Title: [Embedding Arithmetic for Text-driven Image Transformation](https://arxiv.org/abs/2112.03162)

Authors: [Guillaume Couairon](https://arxiv.org/search/cs?searchtype=author&query=Couairon%2C+G), [Matthieu Cord](https://arxiv.org/search/cs?searchtype=author&query=Cord%2C+M), [Matthijs Douze](https://arxiv.org/search/cs?searchtype=author&query=Douze%2C+M), [Holger Schwenk](https://arxiv.org/search/cs?searchtype=author&query=Schwenk%2C+H)

> Latent text representations exhibit geometric regularities, such as the famous analogy: queen is to king what woman is to man. Such structured semantic relations were not demonstrated on image representations. Recent works aiming at bridging this semantic gap embed images and text into a multimodal space, enabling the transfer of text-defined transformations to the image modality. 
> We introduce the SIMAT dataset to evaluate the task of text-driven image transformation. SIMAT contains 6k images and 18k "transformation queries" that aim at either replacing scene elements or changing their pairwise relationships. The goal is to retrieve an image consistent with the (source image, transformation) query. We use an image/text matching oracle (OSCAR) to assess whether the image transformation is successful. The SIMAT dataset will be publicly available. 
> We use SIMAT to show that vanilla CLIP multimodal embeddings are not very well suited for text-driven image transformation, but that a simple finetuning on the COCO dataset can bring dramatic improvements. We also study whether it is beneficial to leverage the geometric properties of pretrained universal sentence encoders (FastText, LASER and LaBSE).

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.03162](https://arxiv.org/abs/2112.03162) [cs.CV]** |
|           | (or **[arXiv:2112.03162v1](https://arxiv.org/abs/2112.03162v1) [cs.CV]** for this version) |





<h2 id="2021-12-7-5">5. Text2Mesh: Text-Driven Neural Stylization for Meshes
</h2>

Title: [Text2Mesh: Text-Driven Neural Stylization for Meshes](https://arxiv.org/abs/2112.03221)

Authors: [Oscar Michel](https://arxiv.org/search/cs?searchtype=author&query=Michel%2C+O), [Roi Bar-On](https://arxiv.org/search/cs?searchtype=author&query=Bar-On%2C+R), [Richard Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+R), [Sagie Benaim](https://arxiv.org/search/cs?searchtype=author&query=Benaim%2C+S), [Rana Hanocka](https://arxiv.org/search/cs?searchtype=author&query=Hanocka%2C+R)

> In this work, we develop intuitive controls for editing the style of 3D objects. Our framework, Text2Mesh, stylizes a 3D mesh by predicting color and local geometric details which conform to a target text prompt. We consider a disentangled representation of a 3D object using a fixed mesh input (content) coupled with a learned neural network, which we term neural style field network. In order to modify style, we obtain a similarity score between a text prompt (describing style) and a stylized mesh by harnessing the representational power of CLIP. Text2Mesh requires neither a pre-trained generative model nor a specialized 3D mesh dataset. It can handle low-quality meshes (non-manifold, boundaries, etc.) with arbitrary genus, and does not require UV parameterization. We demonstrate the ability of our technique to synthesize a myriad of styles over a wide variety of 3D meshes.

| Comments: | project page: [this https URL](https://threedle.github.io/text2mesh/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Graphics (cs.GR) |
| Cite as:  | **[arXiv:2112.03221](https://arxiv.org/abs/2112.03221) [cs.CV]** |
|           | (or **[arXiv:2112.03221v1](https://arxiv.org/abs/2112.03221v1) [cs.CV]** for this version) |





<h2 id="2021-12-7-6">6. CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks
</h2>

Title: [CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks](https://arxiv.org/abs/2112.02714)

Authors: [Zixuan Ke](https://arxiv.org/search/cs?searchtype=author&query=Ke%2C+Z), [Bing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B), [Hu Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Lei Shu](https://arxiv.org/search/cs?searchtype=author&query=Shu%2C+L)

> This paper studies continual learning (CL) of a sequence of aspect sentiment classification(ASC) tasks in a particular CL setting called domain incremental learning (DIL). Each task is from a different domain or product. The DIL setting is particularly suited to ASC because in testing the system needs not know the task/domain to which the test data belongs. To our knowledge, this setting has not been studied before for ASC. This paper proposes a novel model called CLASSIC. The key novelty is a contrastive continual learning method that enables both knowledge transfer across tasks and knowledge distillation from old tasks to the new task, which eliminates the need for task ids in testing. Experimental results show the high effectiveness of CLASSIC.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | EMNLP 2021                                                   |
| Cite as:           | **[arXiv:2112.02714](https://arxiv.org/abs/2112.02714) [cs.CL]** |
|                    | (or **[arXiv:2112.02714v1](https://arxiv.org/abs/2112.02714v1) [cs.CL]** for this version) |





<h2 id="2021-12-7-7">7. Towards More Robust Natural Language Understanding
</h2>

Title: [Towards More Robust Natural Language Understanding](https://arxiv.org/abs/2112.02992)

Authors: [Xinliang Frederick Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X+F)

> Natural Language Understanding (NLU) is a branch of Natural Language Processing (NLP) that uses intelligent computer software to understand texts that encode human knowledge. Recent years have witnessed notable progress across various NLU tasks with deep learning techniques, especially with pretrained language models. Besides proposing more advanced model architectures, constructing more reliable and trustworthy datasets also plays a huge role in improving NLU systems, without which it would be impossible to train a decent NLU model. It's worth noting that the human ability of understanding natural language is flexible and robust. On the contrary, most of existing NLU systems fail to achieve desirable performance on out-of-domain data or struggle on handling challenging items (e.g., inherently ambiguous items, adversarial items) in the real world. Therefore, in order to have NLU models understand human language more effectively, it is expected to prioritize the study on robust natural language understanding. In this thesis, we deem that NLU systems are consisting of two components: NLU models and NLU datasets. As such, we argue that, to achieve robust NLU, the model architecture/training and the dataset are equally important. Specifically, we will focus on three NLU tasks to illustrate the robustness problem in different NLU tasks and our contributions (i.e., novel models and new datasets) to help achieve more robust natural language understanding. Moving forward, the ultimate goal for robust natural language understanding is to build NLU models which can behave humanly. That is, it's expected that robust NLU systems are capable to transfer the knowledge from training corpus to unseen documents more reliably and survive when encountering challenging items even if the system doesn't know a priori of users' inputs.

| Comments: | Undergraduate Research Thesis, The Ohio State University     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2112.02992](https://arxiv.org/abs/2112.02992) [cs.CL]** |
|           | (or **[arXiv:2112.02992v1](https://arxiv.org/abs/2112.02992v1) [cs.CL]** for this version) |





<h2 id="2021-12-7-8">8. Quantifying Adaptability in Pre-trained Language Models with 500 Tasks
</h2>

Title: [Quantifying Adaptability in Pre-trained Language Models with 500 Tasks](https://arxiv.org/abs/2112.03204)

Authors: [Belinda Z. Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B+Z), [Jane Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J), [Madian Khabsa](https://arxiv.org/search/cs?searchtype=author&query=Khabsa%2C+M), [Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&query=Zettlemoyer%2C+L), [Alon Halevy](https://arxiv.org/search/cs?searchtype=author&query=Halevy%2C+A), [Jacob Andreas](https://arxiv.org/search/cs?searchtype=author&query=Andreas%2C+J)

> When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge. Using TaskBench500, we evaluate three facets of adaptability, finding that: (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels. Our experiments show that adaptability to new tasks, like generalization to new examples, can be systematically described and understood, and we conclude with a discussion of additional aspects of adaptability that could be studied using the new benchmark.

| Comments: | 18 pages, 5 figures, 8 tables                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2112.03204](https://arxiv.org/abs/2112.03204) [cs.CL]** |
|           | (or **[arXiv:2112.03204v1](https://arxiv.org/abs/2112.03204v1) [cs.CL]** for this version) |







# 2021-12-6

[Return to Index](#Index)



<h2 id="2021-12-6-1">1. Linear algebra with transformers
</h2>

Title: [Linear algebra with transformers](https://arxiv.org/abs/2112.01898)

Authors: [François Charton](https://arxiv.org/search/cs?searchtype=author&query=Charton%2C+F)

> Most applications of transformers to mathematics, from integration to theorem proving, focus on symbolic computation. In this paper, we show that transformers can be trained to perform numerical calculations with high accuracy. We consider problems of linear algebra: matrix transposition, addition, multiplication, eigenvalues and vectors, singular value decomposition, and inversion. Training small transformers (up to six layers) over datasets of random matrices, we achieve high accuracies (over 90%) on all problems. We also show that trained models can generalize out of their training distribution, and that out-of-domain accuracy can be greatly improved by working from more diverse datasets (in particular, by training from matrices with non-independent and identically distributed coefficients). Finally, we show that few-shot learning can be leveraged to re-train models to solve larger problems.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.01898](https://arxiv.org/abs/2112.01898) [cs.LG]** |
|           | (or **[arXiv:2112.01898v1](https://arxiv.org/abs/2112.01898v1) [cs.LG]** for this version) |





<h2 id="2021-12-6-2">2. Multitask Finetuning for Improving Neural Machine Translation in Indian Languages
</h2>

Title: [Multitask Finetuning for Improving Neural Machine Translation in Indian Languages](https://arxiv.org/abs/2112.01742)

Authors: [Shaily Desai](https://arxiv.org/search/cs?searchtype=author&query=Desai%2C+S), [Atharva Kshirsagar](https://arxiv.org/search/cs?searchtype=author&query=Kshirsagar%2C+A), [Manisha Marathe](https://arxiv.org/search/cs?searchtype=author&query=Marathe%2C+M)

> Transformer based language models have led to impressive results across all domains in Natural Language Processing. Pretraining these models on language modeling tasks and finetuning them on downstream tasks such as Text Classification, Question Answering and Neural Machine Translation has consistently shown exemplary results. In this work, we propose a Multitask Finetuning methodology which combines the Bilingual Machine Translation task with an auxiliary Causal Language Modeling task to improve performance on the former task on Indian Languages. We conduct an empirical study on three language pairs, Marathi-Hindi, Marathi-English and Hindi-English, where we compare the multitask finetuning approach to the standard finetuning approach, for which we use the mBART50 model. Our study indicates that the multitask finetuning method could be a better technique than standard finetuning, and could improve Bilingual Machine Translation across language pairs.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.01742](https://arxiv.org/abs/2112.01742) [cs.CL]** |
|           | (or **[arXiv:2112.01742v1](https://arxiv.org/abs/2112.01742v1) [cs.CL]** for this version) |





<h2 id="2021-12-6-3">3. Translating Politeness Across Cultures: Case of Hindi and English
</h2>

Title: [Translating Politeness Across Cultures: Case of Hindi and English](https://arxiv.org/abs/2112.01822)

Authors: [Ritesh Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+R), [Girish Nath Jha](https://arxiv.org/search/cs?searchtype=author&query=Jha%2C+G+N)

> In this paper, we present a corpus based study of politeness across two languages-English and Hindi. It studies the politeness in a translated parallel corpus of Hindi and English and sees how politeness in a Hindi text is translated into English. We provide a detailed theoretical background in which the comparison is carried out, followed by a brief description of the translated data within this theoretical model. Since politeness may become one of the major reasons of conflict and misunderstanding, it is a very important phenomenon to be studied and understood cross-culturally, particularly for such purposes as machine translation.

| Subjects:          | **Computation and Language (cs.CL)**                         |
| ------------------ | ------------------------------------------------------------ |
| Journal reference: | Proceedings of the 3rd ACM International Conference on Inter-Cultural Collaboration (ICIC-2010), Copenhagen Business School, Denmark, pp. 175-178, 2010 |
| Cite as:           | **[arXiv:2112.01822](https://arxiv.org/abs/2112.01822) [cs.CL]** |
|                    | (or **[arXiv:2112.01822v1](https://arxiv.org/abs/2112.01822v1) [cs.CL]** for this version) |





<h2 id="2021-12-6-4">4. Semantic Segmentation of Legal Documents via Rhetorical Roles
</h2>

Title: [Semantic Segmentation of Legal Documents via Rhetorical Roles](https://arxiv.org/abs/2112.01836)

Authors: [Vijit Malik](https://arxiv.org/search/cs?searchtype=author&query=Malik%2C+V), [Rishabh Sanjay](https://arxiv.org/search/cs?searchtype=author&query=Sanjay%2C+R), [Shouvik Kumar Guha](https://arxiv.org/search/cs?searchtype=author&query=Guha%2C+S+K), [Shubham Kumar Nigam](https://arxiv.org/search/cs?searchtype=author&query=Nigam%2C+S+K), [Angshuman Hazarika](https://arxiv.org/search/cs?searchtype=author&query=Hazarika%2C+A), [Arnab Bhattacharya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharya%2C+A), [Ashutosh Modi](https://arxiv.org/search/cs?searchtype=author&query=Modi%2C+A)

> Legal documents are unstructured, use legal jargon, and have considerable length, making it difficult to process automatically via conventional text processing techniques. A legal document processing system would benefit substantially if the documents could be semantically segmented into coherent units of information. This paper proposes a Rhetorical Roles (RR) system for segmenting a legal document into semantically coherent units: facts, arguments, statute, issue, precedent, ruling, and ratio. With the help of legal experts, we propose a set of 13 fine-grained rhetorical role labels and create a new corpus of legal documents annotated with the proposed RR. We develop a system for segmenting a document into rhetorical role units. In particular, we develop a multitask learning-based deep learning model with document rhetorical role label shift as an auxiliary task for segmenting a legal document. We experiment extensively with various deep learning models for predicting rhetorical roles in a document, and the proposed model shows superior performance over the existing models. Further, we apply RR for predicting the judgment of legal cases and show that the use of RR enhances the prediction compared to the transformer-based models.

| Comments: | 16 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2112.01836](https://arxiv.org/abs/2112.01836) [cs.CL]** |
|           | (or **[arXiv:2112.01836v1](https://arxiv.org/abs/2112.01836v1) [cs.CL]** for this version) |





<h2 id="2021-12-6-5">5. A Proposal of Automatic Error Correction in Text
</h2>

Title: [A Proposal of Automatic Error Correction in Text](https://arxiv.org/abs/2112.01846)

Authors: [Wulfrano A. Luna-Ramírez](https://arxiv.org/search/cs?searchtype=author&query=Luna-Ramírez%2C+W+A), [Carlos R. Jaimez-González](https://arxiv.org/search/cs?searchtype=author&query=Jaimez-González%2C+C+R)

> The great amount of information that can be stored in electronic media is growing up daily. Many of them is got mainly by typing, such as the huge of information obtained from web 2.0 sites; or scaned and processing by an Optical Character Recognition software, like the texts of libraries and goverment offices. Both processes introduce error in texts, so it is difficult to use the data for other purposes than just to read it, i.e. the processing of those texts by other applications like e-learning, learning of languages, electronic tutorials, data minning, information retrieval and even more specialized systems such as tiflologic software, specifically blinded people-oriented applications like automatic reading, where the text would be error free as possible in order to make easier the text to speech task, and so on. In this paper it is showed an application of automatic recognition and correction of ortographic errors in electronic texts. This task is composed of three stages: a) error detection; b) candidate corrections generation; and c) correction -selection of the best candidate. The proposal is based in part of speech text categorization, word similarity, word diccionaries, statistical measures, morphologic analisys and n-grams based language model of Spanish.

| Comments:          | 15 pages, 3 figures, 11 tables, 1 algorithm. Formerly published on Journal of Research in Computer Science - Intl Conference on Computer CORE2012 |
| ------------------ | ------------------------------------------------------------ |
| Subjects:          | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Journal reference: | Luna-Ramírez, A., Jaimez-González, C. R. (2012). A Proposal of Automatic Error Correction in Text. Revista Research in Computing Science: Advances in Computing Science, Vol. 58, pp. 323-337. ISSN: 1870-4069 |
| Cite as:           | **[arXiv:2112.01846](https://arxiv.org/abs/2112.01846) [cs.CL]** |
|                    | (or **[arXiv:2112.01846v1](https://arxiv.org/abs/2112.01846v1) [cs.CL]** for this version) |






# 2021-12-3

[Return to Index](#Index)



<h2 id="2021-12-3-1">1. Consensus Graph Representation Learning for Better Grounded Image Captioning
</h2>

Title: [Consensus Graph Representation Learning for Better Grounded Image Captioning](https://arxiv.org/abs/2112.00974)

Authors: [Wenqiao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W), [Haochen Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+H), [Siliang Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+S), [Jun Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+J), [Qiang Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Q), [Yueting Zhuang](https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Y)

> The contemporary visual captioning models frequently hallucinate objects that are not actually in a scene, due to the visual misclassification or over-reliance on priors that resulting in the semantic inconsistency between the visual information and the target lexical words. The most common way is to encourage the captioning model to dynamically link generated object words or phrases to appropriate regions of the image, i.e., the grounded image captioning (GIC). However, GIC utilizes an auxiliary task (grounding objects) that has not solved the key issue of object hallucination, i.e., the semantic inconsistency. In this paper, we take a novel perspective on the issue above - exploiting the semantic coherency between the visual and language modalities. Specifically, we propose the Consensus Rraph Representation Learning framework (CGRL) for GIC that incorporates a consensus representation into the grounded captioning pipeline. The consensus is learned by aligning the visual graph (e.g., scene graph) to the language graph that consider both the nodes and edges in a graph. With the aligned consensus, the captioning model can capture both the correct linguistic characteristics and visual relevance, and then grounding appropriate image regions further. We validate the effectiveness of our model, with a significant decline in object hallucination (-9% CHAIRi) on the Flickr30k Entities dataset. Besides, our CGRL also evaluated by several automatic metrics and human evaluation, the results indicate that the proposed approach can simultaneously improve the performance of image captioning (+2.9 Cider) and grounding (+2.3 F1LOC).

| Comments: | 9 pages, 5 figures, AAAI 2021                                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2112.00974](https://arxiv.org/abs/2112.00974) [cs.CV]** |
|           | (or **[arXiv:2112.00974v1](https://arxiv.org/abs/2112.00974v1) [cs.CV]** for this version) |





<h2 id="2021-12-3-2">2. A Mixture of Expert Based Deep Neural Network for Improved ASR
</h2>

Title: [A Mixture of Expert Based Deep Neural Network for Improved ASR](https://arxiv.org/abs/2112.01025)

Authors: [Vishwanath Pratap Singh](https://arxiv.org/search/eess?searchtype=author&query=Singh%2C+V+P), [Shakti P. Rath](https://arxiv.org/search/eess?searchtype=author&query=Rath%2C+S+P), [Abhishek Pandey](https://arxiv.org/search/eess?searchtype=author&query=Pandey%2C+A)

> This paper presents a novel deep learning architecture for acoustic model in the context of Automatic Speech Recognition (ASR), termed as MixNet. Besides the conventional layers, such as fully connected layers in DNN-HMM and memory cells in LSTM-HMM, the model uses two additional layers based on Mixture of Experts (MoE). The first MoE layer operating at the input is based on pre-defined broad phonetic classes and the second layer operating at the penultimate layer is based on automatically learned acoustic classes. In natural speech, overlap in distribution across different acoustic classes is inevitable, which leads to inter-class mis-classification. The ASR accuracy is expected to improve if the conventional architecture of acoustic model is modified to make them more suitable to account for such overlaps. MixNet is developed keeping this in mind. Analysis conducted by means of scatter diagram verifies that MoE indeed improves the separation between classes that translates to better ASR accuracy. Experiments are conducted on a large vocabulary ASR task which show that the proposed architecture provides 13.6% and 10.0% relative reduction in word error rates compared to the conventional models, namely, DNN and LSTM respectively, trained using sMBR criteria. In comparison to an existing method developed for phone-classification (by Eigen et al), our proposed method yields a significant improvement.

| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Sound (cs.SD) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.01025](https://arxiv.org/abs/2112.01025) [eess.AS]** |
|           | (or **[arXiv:2112.01025v1](https://arxiv.org/abs/2112.01025v1) [eess.AS]** for this version) |





<h2 id="2021-12-3-3">3. DenseCLIP: Extract Free Dense Labels from CLIP
</h2>

Title: [DenseCLIP: Extract Free Dense Labels from CLIP](https://arxiv.org/abs/2112.01071)

Authors: [Chong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Chen Change Loy](https://arxiv.org/search/cs?searchtype=author&query=Loy%2C+C+C), [Bo Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+B)

> Contrastive Language-Image Pre-training (CLIP) has made a remarkable breakthrough in open-vocabulary zero-shot image recognition. Many recent studies leverage the pre-trained CLIP models for image-level classification and manipulation. In this paper, we further explore the potentials of CLIP for pixel-level dense prediction, specifically in semantic segmentation. Our method, DenseCLIP, in the absence of annotations and fine-tuning, yields reasonable segmentation results on open concepts across various datasets. By adding pseudo labeling and self-training, DenseCLIP+ surpasses SOTA transductive zero-shot semantic segmentation methods by large margins, e.g., mIoUs of unseen classes on PASCAL VOC/PASCAL Context/COCO Stuff are improved from 35.6/20.7/30.3 to 86.1/66.7/54.7. We also test the robustness of DenseCLIP under input corruption and evaluate its capability in discriminating fine-grained objects and novel concepts. Our finding suggests that DenseCLIP can serve as a new reliable source of supervision for dense prediction tasks to achieve annotation-free segmentation.

| Comments: | Tech report, 12 pages, 6 figures                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2112.01071](https://arxiv.org/abs/2112.01071) [cs.CV]** |
|           | (or **[arXiv:2112.01071v1](https://arxiv.org/abs/2112.01071v1) [cs.CV]** for this version) |






# 2021-12-2

[Return to Index](#Index)



<h2 id="2021-12-2-1">1. CLIPstyler: Image Style Transfer with a Single Text Condition
</h2>

Title: [CLIPstyler: Image Style Transfer with a Single Text Condition](https://arxiv.org/abs/2112.00374)

Authors: [Gihyun Kwon](https://arxiv.org/search/cs?searchtype=author&query=Kwon%2C+G), [Jong Chul Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+J+C)

> Existing neural style transfer methods require reference style images to transfer texture information of style images to content images. However, in many practical situations, users may not have reference style images but still be interested in transferring styles by just imagining them. In order to deal with such applications, we propose a new framework that enables a style transfer `without' a style image, but only with a text description of the desired style. Using the pre-trained text-image embedding model of CLIP, we demonstrate the modulation of the style of content images only with a single text condition. Specifically, we propose a patch-wise text-image matching loss with multiview augmentations for realistic texture transfer. Extensive experimental results confirmed the successful image style transfer with realistic textures that reflect semantic query texts.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Image and Video Processing (eess.IV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.00374](https://arxiv.org/abs/2112.00374) [cs.CV]** |
|           | (or **[arXiv:2112.00374v1](https://arxiv.org/abs/2112.00374v1) [cs.CV]** for this version) |





<h2 id="2021-12-2-2">2. Translation-equivariant Image Quantizer for Bi-directional Image-Text Generation
</h2>

Title: [Translation-equivariant Image Quantizer for Bi-directional Image-Text Generation](https://arxiv.org/abs/2112.00384)

Authors: [Woncheol Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+W), [Gyubok Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+G), [Jiyoung Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J), [Joonseok Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J), [Edward Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+E)

> Recently, vector-quantized image modeling has demonstrated impressive performance on generation tasks such as text-to-image generation. However, we discover that the current image quantizers do not satisfy translation equivariance in the quantized space due to aliasing, degrading performance in the downstream text-to-image generation and image-to-text generation, even in simple experimental setups. Instead of focusing on anti-aliasing, we take a direct approach to encourage translation equivariance in the quantized space. In particular, we explore a desirable property of image quantizers, called 'Translation Equivariance in the Quantized Space' and propose a simple but effective way to achieve translation equivariance by regularizing orthogonality in the codebook embedding vectors. Using this method, we improve accuracy by +22% in text-to-image generation and +26% in image-to-text generation, outperforming the VQGAN.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.00384](https://arxiv.org/abs/2112.00384) [cs.CV]** |
|           | (or **[arXiv:2112.00384v1](https://arxiv.org/abs/2112.00384v1) [cs.CV]** for this version) |





<h2 id="2021-12-2-3">3. DPRK-BERT: The Supreme Language Model
</h2>

Title: [DPRK-BERT: The Supreme Language Model](https://arxiv.org/abs/2112.00567)

Authors: [Arda Akdemir](https://arxiv.org/search/cs?searchtype=author&query=Akdemir%2C+A), [Yeojoo Jeon](https://arxiv.org/search/cs?searchtype=author&query=Jeon%2C+Y)

> Deep language models have achieved remarkable success in the NLP domain. The standard way to train a deep language model is to employ unsupervised learning from scratch on a large unlabeled corpus. However, such large corpora are only available for widely-adopted and high-resource languages and domains. This study presents the first deep language model, DPRK-BERT, for the DPRK language. We achieve this by compiling the first unlabeled corpus for the DPRK language and fine-tuning a preexisting the ROK language model. We compare the proposed model with existing approaches and show significant improvements on two DPRK datasets. We also present a cross-lingual version of this model which yields better generalization across the two Korean languages. Finally, we provide various NLP tools related to the DPRK language that would foster future research.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2112.00567](https://arxiv.org/abs/2112.00567) [cs.CL]** |
|           | (or **[arXiv:2112.00567v1](https://arxiv.org/abs/2112.00567v1) [cs.CL]** for this version) |





# 2021-12-1

[Return to Index](#Index)



<h2 id="2021-12-1-1">1. Do We Still Need Automatic Speech Recognition for Spoken Language Understanding?
</h2>

Title: [Do We Still Need Automatic Speech Recognition for Spoken Language Understanding?](https://arxiv.org/abs/2111.14842)

Authors: [Lasse Borgholt](https://arxiv.org/search/eess?searchtype=author&query=Borgholt%2C+L), [Jakob Drachmann Havtorn](https://arxiv.org/search/eess?searchtype=author&query=Havtorn%2C+J+D), [Mostafa Abdou](https://arxiv.org/search/eess?searchtype=author&query=Abdou%2C+M), [Joakim Edin](https://arxiv.org/search/eess?searchtype=author&query=Edin%2C+J), [Lars Maaløe](https://arxiv.org/search/eess?searchtype=author&query=Maaløe%2C+L), [Anders Søgaard](https://arxiv.org/search/eess?searchtype=author&query=Søgaard%2C+A), [Christian Igel](https://arxiv.org/search/eess?searchtype=author&query=Igel%2C+C)

> Spoken language understanding (SLU) tasks are usually solved by first transcribing an utterance with automatic speech recognition (ASR) and then feeding the output to a text-based model. Recent advances in self-supervised representation learning for speech data have focused on improving the ASR component. We investigate whether representation learning for speech has matured enough to replace ASR in SLU. We compare learned speech features from wav2vec 2.0, state-of-the-art ASR transcripts, and the ground truth text as input for a novel speech-based named entity recognition task, a cardiac arrest detection task on real-world emergency calls and two existing SLU benchmarks. We show that learned speech features are superior to ASR transcripts on three classification tasks. For machine translation, ASR transcripts are still the better choice. We highlight the intrinsic robustness of wav2vec 2.0 representations to out-of-vocabulary words as key to better performance.

| Comments: | Under review as a conference paper at ICASSP 2022            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Audio and Speech Processing (eess.AS)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2111.14842](https://arxiv.org/abs/2111.14842) [eess.AS]** |
|           | (or **[arXiv:2111.14842v1](https://arxiv.org/abs/2111.14842v1) [eess.AS]** for this version) |





<h2 id="2021-12-1-2">2. Improvement in Machine Translation with Generative Adversarial Networks
</h2>

Title: [Improvement in Machine Translation with Generative Adversarial Networks](https://arxiv.org/abs/2111.15166)

Authors: [Jay Ahn](https://arxiv.org/search/cs?searchtype=author&query=Ahn%2C+J), [Hari Madhu](https://arxiv.org/search/cs?searchtype=author&query=Madhu%2C+H), [Viet Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+V)

> In this paper, we explore machine translation improvement via Generative Adversarial Network (GAN) architecture. We take inspiration from RelGAN, a model for text generation, and NMT-GAN, an adversarial machine translation model, to implement a model that learns to transform awkward, non-fluent English sentences to fluent ones, while only being trained on monolingual corpora. We utilize a parameter λ to control the amount of deviation from the input sentence, i.e. a trade-off between keeping the original tokens and modifying it to be more fluent. Our results improved upon phrase-based machine translation in some cases. Especially, GAN with a transformer generator shows some promising results. We suggests some directions for future works to build upon this proof-of-concept.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.15166](https://arxiv.org/abs/2111.15166) [cs.CL]** |
|           | (or **[arXiv:2111.15166v1](https://arxiv.org/abs/2111.15166v1) [cs.CL]** for this version) |





<h2 id="2021-12-1-3">3. Pureformer: Do We Even Need Attention?
</h2>

Title: [Pureformer: Do We Even Need Attention?](https://arxiv.org/abs/2111.15588)

Authors: [Uladzislau Yorsh](https://arxiv.org/search/cs?searchtype=author&query=Yorsh%2C+U), [Alexander Kovalenko](https://arxiv.org/search/cs?searchtype=author&query=Kovalenko%2C+A)

> In this paper we propose that the dot product pairwise matching attention layer, which is widely used in transformer-based models, is redundant for the model performance. Attention in its original formulation has to be seen rather as a human-level tool to explore and/or visualize relevancy scores in the sequences. Instead, we present a simple and fast alternative without any approximation that, to the best of our knowledge, outperforms existing attention approximations on the text classification task from the Long-Range Arena benchmark.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2111.15588](https://arxiv.org/abs/2111.15588) [cs.CL]** |
|           | (or **[arXiv:2111.15588v1](https://arxiv.org/abs/2111.15588v1) [cs.CL]** for this version) |


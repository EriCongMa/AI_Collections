# MA C.'s Daily Paper Of Interest - February, 2022

# Index

- [2022-02-28](#2022-02-28)
  - [1. Screening Gender Transfer in Neural Machine Translation](#2022-02-28-1)
  - [2. A Survey of Multilingual Models for Automatic Speech Recognition](#2022-02-28-2)
  - [3. The Reality of Multi-Lingual Machine Translation](#2022-02-28-3)
  
- [2022-02-25](#2022-02-25)
  - [1. Overcoming a Theoretical Limitation of Self-Attention](#2022-02-25-1)
  - [2. Using natural language prompts for machine translation](#2022-02-25-2)
  - [3. Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words](#2022-02-25-3)

- [2022-02-24](#2022-02-24)
  - [1. Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt](#2022-02-24-1)
  - [2. Refining the state-of-the-art in Machine Translation, optimizing NMT for the JA <-> EN language pair by leveraging personal domain expertise](#2022-02-24-2)

- [2022-02-23](#2022-02-23)
  - [1. CaMEL: Mean Teacher Learning for Image Captioning](#2022-02-23-1)
  - [2. A Survey of Vision-Language Pre-Trained Models](#2022-02-23-2)
  - [3. An Overview on Machine Translation Evaluation](#2022-02-23-3)
- [2022-02-22](#2022-02-22)
  - [1. PETCI: A Parallel English Translation Dataset of Chinese Idioms](#2022-02-22-1)
  - [2. CALCS 2021 Shared Task: Machine Translation for Code-Switched Data](#2022-02-22-2)
  - [3. Punctuation Restoration](#2022-02-22-3)
  - [4. -Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning](#2022-02-22-4)
  - [5. USCORE: An Effective Approach to Fully Unsupervised Evaluation Metrics for Machine Translation](#2022-02-22-5)
  - [6. BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models](#2022-02-22-6)
  - [7. Domain Adaptation in Neural Machine Translation using a Qualia-Enriched FrameNet](#2022-02-22-7)
  - [8. Interpreting Language Models with Contrastive Explanations](#2022-02-22-8)
- [2022-02-21](#2022-02-21)
  - [1. Improving English to Sinhala Neural Machine Translation using Part-of-Speech Tag](#2022-02-21-1)
  - [2. VLP: A Survey on Vision-Language Pre-training](#2022-02-21-2)
- [2022-02-18](#2022-02-18)
  - [1. End-to-End Training of Both Translation Models in the Back-Translation Framework](#2022-02-18-1)
  - [2. cosFormer: Rethinking Softmax in Attention](#2022-02-18-2)


- [2022-02-17](#2022-02-17)

  - [1. On the Self Shuffle Language](#2022-02-17-1)
  - [2. ZeroGen: Efficient Zero-shot Learning via Dataset Generation](#2022-02-17-2)
  - [3. Revisiting Parameter-Efficient Tuning: Are We Really There Yet?](#2022-02-17-3)
  - [4. Should You Mask 15% in Masked Language Modeling?](#2022-02-17-4)
- [2022-02-16](#2022-02-16)

  - [1. A Survey on Dynamic Neural Networks for Natural Language Processing](#2022-02-16-1)
  - [2. A Survey on Model Compression for Natural Language Processing](#2022-02-16-2)
  - [3. MuLD: The Multitask Long Document Benchmark](#2022-02-16-3)
  - [4. BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer](#2022-02-16-4)
  - [5. Delving Deeper into Cross-lingual Visual Question Answering](#2022-02-16-5)

- [2022-02-15](#2022-02-15)

  - [1. https://arxiv.org/abs/2202.06045](#2022-02-15-1)
  - [2. A Contrastive Framework for Neural Text Generation](#2022-02-15-2)
  - [3. I-Tuning: Tuning Language Models with Image for Caption Generation](#2022-02-15-3)

- [2022-02-14](#2022-02-14)

  - [1. Including Facial Expressions in Contextual Embeddings for Sign Language Generation](#2022-02-14-1)
  - [2. Evaluating MT Systems: A Theoretical Framework](#2022-02-14-2)

- [2022-02-11](#2022-02-11)
  - [1. SHAS: Approaching optimal Segmentation for End-to-End Speech Translation](#2022-02-11-1)
  - [2. AdaPrompt: Adaptive Model Training for Prompt-based NLP](#2022-02-11-2)
  - [3. Slovene SuperGLUE Benchmark: Translation and Evaluation](#2022-02-11-3)
  - [4. Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding](#2022-02-11-4)
- [2022-02-10](#2022-02-10)

  - [1. Machine Explanations and Human Understanding](#2022-02-10-1)
  - [2. Image Difference Captioning with Pre-training and Contrastive Learning](#2022-02-10-2)
  - [3. Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models](#2022-02-10-3)
  - [4. pNLP-Mixer: an Efficient all-MLP Architecture for Language](#2022-02-10-4)
  - [5. Generating Training Data with Language Models: Towards Zero-Shot Language Understanding](#2022-02-10-5)
- [2022-02-09](#2022-02-09)

  - [1. DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers](#2022-02-09-1)
- [2022-02-08](#2022-02-08)

  - [1. Machine Translation from Signed to Spoken Languages: State of the Art and Challenges](#2022-02-08-1)
  - [2. Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition](#2022-02-08-2)
  - [3. Red Teaming Language Models with Language Models](#2022-02-08-3)
- [2022-02-07](#2022-02-07)

  - [1. Data Scaling Laws in NMT: The Effect of Noise and Architecture](#2022-02-07-1)
  - [2. Temporal Attention for Language Models](#2022-02-07-2)
  - [3. The Ecological Footprint of Neural Machine Translation Systems](#2022-02-07-3)
- [2022-01-28](#2022-01-28)
  - [1. Tackling data scarcity in speech translation using zero-shot multilingual machine translation techniques](#2022-01-28-1)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MAC-Daily-POI-index.md)



# 2022-02-28

[Return to Index](#Index)



<h2 id="2022-02-28-1">1. Screening Gender Transfer in Neural Machine Translation
</h2>

Title: [Screening Gender Transfer in Neural Machine Translation](https://arxiv.org/abs/2202.12568)

Authors: [Guillaume Wisniewski](https://arxiv.org/search/cs?searchtype=author&query=Wisniewski%2C+G), [Lichao Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+L), [Nicolas Ballier](https://arxiv.org/search/cs?searchtype=author&query=Ballier%2C+N), [François Yvon](https://arxiv.org/search/cs?searchtype=author&query=Yvon%2C+F)

> This paper aims at identifying the information flow in state-of-the-art machine translation systems, taking as example the transfer of gender when translating from French into English. Using a controlled set of examples, we experiment several ways to investigate how gender information circulates in a encoder-decoder architecture considering both probing techniques as well as interventions on the internal representations used in the MT system. Our results show that gender information can be found in all token representations built by the encoder and the decoder and lead us to conclude that there are multiple pathways for gender transfer.

| Comments:    | Accepted at BlackBoxNLP'2021                                 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| Cite as:     | **[arXiv:2202.12568](https://arxiv.org/abs/2202.12568) [cs.CL]** |
|              | (or **[arXiv:2202.12568v1](https://arxiv.org/abs/2202.12568v1) [cs.CL]** for this version) |
| Related DOI: | https://doi.org/10.18653/v1/2021.blackboxnlp-1.24Focus to learn more |





<h2 id="2022-02-28-2">2. A Survey of Multilingual Models for Automatic Speech Recognition
</h2>

Title: [A Survey of Multilingual Models for Automatic Speech Recognition](https://arxiv.org/abs/2202.12576)

Authors: [Hemant Yadav](https://arxiv.org/search/cs?searchtype=author&query=Yadav%2C+H), [Sunayana Sitaram](https://arxiv.org/search/cs?searchtype=author&query=Sitaram%2C+S)

> Although Automatic Speech Recognition (ASR) systems have achieved human-like performance for a few languages, the majority of the world's languages do not have usable systems due to the lack of large speech datasets to train these models. Cross-lingual transfer is an attractive solution to this problem, because low-resource languages can potentially benefit from higher-resource languages either through transfer learning, or being jointly trained in the same multilingual model. The problem of cross-lingual transfer has been well studied in ASR, however, recent advances in Self Supervised Learning are opening up avenues for unlabeled speech data to be used in multilingual ASR models, which can pave the way for improved performance on low-resource languages. In this paper, we survey the state of the art in multilingual ASR models that are built with cross-lingual transfer in mind. We present best practices for building multilingual models from research across diverse languages and techniques, discuss open questions and provide recommendations for future work.

| Comments: | 9 pages. Submitted to LREC 2022                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2202.12576](https://arxiv.org/abs/2202.12576) [cs.CL]** |
|           | (or **[arXiv:2202.12576v1](https://arxiv.org/abs/2202.12576v1) [cs.CL]** for this version) |





<h2 id="2022-02-28-3">3. The Reality of Multi-Lingual Machine Translation
</h2>

Title: [The Reality of Multi-Lingual Machine Translation](https://arxiv.org/abs/2202.12814)

Authors: [Tom Kocmi](https://arxiv.org/search/cs?searchtype=author&query=Kocmi%2C+T), [Dominik Macháček](https://arxiv.org/search/cs?searchtype=author&query=Macháček%2C+D), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O)

> Our book "The Reality of Multi-Lingual Machine Translation" discusses the benefits and perils of using more than two languages in machine translation systems. While focused on the particular task of sequence-to-sequence processing and multi-task learning, the book targets somewhat beyond the area of natural language processing. Machine translation is for us a prime example of deep learning applications where human skills and learning capabilities are taken as a benchmark that many try to match and surpass. We document that some of the gains observed in multi-lingual translation may result from simpler effects than the assumed cross-lingual transfer of knowledge. 
> In the first, rather general part, the book will lead you through the motivation for multi-linguality, the versatility of deep neural networks especially in sequence-to-sequence tasks to complications of this learning. We conclude the general part with warnings against too optimistic and unjustified explanations of the gains that neural networks demonstrate. 
> In the second part, we fully delve into multi-lingual models, with a particularly careful examination of transfer learning as one of the more straightforward approaches utilizing additional languages. The recent multi-lingual techniques, including massive models, are surveyed and practical aspects of deploying systems for many languages are discussed. The conclusion highlights the open problem of machine understanding and reminds of two ethical aspects of building large-scale models: the inclusivity of research and its ecological trace.

| Comments: | ISBN 978-80-88132-11-0. arXiv admin note: substantial text overlap with [arXiv:2001.01622](https://arxiv.org/abs/2001.01622) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.12814](https://arxiv.org/abs/2202.12814) [cs.CL]** |
|           | (or **[arXiv:2202.12814v1](https://arxiv.org/abs/2202.12814v1) [cs.CL]** for this version) |





# 2022-02-25

[Return to Index](#Index)



<h2 id="2022-02-25-1">1. Overcoming a Theoretical Limitation of Self-Attention
</h2>

Title: [Overcoming a Theoretical Limitation of Self-Attention](https://arxiv.org/abs/2202.12172)

Authors: [David Chiang](https://arxiv.org/search/cs?searchtype=author&query=Chiang%2C+D), [Peter Cholak](https://arxiv.org/search/cs?searchtype=author&query=Cholak%2C+P)

> Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions become less and less confident (that is, with cross-entropy approaching 1 bit per string) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation suggested by Hahn's lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.

| Comments: | Accepted at ACL 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2202.12172](https://arxiv.org/abs/2202.12172) [cs.LG]** |
|           | (or **[arXiv:2202.12172v1](https://arxiv.org/abs/2202.12172v1) [cs.LG]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.12172Focus to learn more |





<h2 id="2022-02-25-2">2. Using natural language prompts for machine translation
</h2>

Title: [Using natural language prompts for machine translation](https://arxiv.org/abs/2202.11822)

Authors: [Xavier Garcia](https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+X), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

> We explore the use of natural language prompts for controlling various aspects of the outputs generated by machine translation models. We demonstrate that natural language prompts allow us to influence properties like formality or specific dialect of the output. We show that using language names to control the output language of multilingual translation models enables positive transfer for unseen language pairs. This unlocks the ability to translate into languages not seen during fine-tuning by using their English names. We investigate how scale, number of pre-training steps, number of languages in fine-tuning, and language similarity affect this phenomenon.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.11822](https://arxiv.org/abs/2202.11822) [cs.CL]** |
|           | (or **[arXiv:2202.11822v1](https://arxiv.org/abs/2202.11822v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.11822Focus to learn more |





<h2 id="2022-02-25-3">3. Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words
</h2>

Title: [Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words](https://arxiv.org/abs/2202.12142)

Authors: [Zhangyin Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Z), [Duyu Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D), [Cong Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Junwei Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+J), [Shuangzhi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+S), [Xiaocheng Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+X), [Bing Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+B), [Yunbo Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y), [Shuming Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S)

> The standard BERT adopts subword-based tokenization, which may break a word into two or more wordpieces (e.g., converting "lossless" to "loss" and "less"). This will bring inconvenience in following situations: (1) what is the best way to obtain the contextual vector of a word that is divided into multiple wordpieces? (2) how to predict a word via cloze test without knowing the number of wordpieces in advance? In this work, we explore the possibility of developing BERT-style pretrained model over a vocabulary of words instead of wordpieces. We call such word-level BERT model as WordBERT. We train models with different vocabulary sizes, initialization configurations and languages. Results show that, compared to standard wordpiece-based BERT, WordBERT makes significant improvements on cloze test and machine reading comprehension. On many other natural language understanding tasks, including POS tagging, chunking and NER, WordBERT consistently performs better than BERT. Model analysis indicates that the major advantage of WordBERT over BERT lies in the understanding for low-frequency words and rare words. Furthermore, since the pipeline is language-independent, we train WordBERT for Chinese language and obtain significant gains on five natural language understanding datasets. Lastly, the analyse on inference speed illustrates WordBERT has comparable time cost to BERT in natural language understanding tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.12142](https://arxiv.org/abs/2202.12142) [cs.CL]** |
|           | (or **[arXiv:2202.12142v1](https://arxiv.org/abs/2202.12142v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.12142Focus to learn more |







# 2022-02-24

[Return to Index](#Index)



<h2 id="2022-02-24-1">1. Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt
</h2>

Title: [Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt](https://arxiv.org/abs/2202.11451)

Authors: [Lianzhe Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Dongdong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Houfeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Prompt-based tuning has been proven effective for pretrained language models (PLMs). While most of the existing work focuses on the monolingual prompts, we study the multilingual prompts for multilingual PLMs, especially in the zero-shot cross-lingual setting. To alleviate the effort of designing different prompts for multiple languages, we propose a novel model that uses a unified prompt for all languages, called UniPrompt. Different from the discrete prompts and soft prompts, the unified prompt is model-based and language-agnostic. Specifically, the unified prompt is initialized by a multilingual PLM to produce language-independent representation, after which is fused with the text input. During inference, the prompts can be pre-computed so that no extra computation cost is needed. To collocate with the unified prompt, we propose a new initialization method for the target label word to further improve the model's transferability across languages. Extensive experiments show that our proposed methods can significantly outperform the strong baselines across different languages. We will release data and code to facilitate future research.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.11451](https://arxiv.org/abs/2202.11451) [cs.CL]** |
|           | (or **[arXiv:2202.11451v1](https://arxiv.org/abs/2202.11451v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.11451Focus to learn more |





<h2 id="2022-02-24-2">2. Refining the state-of-the-art in Machine Translation, optimizing NMT for the JA <-> EN language pair by leveraging personal domain expertise
</h2>

Title: [Refining the state-of-the-art in Machine Translation, optimizing NMT for the JA <-> EN language pair by leveraging personal domain expertise](https://arxiv.org/abs/2202.11669)

Authors: [Matthew Bieda](https://arxiv.org/search/cs?searchtype=author&query=Bieda%2C+M)

> Documenting the construction of an NMT (Neural Machine Translation) system for En/Ja based on the Transformer architecture leveraging the OpenNMT framework. A systematic exploration of corpora pre-processing, hyperparameter tuning and model architecture is carried out to obtain optimal performance. The system is evaluated using standard auto-evaluation metrics such as BLEU, and my subjective opinion as a Japanese linguist.

| Comments: | 11 pages, 13 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.11669](https://arxiv.org/abs/2202.11669) [cs.CL]** |
|           | (or **[arXiv:2202.11669v1](https://arxiv.org/abs/2202.11669v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.11669Focus to learn more |






# 2022-02-23

[Return to Index](#Index)



<h2 id="2022-02-23-1">1. CaMEL: Mean Teacher Learning for Image Captioning
</h2>

Title: [CaMEL: Mean Teacher Learning for Image Captioning](https://arxiv.org/abs/2202.10492)

Authors: [Manuele Barraco](https://arxiv.org/search/cs?searchtype=author&query=Barraco%2C+M), [Matteo Stefanini](https://arxiv.org/search/cs?searchtype=author&query=Stefanini%2C+M), [Marcella Cornia](https://arxiv.org/search/cs?searchtype=author&query=Cornia%2C+M), [Silvia Cascianelli](https://arxiv.org/search/cs?searchtype=author&query=Cascianelli%2C+S), [Lorenzo Baraldi](https://arxiv.org/search/cs?searchtype=author&query=Baraldi%2C+L), [Rita Cucchiara](https://arxiv.org/search/cs?searchtype=author&query=Cucchiara%2C+R)

> Describing images in natural language is a fundamental step towards the automatic modeling of connections between the visual and textual modalities. In this paper we present CaMEL, a novel Transformer-based architecture for image captioning. Our proposed approach leverages the interaction of two interconnected language models that learn from each other during the training phase. The interplay between the two language models follows a mean teacher learning paradigm with knowledge distillation. Experimentally, we assess the effectiveness of the proposed solution on the COCO dataset and in conjunction with different visual feature extractors. When comparing with existing proposals, we demonstrate that our model provides state-of-the-art caption quality with a significantly reduced number of parameters. According to the CIDEr metric, we obtain a new state of the art on COCO when training without using external data. The source code and trained models are publicly available at: [this https URL](https://github.com/aimagelab/camel).

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multimedia (cs.MM) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.10492](https://arxiv.org/abs/2202.10492) [cs.CV]** |
|           | (or **[arXiv:2202.10492v1](https://arxiv.org/abs/2202.10492v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.10492Focus to learn more |





<h2 id="2022-02-23-2">2. A Survey of Vision-Language Pre-Trained Models
</h2>

Title: [A Survey of Vision-Language Pre-Trained Models](https://arxiv.org/abs/2202.10936)

Authors: [Yifan Du](https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y), [Zikang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Junyi Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Wayne Xin Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W+X)

> As Transformer evolved, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre-training to the field of Vision-and-Language (V-L) learning and improve the performance on downstream tasks becomes a focus of multimodal learning. In this paper, we review the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the core content, we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image representations. We further present widely-used pre-training tasks, after which we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide multimodal researchers a synthesis and pointer to related research.

| Comments: | Under review                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2202.10936](https://arxiv.org/abs/2202.10936) [cs.CV]** |
|           | (or **[arXiv:2202.10936v1](https://arxiv.org/abs/2202.10936v1) [cs.CV]** for this version) |





<h2 id="2022-02-23-3">3. An Overview on Machine Translation Evaluation
</h2>

Title: [An Overview on Machine Translation Evaluation](https://arxiv.org/abs/2202.11027)

Authors: [Lifeng Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+L)

> Since the 1950s, machine translation (MT) has become one of the important tasks of AI and development, and has experienced several different periods and stages of development, including rule-based methods, statistical methods, and recently proposed neural network-based learning methods. Accompanying these staged leaps is the evaluation research and development of MT, especially the important role of evaluation methods in statistical translation and neural translation research. The evaluation task of MT is not only to evaluate the quality of machine translation, but also to give timely feedback to machine translation researchers on the problems existing in machine translation itself, how to improve and how to optimise. In some practical application fields, such as in the absence of reference translations, the quality estimation of machine translation plays an important role as an indicator to reveal the credibility of automatically translated target languages. This report mainly includes the following contents: a brief history of machine translation evaluation (MTE), the classification of research methods on MTE, and the the cutting-edge progress, including human evaluation, automatic evaluation, and evaluation of evaluation methods (meta-evaluation). Manual evaluation and automatic evaluation include reference-translation based and reference-translation independent participation; automatic evaluation methods include traditional n-gram string matching, models applying syntax and semantics, and deep learning models; evaluation of evaluation methods includes estimating the credibility of human evaluations, the reliability of the automatic evaluation, the reliability of the test set, etc. Advances in cutting-edge evaluation methods include task-based evaluation, using pre-trained language models based on big data, and lightweight optimisation models using distillation techniques.

| Comments: | 35 pages, in Chinese                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2202.11027](https://arxiv.org/abs/2202.11027) [cs.CL]** |
|           | (or **[arXiv:2202.11027v1](https://arxiv.org/abs/2202.11027v1) [cs.CL]** for this version) |





# 2022-02-22

[Return to Index](#Index)



<h2 id="2022-02-22-1">1. PETCI: A Parallel English Translation Dataset of Chinese Idioms
</h2>

Title: [PETCI: A Parallel English Translation Dataset of Chinese Idioms](https://arxiv.org/abs/2202.09509)

Authors: [Kenan Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+K) (The University of Chicago)

> Idioms are an important language phenomenon in Chinese, but idiom translation is notoriously hard. Current machine translation models perform poorly on idiom translation, while idioms are sparse in many translation datasets. We present PETCI, a parallel English translation dataset of Chinese idioms, aiming to improve idiom translation by both human and machine. The dataset is built by leveraging human and machine effort. Baseline generation models show unsatisfactory abilities to improve translation, but structure-aware classification models show good performance on distinguishing good translations. Furthermore, the size of PETCI can be easily increased without expertise. Overall, PETCI can be helpful to language learners and machine translation systems.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.09509](https://arxiv.org/abs/2202.09509) [cs.CL]** |
|           | (or **[arXiv:2202.09509v1](https://arxiv.org/abs/2202.09509v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.09509Focus to learn more |





<h2 id="2022-02-22-2">2. CALCS 2021 Shared Task: Machine Translation for Code-Switched Data
</h2>

Title: [CALCS 2021 Shared Task: Machine Translation for Code-Switched Data](https://arxiv.org/abs/2202.09625)

Authors: [Shuguang Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Gustavo Aguilar](https://arxiv.org/search/cs?searchtype=author&query=Aguilar%2C+G), [Anirudh Srinivasan](https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+A), [Mona Diab](https://arxiv.org/search/cs?searchtype=author&query=Diab%2C+M), [Thamar Solorio](https://arxiv.org/search/cs?searchtype=author&query=Solorio%2C+T)

> To date, efforts in the code-switching literature have focused for the most part on language identification, POS, NER, and syntactic parsing. In this paper, we address machine translation for code-switched social media data. We create a community shared task. We provide two modalities for participation: supervised and unsupervised. For the supervised setting, participants are challenged to translate English into Hindi-English (Eng-Hinglish) in a single direction. For the unsupervised setting, we provide the following language pairs: English and Spanish-English (Eng-Spanglish), and English and Modern Standard Arabic-Egyptian Arabic (Eng-MSAEA) in both directions. We share insights and challenges in curating the "into" code-switching language evaluation data. Further, we provide baselines for all language pairs in the shared task. The leaderboard for the shared task comprises 12 individual system submissions corresponding to 5 different teams. The best performance achieved is 12.67% BLEU score for English to Hinglish and 25.72% BLEU score for MSAEA to English.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.09625](https://arxiv.org/abs/2202.09625) [cs.CL]** |
|           | (or **[arXiv:2202.09625v1](https://arxiv.org/abs/2202.09625v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.09625Focus to learn more |





<h2 id="2022-02-22-3">3. Punctuation Restoration
</h2>

Title: [Punctuation Restoration](https://arxiv.org/abs/2202.09695)

Authors: [Viet Dac Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+V+D), [Amir Pouran Ben Veyseh](https://arxiv.org/search/cs?searchtype=author&query=Veyseh%2C+A+P+B), [Franck Dernoncourt](https://arxiv.org/search/cs?searchtype=author&query=Dernoncourt%2C+F), [Thien Huu Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+H)

> Given the increasing number of livestreaming videos, automatic speech recognition and post-processing for livestreaming video transcripts are crucial for efficient data management as well as knowledge mining. A key step in this process is punctuation restoration which restores fundamental text structures such as phrase and sentence boundaries from the video transcripts. This work presents a new human-annotated corpus, called BehancePR, for punctuation restoration in livestreaming video transcripts. Our experiments on BehancePR demonstrate the challenges of punctuation restoration for this domain. Furthermore, we show that popular natural language processing toolkits are incapable of detecting sentence boundary on non-punctuated transcripts of livestreaming videos, calling for more research effort to develop robust models for this area.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.09695](https://arxiv.org/abs/2202.09695) [cs.CL]** |
|           | (or **[arXiv:2202.09695v1](https://arxiv.org/abs/2202.09695v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.09695Focus to learn more |





<h2 id="2022-02-22-4">4. -Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning
</h2>

Title: [-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning](https://arxiv.org/abs/2202.09817)

Authors: [Yitao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Chenxin An](https://arxiv.org/search/cs?searchtype=author&query=An%2C+C), [Xipeng Qiu](https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X)

> With the success of large-scale pre-trained models (PTMs), how efficiently adapting PTMs to downstream tasks has attracted tremendous attention, especially for PTMs with billions of parameters. Although some parameter-efficient tuning paradigms have been proposed to address this problem, they still require large resources to compute the gradients in the training phase. In this paper, we propose -Tuning, an efficient yet effective paradigm to adapt frozen large-scale PTMs to specific downstream tasks. -tuning learns dense representations for labels  defined in a given task and aligns them to fixed feature representation. Without tuning the features of input text and model parameters, -tuning is both parameter-efficient and training-efficient. For DeBERTaXXL with 1.6 billion parameters, -tuning achieves performance more than 96% of full fine-tuning on GLUE Benchmark with only 2% tunable parameters and much fewer training costs.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.09817](https://arxiv.org/abs/2202.09817) [cs.CL]** |
|           | (or **[arXiv:2202.09817v1](https://arxiv.org/abs/2202.09817v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.09817Focus to learn more |





<h2 id="2022-02-22-5">5. USCORE: An Effective Approach to Fully Unsupervised Evaluation Metrics for Machine Translation
</h2>

Title: [USCORE: An Effective Approach to Fully Unsupervised Evaluation Metrics for Machine Translation](https://arxiv.org/abs/2202.10062)

Authors: [Jonas Belouadi](https://arxiv.org/search/cs?searchtype=author&query=Belouadi%2C+J), [Steffen Eger](https://arxiv.org/search/cs?searchtype=author&query=Eger%2C+S)

> The vast majority of evaluation metrics for machine translation are supervised, i.e., (i) assume the existence of reference translations, (ii) are trained on human scores, or (iii) leverage parallel data. This hinders their applicability to cases where such supervision signals are not available. In this work, we develop fully unsupervised evaluation metrics. To do so, we leverage similarities and synergies between evaluation metric induction, parallel corpus mining, and MT systems. In particular, we use an unsupervised evaluation metric to mine pseudo-parallel data, which we use to remap deficient underlying vector spaces (in an iterative manner) and to induce an unsupervised MT system, which then provides pseudo-references as an additional component in the metric. Finally, we also induce unsupervised multilingual sentence embeddings from pseudo-parallel data. We show that our fully unsupervised metrics are effective, i.e., they beat supervised competitors on 4 out of our 5 evaluation datasets.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.10062](https://arxiv.org/abs/2202.10062) [cs.CL]** |
|           | (or **[arXiv:2202.10062v1](https://arxiv.org/abs/2202.10062v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.10062Focus to learn more |





<h2 id="2022-02-22-6">6. BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models
</h2>

Title: [BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models](https://arxiv.org/abs/2202.10101)

Authors: [Lisa Langnickel](https://arxiv.org/search/cs?searchtype=author&query=Langnickel%2C+L), [Alexander Schulz](https://arxiv.org/search/cs?searchtype=author&query=Schulz%2C+A), [Barbara Hammer](https://arxiv.org/search/cs?searchtype=author&query=Hammer%2C+B), [Juliane Fluck](https://arxiv.org/search/cs?searchtype=author&query=Fluck%2C+J)

> Recent developments in transfer learning have boosted the advancements in natural language processing tasks. The performance is, however, dependent on high-quality, manually annotated training data. Especially in the biomedical domain, it has been shown that one training corpus is not enough to learn generic models that are able to efficiently predict on new data. Therefore, state-of-the-art models need the ability of lifelong learning in order to improve performance as soon as new data are available - without the need of retraining the whole model from scratch. We present WEAVER, a simple, yet efficient post-processing method that infuses old knowledge into the new model, thereby reducing catastrophic forgetting. We show that applying WEAVER in a sequential manner results in similar word embedding distributions as doing a combined training on all data at once, while being computationally more efficient. Because there is no need of data sharing, the presented method is also easily applicable to federated learning settings and can for example be beneficial for the mining of electronic health records from different clinics.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.10101](https://arxiv.org/abs/2202.10101) [cs.CL]** |
|           | (or **[arXiv:2202.10101v1](https://arxiv.org/abs/2202.10101v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.10101Focus to learn more |





<h2 id="2022-02-22-7">7. Domain Adaptation in Neural Machine Translation using a Qualia-Enriched FrameNet
</h2>

Title: [Domain Adaptation in Neural Machine Translation using a Qualia-Enriched FrameNet](https://arxiv.org/abs/2202.10287)

Authors: [Alexandre Diniz Costa](https://arxiv.org/search/cs?searchtype=author&query=Costa%2C+A+D), [Mateus Coutinho Marim](https://arxiv.org/search/cs?searchtype=author&query=Marim%2C+M+C), [Ely Edison da Silva Matos](https://arxiv.org/search/cs?searchtype=author&query=da+Silva+Matos%2C+E+E), [Tiago Timponi Torrent](https://arxiv.org/search/cs?searchtype=author&query=Torrent%2C+T+T)

> In this paper we present Scylla, a methodology for domain adaptation of Neural Machine Translation (NMT) systems that make use of a multilingual FrameNet enriched with qualia relations as an external knowledge base. Domain adaptation techniques used in NMT usually require fine-tuning and in-domain training data, which may pose difficulties for those working with lesser-resourced languages and may also lead to performance decay of the NMT system for out-of-domain sentences. Scylla does not require fine-tuning of the NMT model, avoiding the risk of model over-fitting and consequent decrease in performance for out-of-domain translations. Two versions of Scylla are presented: one using the source sentence as input, and another one using the target sentence. We evaluate Scylla in comparison to a state-of-the-art commercial NMT system in an experiment in which 50 sentences from the Sports domain are translated from Brazilian Portuguese to English. The two versions of Scylla significantly outperform the baseline commercial system in HTER.

| Comments:    | Paper submitted to LREC 2022                                 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**                         |
| ACM classes: | E.1                                                          |
| Cite as:     | **[arXiv:2202.10287](https://arxiv.org/abs/2202.10287) [cs.CL]** |
|              | (or **[arXiv:2202.10287v1](https://arxiv.org/abs/2202.10287v1) [cs.CL]** for this version) |
|              | https://doi.org/10.48550/arXiv.2202.10287Focus to learn more |





<h2 id="2022-02-22-8">8. Interpreting Language Models with Contrastive Explanations
</h2>

Title: [Interpreting Language Models with Contrastive Explanations](https://arxiv.org/abs/2202.10419)

Authors: [Kayo Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+K), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable to provide informative explanations. Language models must consider various features to predict a token, such as its part of speech, number, tense, or semantics. Existing explanation methods conflate evidence for all these features into a single explanation, which is less interpretable for human understanding. 
> To disentangle the different decisions in language modeling, we focus on explaining language models contrastively: we look for salient input tokens that explain why the model predicted one token instead of another. We demonstrate that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers. We also identify groups of contrastive decisions where the model uses similar evidence, and we are able to characterize what input tokens models use during various language generation decisions.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.10419](https://arxiv.org/abs/2202.10419) [cs.CL]** |
|           | (or **[arXiv:2202.10419v1](https://arxiv.org/abs/2202.10419v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.10419Focus to learn more |






# 2022-02-21

[Return to Index](#Index)



<h2 id="2022-02-21-1">1. Improving English to Sinhala Neural Machine Translation using Part-of-Speech Tag
</h2>

Title: [Improving English to Sinhala Neural Machine Translation using Part-of-Speech Tag](https://arxiv.org/abs/2202.08882)

Authors: [Ravinga Perera](https://arxiv.org/search/cs?searchtype=author&query=Perera%2C+R), [Thilakshi Fonseka](https://arxiv.org/search/cs?searchtype=author&query=Fonseka%2C+T), [Rashmini Naranpanawa](https://arxiv.org/search/cs?searchtype=author&query=Naranpanawa%2C+R), [Uthayasanker Thayasivam](https://arxiv.org/search/cs?searchtype=author&query=Thayasivam%2C+U)

> The performance of Neural Machine Translation (NMT) depends significantly on the size of the available parallel corpus. Due to this fact, low resource language pairs demonstrate low translation performance compared to high resource language pairs. The translation quality further degrades when NMT is performed for morphologically rich languages. Even though the web contains a large amount of information, most people in Sri Lanka are unable to read and understand English properly. Therefore, there is a huge requirement of translating English content to local languages to share information among locals. Sinhala language is the primary language in Sri Lanka and building an NMT system that can produce quality English to Sinhala translations is difficult due to the syntactic divergence between these two languages under low resource constraints. Thus, in this research, we explore effective methods of incorporating Part of Speech (POS) tags to the Transformer input embedding and positional encoding to further enhance the performance of the baseline English to Sinhala neural machine translation model.

| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2202.08882](https://arxiv.org/abs/2202.08882) [cs.CL]** |
|              | (or **[arXiv:2202.08882v1](https://arxiv.org/abs/2202.08882v1) [cs.CL]** for this version) |
|              | https://doi.org/10.48550/arXiv.2202.08882Focus to learn more |







<h2 id="2022-02-21-2">2. VLP: A Survey on Vision-Language Pre-training
</h2>

Title: [VLP: A Survey on Vision-Language Pre-training](https://arxiv.org/abs/2202.09061)

Authors: [Feilong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+F), [Duzhan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Minglun Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+M), [Xiuyi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Jing Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+J), [Shuang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S), [Bo Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+B)

> In the past few years, the emergence of pre-training models has brought uni-modal fields such as computer vision (CV) and natural language processing (NLP) to a new era. Substantial works have shown they are beneficial for downstream uni-modal tasks and avoid training a new model from scratch. So can such pre-trained models be applied to multi-modal tasks? Researchers have explored this problem and made significant progress. This paper surveys recent advances and new frontiers in vision-language pre-training (VLP), including image-text and video-text pre-training. To give readers a better overall grasp of VLP, we first review its recent advances from five aspects: feature extraction, model architecture, pre-training objectives, pre-training datasets, and downstream tasks. Then, we summarize the specific VLP models in detail. Finally, we discuss the new frontiers in VLP. To the best of our knowledge, this is the first survey on VLP. We hope that this survey can shed light on future research in the VLP field.

| Comments: | A Survey on Vision-Language Pre-training                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2202.09061](https://arxiv.org/abs/2202.09061) [cs.CV]** |
|           | (or **[arXiv:2202.09061v1](https://arxiv.org/abs/2202.09061v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.09061Focus to learn more |







# 2022-02-18

[Return to Index](#Index)



<h2 id="2022-02-18-1">1. End-to-End Training of Both Translation Models in the Back-Translation Framework
</h2>

Title: [End-to-End Training of Both Translation Models in the Back-Translation Framework](https://arxiv.org/abs/2202.08465)

Authors: [DongNyeong Heo](https://arxiv.org/search/cs?searchtype=author&query=Heo%2C+D), [Heeyoul Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+H)

> Semi-supervised learning algorithms in neural machine translation (NMT) have significantly improved translation quality compared to the supervised learning algorithms by using additional monolingual corpora. Among them, back-translation is a theoretically well-structured and cutting-edge method. Given two pre-trained NMT models between source and target languages, one translates a monolingual sentence as a latent sentence, and the other reconstructs the monolingual input sentence given the latent sentence. Therefore, previous works tried to apply the variational auto-encoder's (VAE) training framework to the back-translation framework. However, the discrete property of the latent sentence made it impossible to use backpropagation in the framework. This paper proposes a categorical reparameterization trick that generates a differentiable sentence, with which we practically implement the VAE's training framework for the back-translation and train it by end-to-end backpropagation. In addition, we propose several regularization techniques that are especially advantageous to this framework. In our experiments, we demonstrate that our method makes backpropagation available through the latent sentences and improves the BLEU scores on the datasets of the WMT18 translation task.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.08465](https://arxiv.org/abs/2202.08465) [cs.CL]** |
|           | (or **[arXiv:2202.08465v1](https://arxiv.org/abs/2202.08465v1) [cs.CL]** for this version) |





<h2 id="2022-02-18-2">2. cosFormer: Rethinking Softmax in Attention
</h2>

Title: [cosFormer: Rethinking Softmax in Attention](https://arxiv.org/abs/2202.08791)

Authors: [Zhen Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Z), [Weixuan Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+W), [Hui Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+H), [Dongxu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+D), [Yunshen Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Y), [Baohong Lv](https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+B), [Junjie Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+J), [Lingpeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L), [Yiran Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+Y)

> Transformer has shown great successes in natural language processing, computer vision, and audio processing. As one of its core components, the softmax attention helps to capture long-range dependencies yet prohibits its scale-up due to the quadratic space and time complexity to the sequence length. Kernel methods are often adopted to reduce the complexity by approximating the softmax operator. Nevertheless, due to the approximation errors, their performances vary in different tasks/corpus and suffer crucial performance drops when compared with the vanilla softmax attention. In this paper, we propose a linear transformer called cosFormer that can achieve comparable or better accuracy to the vanilla transformer in both casual and cross attentions. cosFormer is based on two key properties of softmax attention: i). non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme that can concentrate the distribution of the attention matrix. As its linear substitute, cosFormer fulfills these properties with a linear operator and a cosine-based distance re-weighting mechanism. Extensive experiments on language modeling and text understanding tasks demonstrate the effectiveness of our method. We further examine our method on long sequences and achieve state-of-the-art performance on the Long-Range Arena benchmark. The source code is available at [this https URL](https://github.com/OpenNLPLab/cosFormer).

| Comments: | Accepted to ICLR2022. Yiran Zhong is the corresponding author. Zhen Qin, Weixuan Sun, Hui Deng contributed equally to this work |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.08791](https://arxiv.org/abs/2202.08791) [cs.CL]** |
|           | (or **[arXiv:2202.08791v1](https://arxiv.org/abs/2202.08791v1) [cs.CL]** for this version) |



# 2022-02-17

[Return to Index](#Index)



<h2 id="2022-02-17-1">1. On the Self Shuffle Language
</h2>

Title: [On the Self Shuffle Language](https://arxiv.org/abs/2202.07988)

Authors: [Pamela Fleischmann](https://arxiv.org/search/math?searchtype=author&query=Fleischmann%2C+P), [Tero Harju](https://arxiv.org/search/math?searchtype=author&query=Harju%2C+T), [Lukas Haschke](https://arxiv.org/search/math?searchtype=author&query=Haschke%2C+L), [Jonas Höfer](https://arxiv.org/search/math?searchtype=author&query=Höfer%2C+J), [Dirk Nowotka](https://arxiv.org/search/math?searchtype=author&query=Nowotka%2C+D)

> The shuffle product \(u\shuffle v\) of two words \(u\) and \(v\) is the set of all words which can be obtained by interleaving \(u\) and \(v\). Motivated by the paper \emph{The Shuffle Product: New Research Directions} by Restivo (2015) we investigate a special case of the shuffle product. In this work we consider the shuffle of a word with itself called the \emph{self shuffle} or \emph{shuffle square}, showing first that the self shuffle language and the shuffle of the language are in general different sets. We prove that the language of all words arising as a self shuffle of some word is context sensitive but not context free. Furthermore, we show that the self shuffle \(w \shuffle w\) uniquely determines \(w\).

| Subjects:    | **Combinatorics (math.CO)**; Computation and Language (cs.CL) |
| ------------ | ------------------------------------------------------------ |
| MSC classes: | 14J60                                                        |
| ACM classes: | F.2.2; I.2.7                                                 |
| Cite as:     | **[arXiv:2202.07988](https://arxiv.org/abs/2202.07988) [math.CO]** |
|              | (or **[arXiv:2202.07988v1](https://arxiv.org/abs/2202.07988v1) [math.CO]** for this version) |
|              | https://doi.org/10.48550/arXiv.2202.07988Focus to learn more |





<h2 id="2022-02-17-2">2. ZeroGen: Efficient Zero-shot Learning via Dataset Generation
</h2>

Title: [ZeroGen: Efficient Zero-shot Learning via Dataset Generation](https://arxiv.org/abs/2202.07922)

Authors: [Jiacheng Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+J), [Jiahui Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J), [Qintong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Q), [Hang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Jiangtao Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J), [Zhiyong Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Tao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+T), [Lingpeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L)

> There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, ZeroGen. Given a zero-shot task, we first generate a dataset from scratch using PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM) under the supervision of the synthesized dataset. This approach allows highly efficient inference as the final task model only has orders of magnitude fewer parameters comparing to PLMs (e.g., GPT2-XL). Apart from being annotation-free and efficient, we argue that ZeroGen can also provide useful insights from the perspective of data-free model-agnostic knowledge distillation, and unreferenced text generation evaluation. Experiments and analysis on different NLP tasks, namely, text classification, question answering, and natural language inference), show the effectiveness of ZeroGen.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07922](https://arxiv.org/abs/2202.07922) [cs.CL]** |
|           | (or **[arXiv:2202.07922v1](https://arxiv.org/abs/2202.07922v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.07922Focus to learn more |





<h2 id="2022-02-17-3">3. Revisiting Parameter-Efficient Tuning: Are We Really There Yet?
</h2>

Title: [Revisiting Parameter-Efficient Tuning: Are We Really There Yet?](https://arxiv.org/abs/2202.07962)

Authors: [Guanzheng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Fangyu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Zaiqiao Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Z), [Shangsong Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+S)

> Parameter-efficient tuning (PETuning) methods have been deemed by many as the new paradigm for using pretrained language models (PLMs). By tuning just a fraction amount of parameters comparing to full model finetuning, PETuning methods claim to have achieved performance on par with or even better than finetuning. In this work, we take a step back and re-examine these PETuning methods by conducting the first comprehensive investigation into the training and evaluation of PETuning methods. We found the problematic validation and testing practice in current studies, when accompanied by the instability nature of PETuning methods, has led to unreliable conclusions. When being compared under a truly fair evaluation protocol, PETuning cannot yield consistently competitive performance while finetuning remains to be the best-performing method in medium- and high-resource settings. We delve deeper into the cause of the instability and observed that model size does not explain the phenomenon but training iteration positively correlates with the stability.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07962](https://arxiv.org/abs/2202.07962) [cs.CL]** |
|           | (or **[arXiv:2202.07962v1](https://arxiv.org/abs/2202.07962v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.07962Focus to learn more |





<h2 id="2022-02-17-4">4. Should You Mask 15% in Masked Language Modeling?
</h2>

Title: [Should You Mask 15% in Masked Language Modeling?](https://arxiv.org/abs/2202.08005)

Authors: [Alexander Wettig](https://arxiv.org/search/cs?searchtype=author&query=Wettig%2C+A), [Tianyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+T), [Zexuan Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+Z), [Danqi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D)

> Masked language models conventionally use a masking rate of 15% due to the belief that more masking would provide insufficient context to learn good representations, and less masking would make training too expensive. Surprisingly, we find that masking up to 40% of input tokens can outperform the 15% baseline, and even masking 80% can preserve most of the performance, as measured by fine-tuning on downstream tasks. Increasing the masking rates has two distinct effects, which we investigate through careful ablations: (1) A larger proportion of input tokens are corrupted, reducing the context size and creating a harder task, and (2) models perform more predictions, which benefits training. We observe that larger models in particular favor higher masking rates, as they have more capacity to perform the harder task. We also connect our findings to sophisticated masking schemes such as span masking and PMI masking, as well as BERT's curious 80-10-10 corruption strategy, and find that simple uniform masking with [MASK] replacements can be competitive at higher masking rates. Our results contribute to a better understanding of masked language modeling and point to new avenues for efficient pre-training.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.08005](https://arxiv.org/abs/2202.08005) [cs.CL]** |
|           | (or **[arXiv:2202.08005v1](https://arxiv.org/abs/2202.08005v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.08005Focus to learn more |









# 2022-02-16

[Return to Index](#Index)



<h2 id="2022-02-16-1">1. A Survey on Dynamic Neural Networks for Natural Language Processing
</h2>

Title: [A Survey on Dynamic Neural Networks for Natural Language Processing](https://arxiv.org/abs/2202.07101)

Authors: [Canwen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Julian McAuley](https://arxiv.org/search/cs?searchtype=author&query=McAuley%2C+J)

> Effectively scaling large Transformer models is a main driver of recent advances in natural language processing. Dynamic neural networks, as an emerging research direction, are capable of scaling up neural networks with sub-linear increases in computation and time by dynamically adjusting their computational path based on the input. Dynamic neural networks could be a promising solution to the growing parameter numbers of pretrained language models, allowing both model pretraining with trillions of parameters and faster inference on mobile devices. In this survey, we summarize progress of three types of dynamic neural networks in NLP: skimming, mixture of experts, and early exit. We also highlight current challenges in dynamic neural networks and directions for future research.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07101](https://arxiv.org/abs/2202.07101) [cs.CL]** |
|           | (or **[arXiv:2202.07101v1](https://arxiv.org/abs/2202.07101v1) [cs.CL]** for this version) |





<h2 id="2022-02-16-2">2. A Survey on Model Compression for Natural Language Processing
</h2>

Title: [A Survey on Model Compression for Natural Language Processing](https://arxiv.org/abs/2202.07105)

Authors: [Canwen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Julian McAuley](https://arxiv.org/search/cs?searchtype=author&query=McAuley%2C+J)

> With recent developments in new architectures like Transformer and pretraining techniques, significant progress has been made in applications of natural language processing (NLP). However, the high energy cost and long inference delay of Transformer is preventing NLP from entering broader scenarios including edge and mobile computing. Efficient NLP research aims to comprehensively consider computation, time and carbon emission for the entire life-cycle of NLP, including data preparation, model training and inference. In this survey, we focus on the inference stage and review the current state of model compression for NLP, including the benchmarks, metrics and methodology. We outline the current obstacles and future research directions.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07105](https://arxiv.org/abs/2202.07105) [cs.CL]** |
|           | (or **[arXiv:2202.07105v1](https://arxiv.org/abs/2202.07105v1) [cs.CL]** for this version) |





<h2 id="2022-02-16-3">3. MuLD: The Multitask Long Document Benchmark
</h2>

Title: [MuLD: The Multitask Long Document Benchmark](https://arxiv.org/abs/2202.07362)

Authors: [G Thomas Hudson](https://arxiv.org/search/cs?searchtype=author&query=Hudson%2C+G+T), [Noura Al Moubayed](https://arxiv.org/search/cs?searchtype=author&query=Moubayed%2C+N+A)

> The impressive progress in NLP techniques has been driven by the development of multi-task benchmarks such as GLUE and SuperGLUE. While these benchmarks focus on tasks for one or two input sentences, there has been exciting work in designing efficient techniques for processing much longer inputs. In this paper, we present MuLD: a new long document benchmark consisting of only documents over 10,000 tokens. By modifying existing NLP tasks, we create a diverse benchmark which requires models to successfully model long-term dependencies in the text. We evaluate how existing models perform, and find that our benchmark is much more challenging than their `short document' equivalents. Furthermore, by evaluating both regular and efficient transformers, we show that models with increased context length are better able to solve the tasks presented, suggesting that future improvements in these models are vital for solving similar long document problems. We release the data and code for baselines to encourage further research on efficient NLP models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07362](https://arxiv.org/abs/2202.07362) [cs.CL]** |
|           | (or **[arXiv:2202.07362v1](https://arxiv.org/abs/2202.07362v1) [cs.CL]** for this version) |





<h2 id="2022-02-16-4">4. BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer
</h2>

Title: [BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer](https://arxiv.org/abs/2202.07543)

Authors: [Ana-Maria Bucur](https://arxiv.org/search/cs?searchtype=author&query=Bucur%2C+A), [Adrian Cosma](https://arxiv.org/search/cs?searchtype=author&query=Cosma%2C+A), [Ioan-Bogdan Iordache](https://arxiv.org/search/cs?searchtype=author&query=Iordache%2C+I)

> Memes are prevalent on the internet and continue to grow and evolve alongside our culture. An automatic understanding of memes propagating on the internet can shed light on the general sentiment and cultural attitudes of people. In this work, we present team BLUE's solution for the second edition of the MEMOTION competition. We showcase two approaches for meme classification (i.e. sentiment, humour, offensive, sarcasm and motivation levels) using a text-only method using BERT, and a Multi-Modal-Multi-Task transformer network that operates on both the meme image and its caption to output the final scores. In both approaches, we leverage state-of-the-art pretrained models for text (BERT, Sentence Transformer) and image processing (EfficientNetV4, CLIP). Through our efforts, we obtain first place in task A, second place in task B and third place in task C. In addition, our team obtained the highest average score for all three tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07543](https://arxiv.org/abs/2202.07543) [cs.CL]** |
|           | (or **[arXiv:2202.07543v1](https://arxiv.org/abs/2202.07543v1) [cs.CL]** for this version) |





<h2 id="2022-02-16-5">5. Delving Deeper into Cross-lingual Visual Question Answering
</h2>

Title: [Delving Deeper into Cross-lingual Visual Question Answering](https://arxiv.org/abs/2202.07630)

Authors: [Chen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Jonas Pfeiffer](https://arxiv.org/search/cs?searchtype=author&query=Pfeiffer%2C+J), [Anna Korhonen](https://arxiv.org/search/cs?searchtype=author&query=Korhonen%2C+A), [Ivan Vulic](https://arxiv.org/search/cs?searchtype=author&query=Vulic%2C+I), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> Visual question answering (VQA) is one of the crucial vision-and-language tasks. Yet, the bulk of research until recently has focused only on the English language due to the lack of appropriate evaluation resources. Previous work on cross-lingual VQA has reported poor zero-shot transfer performance of current multilingual multimodal Transformers and large gaps to monolingual performance, attributed mostly to misalignment of text embeddings between the source and target languages, without providing any additional deeper analyses. In this work, we delve deeper and address different aspects of cross-lingual VQA holistically, aiming to understand the impact of input data, fine-tuning and evaluation regimes, and interactions between the two modalities in cross-lingual setups. 1) We tackle low transfer performance via novel methods that substantially reduce the gap to monolingual English performance, yielding +10 accuracy points over existing transfer methods. 2) We study and dissect cross-lingual VQA across different question types of varying complexity, across different multilingual multi-modal Transformers, and in zero-shot and few-shot scenarios. 3) We further conduct extensive analyses on modality biases in training data and models, aimed to further understand why zero-shot performance gaps remain for some question types and languages. We hope that the novel methods and detailed analyses will guide further progress in multilingual VQA.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07630](https://arxiv.org/abs/2202.07630) [cs.CL]** |
|           | (or **[arXiv:2202.07630v1](https://arxiv.org/abs/2202.07630v1) [cs.CL]** for this version) |





# 2022-02-15

[Return to Index](#Index)



<h2 id="2022-02-15-1">1. USTED: Improving ASR with a Unified Speech and Text Encoder-Decoder
</h2>

Title: [USTED: Improving ASR with a Unified Speech and Text Encoder-Decoder](https://arxiv.org/abs/2202.06045)

Authors:[Bolaji Yusuf](https://arxiv.org/search/cs?searchtype=author&query=Yusuf%2C+B), [Ankur Gandhe](https://arxiv.org/search/cs?searchtype=author&query=Gandhe%2C+A), [Alex Sokolov](https://arxiv.org/search/cs?searchtype=author&query=Sokolov%2C+A)

> Improving end-to-end speech recognition by incorporating external text data has been a longstanding research topic. There has been a recent focus on training E2E ASR models that get the performance benefits of external text data without incurring the extra cost of evaluating an external language model at inference time. In this work, we propose training ASR model jointly with a set of text-to-text auxiliary tasks with which it shares a decoder and parts of the encoder. When we jointly train ASR and masked language model with the 960-hour Librispeech and Opensubtitles data respectively, we observe WER reductions of 16% and 20% on test-other and test-clean respectively over an ASR-only baseline without any extra cost at inference time, and reductions of 6% and 8% compared to a stronger MUTE-L baseline which trains the decoder with the same text data as our model. We achieve further improvements when we train masked language model on Librispeech data or when we use machine translation as the auxiliary task, without significantly sacrificing performance on the task itself.

| Comments: | 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2202.06045](https://arxiv.org/abs/2202.06045) [cs.CL]** |
|           | (or **[arXiv:2202.06045v1](https://arxiv.org/abs/2202.06045v1) [cs.CL]** for this version) |





<h2 id="2022-02-15-2">2. A Contrastive Framework for Neural Text Generation
</h2>

Title: [A Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417)

Authors:[Yixuan Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y), [Tian Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+T), [Yan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Dani Yogatama](https://arxiv.org/search/cs?searchtype=author&query=Yogatama%2C+D), [Lingpeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L), [Nigel Collier](https://arxiv.org/search/cs?searchtype=author&query=Collier%2C+N)

> Text generation is of great importance to many natural language processing applications. However, maximization-based decoding methods (e.g. beam search) of neural language models often lead to degenerate solutions -- the generated text is unnatural and contains undesirable repetitions. Existing approaches introduce stochasticity via sampling or modify training objectives to decrease probabilities of certain tokens (e.g., unlikelihood training). However, they often lead to solutions that lack coherence. In this work, we show that an underlying reason for model degeneration is the anisotropic distribution of token representations. We present a contrastive solution: (i) SimCTG, a contrastive training objective to calibrate the model's representation space, and (ii) a decoding method -- contrastive search -- to encourage diversity while maintaining coherence in the generated text. Extensive experiments and analyses on three benchmarks from two languages demonstrate that our proposed approach outperforms state-of-the-art text generation methods as evaluated by both human and automatic metrics.

| Comments: | 22 pages, 8 figures, and 8 tables                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.06417](https://arxiv.org/abs/2202.06417) [cs.CL]** |
|           | (or **[arXiv:2202.06417v1](https://arxiv.org/abs/2202.06417v1) [cs.CL]** for this version) |





<h2 id="2022-02-15-3">3. I-Tuning: Tuning Language Models with Image for Caption Generation
</h2>

Title: [I-Tuning: Tuning Language Models with Image for Caption Generation](https://arxiv.org/abs/2202.06574)

Authors:[Ziyang Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Z), [Yadong Xi](https://arxiv.org/search/cs?searchtype=author&query=Xi%2C+Y), [Rongsheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Jing Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+J)

> Recently, tuning the pre-trained language model (PLM) in a parameter-efficient manner becomes a popular topic in the natural language processing area. However, most of them focus on tuning the PLM with the text-only information. In this work, we propose a new perspective to tune the frozen PLM with images for caption generation. We denote our method as I-Tuning, which can automatically filter the vision information from images to adjust the output hidden states of PLM. Evaluating on the image captioning tasks (MSCOCO and Flickr30k Captioning), our method achieves comparable or even better performance than the previous models which have 2-4 times more trainable parameters and/or consume a large amount of cross-modal pre-training data.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2202.06574](https://arxiv.org/abs/2202.06574) [cs.CL]** |
|           | (or **[arXiv:2202.06574v1](https://arxiv.org/abs/2202.06574v1) [cs.CL]** for this version) |





# 2022-02-14

[Return to Index](#Index)



<h2 id="2022-02-14-1">1. Including Facial Expressions in Contextual Embeddings for Sign Language Generation
</h2>

Title: [Including Facial Expressions in Contextual Embeddings for Sign Language Generation](https://arxiv.org/abs/2202.05383)

Authors: [Carla Viegas](https://arxiv.org/search/cs?searchtype=author&query=Viegas%2C+C), [Mert İnan](https://arxiv.org/search/cs?searchtype=author&query=İnan%2C+M), [Lorna Quandt](https://arxiv.org/search/cs?searchtype=author&query=Quandt%2C+L), [Malihe Alikhani](https://arxiv.org/search/cs?searchtype=author&query=Alikhani%2C+M)

> State-of-the-art sign language generation frameworks lack expressivity and naturalness which is the result of only focusing manual signs, neglecting the affective, grammatical and semantic functions of facial expressions. The purpose of this work is to augment semantic representation of sign language through grounding facial expressions. We study the effect of modeling the relationship between text, gloss, and facial expressions on the performance of the sign generation systems. In particular, we propose a Dual Encoder Transformer able to generate manual signs as well as facial expressions by capturing the similarities and differences found in text and sign gloss annotation. We take into consideration the role of facial muscle activity to express intensities of manual signs by being the first to employ facial action units in sign language generation. We perform a series of experiments showing that our proposed model improves the quality of automatically generated sign language.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.05383](https://arxiv.org/abs/2202.05383) [cs.CL]** |
|           | (or **[arXiv:2202.05383v1](https://arxiv.org/abs/2202.05383v1) [cs.CL]** for this version) |





<h2 id="2022-02-14-2">2. Evaluating MT Systems: A Theoretical Framework
</h2>

Title: [Evaluating MT Systems: A Theoretical Framework](https://arxiv.org/abs/2202.05806)

Authors: [Rajeev Sangal](https://arxiv.org/search/cs?searchtype=author&query=Sangal%2C+R)

> This paper outlines a theoretical framework using which different automatic metrics can be designed for evaluation of Machine Translation systems. It introduces the concept of {\em cognitive ease} which depends on {\em adequacy} and {\em lack of fluency}. Thus, cognitive ease becomes the main parameter to be measured rather than comprehensibility. The framework allows the components of cognitive ease to be broken up and computed based on different linguistic levels etc. Independence of dimensions and linearly combining them provides for a highly modular approach. 
> The paper places the existing automatic methods in an overall framework, to understand them better and to improve upon them in future. It can also be used to evaluate the newer types of MT systems, such as speech to speech translation and discourse translation.

| Comments: | 18 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.05806](https://arxiv.org/abs/2202.05806) [cs.CL]** |
|           | (or **[arXiv:2202.05806v1](https://arxiv.org/abs/2202.05806v1) [cs.CL]** for this version) |



# 2022-02-11

[Return to Index](#Index)



<h2 id="2022-02-11-1">1. SHAS: Approaching optimal Segmentation for End-to-End Speech Translation
</h2>

Title: [SHAS: Approaching optimal Segmentation for End-to-End Speech Translation](https://arxiv.org/abs/2202.04774)

Authors: [Ioannis Tsiamas](https://arxiv.org/search/cs?searchtype=author&query=Tsiamas%2C+I), [Gerard I. Gállego](https://arxiv.org/search/cs?searchtype=author&query=Gállego%2C+G+I), [José A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

> Speech translation models are unable to directly process long audios, like TED talks, which have to be split into shorter segments. Speech translation datasets provide manual segmentations of the audios, which are not available in real-world scenarios, and existing segmentation methods usually significantly reduce translation quality at inference time. To bridge the gap between the manual segmentation of training and the automatic one at inference, we propose Supervised Hybrid Audio Segmentation (SHAS), a method that can effectively learn the optimal segmentation from any manually segmented speech corpus. First, we train a classifier to identify the included frames in a segmentation, using speech representations from a pre-trained wav2vec 2.0. The optimal splitting points are then found by a probabilistic Divide-and-Conquer algorithm that progressively splits at the frame of lowest probability until all segments are below a pre-specified length. Experiments on MuST-C and mTEDx show that the translation of the segments produced by our method approaches the quality of the manual segmentation on 5 languages pairs. Namely, SHAS retains 95-98% of the manual segmentation's BLEU score, compared to the 87-93% of the best existing methods. Our method is additionally generalizable to different domains and achieves high zero-shot performance in unseen languages.

| Comments: | 7 pages including appendix                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2202.04774](https://arxiv.org/abs/2202.04774) [cs.SD]** |
|           | (or **[arXiv:2202.04774v1](https://arxiv.org/abs/2202.04774v1) [cs.SD]** for this version) |





<h2 id="2022-02-11-2">2. AdaPrompt: Adaptive Model Training for Prompt-based NLP
</h2>

Title: [AdaPrompt: Adaptive Model Training for Prompt-based NLP](https://arxiv.org/abs/2202.04824)

Authors: [Yulong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in community. The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled by pre-trained language models (PLMs). However, for prompt learning, there are still two salient gaps between NLP tasks and pretraining. First, prompt information is not necessarily sufficiently present during LM pretraining. Second, task-specific data are not necessarily well represented during pretraining. We address these two issues by proposing AdaPrompt, adaptively retrieving external data for continual pretraining of PLMs by making use of both task and prompt characteristics. In addition, we make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers. Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings. In addition, in zero-shot settings, our method outperforms standard prompt-based methods by up to 26.35\% relative error reduction.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.04824](https://arxiv.org/abs/2202.04824) [cs.CL]** |
|           | (or **[arXiv:2202.04824v1](https://arxiv.org/abs/2202.04824v1) [cs.CL]** for this version) |





<h2 id="2022-02-11-3">3. Slovene SuperGLUE Benchmark: Translation and Evaluation
</h2>

Title: [Slovene SuperGLUE Benchmark: Translation and Evaluation](https://arxiv.org/abs/2202.04994)

Authors: [Aleš Žagar](https://arxiv.org/search/cs?searchtype=author&query=Žagar%2C+A), [Marko Robnik-Šikonja](https://arxiv.org/search/cs?searchtype=author&query=Robnik-Šikonja%2C+M)

> We present a Slovene combined machine-human translated SuperGLUE benchmark. We describe the translation process and problems arising due to differences in morphology and grammar. We evaluate the translated datasets in several modes: monolingual, cross-lingual, and multilingual, taking into account differences between machine and human translated training sets. The results show that the monolingual Slovene SloBERTa model is superior to massively multilingual and trilingual BERT models, but these also show a good cross-lingual performance on certain tasks. The performance of Slovene models still lags behind the best English models.

| Comments: | arXiv admin note: text overlap with [arXiv:2107.10614](https://arxiv.org/abs/2107.10614) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.04994](https://arxiv.org/abs/2202.04994) [cs.CL]** |
|           | (or **[arXiv:2202.04994v1](https://arxiv.org/abs/2202.04994v1) [cs.CL]** for this version) |





<h2 id="2022-02-11-4">4. Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding
</h2>

Title: [Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding](https://arxiv.org/abs/2202.05209)

Authors: [Peter Sullivan](https://arxiv.org/search/cs?searchtype=author&query=Sullivan%2C+P), [Toshiko Shibano](https://arxiv.org/search/cs?searchtype=author&query=Shibano%2C+T), [Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+M)

> ASR systems designed for native English (L1) usually underperform on non-native English (L2). To address this performance gap, \textbf{(i)} we extend our previous work to investigate fine-tuning of a pre-trained wav2vec 2.0 model \cite{baevski2020wav2vec,xu2021self} under a rich set of L1 and L2 training conditions. We further \textbf{(ii)} incorporate language model decoding in the ASR system, along with the fine-tuning method. Quantifying gains acquired from each of these two approaches separately and an error analysis allows us to identify different sources of improvement within our models. We find that while the large self-trained wav2vec 2.0 may be internalizing sufficient decoding knowledge for clean L1 speech \cite{xu2021self}, this does not hold for L2 speech and accounts for the utility of employing language model decoding on L2 data.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:2110.00678](https://arxiv.org/abs/2110.00678) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2202.05209](https://arxiv.org/abs/2202.05209) [cs.CL]** |
|           | (or **[arXiv:2202.05209v1](https://arxiv.org/abs/2202.05209v1) [cs.CL]** for this version) |





# 2022-02-10

[Return to Index](#Index)



<h2 id="2022-02-10-1">1. Machine Explanations and Human Understanding
</h2>

Title: [Machine Explanations and Human Understanding](https://arxiv.org/abs/2202.04092)

Authors: [Chacha Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Shi Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S), [Amit Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+A), [Chenhao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+C)

> Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human understanding, and show how human intuitions play a central role in enabling human understanding. Specifically, we identify three core concepts of interest that cover all existing quantitative measures of understanding in the context of human-AI decision making: task decision boundary, model decision boundary, and model error. Our key result is that without assumptions about task-specific intuitions, explanations may potentially improve human understanding of model decision boundary, but they cannot improve human understanding of task decision boundary or model error. To achieve complementary human-AI performance, we articulate possible ways on how explanations need to work with human intuitions. For instance, human intuitions about the relevance of features (e.g., education is more important than age in predicting a person's income) can be critical in detecting model error. We validate the importance of human intuitions in shaping the outcome of machine explanations with empirical human-subject studies. Overall, our work provides a general framework along with actionable implications for future algorithmic development and empirical experiments of machine explanations.

| Comments: | 26 pages, 13 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2202.04092](https://arxiv.org/abs/2202.04092) [cs.AI]** |
|           | (or **[arXiv:2202.04092v1](https://arxiv.org/abs/2202.04092v1) [cs.AI]** for this version) |





<h2 id="2022-02-10-2">2. Image Difference Captioning with Pre-training and Contrastive Learning
</h2>

Title: [Image Difference Captioning with Pre-training and Contrastive Learning](https://arxiv.org/abs/2202.04298)

Authors: [Linli Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+L), [Weiying Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Qin Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Q)

> The Image Difference Captioning (IDC) task aims to describe the visual differences between two similar images with natural language. The major challenges of this task lie in two aspects: 1) fine-grained visual differences that require learning stronger vision and language association and 2) high-cost of manual annotations that leads to limited supervised data. To address these challenges, we propose a new modeling framework following the pre-training-finetuning paradigm. Specifically, we design three self-supervised tasks and contrastive learning strategies to align visual differences and text descriptions at a fine-grained level. Moreover, we propose a data expansion strategy to utilize extra cross-task supervision information, such as data for fine-grained image classification, to alleviate the limitation of available supervised IDC data. Extensive experiments on two IDC benchmark datasets, CLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed modeling framework. The codes and models will be released at [this https URL](https://github.com/yaolinli/IDC).

| Comments: | Accepted to AAAI2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Multimedia (cs.MM)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2202.04298](https://arxiv.org/abs/2202.04298) [cs.MM]** |
|           | (or **[arXiv:2202.04298v1](https://arxiv.org/abs/2202.04298v1) [cs.MM]** for this version) |





<h2 id="2022-02-10-3">3. Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models
</h2>

Title: [Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models](https://arxiv.org/abs/2202.04173)

Authors: [Boxin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Wei Ping](https://arxiv.org/search/cs?searchtype=author&query=Ping%2C+W), [Chaowei Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C), [Peng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P), [Mostofa Patwary](https://arxiv.org/search/cs?searchtype=author&query=Patwary%2C+M), [Mohammad Shoeybi](https://arxiv.org/search/cs?searchtype=author&query=Shoeybi%2C+M), [Bo Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Anima Anandkumar](https://arxiv.org/search/cs?searchtype=author&query=Anandkumar%2C+A), [Bryan Catanzaro](https://arxiv.org/search/cs?searchtype=author&query=Catanzaro%2C+B)

> Pre-trained language models (LMs) are shown to easily generate toxic language. In this work, we systematically explore domain-adaptive training to reduce the toxicity of language models. We conduct this study on three dimensions: training corpus, model size, and parameter efficiency. For the training corpus, we propose to leverage the generative power of LMs and generate nontoxic datasets for domain-adaptive training, which mitigates the exposure bias and is shown to be more data-efficient than using a curated pre-training corpus. We demonstrate that the self-generation method consistently outperforms the existing baselines across various model sizes on both automatic and human evaluations, even when it uses a 1/3 smaller training corpus. We then comprehensively study detoxifying LMs with parameter sizes ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never been studied before. We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify. We also explore parameter-efficient training methods for detoxification. We demonstrate that adding and training adapter-only layers in LMs not only saves a lot of parameters but also achieves a better trade-off between toxicity and perplexity than whole model adaptation for the large-scale models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.04173](https://arxiv.org/abs/2202.04173) [cs.CL]** |
|           | (or **[arXiv:2202.04173v1](https://arxiv.org/abs/2202.04173v1) [cs.CL]** for this version) |





<h2 id="2022-02-10-4">4. pNLP-Mixer: an Efficient all-MLP Architecture for Language
</h2>

Title: [pNLP-Mixer: an Efficient all-MLP Architecture for Language](https://arxiv.org/abs/2202.04350)

Authors: [Francesco Fusco](https://arxiv.org/search/cs?searchtype=author&query=Fusco%2C+F), [Damian Pascual](https://arxiv.org/search/cs?searchtype=author&query=Pascual%2C+D), [Peter Staar](https://arxiv.org/search/cs?searchtype=author&query=Staar%2C+P)

> Large pre-trained language models drastically changed the natural language processing(NLP) landscape. Nowadays, they represent the go-to framework to tackle diverse NLP tasks, even with a limited number of annotations. However, using those models in production, either in the cloud or at the edge, remains a challenge due to the memory footprint and/or inference costs. As an alternative, recent work on efficient NLP has shown that small weight-efficient models can reach competitive performance at a fraction of the costs. Here, we introduce pNLP-Mixer, an embbedding-free model based on the MLP-Mixer architecture that achieves high weight-efficiency thanks to a novel linguistically informed projection layer. We evaluate our model on two multi-lingual semantic parsing datasets, MTOP and multiATIS. On MTOP our pNLP-Mixer almost matches the performance of mBERT, which has 38 times more parameters, and outperforms the state-of-the-art of tiny models (pQRNN) with 3 times fewer parameters. On a long-sequence classification task (Hyperpartisan) our pNLP-Mixer without pretraining outperforms RoBERTa, which has 100 times more parameters, demonstrating the potential of this architecture.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2202.04350](https://arxiv.org/abs/2202.04350) [cs.CL]** |
|           | (or **[arXiv:2202.04350v1](https://arxiv.org/abs/2202.04350v1) [cs.CL]** for this version) |





<h2 id="2022-02-10-5">5. Generating Training Data with Language Models: Towards Zero-Shot Language Understanding
</h2>

Title: [Generating Training Data with Language Models: Towards Zero-Shot Language Understanding](https://arxiv.org/abs/2202.04538)

Authors: [Yu Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Jiaxin Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Yu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Jiawei Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J)

> Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class.

| Comments: | Code: [this https URL](https://github.com/yumeng5/SuperGen)  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2202.04538](https://arxiv.org/abs/2202.04538) [cs.CL]** |
|           | (or **[arXiv:2202.04538v1](https://arxiv.org/abs/2202.04538v1) [cs.CL]** for this version) |





# 2022-02-09

[Return to Index](#Index)



<h2 id="2022-02-08-1">1. DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers
</h2>

Title: [DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers](https://arxiv.org/abs/2202.04053)

Authors: [Jaemin Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+J), [Abhay Zala](https://arxiv.org/search/cs?searchtype=author&query=Zala%2C+A), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Generating images from textual descriptions has gained a lot of attention. Recently, DALL-E, a multimodal transformer language model, and its variants have shown high-quality text-to-image generation capabilities with a simple architecture and training objective, powered by large-scale training data and computation. However, despite the interesting image generation results, there has not been a detailed analysis on how to evaluate such models. In this work, we investigate the reasoning capabilities and social biases of such text-to-image generative transformers in detail. First, we measure four visual reasoning skills: object recognition, object counting, color recognition, and spatial relation understanding. For this, we propose PaintSkills, a diagnostic dataset and evaluation toolkit that measures these four visual reasoning skills. Second, we measure the text alignment and quality of the generated images based on pretrained image captioning, image-text retrieval, and image classification models. Third, we assess social biases in the models. For this, we suggest evaluation of gender and racial biases of text-to-image generation models based on a pretrained image-text retrieval model and human evaluation. In our experiments, we show that recent text-to-image models perform better in recognizing and counting objects than recognizing colors and understanding spatial relations, while there exists a large gap between model performances and oracle accuracy on all skills. Next, we demonstrate that recent text-to-image models learn specific gender/racial biases from web image-text pairs. We also show that our automatic evaluations of visual reasoning skills and gender bias are highly correlated with human judgments. We hope our work will help guide future progress in improving text-to-image models on visual reasoning skills and social biases. Code and data at: [this https URL](https://github.com/j-min/DallEval)

| Comments: | 20 pages, 10 figures, 13 tables                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2202.04053](https://arxiv.org/abs/2202.04053) [cs.CV]** |
|           | (or **[arXiv:2202.04053v1](https://arxiv.org/abs/2202.04053v1) [cs.CV]** for this version) |







# 2022-02-08

[Return to Index](#Index)



<h2 id="2022-02-08-1">1. Machine Translation from Signed to Spoken Languages: State of the Art and Challenges
</h2>

Title: [Machine Translation from Signed to Spoken Languages: State of the Art and Challenges](https://arxiv.org/abs/2202.03086)

Authors: [Mathieu De Coster](https://arxiv.org/search/cs?searchtype=author&query=De+Coster%2C+M), [Dimitar Shterionov](https://arxiv.org/search/cs?searchtype=author&query=Shterionov%2C+D), [Mieke Van Herreweghe](https://arxiv.org/search/cs?searchtype=author&query=Van+Herreweghe%2C+M), [Joni Dambre](https://arxiv.org/search/cs?searchtype=author&query=Dambre%2C+J)

> Automatic translation from signed to spoken languages is an interdisciplinary research domain, lying on the intersection of computer vision, machine translation and linguistics. Nevertheless, research in this domain is performed mostly by computer scientists in isolation. As the domain is becoming increasingly popular - the majority of scientific papers on the topic of sign language translation have been published in the past three years - we provide an overview of the state of the art as well as some required background in the different related disciplines. We give a high-level introduction to sign language linguistics and machine translation to illustrate the requirements of automatic sign language translation. We present a systematic literature review to illustrate the state of the art in the domain and then, harking back to the requirements, lay out several challenges for future research. We find that significant advances have been made on the shoulders of spoken language machine translation research. However, current approaches are often not linguistically motivated or are not adapted to the different input modality of sign languages. We explore challenges related to the representation of sign language data, the collection of datasets, the need for interdisciplinary research and requirements for moving beyond research, towards applications. Based on our findings, we advocate for interdisciplinary research and to base future research on linguistic analysis of sign languages. Furthermore, the inclusion of deaf and hearing end users of sign language translation applications in use case identification, data collection and evaluation is of the utmost importance in the creation of useful sign language translation models. We recommend iterative, human-in-the-loop, design and development of sign language translation models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.03086](https://arxiv.org/abs/2202.03086) [cs.CL]** |
|           | (or **[arXiv:2202.03086v1](https://arxiv.org/abs/2202.03086v1) [cs.CL]** for this version) |





<h2 id="2022-02-08-2">2. Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition
</h2>

Title: [Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition](https://arxiv.org/abs/2202.03218)

Authors: [Bethan Thomas](https://arxiv.org/search/cs?searchtype=author&query=Thomas%2C+B), [Samuel Kessler](https://arxiv.org/search/cs?searchtype=author&query=Kessler%2C+S), [Salah Karout](https://arxiv.org/search/cs?searchtype=author&query=Karout%2C+S)

> Self-supervised learning (SSL) is a powerful tool that allows learning of underlying representations from unlabeled data. Transformer based models such as wav2vec 2.0 and HuBERT are leading the field in the speech domain. Generally these models are fine-tuned on a small amount of labeled data for a downstream task such as Automatic Speech Recognition (ASR). This involves re-training the majority of the model for each task. Adapters are small lightweight modules which are commonly used in Natural Language Processing (NLP) to adapt pre-trained models to new tasks. In this paper we propose applying adapters to wav2vec 2.0 to reduce the number of parameters required for downstream ASR tasks, and increase scalability of the model to multiple tasks or languages. Using adapters we can perform ASR while training fewer than 10% of parameters per task compared to full fine-tuning with little degradation of performance. Ablations show that applying adapters into just the top few layers of the pre-trained network gives similar performance to full transfer, supporting the theory that higher pre-trained layers encode more phonemic information, and further optimizing efficiency.

| Comments: | 5 Pages, 4 figures. Accepted to ICASSP 2022                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2202.03218](https://arxiv.org/abs/2202.03218) [cs.CL]** |
|           | (or **[arXiv:2202.03218v1](https://arxiv.org/abs/2202.03218v1) [cs.CL]** for this version) |





<h2 id="2022-02-08-3">3. Red Teaming Language Models with Language Models
</h2>

Title: [Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286)

Authors: [Ethan Perez](https://arxiv.org/search/cs?searchtype=author&query=Perez%2C+E), [Saffron Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Francis Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+F), [Trevor Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+T), [Roman Ring](https://arxiv.org/search/cs?searchtype=author&query=Ring%2C+R), [John Aslanides](https://arxiv.org/search/cs?searchtype=author&query=Aslanides%2C+J), [Amelia Glaese](https://arxiv.org/search/cs?searchtype=author&query=Glaese%2C+A), [Nat McAleese](https://arxiv.org/search/cs?searchtype=author&query=McAleese%2C+N), [Geoffrey Irving](https://arxiv.org/search/cs?searchtype=author&query=Irving%2C+G)

> Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ("red teaming") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.03286](https://arxiv.org/abs/2202.03286) [cs.CL]** |
|           | (or **[arXiv:2202.03286v1](https://arxiv.org/abs/2202.03286v1) [cs.CL]** for this version) |







# 2022-02-07

[Return to Index](#Index)



<h2 id="2022-02-07-1">1. Data Scaling Laws in NMT: The Effect of Noise and Architecture
</h2>

Title: [Data Scaling Laws in NMT: The Effect of Noise and Architecture](https://arxiv.org/abs/2202.01994)

Authors: [Yamini Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+Y), [Behrooz Ghorbani](https://arxiv.org/search/cs?searchtype=author&query=Ghorbani%2C+B), [Ankush Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+A), [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Maxim Krikun](https://arxiv.org/search/cs?searchtype=author&query=Krikun%2C+M), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [Behnam Neyshabur](https://arxiv.org/search/cs?searchtype=author&query=Neyshabur%2C+B), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

> In this work, we study the effect of varying the architecture and training data quality on the data scaling properties of Neural Machine Translation (NMT). First, we establish that the test loss of encoder-decoder transformer models scales as a power law in the number of training samples, with a dependence on the model size. Then, we systematically vary aspects of the training setup to understand how they impact the data scaling laws. In particular, we change the following (1) Architecture and task setup: We compare to a transformer-LSTM hybrid, and a decoder-only transformer with a language modeling loss (2) Noise level in the training distribution: We experiment with filtering, and adding iid synthetic noise. In all the above cases, we find that the data scaling exponents are minimally impacted, suggesting that marginally worse architectures or training data can be compensated for by adding more data. Lastly, we find that using back-translated data instead of parallel data, can significantly degrade the scaling exponent.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.01994](https://arxiv.org/abs/2202.01994) [cs.LG]** |
|           | (or **[arXiv:2202.01994v1](https://arxiv.org/abs/2202.01994v1) [cs.LG]** for this version) |





<h2 id="2022-02-07-2">2. Temporal Attention for Language Models
</h2>

Title: [Temporal Attention for Language Models](https://arxiv.org/abs/2202.02093)

Authors: [Guy D. Rosin](https://arxiv.org/search/cs?searchtype=author&query=Rosin%2C+G+D), [Kira Radinsky](https://arxiv.org/search/cs?searchtype=author&query=Radinsky%2C+K)

> Pretrained language models based on the transformer architecture have shown great success in NLP. Textual training data often comes from the web and is thus tagged with time-specific information, but most language models ignore this information. They are trained on the textual data alone, limiting their ability to generalize temporally. In this work, we extend the key component of the transformer architecture, i.e., the self-attention mechanism, and propose temporal attention - a time-aware self-attention mechanism. Temporal attention can be applied to any transformer model and requires the input texts to be accompanied with their relevant time points. It allows the transformer to capture this temporal information and create time-specific contextualized word representations. We leverage these representations for the task of semantic change detection; we apply our proposed mechanism to BERT and experiment on three datasets in different languages (English, German, and Latin) that also vary in time, size, and genre. Our proposed model achieves state-of-the-art results on all the datasets.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.02093](https://arxiv.org/abs/2202.02093) [cs.CL]** |
|           | (or **[arXiv:2202.02093v1](https://arxiv.org/abs/2202.02093v1) [cs.CL]** for this version) |





<h2 id="2022-02-07-3">3. The Ecological Footprint of Neural Machine Translation Systems
</h2>

Title: [The Ecological Footprint of Neural Machine Translation Systems](https://arxiv.org/abs/2202.02170)

Authors: [Dimitar Sherionov](https://arxiv.org/search/cs?searchtype=author&query=Sherionov%2C+D), [Eva Vanmassenhove](https://arxiv.org/search/cs?searchtype=author&query=Vanmassenhove%2C+E)

> Over the past decade, deep learning (DL) has led to significant advancements in various fields of artificial intelligence, including machine translation (MT). These advancements would not be possible without the ever-growing volumes of data and the hardware that allows large DL models to be trained efficiently. Due to the large amount of computing cores as well as dedicated memory, graphics processing units (GPUs) are a more effective hardware solution for training and inference with DL models than central processing units (CPUs). However, the former is very power demanding. The electrical power consumption has economical as well as ecological implications. 
> This chapter focuses on the ecological footprint of neural MT systems. It starts from the power drain during the training of and the inference with neural MT models and moves towards the environment impact, in terms of carbon dioxide emissions. Different architectures (RNN and Transformer) and different GPUs (consumer-grate NVidia 1080Ti and workstation-grade NVidia P100) are compared. Then, the overall CO2 offload is calculated for Ireland and the Netherlands. The NMT models and their ecological impact are compared to common household appliances to draw a more clear picture. 
> The last part of this chapter analyses quantization, a technique for reducing the size and complexity of models, as a way to reduce power consumption. As quantized models can run on CPUs, they present a power-efficient inference solution without depending on a GPU.

| Comments: | 25 pages, 3 figures, 10 tables                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.02170](https://arxiv.org/abs/2202.02170) [cs.CL]** |
|           | (or **[arXiv:2202.02170v1](https://arxiv.org/abs/2202.02170v1) [cs.CL]** for this version) |




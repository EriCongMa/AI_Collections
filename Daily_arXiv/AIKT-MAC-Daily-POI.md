# MA C.'s Daily Paper Of Interest - February, 2022

# Index

- [2022-02-21](#2022-02-21)
  - [1. Improving English to Sinhala Neural Machine Translation using Part-of-Speech Tag](#2022-02-21-1)
  - [2. VLP: A Survey on Vision-Language Pre-training](#2022-02-21-2)
  
- [2022-02-18](#2022-02-18)
  - [1. End-to-End Training of Both Translation Models in the Back-Translation Framework](#2022-02-18-1)
  - [2. cosFormer: Rethinking Softmax in Attention](#2022-02-18-2)


- [2022-02-17](#2022-02-17)

  - [1. On the Self Shuffle Language](#2022-02-17-1)
  - [2. ZeroGen: Efficient Zero-shot Learning via Dataset Generation](#2022-02-17-2)
  - [3. Revisiting Parameter-Efficient Tuning: Are We Really There Yet?](#2022-02-17-3)
  - [4. Should You Mask 15% in Masked Language Modeling?](#2022-02-17-4)
- [2022-02-16](#2022-02-16)

  - [1. A Survey on Dynamic Neural Networks for Natural Language Processing](#2022-02-16-1)
  - [2. A Survey on Model Compression for Natural Language Processing](#2022-02-16-2)
  - [3. MuLD: The Multitask Long Document Benchmark](#2022-02-16-3)
  - [4. BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer](#2022-02-16-4)
  - [5. Delving Deeper into Cross-lingual Visual Question Answering](#2022-02-16-5)

- [2022-02-15](#2022-02-15)

  - [1. https://arxiv.org/abs/2202.06045](#2022-02-15-1)
  - [2. A Contrastive Framework for Neural Text Generation](#2022-02-15-2)
  - [3. I-Tuning: Tuning Language Models with Image for Caption Generation](#2022-02-15-3)

- [2022-02-14](#2022-02-14)

  - [1. Including Facial Expressions in Contextual Embeddings for Sign Language Generation](#2022-02-14-1)
  - [2. Evaluating MT Systems: A Theoretical Framework](#2022-02-14-2)

- [2022-02-11](#2022-02-11)
  - [1. SHAS: Approaching optimal Segmentation for End-to-End Speech Translation](#2022-02-11-1)
  - [2. AdaPrompt: Adaptive Model Training for Prompt-based NLP](#2022-02-11-2)
  - [3. Slovene SuperGLUE Benchmark: Translation and Evaluation](#2022-02-11-3)
  - [4. Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding](#2022-02-11-4)
- [2022-02-10](#2022-02-10)

  - [1. Machine Explanations and Human Understanding](#2022-02-10-1)
  - [2. Image Difference Captioning with Pre-training and Contrastive Learning](#2022-02-10-2)
  - [3. Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models](#2022-02-10-3)
  - [4. pNLP-Mixer: an Efficient all-MLP Architecture for Language](#2022-02-10-4)
  - [5. Generating Training Data with Language Models: Towards Zero-Shot Language Understanding](#2022-02-10-5)
- [2022-02-09](#2022-02-09)

  - [1. DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers](#2022-02-09-1)
- [2022-02-08](#2022-02-08)

  - [1. Machine Translation from Signed to Spoken Languages: State of the Art and Challenges](#2022-02-08-1)
  - [2. Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition](#2022-02-08-2)
  - [3. Red Teaming Language Models with Language Models](#2022-02-08-3)
- [2022-02-07](#2022-02-07)

  - [1. Data Scaling Laws in NMT: The Effect of Noise and Architecture](#2022-02-07-1)
  - [2. Temporal Attention for Language Models](#2022-02-07-2)
  - [3. The Ecological Footprint of Neural Machine Translation Systems](#2022-02-07-3)
- [2022-01-28](#2022-01-28)
  - [1. Tackling data scarcity in speech translation using zero-shot multilingual machine translation techniques](#2022-01-28-1)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MAC-Daily-POI-index.md)



# 2022-02-21

[Return to Index](#Index)



<h2 id="2022-02-21-1">1. Improving English to Sinhala Neural Machine Translation using Part-of-Speech Tag
</h2>

Title: [Improving English to Sinhala Neural Machine Translation using Part-of-Speech Tag](https://arxiv.org/abs/2202.08882)

Authors: [Ravinga Perera](https://arxiv.org/search/cs?searchtype=author&query=Perera%2C+R), [Thilakshi Fonseka](https://arxiv.org/search/cs?searchtype=author&query=Fonseka%2C+T), [Rashmini Naranpanawa](https://arxiv.org/search/cs?searchtype=author&query=Naranpanawa%2C+R), [Uthayasanker Thayasivam](https://arxiv.org/search/cs?searchtype=author&query=Thayasivam%2C+U)

> The performance of Neural Machine Translation (NMT) depends significantly on the size of the available parallel corpus. Due to this fact, low resource language pairs demonstrate low translation performance compared to high resource language pairs. The translation quality further degrades when NMT is performed for morphologically rich languages. Even though the web contains a large amount of information, most people in Sri Lanka are unable to read and understand English properly. Therefore, there is a huge requirement of translating English content to local languages to share information among locals. Sinhala language is the primary language in Sri Lanka and building an NMT system that can produce quality English to Sinhala translations is difficult due to the syntactic divergence between these two languages under low resource constraints. Thus, in this research, we explore effective methods of incorporating Part of Speech (POS) tags to the Transformer input embedding and positional encoding to further enhance the performance of the baseline English to Sinhala neural machine translation model.

| Subjects:    | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| ------------ | ------------------------------------------------------------ |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2202.08882](https://arxiv.org/abs/2202.08882) [cs.CL]** |
|              | (or **[arXiv:2202.08882v1](https://arxiv.org/abs/2202.08882v1) [cs.CL]** for this version) |
|              | https://doi.org/10.48550/arXiv.2202.08882Focus to learn more |







<h2 id="2022-02-21-2">2. VLP: A Survey on Vision-Language Pre-training
</h2>

Title: [VLP: A Survey on Vision-Language Pre-training](https://arxiv.org/abs/2202.09061)

Authors: [Feilong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+F), [Duzhan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D), [Minglun Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+M), [Xiuyi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Jing Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+J), [Shuang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S), [Bo Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+B)

> In the past few years, the emergence of pre-training models has brought uni-modal fields such as computer vision (CV) and natural language processing (NLP) to a new era. Substantial works have shown they are beneficial for downstream uni-modal tasks and avoid training a new model from scratch. So can such pre-trained models be applied to multi-modal tasks? Researchers have explored this problem and made significant progress. This paper surveys recent advances and new frontiers in vision-language pre-training (VLP), including image-text and video-text pre-training. To give readers a better overall grasp of VLP, we first review its recent advances from five aspects: feature extraction, model architecture, pre-training objectives, pre-training datasets, and downstream tasks. Then, we summarize the specific VLP models in detail. Finally, we discuss the new frontiers in VLP. To the best of our knowledge, this is the first survey on VLP. We hope that this survey can shed light on future research in the VLP field.

| Comments: | A Survey on Vision-Language Pre-training                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2202.09061](https://arxiv.org/abs/2202.09061) [cs.CV]** |
|           | (or **[arXiv:2202.09061v1](https://arxiv.org/abs/2202.09061v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.09061Focus to learn more |







# 2022-02-18

[Return to Index](#Index)



<h2 id="2022-02-18-1">1. End-to-End Training of Both Translation Models in the Back-Translation Framework
</h2>

Title: [End-to-End Training of Both Translation Models in the Back-Translation Framework](https://arxiv.org/abs/2202.08465)

Authors: [DongNyeong Heo](https://arxiv.org/search/cs?searchtype=author&query=Heo%2C+D), [Heeyoul Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+H)

> Semi-supervised learning algorithms in neural machine translation (NMT) have significantly improved translation quality compared to the supervised learning algorithms by using additional monolingual corpora. Among them, back-translation is a theoretically well-structured and cutting-edge method. Given two pre-trained NMT models between source and target languages, one translates a monolingual sentence as a latent sentence, and the other reconstructs the monolingual input sentence given the latent sentence. Therefore, previous works tried to apply the variational auto-encoder's (VAE) training framework to the back-translation framework. However, the discrete property of the latent sentence made it impossible to use backpropagation in the framework. This paper proposes a categorical reparameterization trick that generates a differentiable sentence, with which we practically implement the VAE's training framework for the back-translation and train it by end-to-end backpropagation. In addition, we propose several regularization techniques that are especially advantageous to this framework. In our experiments, we demonstrate that our method makes backpropagation available through the latent sentences and improves the BLEU scores on the datasets of the WMT18 translation task.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.08465](https://arxiv.org/abs/2202.08465) [cs.CL]** |
|           | (or **[arXiv:2202.08465v1](https://arxiv.org/abs/2202.08465v1) [cs.CL]** for this version) |





<h2 id="2022-02-18-2">2. cosFormer: Rethinking Softmax in Attention
</h2>

Title: [cosFormer: Rethinking Softmax in Attention](https://arxiv.org/abs/2202.08791)

Authors: [Zhen Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Z), [Weixuan Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+W), [Hui Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+H), [Dongxu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+D), [Yunshen Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Y), [Baohong Lv](https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+B), [Junjie Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+J), [Lingpeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L), [Yiran Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+Y)

> Transformer has shown great successes in natural language processing, computer vision, and audio processing. As one of its core components, the softmax attention helps to capture long-range dependencies yet prohibits its scale-up due to the quadratic space and time complexity to the sequence length. Kernel methods are often adopted to reduce the complexity by approximating the softmax operator. Nevertheless, due to the approximation errors, their performances vary in different tasks/corpus and suffer crucial performance drops when compared with the vanilla softmax attention. In this paper, we propose a linear transformer called cosFormer that can achieve comparable or better accuracy to the vanilla transformer in both casual and cross attentions. cosFormer is based on two key properties of softmax attention: i). non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme that can concentrate the distribution of the attention matrix. As its linear substitute, cosFormer fulfills these properties with a linear operator and a cosine-based distance re-weighting mechanism. Extensive experiments on language modeling and text understanding tasks demonstrate the effectiveness of our method. We further examine our method on long sequences and achieve state-of-the-art performance on the Long-Range Arena benchmark. The source code is available at [this https URL](https://github.com/OpenNLPLab/cosFormer).

| Comments: | Accepted to ICLR2022. Yiran Zhong is the corresponding author. Zhen Qin, Weixuan Sun, Hui Deng contributed equally to this work |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.08791](https://arxiv.org/abs/2202.08791) [cs.CL]** |
|           | (or **[arXiv:2202.08791v1](https://arxiv.org/abs/2202.08791v1) [cs.CL]** for this version) |



# 2022-02-17

[Return to Index](#Index)



<h2 id="2022-02-17-1">1. On the Self Shuffle Language
</h2>

Title: [On the Self Shuffle Language](https://arxiv.org/abs/2202.07988)

Authors: [Pamela Fleischmann](https://arxiv.org/search/math?searchtype=author&query=Fleischmann%2C+P), [Tero Harju](https://arxiv.org/search/math?searchtype=author&query=Harju%2C+T), [Lukas Haschke](https://arxiv.org/search/math?searchtype=author&query=Haschke%2C+L), [Jonas Höfer](https://arxiv.org/search/math?searchtype=author&query=Höfer%2C+J), [Dirk Nowotka](https://arxiv.org/search/math?searchtype=author&query=Nowotka%2C+D)

> The shuffle product \(u\shuffle v\) of two words \(u\) and \(v\) is the set of all words which can be obtained by interleaving \(u\) and \(v\). Motivated by the paper \emph{The Shuffle Product: New Research Directions} by Restivo (2015) we investigate a special case of the shuffle product. In this work we consider the shuffle of a word with itself called the \emph{self shuffle} or \emph{shuffle square}, showing first that the self shuffle language and the shuffle of the language are in general different sets. We prove that the language of all words arising as a self shuffle of some word is context sensitive but not context free. Furthermore, we show that the self shuffle \(w \shuffle w\) uniquely determines \(w\).

| Subjects:    | **Combinatorics (math.CO)**; Computation and Language (cs.CL) |
| ------------ | ------------------------------------------------------------ |
| MSC classes: | 14J60                                                        |
| ACM classes: | F.2.2; I.2.7                                                 |
| Cite as:     | **[arXiv:2202.07988](https://arxiv.org/abs/2202.07988) [math.CO]** |
|              | (or **[arXiv:2202.07988v1](https://arxiv.org/abs/2202.07988v1) [math.CO]** for this version) |
|              | https://doi.org/10.48550/arXiv.2202.07988Focus to learn more |





<h2 id="2022-02-17-2">2. ZeroGen: Efficient Zero-shot Learning via Dataset Generation
</h2>

Title: [ZeroGen: Efficient Zero-shot Learning via Dataset Generation](https://arxiv.org/abs/2202.07922)

Authors: [Jiacheng Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+J), [Jiahui Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J), [Qintong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Q), [Hang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H), [Jiangtao Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J), [Zhiyong Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Tao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+T), [Lingpeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L)

> There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, ZeroGen. Given a zero-shot task, we first generate a dataset from scratch using PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM) under the supervision of the synthesized dataset. This approach allows highly efficient inference as the final task model only has orders of magnitude fewer parameters comparing to PLMs (e.g., GPT2-XL). Apart from being annotation-free and efficient, we argue that ZeroGen can also provide useful insights from the perspective of data-free model-agnostic knowledge distillation, and unreferenced text generation evaluation. Experiments and analysis on different NLP tasks, namely, text classification, question answering, and natural language inference), show the effectiveness of ZeroGen.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07922](https://arxiv.org/abs/2202.07922) [cs.CL]** |
|           | (or **[arXiv:2202.07922v1](https://arxiv.org/abs/2202.07922v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.07922Focus to learn more |





<h2 id="2022-02-17-3">3. Revisiting Parameter-Efficient Tuning: Are We Really There Yet?
</h2>

Title: [Revisiting Parameter-Efficient Tuning: Are We Really There Yet?](https://arxiv.org/abs/2202.07962)

Authors: [Guanzheng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G), [Fangyu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Zaiqiao Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Z), [Shangsong Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+S)

> Parameter-efficient tuning (PETuning) methods have been deemed by many as the new paradigm for using pretrained language models (PLMs). By tuning just a fraction amount of parameters comparing to full model finetuning, PETuning methods claim to have achieved performance on par with or even better than finetuning. In this work, we take a step back and re-examine these PETuning methods by conducting the first comprehensive investigation into the training and evaluation of PETuning methods. We found the problematic validation and testing practice in current studies, when accompanied by the instability nature of PETuning methods, has led to unreliable conclusions. When being compared under a truly fair evaluation protocol, PETuning cannot yield consistently competitive performance while finetuning remains to be the best-performing method in medium- and high-resource settings. We delve deeper into the cause of the instability and observed that model size does not explain the phenomenon but training iteration positively correlates with the stability.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07962](https://arxiv.org/abs/2202.07962) [cs.CL]** |
|           | (or **[arXiv:2202.07962v1](https://arxiv.org/abs/2202.07962v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.07962Focus to learn more |





<h2 id="2022-02-17-4">4. Should You Mask 15% in Masked Language Modeling?
</h2>

Title: [Should You Mask 15% in Masked Language Modeling?](https://arxiv.org/abs/2202.08005)

Authors: [Alexander Wettig](https://arxiv.org/search/cs?searchtype=author&query=Wettig%2C+A), [Tianyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+T), [Zexuan Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+Z), [Danqi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D)

> Masked language models conventionally use a masking rate of 15% due to the belief that more masking would provide insufficient context to learn good representations, and less masking would make training too expensive. Surprisingly, we find that masking up to 40% of input tokens can outperform the 15% baseline, and even masking 80% can preserve most of the performance, as measured by fine-tuning on downstream tasks. Increasing the masking rates has two distinct effects, which we investigate through careful ablations: (1) A larger proportion of input tokens are corrupted, reducing the context size and creating a harder task, and (2) models perform more predictions, which benefits training. We observe that larger models in particular favor higher masking rates, as they have more capacity to perform the harder task. We also connect our findings to sophisticated masking schemes such as span masking and PMI masking, as well as BERT's curious 80-10-10 corruption strategy, and find that simple uniform masking with [MASK] replacements can be competitive at higher masking rates. Our results contribute to a better understanding of masked language modeling and point to new avenues for efficient pre-training.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.08005](https://arxiv.org/abs/2202.08005) [cs.CL]** |
|           | (or **[arXiv:2202.08005v1](https://arxiv.org/abs/2202.08005v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2202.08005Focus to learn more |









# 2022-02-16

[Return to Index](#Index)



<h2 id="2022-02-16-1">1. A Survey on Dynamic Neural Networks for Natural Language Processing
</h2>

Title: [A Survey on Dynamic Neural Networks for Natural Language Processing](https://arxiv.org/abs/2202.07101)

Authors: [Canwen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Julian McAuley](https://arxiv.org/search/cs?searchtype=author&query=McAuley%2C+J)

> Effectively scaling large Transformer models is a main driver of recent advances in natural language processing. Dynamic neural networks, as an emerging research direction, are capable of scaling up neural networks with sub-linear increases in computation and time by dynamically adjusting their computational path based on the input. Dynamic neural networks could be a promising solution to the growing parameter numbers of pretrained language models, allowing both model pretraining with trillions of parameters and faster inference on mobile devices. In this survey, we summarize progress of three types of dynamic neural networks in NLP: skimming, mixture of experts, and early exit. We also highlight current challenges in dynamic neural networks and directions for future research.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07101](https://arxiv.org/abs/2202.07101) [cs.CL]** |
|           | (or **[arXiv:2202.07101v1](https://arxiv.org/abs/2202.07101v1) [cs.CL]** for this version) |





<h2 id="2022-02-16-2">2. A Survey on Model Compression for Natural Language Processing
</h2>

Title: [A Survey on Model Compression for Natural Language Processing](https://arxiv.org/abs/2202.07105)

Authors: [Canwen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C), [Julian McAuley](https://arxiv.org/search/cs?searchtype=author&query=McAuley%2C+J)

> With recent developments in new architectures like Transformer and pretraining techniques, significant progress has been made in applications of natural language processing (NLP). However, the high energy cost and long inference delay of Transformer is preventing NLP from entering broader scenarios including edge and mobile computing. Efficient NLP research aims to comprehensively consider computation, time and carbon emission for the entire life-cycle of NLP, including data preparation, model training and inference. In this survey, we focus on the inference stage and review the current state of model compression for NLP, including the benchmarks, metrics and methodology. We outline the current obstacles and future research directions.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07105](https://arxiv.org/abs/2202.07105) [cs.CL]** |
|           | (or **[arXiv:2202.07105v1](https://arxiv.org/abs/2202.07105v1) [cs.CL]** for this version) |





<h2 id="2022-02-16-3">3. MuLD: The Multitask Long Document Benchmark
</h2>

Title: [MuLD: The Multitask Long Document Benchmark](https://arxiv.org/abs/2202.07362)

Authors: [G Thomas Hudson](https://arxiv.org/search/cs?searchtype=author&query=Hudson%2C+G+T), [Noura Al Moubayed](https://arxiv.org/search/cs?searchtype=author&query=Moubayed%2C+N+A)

> The impressive progress in NLP techniques has been driven by the development of multi-task benchmarks such as GLUE and SuperGLUE. While these benchmarks focus on tasks for one or two input sentences, there has been exciting work in designing efficient techniques for processing much longer inputs. In this paper, we present MuLD: a new long document benchmark consisting of only documents over 10,000 tokens. By modifying existing NLP tasks, we create a diverse benchmark which requires models to successfully model long-term dependencies in the text. We evaluate how existing models perform, and find that our benchmark is much more challenging than their `short document' equivalents. Furthermore, by evaluating both regular and efficient transformers, we show that models with increased context length are better able to solve the tasks presented, suggesting that future improvements in these models are vital for solving similar long document problems. We release the data and code for baselines to encourage further research on efficient NLP models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07362](https://arxiv.org/abs/2202.07362) [cs.CL]** |
|           | (or **[arXiv:2202.07362v1](https://arxiv.org/abs/2202.07362v1) [cs.CL]** for this version) |





<h2 id="2022-02-16-4">4. BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer
</h2>

Title: [BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer](https://arxiv.org/abs/2202.07543)

Authors: [Ana-Maria Bucur](https://arxiv.org/search/cs?searchtype=author&query=Bucur%2C+A), [Adrian Cosma](https://arxiv.org/search/cs?searchtype=author&query=Cosma%2C+A), [Ioan-Bogdan Iordache](https://arxiv.org/search/cs?searchtype=author&query=Iordache%2C+I)

> Memes are prevalent on the internet and continue to grow and evolve alongside our culture. An automatic understanding of memes propagating on the internet can shed light on the general sentiment and cultural attitudes of people. In this work, we present team BLUE's solution for the second edition of the MEMOTION competition. We showcase two approaches for meme classification (i.e. sentiment, humour, offensive, sarcasm and motivation levels) using a text-only method using BERT, and a Multi-Modal-Multi-Task transformer network that operates on both the meme image and its caption to output the final scores. In both approaches, we leverage state-of-the-art pretrained models for text (BERT, Sentence Transformer) and image processing (EfficientNetV4, CLIP). Through our efforts, we obtain first place in task A, second place in task B and third place in task C. In addition, our team obtained the highest average score for all three tasks.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07543](https://arxiv.org/abs/2202.07543) [cs.CL]** |
|           | (or **[arXiv:2202.07543v1](https://arxiv.org/abs/2202.07543v1) [cs.CL]** for this version) |





<h2 id="2022-02-16-5">5. Delving Deeper into Cross-lingual Visual Question Answering
</h2>

Title: [Delving Deeper into Cross-lingual Visual Question Answering](https://arxiv.org/abs/2202.07630)

Authors: [Chen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Jonas Pfeiffer](https://arxiv.org/search/cs?searchtype=author&query=Pfeiffer%2C+J), [Anna Korhonen](https://arxiv.org/search/cs?searchtype=author&query=Korhonen%2C+A), [Ivan Vulic](https://arxiv.org/search/cs?searchtype=author&query=Vulic%2C+I), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> Visual question answering (VQA) is one of the crucial vision-and-language tasks. Yet, the bulk of research until recently has focused only on the English language due to the lack of appropriate evaluation resources. Previous work on cross-lingual VQA has reported poor zero-shot transfer performance of current multilingual multimodal Transformers and large gaps to monolingual performance, attributed mostly to misalignment of text embeddings between the source and target languages, without providing any additional deeper analyses. In this work, we delve deeper and address different aspects of cross-lingual VQA holistically, aiming to understand the impact of input data, fine-tuning and evaluation regimes, and interactions between the two modalities in cross-lingual setups. 1) We tackle low transfer performance via novel methods that substantially reduce the gap to monolingual English performance, yielding +10 accuracy points over existing transfer methods. 2) We study and dissect cross-lingual VQA across different question types of varying complexity, across different multilingual multi-modal Transformers, and in zero-shot and few-shot scenarios. 3) We further conduct extensive analyses on modality biases in training data and models, aimed to further understand why zero-shot performance gaps remain for some question types and languages. We hope that the novel methods and detailed analyses will guide further progress in multilingual VQA.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.07630](https://arxiv.org/abs/2202.07630) [cs.CL]** |
|           | (or **[arXiv:2202.07630v1](https://arxiv.org/abs/2202.07630v1) [cs.CL]** for this version) |





# 2022-02-15

[Return to Index](#Index)



<h2 id="2022-02-15-1">1. USTED: Improving ASR with a Unified Speech and Text Encoder-Decoder
</h2>

Title: [USTED: Improving ASR with a Unified Speech and Text Encoder-Decoder](https://arxiv.org/abs/2202.06045)

Authors:[Bolaji Yusuf](https://arxiv.org/search/cs?searchtype=author&query=Yusuf%2C+B), [Ankur Gandhe](https://arxiv.org/search/cs?searchtype=author&query=Gandhe%2C+A), [Alex Sokolov](https://arxiv.org/search/cs?searchtype=author&query=Sokolov%2C+A)

> Improving end-to-end speech recognition by incorporating external text data has been a longstanding research topic. There has been a recent focus on training E2E ASR models that get the performance benefits of external text data without incurring the extra cost of evaluating an external language model at inference time. In this work, we propose training ASR model jointly with a set of text-to-text auxiliary tasks with which it shares a decoder and parts of the encoder. When we jointly train ASR and masked language model with the 960-hour Librispeech and Opensubtitles data respectively, we observe WER reductions of 16% and 20% on test-other and test-clean respectively over an ASR-only baseline without any extra cost at inference time, and reductions of 6% and 8% compared to a stronger MUTE-L baseline which trains the decoder with the same text data as our model. We achieve further improvements when we train masked language model on Librispeech data or when we use machine translation as the auxiliary task, without significantly sacrificing performance on the task itself.

| Comments: | 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2202.06045](https://arxiv.org/abs/2202.06045) [cs.CL]** |
|           | (or **[arXiv:2202.06045v1](https://arxiv.org/abs/2202.06045v1) [cs.CL]** for this version) |





<h2 id="2022-02-15-2">2. A Contrastive Framework for Neural Text Generation
</h2>

Title: [A Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417)

Authors:[Yixuan Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y), [Tian Lan](https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+T), [Yan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y), [Dani Yogatama](https://arxiv.org/search/cs?searchtype=author&query=Yogatama%2C+D), [Lingpeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L), [Nigel Collier](https://arxiv.org/search/cs?searchtype=author&query=Collier%2C+N)

> Text generation is of great importance to many natural language processing applications. However, maximization-based decoding methods (e.g. beam search) of neural language models often lead to degenerate solutions -- the generated text is unnatural and contains undesirable repetitions. Existing approaches introduce stochasticity via sampling or modify training objectives to decrease probabilities of certain tokens (e.g., unlikelihood training). However, they often lead to solutions that lack coherence. In this work, we show that an underlying reason for model degeneration is the anisotropic distribution of token representations. We present a contrastive solution: (i) SimCTG, a contrastive training objective to calibrate the model's representation space, and (ii) a decoding method -- contrastive search -- to encourage diversity while maintaining coherence in the generated text. Extensive experiments and analyses on three benchmarks from two languages demonstrate that our proposed approach outperforms state-of-the-art text generation methods as evaluated by both human and automatic metrics.

| Comments: | 22 pages, 8 figures, and 8 tables                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.06417](https://arxiv.org/abs/2202.06417) [cs.CL]** |
|           | (or **[arXiv:2202.06417v1](https://arxiv.org/abs/2202.06417v1) [cs.CL]** for this version) |





<h2 id="2022-02-15-3">3. I-Tuning: Tuning Language Models with Image for Caption Generation
</h2>

Title: [I-Tuning: Tuning Language Models with Image for Caption Generation](https://arxiv.org/abs/2202.06574)

Authors:[Ziyang Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Z), [Yadong Xi](https://arxiv.org/search/cs?searchtype=author&query=Xi%2C+Y), [Rongsheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R), [Jing Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+J)

> Recently, tuning the pre-trained language model (PLM) in a parameter-efficient manner becomes a popular topic in the natural language processing area. However, most of them focus on tuning the PLM with the text-only information. In this work, we propose a new perspective to tune the frozen PLM with images for caption generation. We denote our method as I-Tuning, which can automatically filter the vision information from images to adjust the output hidden states of PLM. Evaluating on the image captioning tasks (MSCOCO and Flickr30k Captioning), our method achieves comparable or even better performance than the previous models which have 2-4 times more trainable parameters and/or consume a large amount of cross-modal pre-training data.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2202.06574](https://arxiv.org/abs/2202.06574) [cs.CL]** |
|           | (or **[arXiv:2202.06574v1](https://arxiv.org/abs/2202.06574v1) [cs.CL]** for this version) |





# 2022-02-14

[Return to Index](#Index)



<h2 id="2022-02-14-1">1. Including Facial Expressions in Contextual Embeddings for Sign Language Generation
</h2>

Title: [Including Facial Expressions in Contextual Embeddings for Sign Language Generation](https://arxiv.org/abs/2202.05383)

Authors: [Carla Viegas](https://arxiv.org/search/cs?searchtype=author&query=Viegas%2C+C), [Mert İnan](https://arxiv.org/search/cs?searchtype=author&query=İnan%2C+M), [Lorna Quandt](https://arxiv.org/search/cs?searchtype=author&query=Quandt%2C+L), [Malihe Alikhani](https://arxiv.org/search/cs?searchtype=author&query=Alikhani%2C+M)

> State-of-the-art sign language generation frameworks lack expressivity and naturalness which is the result of only focusing manual signs, neglecting the affective, grammatical and semantic functions of facial expressions. The purpose of this work is to augment semantic representation of sign language through grounding facial expressions. We study the effect of modeling the relationship between text, gloss, and facial expressions on the performance of the sign generation systems. In particular, we propose a Dual Encoder Transformer able to generate manual signs as well as facial expressions by capturing the similarities and differences found in text and sign gloss annotation. We take into consideration the role of facial muscle activity to express intensities of manual signs by being the first to employ facial action units in sign language generation. We perform a series of experiments showing that our proposed model improves the quality of automatically generated sign language.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.05383](https://arxiv.org/abs/2202.05383) [cs.CL]** |
|           | (or **[arXiv:2202.05383v1](https://arxiv.org/abs/2202.05383v1) [cs.CL]** for this version) |





<h2 id="2022-02-14-2">2. Evaluating MT Systems: A Theoretical Framework
</h2>

Title: [Evaluating MT Systems: A Theoretical Framework](https://arxiv.org/abs/2202.05806)

Authors: [Rajeev Sangal](https://arxiv.org/search/cs?searchtype=author&query=Sangal%2C+R)

> This paper outlines a theoretical framework using which different automatic metrics can be designed for evaluation of Machine Translation systems. It introduces the concept of {\em cognitive ease} which depends on {\em adequacy} and {\em lack of fluency}. Thus, cognitive ease becomes the main parameter to be measured rather than comprehensibility. The framework allows the components of cognitive ease to be broken up and computed based on different linguistic levels etc. Independence of dimensions and linearly combining them provides for a highly modular approach. 
> The paper places the existing automatic methods in an overall framework, to understand them better and to improve upon them in future. It can also be used to evaluate the newer types of MT systems, such as speech to speech translation and discourse translation.

| Comments: | 18 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.05806](https://arxiv.org/abs/2202.05806) [cs.CL]** |
|           | (or **[arXiv:2202.05806v1](https://arxiv.org/abs/2202.05806v1) [cs.CL]** for this version) |



# 2022-02-11

[Return to Index](#Index)



<h2 id="2022-02-11-1">1. SHAS: Approaching optimal Segmentation for End-to-End Speech Translation
</h2>

Title: [SHAS: Approaching optimal Segmentation for End-to-End Speech Translation](https://arxiv.org/abs/2202.04774)

Authors: [Ioannis Tsiamas](https://arxiv.org/search/cs?searchtype=author&query=Tsiamas%2C+I), [Gerard I. Gállego](https://arxiv.org/search/cs?searchtype=author&query=Gállego%2C+G+I), [José A. R. Fonollosa](https://arxiv.org/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R), [Marta R. Costa-jussà](https://arxiv.org/search/cs?searchtype=author&query=Costa-jussà%2C+M+R)

> Speech translation models are unable to directly process long audios, like TED talks, which have to be split into shorter segments. Speech translation datasets provide manual segmentations of the audios, which are not available in real-world scenarios, and existing segmentation methods usually significantly reduce translation quality at inference time. To bridge the gap between the manual segmentation of training and the automatic one at inference, we propose Supervised Hybrid Audio Segmentation (SHAS), a method that can effectively learn the optimal segmentation from any manually segmented speech corpus. First, we train a classifier to identify the included frames in a segmentation, using speech representations from a pre-trained wav2vec 2.0. The optimal splitting points are then found by a probabilistic Divide-and-Conquer algorithm that progressively splits at the frame of lowest probability until all segments are below a pre-specified length. Experiments on MuST-C and mTEDx show that the translation of the segments produced by our method approaches the quality of the manual segmentation on 5 languages pairs. Namely, SHAS retains 95-98% of the manual segmentation's BLEU score, compared to the 87-93% of the best existing methods. Our method is additionally generalizable to different domains and achieves high zero-shot performance in unseen languages.

| Comments: | 7 pages including appendix                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Sound (cs.SD)**; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2202.04774](https://arxiv.org/abs/2202.04774) [cs.SD]** |
|           | (or **[arXiv:2202.04774v1](https://arxiv.org/abs/2202.04774v1) [cs.SD]** for this version) |





<h2 id="2022-02-11-2">2. AdaPrompt: Adaptive Model Training for Prompt-based NLP
</h2>

Title: [AdaPrompt: Adaptive Model Training for Prompt-based NLP](https://arxiv.org/abs/2202.04824)

Authors: [Yulong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in community. The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled by pre-trained language models (PLMs). However, for prompt learning, there are still two salient gaps between NLP tasks and pretraining. First, prompt information is not necessarily sufficiently present during LM pretraining. Second, task-specific data are not necessarily well represented during pretraining. We address these two issues by proposing AdaPrompt, adaptively retrieving external data for continual pretraining of PLMs by making use of both task and prompt characteristics. In addition, we make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers. Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings. In addition, in zero-shot settings, our method outperforms standard prompt-based methods by up to 26.35\% relative error reduction.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.04824](https://arxiv.org/abs/2202.04824) [cs.CL]** |
|           | (or **[arXiv:2202.04824v1](https://arxiv.org/abs/2202.04824v1) [cs.CL]** for this version) |





<h2 id="2022-02-11-3">3. Slovene SuperGLUE Benchmark: Translation and Evaluation
</h2>

Title: [Slovene SuperGLUE Benchmark: Translation and Evaluation](https://arxiv.org/abs/2202.04994)

Authors: [Aleš Žagar](https://arxiv.org/search/cs?searchtype=author&query=Žagar%2C+A), [Marko Robnik-Šikonja](https://arxiv.org/search/cs?searchtype=author&query=Robnik-Šikonja%2C+M)

> We present a Slovene combined machine-human translated SuperGLUE benchmark. We describe the translation process and problems arising due to differences in morphology and grammar. We evaluate the translated datasets in several modes: monolingual, cross-lingual, and multilingual, taking into account differences between machine and human translated training sets. The results show that the monolingual Slovene SloBERTa model is superior to massively multilingual and trilingual BERT models, but these also show a good cross-lingual performance on certain tasks. The performance of Slovene models still lags behind the best English models.

| Comments: | arXiv admin note: text overlap with [arXiv:2107.10614](https://arxiv.org/abs/2107.10614) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.04994](https://arxiv.org/abs/2202.04994) [cs.CL]** |
|           | (or **[arXiv:2202.04994v1](https://arxiv.org/abs/2202.04994v1) [cs.CL]** for this version) |





<h2 id="2022-02-11-4">4. Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding
</h2>

Title: [Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding](https://arxiv.org/abs/2202.05209)

Authors: [Peter Sullivan](https://arxiv.org/search/cs?searchtype=author&query=Sullivan%2C+P), [Toshiko Shibano](https://arxiv.org/search/cs?searchtype=author&query=Shibano%2C+T), [Muhammad Abdul-Mageed](https://arxiv.org/search/cs?searchtype=author&query=Abdul-Mageed%2C+M)

> ASR systems designed for native English (L1) usually underperform on non-native English (L2). To address this performance gap, \textbf{(i)} we extend our previous work to investigate fine-tuning of a pre-trained wav2vec 2.0 model \cite{baevski2020wav2vec,xu2021self} under a rich set of L1 and L2 training conditions. We further \textbf{(ii)} incorporate language model decoding in the ASR system, along with the fine-tuning method. Quantifying gains acquired from each of these two approaches separately and an error analysis allows us to identify different sources of improvement within our models. We find that while the large self-trained wav2vec 2.0 may be internalizing sufficient decoding knowledge for clean L1 speech \cite{xu2021self}, this does not hold for L2 speech and accounts for the utility of employing language model decoding on L2 data.

| Comments: | arXiv admin note: substantial text overlap with [arXiv:2110.00678](https://arxiv.org/abs/2110.00678) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2202.05209](https://arxiv.org/abs/2202.05209) [cs.CL]** |
|           | (or **[arXiv:2202.05209v1](https://arxiv.org/abs/2202.05209v1) [cs.CL]** for this version) |





# 2022-02-10

[Return to Index](#Index)



<h2 id="2022-02-10-1">1. Machine Explanations and Human Understanding
</h2>

Title: [Machine Explanations and Human Understanding](https://arxiv.org/abs/2202.04092)

Authors: [Chacha Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C), [Shi Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S), [Amit Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+A), [Chenhao Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+C)

> Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human understanding, and show how human intuitions play a central role in enabling human understanding. Specifically, we identify three core concepts of interest that cover all existing quantitative measures of understanding in the context of human-AI decision making: task decision boundary, model decision boundary, and model error. Our key result is that without assumptions about task-specific intuitions, explanations may potentially improve human understanding of model decision boundary, but they cannot improve human understanding of task decision boundary or model error. To achieve complementary human-AI performance, we articulate possible ways on how explanations need to work with human intuitions. For instance, human intuitions about the relevance of features (e.g., education is more important than age in predicting a person's income) can be critical in detecting model error. We validate the importance of human intuitions in shaping the outcome of machine explanations with empirical human-subject studies. Overall, our work provides a general framework along with actionable implications for future algorithmic development and empirical experiments of machine explanations.

| Comments: | 26 pages, 13 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Artificial Intelligence (cs.AI)**; Computation and Language (cs.CL); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC) |
| Cite as:  | **[arXiv:2202.04092](https://arxiv.org/abs/2202.04092) [cs.AI]** |
|           | (or **[arXiv:2202.04092v1](https://arxiv.org/abs/2202.04092v1) [cs.AI]** for this version) |





<h2 id="2022-02-10-2">2. Image Difference Captioning with Pre-training and Contrastive Learning
</h2>

Title: [Image Difference Captioning with Pre-training and Contrastive Learning](https://arxiv.org/abs/2202.04298)

Authors: [Linli Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+L), [Weiying Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Qin Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Q)

> The Image Difference Captioning (IDC) task aims to describe the visual differences between two similar images with natural language. The major challenges of this task lie in two aspects: 1) fine-grained visual differences that require learning stronger vision and language association and 2) high-cost of manual annotations that leads to limited supervised data. To address these challenges, we propose a new modeling framework following the pre-training-finetuning paradigm. Specifically, we design three self-supervised tasks and contrastive learning strategies to align visual differences and text descriptions at a fine-grained level. Moreover, we propose a data expansion strategy to utilize extra cross-task supervision information, such as data for fine-grained image classification, to alleviate the limitation of available supervised IDC data. Extensive experiments on two IDC benchmark datasets, CLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed modeling framework. The codes and models will be released at [this https URL](https://github.com/yaolinli/IDC).

| Comments: | Accepted to AAAI2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Multimedia (cs.MM)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2202.04298](https://arxiv.org/abs/2202.04298) [cs.MM]** |
|           | (or **[arXiv:2202.04298v1](https://arxiv.org/abs/2202.04298v1) [cs.MM]** for this version) |





<h2 id="2022-02-10-3">3. Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models
</h2>

Title: [Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models](https://arxiv.org/abs/2202.04173)

Authors: [Boxin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B), [Wei Ping](https://arxiv.org/search/cs?searchtype=author&query=Ping%2C+W), [Chaowei Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C), [Peng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P), [Mostofa Patwary](https://arxiv.org/search/cs?searchtype=author&query=Patwary%2C+M), [Mohammad Shoeybi](https://arxiv.org/search/cs?searchtype=author&query=Shoeybi%2C+M), [Bo Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Anima Anandkumar](https://arxiv.org/search/cs?searchtype=author&query=Anandkumar%2C+A), [Bryan Catanzaro](https://arxiv.org/search/cs?searchtype=author&query=Catanzaro%2C+B)

> Pre-trained language models (LMs) are shown to easily generate toxic language. In this work, we systematically explore domain-adaptive training to reduce the toxicity of language models. We conduct this study on three dimensions: training corpus, model size, and parameter efficiency. For the training corpus, we propose to leverage the generative power of LMs and generate nontoxic datasets for domain-adaptive training, which mitigates the exposure bias and is shown to be more data-efficient than using a curated pre-training corpus. We demonstrate that the self-generation method consistently outperforms the existing baselines across various model sizes on both automatic and human evaluations, even when it uses a 1/3 smaller training corpus. We then comprehensively study detoxifying LMs with parameter sizes ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never been studied before. We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify. We also explore parameter-efficient training methods for detoxification. We demonstrate that adding and training adapter-only layers in LMs not only saves a lot of parameters but also achieves a better trade-off between toxicity and perplexity than whole model adaptation for the large-scale models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.04173](https://arxiv.org/abs/2202.04173) [cs.CL]** |
|           | (or **[arXiv:2202.04173v1](https://arxiv.org/abs/2202.04173v1) [cs.CL]** for this version) |





<h2 id="2022-02-10-4">4. pNLP-Mixer: an Efficient all-MLP Architecture for Language
</h2>

Title: [pNLP-Mixer: an Efficient all-MLP Architecture for Language](https://arxiv.org/abs/2202.04350)

Authors: [Francesco Fusco](https://arxiv.org/search/cs?searchtype=author&query=Fusco%2C+F), [Damian Pascual](https://arxiv.org/search/cs?searchtype=author&query=Pascual%2C+D), [Peter Staar](https://arxiv.org/search/cs?searchtype=author&query=Staar%2C+P)

> Large pre-trained language models drastically changed the natural language processing(NLP) landscape. Nowadays, they represent the go-to framework to tackle diverse NLP tasks, even with a limited number of annotations. However, using those models in production, either in the cloud or at the edge, remains a challenge due to the memory footprint and/or inference costs. As an alternative, recent work on efficient NLP has shown that small weight-efficient models can reach competitive performance at a fraction of the costs. Here, we introduce pNLP-Mixer, an embbedding-free model based on the MLP-Mixer architecture that achieves high weight-efficiency thanks to a novel linguistically informed projection layer. We evaluate our model on two multi-lingual semantic parsing datasets, MTOP and multiATIS. On MTOP our pNLP-Mixer almost matches the performance of mBERT, which has 38 times more parameters, and outperforms the state-of-the-art of tiny models (pQRNN) with 3 times fewer parameters. On a long-sequence classification task (Hyperpartisan) our pNLP-Mixer without pretraining outperforms RoBERTa, which has 100 times more parameters, demonstrating the potential of this architecture.

| Comments: | Preprint                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2202.04350](https://arxiv.org/abs/2202.04350) [cs.CL]** |
|           | (or **[arXiv:2202.04350v1](https://arxiv.org/abs/2202.04350v1) [cs.CL]** for this version) |





<h2 id="2022-02-10-5">5. Generating Training Data with Language Models: Towards Zero-Shot Language Understanding
</h2>

Title: [Generating Training Data with Language Models: Towards Zero-Shot Language Understanding](https://arxiv.org/abs/2202.04538)

Authors: [Yu Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y), [Jiaxin Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Yu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Jiawei Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J)

> Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class.

| Comments: | Code: [this https URL](https://github.com/yumeng5/SuperGen)  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2202.04538](https://arxiv.org/abs/2202.04538) [cs.CL]** |
|           | (or **[arXiv:2202.04538v1](https://arxiv.org/abs/2202.04538v1) [cs.CL]** for this version) |





# 2022-02-09

[Return to Index](#Index)



<h2 id="2022-02-08-1">1. DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers
</h2>

Title: [DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers](https://arxiv.org/abs/2202.04053)

Authors: [Jaemin Cho](https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+J), [Abhay Zala](https://arxiv.org/search/cs?searchtype=author&query=Zala%2C+A), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M)

> Generating images from textual descriptions has gained a lot of attention. Recently, DALL-E, a multimodal transformer language model, and its variants have shown high-quality text-to-image generation capabilities with a simple architecture and training objective, powered by large-scale training data and computation. However, despite the interesting image generation results, there has not been a detailed analysis on how to evaluate such models. In this work, we investigate the reasoning capabilities and social biases of such text-to-image generative transformers in detail. First, we measure four visual reasoning skills: object recognition, object counting, color recognition, and spatial relation understanding. For this, we propose PaintSkills, a diagnostic dataset and evaluation toolkit that measures these four visual reasoning skills. Second, we measure the text alignment and quality of the generated images based on pretrained image captioning, image-text retrieval, and image classification models. Third, we assess social biases in the models. For this, we suggest evaluation of gender and racial biases of text-to-image generation models based on a pretrained image-text retrieval model and human evaluation. In our experiments, we show that recent text-to-image models perform better in recognizing and counting objects than recognizing colors and understanding spatial relations, while there exists a large gap between model performances and oracle accuracy on all skills. Next, we demonstrate that recent text-to-image models learn specific gender/racial biases from web image-text pairs. We also show that our automatic evaluations of visual reasoning skills and gender bias are highly correlated with human judgments. We hope our work will help guide future progress in improving text-to-image models on visual reasoning skills and social biases. Code and data at: [this https URL](https://github.com/j-min/DallEval)

| Comments: | 20 pages, 10 figures, 13 tables                              |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2202.04053](https://arxiv.org/abs/2202.04053) [cs.CV]** |
|           | (or **[arXiv:2202.04053v1](https://arxiv.org/abs/2202.04053v1) [cs.CV]** for this version) |







# 2022-02-08

[Return to Index](#Index)



<h2 id="2022-02-08-1">1. Machine Translation from Signed to Spoken Languages: State of the Art and Challenges
</h2>

Title: [Machine Translation from Signed to Spoken Languages: State of the Art and Challenges](https://arxiv.org/abs/2202.03086)

Authors: [Mathieu De Coster](https://arxiv.org/search/cs?searchtype=author&query=De+Coster%2C+M), [Dimitar Shterionov](https://arxiv.org/search/cs?searchtype=author&query=Shterionov%2C+D), [Mieke Van Herreweghe](https://arxiv.org/search/cs?searchtype=author&query=Van+Herreweghe%2C+M), [Joni Dambre](https://arxiv.org/search/cs?searchtype=author&query=Dambre%2C+J)

> Automatic translation from signed to spoken languages is an interdisciplinary research domain, lying on the intersection of computer vision, machine translation and linguistics. Nevertheless, research in this domain is performed mostly by computer scientists in isolation. As the domain is becoming increasingly popular - the majority of scientific papers on the topic of sign language translation have been published in the past three years - we provide an overview of the state of the art as well as some required background in the different related disciplines. We give a high-level introduction to sign language linguistics and machine translation to illustrate the requirements of automatic sign language translation. We present a systematic literature review to illustrate the state of the art in the domain and then, harking back to the requirements, lay out several challenges for future research. We find that significant advances have been made on the shoulders of spoken language machine translation research. However, current approaches are often not linguistically motivated or are not adapted to the different input modality of sign languages. We explore challenges related to the representation of sign language data, the collection of datasets, the need for interdisciplinary research and requirements for moving beyond research, towards applications. Based on our findings, we advocate for interdisciplinary research and to base future research on linguistic analysis of sign languages. Furthermore, the inclusion of deaf and hearing end users of sign language translation applications in use case identification, data collection and evaluation is of the utmost importance in the creation of useful sign language translation models. We recommend iterative, human-in-the-loop, design and development of sign language translation models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.03086](https://arxiv.org/abs/2202.03086) [cs.CL]** |
|           | (or **[arXiv:2202.03086v1](https://arxiv.org/abs/2202.03086v1) [cs.CL]** for this version) |





<h2 id="2022-02-08-2">2. Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition
</h2>

Title: [Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition](https://arxiv.org/abs/2202.03218)

Authors: [Bethan Thomas](https://arxiv.org/search/cs?searchtype=author&query=Thomas%2C+B), [Samuel Kessler](https://arxiv.org/search/cs?searchtype=author&query=Kessler%2C+S), [Salah Karout](https://arxiv.org/search/cs?searchtype=author&query=Karout%2C+S)

> Self-supervised learning (SSL) is a powerful tool that allows learning of underlying representations from unlabeled data. Transformer based models such as wav2vec 2.0 and HuBERT are leading the field in the speech domain. Generally these models are fine-tuned on a small amount of labeled data for a downstream task such as Automatic Speech Recognition (ASR). This involves re-training the majority of the model for each task. Adapters are small lightweight modules which are commonly used in Natural Language Processing (NLP) to adapt pre-trained models to new tasks. In this paper we propose applying adapters to wav2vec 2.0 to reduce the number of parameters required for downstream ASR tasks, and increase scalability of the model to multiple tasks or languages. Using adapters we can perform ASR while training fewer than 10% of parameters per task compared to full fine-tuning with little degradation of performance. Ablations show that applying adapters into just the top few layers of the pre-trained network gives similar performance to full transfer, supporting the theory that higher pre-trained layers encode more phonemic information, and further optimizing efficiency.

| Comments: | 5 Pages, 4 figures. Accepted to ICASSP 2022                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2202.03218](https://arxiv.org/abs/2202.03218) [cs.CL]** |
|           | (or **[arXiv:2202.03218v1](https://arxiv.org/abs/2202.03218v1) [cs.CL]** for this version) |





<h2 id="2022-02-08-3">3. Red Teaming Language Models with Language Models
</h2>

Title: [Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286)

Authors: [Ethan Perez](https://arxiv.org/search/cs?searchtype=author&query=Perez%2C+E), [Saffron Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Francis Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+F), [Trevor Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+T), [Roman Ring](https://arxiv.org/search/cs?searchtype=author&query=Ring%2C+R), [John Aslanides](https://arxiv.org/search/cs?searchtype=author&query=Aslanides%2C+J), [Amelia Glaese](https://arxiv.org/search/cs?searchtype=author&query=Glaese%2C+A), [Nat McAleese](https://arxiv.org/search/cs?searchtype=author&query=McAleese%2C+N), [Geoffrey Irving](https://arxiv.org/search/cs?searchtype=author&query=Irving%2C+G)

> Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ("red teaming") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.03286](https://arxiv.org/abs/2202.03286) [cs.CL]** |
|           | (or **[arXiv:2202.03286v1](https://arxiv.org/abs/2202.03286v1) [cs.CL]** for this version) |







# 2022-02-07

[Return to Index](#Index)



<h2 id="2022-02-07-1">1. Data Scaling Laws in NMT: The Effect of Noise and Architecture
</h2>

Title: [Data Scaling Laws in NMT: The Effect of Noise and Architecture](https://arxiv.org/abs/2202.01994)

Authors: [Yamini Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+Y), [Behrooz Ghorbani](https://arxiv.org/search/cs?searchtype=author&query=Ghorbani%2C+B), [Ankush Garg](https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+A), [Biao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B), [Maxim Krikun](https://arxiv.org/search/cs?searchtype=author&query=Krikun%2C+M), [Colin Cherry](https://arxiv.org/search/cs?searchtype=author&query=Cherry%2C+C), [Behnam Neyshabur](https://arxiv.org/search/cs?searchtype=author&query=Neyshabur%2C+B), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O)

> In this work, we study the effect of varying the architecture and training data quality on the data scaling properties of Neural Machine Translation (NMT). First, we establish that the test loss of encoder-decoder transformer models scales as a power law in the number of training samples, with a dependence on the model size. Then, we systematically vary aspects of the training setup to understand how they impact the data scaling laws. In particular, we change the following (1) Architecture and task setup: We compare to a transformer-LSTM hybrid, and a decoder-only transformer with a language modeling loss (2) Noise level in the training distribution: We experiment with filtering, and adding iid synthetic noise. In all the above cases, we find that the data scaling exponents are minimally impacted, suggesting that marginally worse architectures or training data can be compensated for by adding more data. Lastly, we find that using back-translated data instead of parallel data, can significantly degrade the scaling exponent.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2202.01994](https://arxiv.org/abs/2202.01994) [cs.LG]** |
|           | (or **[arXiv:2202.01994v1](https://arxiv.org/abs/2202.01994v1) [cs.LG]** for this version) |





<h2 id="2022-02-07-2">2. Temporal Attention for Language Models
</h2>

Title: [Temporal Attention for Language Models](https://arxiv.org/abs/2202.02093)

Authors: [Guy D. Rosin](https://arxiv.org/search/cs?searchtype=author&query=Rosin%2C+G+D), [Kira Radinsky](https://arxiv.org/search/cs?searchtype=author&query=Radinsky%2C+K)

> Pretrained language models based on the transformer architecture have shown great success in NLP. Textual training data often comes from the web and is thus tagged with time-specific information, but most language models ignore this information. They are trained on the textual data alone, limiting their ability to generalize temporally. In this work, we extend the key component of the transformer architecture, i.e., the self-attention mechanism, and propose temporal attention - a time-aware self-attention mechanism. Temporal attention can be applied to any transformer model and requires the input texts to be accompanied with their relevant time points. It allows the transformer to capture this temporal information and create time-specific contextualized word representations. We leverage these representations for the task of semantic change detection; we apply our proposed mechanism to BERT and experiment on three datasets in different languages (English, German, and Latin) that also vary in time, size, and genre. Our proposed model achieves state-of-the-art results on all the datasets.

| Comments: | 8 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.02093](https://arxiv.org/abs/2202.02093) [cs.CL]** |
|           | (or **[arXiv:2202.02093v1](https://arxiv.org/abs/2202.02093v1) [cs.CL]** for this version) |





<h2 id="2022-02-07-3">3. The Ecological Footprint of Neural Machine Translation Systems
</h2>

Title: [The Ecological Footprint of Neural Machine Translation Systems](https://arxiv.org/abs/2202.02170)

Authors: [Dimitar Sherionov](https://arxiv.org/search/cs?searchtype=author&query=Sherionov%2C+D), [Eva Vanmassenhove](https://arxiv.org/search/cs?searchtype=author&query=Vanmassenhove%2C+E)

> Over the past decade, deep learning (DL) has led to significant advancements in various fields of artificial intelligence, including machine translation (MT). These advancements would not be possible without the ever-growing volumes of data and the hardware that allows large DL models to be trained efficiently. Due to the large amount of computing cores as well as dedicated memory, graphics processing units (GPUs) are a more effective hardware solution for training and inference with DL models than central processing units (CPUs). However, the former is very power demanding. The electrical power consumption has economical as well as ecological implications. 
> This chapter focuses on the ecological footprint of neural MT systems. It starts from the power drain during the training of and the inference with neural MT models and moves towards the environment impact, in terms of carbon dioxide emissions. Different architectures (RNN and Transformer) and different GPUs (consumer-grate NVidia 1080Ti and workstation-grade NVidia P100) are compared. Then, the overall CO2 offload is calculated for Ireland and the Netherlands. The NMT models and their ecological impact are compared to common household appliances to draw a more clear picture. 
> The last part of this chapter analyses quantization, a technique for reducing the size and complexity of models, as a way to reduce power consumption. As quantized models can run on CPUs, they present a power-efficient inference solution without depending on a GPU.

| Comments: | 25 pages, 3 figures, 10 tables                               |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2202.02170](https://arxiv.org/abs/2202.02170) [cs.CL]** |
|           | (or **[arXiv:2202.02170v1](https://arxiv.org/abs/2202.02170v1) [cs.CL]** for this version) |




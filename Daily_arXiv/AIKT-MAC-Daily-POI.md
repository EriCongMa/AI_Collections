# MA C.'s Daily Paper Of Interest - October a., 2022

# Index

- [2022-10-20](#2022-10-20)
  - [1. RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses](#2022-10-20-1)
  
  - [2. A Survey of Active Learning for Natural Language Processing](#2022-10-20-2)
  
  - [3. Simple and Effective Unsupervised Speech Translation](#2022-10-20-3)
  
  - [4. The Devil in Linear Transformer](#2022-10-20-4)
  
  - [5. Hybrid-Regressive Neural Machine Translation](#2022-10-20-5)
  
  - [6. Language Does More Than Describe: On The Lack Of Figurative Speech in Text-To-Image Models](#2022-10-20-6)
  
  - [7. DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models](#2022-10-20-7)
  
  - [8. Language Models Understand Us, Poorly](#2022-10-20-8)
  
  - [9. TabLLM: Few-shot Classification of Tabular Data with Large Language Models](#2022-10-20-9)
  
- [2022-10-19](#2022-10-19)
  - [1. Deepfake Text Detection: Limitations and Opportunities](#2022-10-19-1)

  - [2. Personalization of CTC Speech Recognition Models](#2022-10-19-2)

  - [3. DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation](#2022-10-19-3)

  - [4. Discrete Cross-Modal Alignment Enables Zero-Shot Speech Translation](#2022-10-19-4)

  - [5. Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks](#2022-10-19-5)

  - [6. Simultaneous Translation for Unsegmented Input: A Sliding Window Approach](#2022-10-19-6)

- [2022-10-18](#2022-10-18)
  - [1. TestAug: A Framework for Augmenting Capability-based NLP Tests](#2022-10-18-1)

  - [2. Vision-Language Pre-training: Basics, Recent Advances, and Future Trends](#2022-10-18-2)

  - [3. Generating Synthetic Speech from SpokenVocab for Speech Translation](#2022-10-18-3)

  - [4. Modeling Context With Linear Attention for Scalable Document-Level Translation](#2022-10-18-4)

  - [5. Towards Robust k-Nearest-Neighbor Machine Translation](#2022-10-18-5)

  - [6. DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models](#2022-10-18-6)

  - [7. Language-agnostic Code-Switching in End-To-End Speech Recognition](#2022-10-18-7)

  - [8. Table-To-Text generation and pre-training with TabT5](#2022-10-18-8)

- [2022-10-14](#2022-10-14)
  - [1. Why self-attention is Natural for Sequence-to-Sequence Problems? A Perspective from Symmetries](#2022-10-14-1)

  - [2. Categorizing Semantic Representations for Neural Machine Translation](#2022-10-14-2)

  - [3. Large Language Models are few(1)-shot Table Reasoners](#2022-10-14-3)

  - [4. Low-resource Neural Machine Translation with Cross-modal Alignment](#2022-10-14-4)

  - [5. SDW-ASL: A Dynamic System to Generate Large Scale Dataset for Continuous American Sign Language](#2022-10-14-5)

  - [6. CS-Insights: A System for Analyzing Computer Science Research](#2022-10-14-6)

  - [7. Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text](#2022-10-14-7)

  - [8. DICTDIS: Dictionary Constrained Disambiguation for Improved NMT](#2022-10-14-8)

  - [9. Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation](#2022-10-14-9)

  - [10. Incorporating Context into Subword Vocabularies](#2022-10-14-10)

  - [11. A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models](#2022-10-14-11)

  - [12. How (Not) To Evaluate Explanation Quality](#2022-10-14-12)

  - [13. You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models](#2022-10-14-13)

  - [14. Language Model Decoding as Likelihood-Utility Alignment](#2022-10-14-14)

- [2022-10-13](#2022-10-13)
  - [1. One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks](#2022-10-13-1)
  - [2. Foundation Transformers](#2022-10-13-2)
  - [3. Shapley Head Pruning: Identifying and Removing Interference in Multilingual Transformers](#2022-10-13-3)
  - [4. Scaling Up Deliberation for Multilingual ASR](#2022-10-13-4)
  - [5. CLIP also Understands Text: Prompting CLIP for Phrase Understanding](#2022-10-13-5)
  - [6. Non-Autoregressive Machine Translation with Translation Memories](#2022-10-13-6)
  - [7. Using Massive Multilingual Pre-Trained Language Models Towards Real Zero-Shot Neural Machine Translation in Clinical Domain](#2022-10-13-7)
  - [8. Improved Data Augmentation for Translation Suggestion](#2022-10-13-8)
  - [9. ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding](#2022-10-13-9)
  - [10. Pruning Pre-trained Language Models Without Fine-Tuning](#2022-10-13-10)
  - [11. A context-aware knowledge transferring strategy for CTC-based ASR](#2022-10-13-11)
  - [12. Back to the Future: On Potential Histories in NLP](#2022-10-13-12)
  - [13. Task Compass: Scaling Multi-task Pre-training with Task Prefix](#2022-10-13-13)
  - [14. Changing the Representation: Examining Language Representation for Neural Sign Language Production](#2022-10-13-14)
  - [15. InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings](#2022-10-13-15)
  - [16. Are Sample-Efficient NLP Models More Robust?](#2022-10-13-16)

- [2022-10-12](#2022-10-12)
  - [1. Markup-to-Image Diffusion Models with Scheduled Sampling](#2022-10-12-1)

  - [2. MAP: Modality-Agnostic Uncertainty-Aware Vision-Language Pre-training Model](#2022-10-12-2)

  - [3. Multilingual Representation Distillation with Contrastive Learning](#2022-10-12-3)

  - [4. Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions](#2022-10-12-4)

  - [5. Checks and Strategies for Enabling Code-Switched Machine Translation](#2022-10-12-5)

  - [6. Mixture of Attention Heads: Selecting Attention Heads Per Token](#2022-10-12-6)

  - [7. Understanding the Failure of Batch Normalization for Transformers in NLP](#2022-10-12-7)

  - [8. Can Language Models Be Specific? How?](#2022-10-12-8)

  - [9. Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation](#2022-10-12-9)

  - [10. CTC Alignments Improve Autoregressive Translation](#2022-10-12-10)

  - [11. Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting](#2022-10-12-11)

  - [12. Like a bilingual baby: The advantage of visually grounding a bilingual language model](#2022-10-12-12)

  - [13. Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems](#2022-10-12-13)

  - [14. Enriching Biomedical Knowledge for Low-resource Language Through Translation](#2022-10-12-14)

  - [15. MTet: Multi-domain Translation for English and Vietnamese](#2022-10-12-15)

  - [16. Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models](#2022-10-12-16)
- [2022-10-11](#2022-10-11)
  - [1. AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models](#2022-10-11-1)

  - [2. MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning](#2022-10-11-2)

  - [3. What the DAAM: Interpreting Stable Diffusion Using Cross Attention](#2022-10-11-3)

  - [4. Visualize Before You Write: Imagination-Guided Open-Ended Text Generation](#2022-10-11-4)

  - [5. Breaking BERT: Evaluating and Optimizing Sparsified Attention](#2022-10-11-5)

  - [6. Improving End-to-End Text Image Translation From the Auxiliary Text Translation Task](#2022-10-11-6)

  - [7. Detecting Label Errors in Token Classification Data](#2022-10-11-7)

  - [8. Sparse Teachers Can Be Dense with Knowledge](#2022-10-11-8)

  - [9. Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation](#2022-10-11-9)

  - [10. SDA: Simple Discrete Augmentation for Contrastive Sentence Representation Learning](#2022-10-11-10)

  - [11. KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text Classification](#2022-10-11-11)

  - [12. Cross-Align: Modeling Deep Cross-lingual Interactions for Word Alignment](#2022-10-11-12)

  - [13. SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters](#2022-10-11-13)

  - [14. Parameter-Efficient Tuning with Special Token Adaptation](#2022-10-11-14)

  - [15. Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation](#2022-10-11-15)

  - [16. Automatic Evaluation and Analysis of Idioms in Neural Machine Translation](#2022-10-11-16)

  - [17. A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language Processing](#2022-10-11-17)
- [2022-10-10](#2022-10-10)
  - [1. Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](#2022-10-10-1)

  - [2. NMTSloth: Understanding and Testing Efficiency Degradation of Neural Machine Translation Systems](#2022-10-10-2)

  - [3. SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training](#2022-10-10-3)
- [Other Columns](https://github.com/EriCongMa/AI_Collections/blob/main/Daily_arXiv/AIKT-MAC-Daily-POI-index.md)



# 2022-10-20

[Return to Index](#Index)



<h2 id="2022-10-20-1">1. RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses
</h2>

Title: [RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses](https://arxiv.org/abs/2210.10634)

Authors: [Honglei Zhuang](https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+H), [Zhen Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Z), [Rolf Jagerman](https://arxiv.org/search/cs?searchtype=author&query=Jagerman%2C+R), [Kai Hui](https://arxiv.org/search/cs?searchtype=author&query=Hui%2C+K), [Ji Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+J), [Jing Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+J), [Jianmo Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+J), [Xuanhui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Michael Bendersky](https://arxiv.org/search/cs?searchtype=author&query=Bendersky%2C+M)

> Recently, substantial progress has been made in text ranking based on pretrained language models such as BERT. However, there are limited studies on how to leverage more powerful sequence-to-sequence models such as T5. Existing attempts usually formulate text ranking as classification and rely on postprocessing to obtain a ranked list. In this paper, we propose RankT5 and study two T5-based ranking model structures, an encoder-decoder and an encoder-only one, so that they not only can directly output ranking scores for each query-document pair, but also can be fine-tuned with "pairwise" or "listwise" ranking losses to optimize ranking performances. Our experiments show that the proposed models with ranking losses can achieve substantial ranking performance gains on different public text ranking data sets. Moreover, when fine-tuned with listwise ranking losses, the ranking model appears to have better zero-shot ranking performance on out-of-domain data sets compared to the model fine-tuned with classification losses.

| Comments: | 13 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Information Retrieval (cs.IR)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2210.10634](https://arxiv.org/abs/2210.10634) [cs.IR]** |
|           | (or **[arXiv:2210.10634v1](https://arxiv.org/abs/2210.10634v1) [cs.IR]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.10634Focus to learn more |





<h2 id="2022-10-20-2">2. A Survey of Active Learning for Natural Language Processing
</h2>

Title: [A Survey of Active Learning for Natural Language Processing](https://arxiv.org/abs/2210.10109)

Authors: [Zhisong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Emma Strubell](https://arxiv.org/search/cs?searchtype=author&query=Strubell%2C+E), [Eduard Hovy](https://arxiv.org/search/cs?searchtype=author&query=Hovy%2C+E)

> In this work, we provide a survey of active learning (AL) for its applications in natural language processing (NLP). In addition to a fine-grained categorization of query strategies, we also investigate several other important aspects of applying AL to NLP problems. These include AL for structured prediction tasks, annotation cost, model learning (especially with deep neural models), and starting and stopping AL. Finally, we conclude with a discussion of related topics and future directions.

| Comments: | EMNLP 2022                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.10109](https://arxiv.org/abs/2210.10109) [cs.CL]** |
|           | (or **[arXiv:2210.10109v1](https://arxiv.org/abs/2210.10109v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.10109Focus to learn more |





<h2 id="2022-10-20-3">3. Simple and Effective Unsupervised Speech Translation
</h2>

Title: [Simple and Effective Unsupervised Speech Translation](https://arxiv.org/abs/2210.10191)

Authors: [Changhan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Hirofumi Inaguma](https://arxiv.org/search/cs?searchtype=author&query=Inaguma%2C+H), [Peng-Jen Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P), [Ilia Kulikov](https://arxiv.org/search/cs?searchtype=author&query=Kulikov%2C+I), [Yun Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y), [Wei-Ning Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+W), [Michael Auli](https://arxiv.org/search/cs?searchtype=author&query=Auli%2C+M), [Juan Pino](https://arxiv.org/search/cs?searchtype=author&query=Pino%2C+J)

> The amount of labeled data to train models for speech tasks is limited for most languages, however, the data scarcity is exacerbated for speech translation which requires labeled data covering two different languages. To address this issue, we study a simple and effective approach to build speech translation systems without labeled data by leveraging recent advances in unsupervised speech recognition, machine translation and speech synthesis, either in a pipeline approach, or to generate pseudo-labels for training end-to-end speech translation models. Furthermore, we present an unsupervised domain adaptation technique for pre-trained speech models which improves the performance of downstream unsupervised speech recognition, especially for low-resource settings. Experiments show that unsupervised speech-to-text translation outperforms the previous unsupervised state of the art by 3.2 BLEU on the Libri-Trans benchmark, on CoVoST 2, our best systems outperform the best supervised end-to-end models (without pre-training) from only two years ago by an average of 5.0 BLEU over five X-En directions. We also report competitive results on MuST-C and CVSS benchmarks.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.10191](https://arxiv.org/abs/2210.10191) [cs.CL]** |
|           | (or **[arXiv:2210.10191v1](https://arxiv.org/abs/2210.10191v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.10191Focus to learn more |





<h2 id="2022-10-20-4">4. The Devil in Linear Transformer
</h2>

Title: [The Devil in Linear Transformer](https://arxiv.org/abs/2210.10340)

Authors: [Zhen Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Z), [XiaoDong Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Weixuan Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+W), [Dongxu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+D), [Lingpeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L), [Nick Barnes](https://arxiv.org/search/cs?searchtype=author&query=Barnes%2C+N), [Yiran Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+Y)

> Linear transformers aim to reduce the quadratic space-time complexity of vanilla transformers. However, they usually suffer from degraded performances on various tasks and corpus. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, transNormer, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient. The code is available at [this https URL](https://github.com/OpenNLPLab/Transnormer) .

| Comments: | accepted to EMNLP2022                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.10340](https://arxiv.org/abs/2210.10340) [cs.CL]** |
|           | (or **[arXiv:2210.10340v1](https://arxiv.org/abs/2210.10340v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.10340Focus to learn more |







<h2 id="2022-10-20-5">5. Hybrid-Regressive Neural Machine Translation
</h2>

Title: [Hybrid-Regressive Neural Machine Translation](https://arxiv.org/abs/2210.10416)

Authors: [Qiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q), [Xinhui Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X), [Ming Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M)

> In this work, we empirically confirm that non-autoregressive translation with an iterative refinement mechanism (IR-NAT) suffers from poor acceleration robustness because it is more sensitive to decoding batch size and computing device setting than autoregressive translation (AT). Inspired by it, we attempt to investigate how to combine the strengths of autoregressive and non-autoregressive translation paradigms better. To this end, we demonstrate through synthetic experiments that prompting a small number of AT's predictions can promote one-shot non-autoregressive translation to achieve the equivalent performance of IR-NAT. Following this line, we propose a new two-stage translation prototype called hybrid-regressive translation (HRT). Specifically, HRT first generates discontinuous sequences via autoregression (e.g., make a prediction every k tokens, k>1) and then fills in all previously skipped tokens at once in a non-autoregressive manner. We also propose a bag of techniques to effectively and efficiently train HRT without adding any model parameters. HRT achieves the state-of-the-art BLEU score of 28.49 on the WMT En-De task and is at least 1.5x faster than AT, regardless of batch size and device. In addition, another bonus of HRT is that it successfully inherits the good characteristics of AT in the deep-encoder-shallow-decoder architecture. Concretely, compared to the vanilla HRT with a 6-layer encoder and 6-layer decoder, the inference speed of HRT with a 12-layer encoder and 1-layer decoder is further doubled on both GPU and CPU without BLEU loss.

| Comments: | Submitted to ICLR 2023                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.10416](https://arxiv.org/abs/2210.10416) [cs.CL]** |
|           | (or **[arXiv:2210.10416v1](https://arxiv.org/abs/2210.10416v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.10416Focus to learn more |





<h2 id="2022-10-20-6">6. Language Does More Than Describe: On The Lack Of Figurative Speech in Text-To-Image Models
</h2>

Title: [Language Does More Than Describe: On The Lack Of Figurative Speech in Text-To-Image Models](https://arxiv.org/abs/2210.10578)

Authors: [Ricardo Kleinlein](https://arxiv.org/search/cs?searchtype=author&query=Kleinlein%2C+R), [Cristina Luna-Jiménez](https://arxiv.org/search/cs?searchtype=author&query=Luna-Jiménez%2C+C), [Fernando Fernández-Martínez](https://arxiv.org/search/cs?searchtype=author&query=Fernández-Martínez%2C+F)

> The impressive capacity shown by recent text-to-image diffusion models to generate high-quality pictures from textual input prompts has leveraged the debate about the very definition of art. Nonetheless, these models have been trained using text data collected from content-based labelling protocols that focus on describing the items and actions in an image but neglect any subjective appraisal. Consequently, these automatic systems need rigorous descriptions of the elements and the pictorial style of the image to be generated, otherwise failing to deliver. As potential indicators of the actual artistic capabilities of current generative models, we characterise the sentimentality, objectiveness and degree of abstraction of publicly available text data used to train current text-to-image diffusion models. Considering the sharp difference observed between their language style and that typically employed in artistic contexts, we suggest generative models should incorporate additional sources of subjective information in their training in order to overcome (or at least to alleviate) some of their current limitations, thus effectively unleashing a truly artistic and creative generation.

| Comments: | NeurIPS 2022 Machine Learning for Creativity and Design Workshop |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.10578](https://arxiv.org/abs/2210.10578) [cs.CL]** |
|           | (or **[arXiv:2210.10578v1](https://arxiv.org/abs/2210.10578v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.10578Focus to learn more |



<h2 id="2022-10-20-7">7. DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models
</h2>

Title: [DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models](https://arxiv.org/abs/2210.10606)

Authors: [Royi Rassin](https://arxiv.org/search/cs?searchtype=author&query=Rassin%2C+R), [Shauli Ravfogel](https://arxiv.org/search/cs?searchtype=author&query=Ravfogel%2C+S), [Yoav Goldberg](https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y)

> We study the way DALLE-2 maps symbols (words) in the prompt to their references (entities or properties of entities in the generated image). We show that in stark contrast to the way human process language, DALLE-2 does not follow the constraint that each word has a single role in the interpretation, and sometimes re-use the same symbol for different purposes. We collect a set of stimuli that reflect the phenomenon: we show that DALLE-2 depicts both senses of nouns with multiple senses at once; and that a given word can modify the properties of two distinct entities in the image, or can be depicted as one object and also modify the properties of another object, creating a semantic leakage of properties between entities. Taken together, our study highlights the differences between DALLE-2 and human language processing and opens an avenue for future study on the inductive biases of text-to-image models.

| Comments: | 5 pages, BlackboxNLP @ EMNLP 2022                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.10606](https://arxiv.org/abs/2210.10606) [cs.CL]** |
|           | (or **[arXiv:2210.10606v1](https://arxiv.org/abs/2210.10606v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.10606Focus to learn more |







<h2 id="2022-10-20-8">8. Language Models Understand Us, Poorly
</h2>

Title: [Language Models Understand Us, Poorly](https://arxiv.org/abs/2210.10684)

Authors: [Jared Moore](https://arxiv.org/search/cs?searchtype=author&query=Moore%2C+J)

> Some claim language models understand us. Others won't hear it. To clarify, I investigate three views of human language understanding: as-mapping, as-reliability and as-representation. I argue that while behavioral reliability is necessary for understanding, internal representations are sufficient; they climb the right hill. I review state-of-the-art language and multi-modal models: they are pragmatically challenged by under-specification of form. I question the Scaling Paradigm: limits on resources may prohibit scaled-up models from approaching understanding. Last, I describe how as-representation advances a science of understanding. We need work which probes model internals, adds more of human language, and measures what models can learn.

| Comments:    | 5 pages, 1 figure, to be published in Findings of EMNLP 2022 |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| ACM classes: | I.2.7                                                        |
| Cite as:     | **[arXiv:2210.10684](https://arxiv.org/abs/2210.10684) [cs.CL]** |
|              | (or **[arXiv:2210.10684v1](https://arxiv.org/abs/2210.10684v1) [cs.CL]** for this version) |
|              | https://doi.org/10.48550/arXiv.2210.10684Focus to learn more |



<h2 id="2022-10-20-9">9. TabLLM: Few-shot Classification of Tabular Data with Large Language Models
</h2>

Title: [TabLLM: Few-shot Classification of Tabular Data with Large Language Models](https://arxiv.org/abs/2210.10723)

Authors: [Stefan Hegselmann](https://arxiv.org/search/cs?searchtype=author&query=Hegselmann%2C+S), [Alejandro Buendia](https://arxiv.org/search/cs?searchtype=author&query=Buendia%2C+A), [Hunter Lang](https://arxiv.org/search/cs?searchtype=author&query=Lang%2C+H), [Monica Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal%2C+M), [Xiaoyi Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X), [David Sontag](https://arxiv.org/search/cs?searchtype=author&query=Sontag%2C+D)

> We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.10723](https://arxiv.org/abs/2210.10723) [cs.CL]** |
|           | (or **[arXiv:2210.10723v1](https://arxiv.org/abs/2210.10723v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.10723Focus to learn more |



# 2022-10-19

[Return to Index](#Index)



<h2 id="2022-10-19-1">1. Deepfake Text Detection: Limitations and Opportunities
</h2>

Title: [Deepfake Text Detection: Limitations and Opportunities](https://arxiv.org/abs/2210.09421)

Authors: [Jiameng Pu](https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+J), [Zain Sarwar](https://arxiv.org/search/cs?searchtype=author&query=Sarwar%2C+Z), [Sifat Muhammad Abdullah](https://arxiv.org/search/cs?searchtype=author&query=Abdullah%2C+S+M), [Abdullah Rehman](https://arxiv.org/search/cs?searchtype=author&query=Rehman%2C+A), [Yoonjin Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y), [Parantapa Bhattacharya](https://arxiv.org/search/cs?searchtype=author&query=Bhattacharya%2C+P), [Mobin Javed](https://arxiv.org/search/cs?searchtype=author&query=Javed%2C+M), [Bimal Viswanath](https://arxiv.org/search/cs?searchtype=author&query=Viswanath%2C+B)

> Recent advances in generative models for language have enabled the creation of convincing synthetic text or deepfake text. Prior work has demonstrated the potential for misuse of deepfake text to mislead content consumers. Therefore, deepfake text detection, the task of discriminating between human and machine-generated text, is becoming increasingly critical. Several defenses have been proposed for deepfake text detection. However, we lack a thorough understanding of their real-world applicability. In this paper, we collect deepfake text from 4 online services powered by Transformer-based tools to evaluate the generalization ability of the defenses on content in the wild. We develop several low-cost adversarial attacks, and investigate the robustness of existing defenses against an adaptive attacker. We find that many defenses show significant degradation in performance under our evaluation scenarios compared to their original claimed performance. Our evaluation shows that tapping into the semantic information in the text content is a promising approach for improving the robustness and generalization performance of deepfake text detection schemes.

| Comments: | Accepted to IEEE S&P 2023; First two authors contributed equally to this work; 18 pages, 7 figures |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Cryptography and Security (cs.CR)**; Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.09421](https://arxiv.org/abs/2210.09421) [cs.CR]** |
|           | (or **[arXiv:2210.09421v1](https://arxiv.org/abs/2210.09421v1) [cs.CR]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.09421Focus to learn more |



<h2 id="2022-10-19-2">2. Personalization of CTC Speech Recognition Models
</h2>

Title: [Personalization of CTC Speech Recognition Models](https://arxiv.org/abs/2210.09510)

Authors: [Saket Dingliwal](https://arxiv.org/search/cs?searchtype=author&query=Dingliwal%2C+S), [Monica Sunkara](https://arxiv.org/search/cs?searchtype=author&query=Sunkara%2C+M), [Srikanth Ronanki](https://arxiv.org/search/cs?searchtype=author&query=Ronanki%2C+S), [Jeff Farris](https://arxiv.org/search/cs?searchtype=author&query=Farris%2C+J), [Katrin Kirchhoff](https://arxiv.org/search/cs?searchtype=author&query=Kirchhoff%2C+K), [Sravan Bodapati](https://arxiv.org/search/cs?searchtype=author&query=Bodapati%2C+S)

> End-to-end speech recognition models trained using joint Connectionist Temporal Classification (CTC)-Attention loss have gained popularity recently. In these models, a non-autoregressive CTC decoder is often used at inference time due to its speed and simplicity. However, such models are hard to personalize because of their conditional independence assumption that prevents output tokens from previous time steps to influence future predictions. To tackle this, we propose a novel two-way approach that first biases the encoder with attention over a predefined list of rare long-tail and out-of-vocabulary (OOV) words and then uses dynamic boosting and phone alignment network during decoding to further bias the subword predictions. We evaluate our approach on open-source VoxPopuli and in-house medical datasets to showcase a 60% improvement in F1 score on domain-specific rare words over a strong CTC baseline.

| Comments: | To appear in SLT 2022                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2210.09510](https://arxiv.org/abs/2210.09510) [cs.CL]** |
|           | (or **[arXiv:2210.09510v1](https://arxiv.org/abs/2210.09510v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.09510Focus to learn more |



<h2 id="2022-10-19-3">3. DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation
</h2>

Title: [DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation](https://arxiv.org/abs/2210.09551)

Authors: [Hanqing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Dawei Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D)

> Prompt learning with immensely large Casual Language Models (CLMs) has been shown promising for attribute-controllable text generation (CTG). However, vanilla prompt tuning tends to imitate training corpus characteristics beyond the control attributes, resulting in a poor generalization ability. Moreover, it is less able to capture the relationship between different attributes, further limiting the control performance. In this paper, we propose a new CTG approach, namely DisCup, which incorporates the attribute knowledge of discriminator to optimize the control-prompts, steering a frozen CLM to produce attribute-specific texts. Specifically, the frozen CLM model, capable of producing multitudinous texts, is first used to generate the next-token candidates based on the context, so as to ensure the diversity of tokens to be predicted. Then, we leverage an attribute-discriminator to select desired/undesired tokens from those candidates, providing the inter-attribute knowledge. Finally, we bridge the above two traits by an unlikelihood objective for prompt-tuning. Extensive experimental results show that DisCup can achieve a new state-of-the-art control performance while maintaining an efficient and high-quality text generation, only relying on around 10 virtual tokens.

| Comments: | Accepted at EMNLP2022                                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.09551](https://arxiv.org/abs/2210.09551) [cs.CL]** |
|           | (or **[arXiv:2210.09551v1](https://arxiv.org/abs/2210.09551v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.09551Focus to learn more |





<h2 id="2022-10-19-4">4. Discrete Cross-Modal Alignment Enables Zero-Shot Speech Translation
</h2>

Title: [Discrete Cross-Modal Alignment Enables Zero-Shot Speech Translation](https://arxiv.org/abs/2210.09556)

Authors: [Chen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Yuchen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Boxing Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B), [Jiajun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Wei Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W), [Zhongqiang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z), [Chengqing Zong](https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+C)

> End-to-end Speech Translation (ST) aims at translating the source language speech into target language text without generating the intermediate transcriptions. However, the training of end-to-end methods relies on parallel ST data, which are difficult and expensive to obtain. Fortunately, the supervised data for automatic speech recognition (ASR) and machine translation (MT) are usually more accessible, making zero-shot speech translation a potential direction. Existing zero-shot methods fail to align the two modalities of speech and text into a shared semantic space, resulting in much worse performance compared to the supervised ST methods. In order to enable zero-shot ST, we propose a novel Discrete Cross-Modal Alignment (DCMA) method that employs a shared discrete vocabulary space to accommodate and match both modalities of speech and text. Specifically, we introduce a vector quantization module to discretize the continuous representations of speech and text into a finite set of virtual tokens, and use ASR data to map corresponding speech and text to the same virtual token in a shared codebook. This way, source language speech can be embedded in the same semantic space as the source language text, which can be then transformed into target language text with an MT module. Experiments on multiple language pairs demonstrate that our zero-shot ST method significantly improves the SOTA, and even performers on par with the strong supervised ST baselines.

| Comments: | Accepted by the main conference of EMNLP 2022                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2210.09556](https://arxiv.org/abs/2210.09556) [cs.CL]** |
|           | (or **[arXiv:2210.09556v1](https://arxiv.org/abs/2210.09556v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.09556Focus to learn more |



<h2 id="2022-10-19-5">5. Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks
</h2>

Title: [Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks](https://arxiv.org/abs/2210.09588)

Authors: [Jaehoon Oh](https://arxiv.org/search/cs?searchtype=author&query=Oh%2C+J), [Jongwoo Ko](https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+J), [Se-Young Yun](https://arxiv.org/search/cs?searchtype=author&query=Yun%2C+S)

> Translation has played a crucial role in improving the performance on multilingual tasks: (1) to generate the target language data from the source language data for training and (2) to generate the source language data from the target language data for inference. However, prior works have not considered the use of both translations simultaneously. This paper shows that combining them can synergize the results on various multilingual sentence classification tasks. We empirically find that translation artifacts stylized by translators are the main factor of the performance gain. Based on this analysis, we adopt two training methods, SupCon and MixUp, considering translation artifacts. Furthermore, we propose a cross-lingual fine-tuning algorithm called MUSC, which uses SupCon and MixUp jointly and improves the performance. Our code is available at [this https URL](https://github.com/jongwooko/MUSC).

| Comments: | The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.09588](https://arxiv.org/abs/2210.09588) [cs.CL]** |
|           | (or **[arXiv:2210.09588v1](https://arxiv.org/abs/2210.09588v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.09588Focus to learn more |



<h2 id="2022-10-19-6">6. Simultaneous Translation for Unsegmented Input: A Sliding Window Approach
</h2>

Title: [Simultaneous Translation for Unsegmented Input: A Sliding Window Approach](https://arxiv.org/abs/2210.09754)

Authors: [Sukanta Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen%2C+S), [Ondřej Bojar](https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O), [Barry Haddow](https://arxiv.org/search/cs?searchtype=author&query=Haddow%2C+B)

> In the cascaded approach to spoken language translation (SLT), the ASR output is typically punctuated and segmented into sentences before being passed to MT, since the latter is typically trained on written text. However, erroneous segmentation, due to poor sentence-final punctuation by the ASR system, leads to degradation in translation quality, especially in the simultaneous (online) setting where the input is continuously updated. To reduce the influence of automatic segmentation, we present a sliding window approach to translate raw ASR outputs (online or offline) without needing to rely on an automatic segmenter. We train translation models using parallel windows (instead of parallel sentences) extracted from the original training data. At test time, we translate at the window level and join the translated windows using a simple approach to generate the final translation. Experiments on English-to-German and English-to-Czech show that our approach improves 1.3--2.0 BLEU points over the usual ASR-segmenter pipeline, and the fixed-length window considerably reduces flicker compared to a baseline retranslation-based online SLT system.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.09754](https://arxiv.org/abs/2210.09754) [cs.CL]** |
|           | (or **[arXiv:2210.09754v1](https://arxiv.org/abs/2210.09754v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.09754Focus to learn more |






# 2022-10-18

[Return to Index](#Index)



<h2 id="2022-10-18-1">1. TestAug: A Framework for Augmenting Capability-based NLP Tests
</h2>

Title: [TestAug: A Framework for Augmenting Capability-based NLP Tests](https://arxiv.org/abs/2210.08097)

Authors: [Guanqun Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+G), [Mirazul Haque](https://arxiv.org/search/cs?searchtype=author&query=Haque%2C+M), [Qiaochu Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Q), [Wei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W), [Xueqing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X)

> The recently proposed capability-based NLP testing allows model developers to test the functional capabilities of NLP models, revealing functional failures that cannot be detected by the traditional heldout mechanism. However, existing work on capability-based testing requires extensive manual efforts and domain expertise in creating the test cases. In this paper, we investigate a low-cost approach for the test case generation by leveraging the GPT-3 engine. We further propose to use a classifier to remove the invalid outputs from GPT-3 and expand the outputs into templates to generate more test cases. Our experiments show that TestAug has three advantages over the existing work on behavioral testing: (1) TestAug can find more bugs than existing work; (2) The test cases in TestAug are more diverse; and (3) TestAug largely saves the manual efforts in creating the test suites. The code and data for TestAug can be found at our project website ([this https URL](https://guanqun-yang.github.io/testaug/)) and GitHub ([this https URL](https://github.com/guanqun-yang/testaug)).

| Comments: | Accepted by COLING 2022; Presentation Video: [this https URL](https://www.youtube.com/watch?v=X0p8J57qxeg;) Website: [this https URL](https://guanqun-yang.github.io/testaug/;) GitHub: [this https URL](https://github.com/guanqun-yang/testaug) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Software Engineering (cs.SE)**; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.08097](https://arxiv.org/abs/2210.08097) [cs.SE]** |
|           | (or **[arXiv:2210.08097v1](https://arxiv.org/abs/2210.08097v1) [cs.SE]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.08097Focus to learn more |





<h2 id="2022-10-18-2">2. Vision-Language Pre-training: Basics, Recent Advances, and Future Trends
</h2>

Title: [Vision-Language Pre-training: Basics, Recent Advances, and Future Trends](https://arxiv.org/abs/2210.09263)

Authors: [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Linjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Chunyuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Zicheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J)

> This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: (i) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; (ii) VLP for core computer vision tasks, such as (open-set) image classification, object detection, and segmentation; and (iii) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, unified modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.

| Comments: | A survey paper/book on Vision-Language Pre-training (102 pages) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2210.09263](https://arxiv.org/abs/2210.09263) [cs.CV]** |
|           | (or **[arXiv:2210.09263v1](https://arxiv.org/abs/2210.09263v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.09263Focus to learn more |





<h2 id="2022-10-18-3">3. Generating Synthetic Speech from SpokenVocab for Speech Translation
</h2>

Title: [Generating Synthetic Speech from SpokenVocab for Speech Translation](https://arxiv.org/abs/2210.08174)

Authors: [Jinming Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J), [Gholamreza Haffar](https://arxiv.org/search/cs?searchtype=author&query=Haffar%2C+G), [Ehsan Shareghi](https://arxiv.org/search/cs?searchtype=author&query=Shareghi%2C+E)

> Training end-to-end speech translation (ST) systems requires sufficiently large-scale data, which is unavailable for most language pairs and domains. One practical solution to the data scarcity issue is to convert machine translation data (MT) to ST data via text-to-speech (TTS) systems. Yet, using TTS systems can be tedious and slow, as the conversion needs to be done for each MT dataset. In this work, we propose a simple, scalable and effective data augmentation technique, i.e., SpokenVocab, to convert MT data to ST data on-the-fly. The idea is to retrieve and stitch audio snippets from a SpokenVocab bank according to words in an MT sequence. Our experiments on multiple language pairs from Must-C show that this method outperforms strong baselines by an average of 1.83 BLEU scores, and it performs equally well as TTS-generated speech. We also showcase how SpokenVocab can be applied in code-switching ST for which often no TTS systems exit. Our code is available at [this https URL](https://github.com/mingzi151/SpokenVocab)

| Comments: | 7 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2210.08174](https://arxiv.org/abs/2210.08174) [cs.CL]** |
|           | (or **[arXiv:2210.08174v1](https://arxiv.org/abs/2210.08174v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.08174Focus to learn more |





<h2 id="2022-10-18-4">4. Modeling Context With Linear Attention for Scalable Document-Level Translation
</h2>

Title: [Modeling Context With Linear Attention for Scalable Document-Level Translation](https://arxiv.org/abs/2210.08431)

Authors: [Zhaofeng Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [Hao Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+H), [Nikolaos Pappas](https://arxiv.org/search/cs?searchtype=author&query=Pappas%2C+N), [Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A)

> Document-level machine translation leverages inter-sentence dependencies to produce more coherent and consistent translations. However, these models, predominantly based on transformers, are difficult to scale to long documents as their attention layers have quadratic complexity in the sequence length. Recent efforts on efficient attention improve scalability, but their effect on document translation remains unexplored. In this work, we investigate the efficacy of a recent linear attention model by Peng et al. (2021) on document translation and augment it with a sentential gate to promote a recency inductive bias. We evaluate the model on IWSLT 2015 and OpenSubtitles 2018 against the transformer, demonstrating substantially increased decoding speed on long sequences with similar or better BLEU scores. We show that sentential gating further improves translation quality on IWSLT.

| Comments: | Findings of EMNLP 2022                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.08431](https://arxiv.org/abs/2210.08431) [cs.CL]** |
|           | (or **[arXiv:2210.08431v1](https://arxiv.org/abs/2210.08431v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.08431Focus to learn more |





<h2 id="2022-10-18-1">5. Towards Robust k-Nearest-Neighbor Machine Translation
</h2>

Title: [Towards Robust k-Nearest-Neighbor Machine Translation](https://arxiv.org/abs/2210.08808)

Authors: [Hui Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+H), [Ziyao Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Z), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Chulun Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+C), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Degen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+D), [Jinsong Su](https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J)

> k-Nearest-Neighbor Machine Translation (kNN-MT) becomes an important research direction of NMT in recent years. Its main idea is to retrieve useful key-value pairs from an additional datastore to modify translations without updating the NMT model. However, the underlying retrieved noisy pairs will dramatically deteriorate the model performance. In this paper, we conduct a preliminary study and find that this problem results from not fully exploiting the prediction of the NMT model. To alleviate the impact of noise, we propose a confidence-enhanced kNN-MT model with robust training. Concretely, we introduce the NMT confidence to refine the modeling of two important components of kNN-MT: kNN distribution and the interpolation weight. Meanwhile we inject two types of perturbations into the retrieved pairs for robust training. Experimental results on four benchmark datasets demonstrate that our model not only achieves significant improvements over current kNN-MT models, but also exhibits better robustness. Our code is available at [this https URL](https://github.com/DeepLearnXMU/Robust-knn-mt).

| Comments: | Accepted to EMNLP 2022                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.08808](https://arxiv.org/abs/2210.08808) [cs.CL]** |
|           | (or **[arXiv:2210.08808v1](https://arxiv.org/abs/2210.08808v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.08808Focus to learn more |





<h2 id="2022-10-18-6">6. DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models
</h2>

Title: [DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models](https://arxiv.org/abs/2210.08933)

Authors: [Shansan Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+S), [Mukai Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M), [Jiangtao Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J), [Zhiyong Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z), [LingPeng Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+L)

> Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is difficult due to the discrete nature of text. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks.

| Comments: | 18 pages                                                     |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.08933](https://arxiv.org/abs/2210.08933) [cs.CL]** |
|           | (or **[arXiv:2210.08933v1](https://arxiv.org/abs/2210.08933v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.08933Focus to learn more |





<h2 id="2022-10-18-7">7. Language-agnostic Code-Switching in End-To-End Speech Recognition
</h2>

Title: [Language-agnostic Code-Switching in End-To-End Speech Recognition](https://arxiv.org/abs/2210.08992)

Authors: [Enes Yavuz Ugan](https://arxiv.org/search/cs?searchtype=author&query=Ugan%2C+E+Y), [Christian Huber](https://arxiv.org/search/cs?searchtype=author&query=Huber%2C+C), [Juan Hussain](https://arxiv.org/search/cs?searchtype=author&query=Hussain%2C+J), [Alexander Waibel](https://arxiv.org/search/cs?searchtype=author&query=Waibel%2C+A)

> Code-Switching (CS) is referred to the phenomenon of alternately using words and phrases from different languages. While today's neural end-to-end (E2E) models deliver state-of-the-art performances on the task of automatic speech recognition (ASR) it is commonly known that these systems are very data-intensive. However, there is only a few transcribed and aligned CS speech available. To overcome this problem and train multilingual systems which can transcribe CS speech, we propose a simple yet effective data augmentation in which audio and corresponding labels of different source languages are concatenated. By using this training data, our E2E model improves on transcribing CS speech and improves performance over the multilingual model, as well. The results show that this augmentation technique can even improve the model's performance on inter-sentential language switches not seen during training by 5,03\% WER.

| Comments: | 6 pages                                                      |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2210.08992](https://arxiv.org/abs/2210.08992) [cs.CL]** |
|           | (or **[arXiv:2210.08992v1](https://arxiv.org/abs/2210.08992v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.08992Focus to learn more |





<h2 id="2022-10-18-8">8. Table-To-Text generation and pre-training with TabT5
</h2>

Title: [Table-To-Text generation and pre-training with TabT5](https://arxiv.org/abs/2210.09162)

Authors: [Ewa Andrejczuk](https://arxiv.org/search/cs?searchtype=author&query=Andrejczuk%2C+E), [Julian Martin Eisenschlos](https://arxiv.org/search/cs?searchtype=author&query=Eisenschlos%2C+J+M), [Francesco Piccinno](https://arxiv.org/search/cs?searchtype=author&query=Piccinno%2C+F), [Syrine Krichene](https://arxiv.org/search/cs?searchtype=author&query=Krichene%2C+S), [Yasemin Altun](https://arxiv.org/search/cs?searchtype=author&query=Altun%2C+Y)

> Encoder-only transformer models have been successfully applied to different table understanding tasks, as in TAPAS (Herzig et al., 2020). A major limitation of these architectures is that they are constrained to classification-like tasks such as cell selection or entailment detection. We present TABT5, an encoder-decoder model that generates natural language text based on tables and textual inputs. TABT5 overcomes the encoder-only limitation by incorporating a decoder component and leverages the input structure with table specific embeddings and pre-training. TABT5 achieves new state-of-the-art results on several domains, including spreadsheet formula prediction with a 15% increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and data-to-text generation with a 2.5% increase in BLEU.

| Comments: | Accepted to Findings of EMNLP 2022                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.09162](https://arxiv.org/abs/2210.09162) [cs.CL]** |
|           | (or **[arXiv:2210.09162v1](https://arxiv.org/abs/2210.09162v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.09162Focus to learn more |






# 2022-10-14

[Return to Index](#Index)



<h2 id="2022-10-14-1">1. Why self-attention is Natural for Sequence-to-Sequence Problems? A Perspective from Symmetries
</h2>

Title: [Why self-attention is Natural for Sequence-to-Sequence Problems? A Perspective from Symmetries](https://arxiv.org/abs/2210.06741)

Authors: [Chao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+C), [Lexing Ying](https://arxiv.org/search/cs?searchtype=author&query=Ying%2C+L)

> In this paper, we show that structures similar to self-attention are natural to learn many sequence-to-sequence problems from the perspective of symmetry. Inspired by language processing applications, we study the orthogonal equivariance of seq2seq functions with knowledge, which are functions taking two inputs -- an input sequence and a ``knowledge'' -- and outputting another sequence. The knowledge consists of a set of vectors in the same embedding space as the input sequence, containing the information of the language used to process the input sequence. We show that orthogonal equivariance in the embedding space is natural for seq2seq functions with knowledge, and under such equivariance the function must take the form close to the self-attention. This shows that network structures similar to self-attention are the right structures to represent the target function of many seq2seq problems. The representation can be further refined if a ``finite information principle'' is considered, or a permutation equivariance holds for the elements of the input sequence.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.06741](https://arxiv.org/abs/2210.06741) [cs.LG]** |
|           | (or **[arXiv:2210.06741v1](https://arxiv.org/abs/2210.06741v1) [cs.LG]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06741Focus to learn more |





<h2 id="2022-10-14-2">2. Categorizing Semantic Representations for Neural Machine Translation
</h2>

Title: [Categorizing Semantic Representations for Neural Machine Translation](https://arxiv.org/abs/2210.06709)

Authors: [Yongjing Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y), [Yafu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Yue Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y)

> Modern neural machine translation (NMT) models have achieved competitive performance in standard benchmarks. However, they have recently been shown to suffer limitation in compositional generalization, failing to effectively learn the translation of atoms (e.g., words) and their semantic composition (e.g., modification) from seen compounds (e.g., phrases), and thus suffering from significantly weakened translation performance on unseen compounds during inference. We address this issue by introducing categorization to the source contextualized representations. The main idea is to enhance generalization by reducing sparsity and overfitting, which is achieved by finding prototypes of token representations over the training set and integrating their embeddings into the source encoding. Experiments on a dedicated MT dataset (i.e., CoGnition) show that our method reduces compositional generalization error rates by 24\% error reduction. In addition, our conceptually simple method gives consistently better results than the Transformer baseline on a range of general MT datasets.

| Comments: | COLING 2022                                                  |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.06709](https://arxiv.org/abs/2210.06709) [cs.CL]** |
|           | (or **[arXiv:2210.06709v1](https://arxiv.org/abs/2210.06709v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06709Focus to learn more |







<h2 id="2022-10-14-3">3. Large Language Models are few(1)-shot Table Reasoners
</h2>

Title: [Large Language Models are few(1)-shot Table Reasoners](https://arxiv.org/abs/2210.06710)

Authors: [Wenhu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W)

> Recent literature has shown that large language models (LLMs) are generally excellent few-shot reasoners to solve text reasoning tasks. However, the capability of LLMs on table reasoning tasks is yet to be explored. In this paper, we aim at understanding how well LLMs can perform on these table tasks with few-shot in-context learning. Specifically, we evaluate LLMs on popular table QA and fact verification datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are really competent at complex reasoning over table structures. When combined with `chain of thoughts' prompting, GPT-3 is able to achieve very strong performance with only a 1-shot demonstration. We further manually study the reasoning chains elicited from LLMs and found that these reasoning chains are highly consistent with the `ground truth' semantic form. We believe that our study opens new possibilities to employ LLMs on different table-based reasoning tasks under few-shot scenario.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.06710](https://arxiv.org/abs/2210.06710) [cs.CL]** |
|           | (or **[arXiv:2210.06710v1](https://arxiv.org/abs/2210.06710v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06710Focus to learn more |







<h2 id="2022-10-14-4">4. Low-resource Neural Machine Translation with Cross-modal Alignment
</h2>

Title: [Low-resource Neural Machine Translation with Cross-modal Alignment](https://arxiv.org/abs/2210.06716)

Authors: [Zhe Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Qingkai Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Q), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y)

> How to achieve neural machine translation with limited parallel data? Existing techniques often rely on large-scale monolingual corpora, which is impractical for some low-resource languages. In this paper, we turn to connect several low-resource languages to a particular high-resource one by additional visual modality. Specifically, we propose a cross-modal contrastive learning method to learn a shared space for all languages, where both a coarse-grained sentence-level objective and a fine-grained token-level one are introduced. Experimental results and further analysis show that our method can effectively learn the cross-modal and cross-lingual alignment with a small amount of image-text pairs and achieves significant improvements over the text-only baseline under both zero-shot and few-shot scenarios.

| Comments: | Accepted to EMNLP 2022                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.06716](https://arxiv.org/abs/2210.06716) [cs.CL]** |
|           | (or **[arXiv:2210.06716v1](https://arxiv.org/abs/2210.06716v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06716Focus to learn more |







<h2 id="2022-10-14-5">5. SDW-ASL: A Dynamic System to Generate Large Scale Dataset for Continuous American Sign Language
</h2>

Title: [SDW-ASL: A Dynamic System to Generate Large Scale Dataset for Continuous American Sign Language](https://arxiv.org/abs/2210.06791)

Authors: [Yehong Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y)

> Despite tremendous progress in natural language processing using deep learning techniques in recent years, sign language production and comprehension has advanced very little. One critical barrier is the lack of largescale datasets available to the public due to the unbearable cost of labeled data generation. Efforts to provide public data for American Sign Language (ASL) comprehension have yielded two datasets, comprising more than thousand video clips. These datasets are large enough to enable a meaningful start to deep learning research on sign languages but are far too small to lead to any solution that can be practically deployed. So far, there is still no suitable dataset for ASL production. We proposed a system that can generate large scale ASL datasets for continuous ASL. It is suitable for general ASL processing and is particularly useful for ASL production. The continuous ASL dataset contains English labeled human articulations in condensed body pose data formats. To better serve the research community, we are releasing the first version of our ASL dataset, which contains 30k sentences, 416k words, a vocabulary of 18k words, in a total of 104 hours. This is the largest continuous sign language dataset published to date in terms of video duration. We also describe a system that can evolve and expand the dataset to incorporate better data processing techniques and more contents when available. It is our hope that the release of this ASL dataset and the sustainable dataset generation system to the public will propel better deep-learning research in ASL natural language processing.

| Comments: | There are 9 pages, 3 figures and 1 table. The ASL dataset viewing system is at [this https URL](https://adeddb94ac1d.ngrok.io/) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.06791](https://arxiv.org/abs/2210.06791) [cs.CL]** |
|           | (or **[arXiv:2210.06791v1](https://arxiv.org/abs/2210.06791v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06791Focus to learn more |







<h2 id="2022-10-14-6">6. CS-Insights: A System for Analyzing Computer Science Research
</h2>

Title: [CS-Insights: A System for Analyzing Computer Science Research](https://arxiv.org/abs/2210.06878)

Authors: [Terry Ruas](https://arxiv.org/search/cs?searchtype=author&query=Ruas%2C+T), [Jan Philip Wahle](https://arxiv.org/search/cs?searchtype=author&query=Wahle%2C+J+P), [Lennart Küll](https://arxiv.org/search/cs?searchtype=author&query=Küll%2C+L), [Saif M. Mohammad](https://arxiv.org/search/cs?searchtype=author&query=Mohammad%2C+S+M), [Bela Gipp](https://arxiv.org/search/cs?searchtype=author&query=Gipp%2C+B)

> This paper presents CS-Insights, an interactive web application to analyze computer science publications from DBLP through multiple perspectives. The dedicated interfaces allow its users to identify trends in research activity, productivity, accessibility, author's productivity, venues' statistics, topics of interest, and the impact of computer science research on other fields. CS-Insightsis publicly available, and its modular architecture can be easily adapted to domains other than computer science.

| Subjects: | **Computation and Language (cs.CL)**; Digital Libraries (cs.DL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.06878](https://arxiv.org/abs/2210.06878) [cs.CL]** |
|           | (or **[arXiv:2210.06878v1](https://arxiv.org/abs/2210.06878v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06878Focus to learn more |









<h2 id="2022-10-14-7">7. Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text
</h2>

Title: [Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text](https://arxiv.org/abs/2210.06990)

Authors: [Marwa Gaser](https://arxiv.org/search/cs?searchtype=author&query=Gaser%2C+M), [Manuel Mager](https://arxiv.org/search/cs?searchtype=author&query=Mager%2C+M), [Injy Hamed](https://arxiv.org/search/cs?searchtype=author&query=Hamed%2C+I), [Nizar Habash](https://arxiv.org/search/cs?searchtype=author&query=Habash%2C+N), [Slim Abdennadher](https://arxiv.org/search/cs?searchtype=author&query=Abdennadher%2C+S), [Ngoc Thang Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+N+T)

> Data sparsity is one of the main challenges posed by Code-switching (CS), which is further exacerbated in the case of morphologically rich languages. For the task of Machine Translation (MT), morphological segmentation has proven successful in alleviating data sparsity in monolingual contexts; however, it has not been investigated for CS settings. In this paper, we study the effectiveness of different segmentation approaches on MT performance, covering morphology-based and frequency-based segmentation techniques. We experiment on MT from code-switched Arabic-English to English. We provide detailed analysis, examining a variety of conditions, such as data size and sentences with different degrees in CS. Empirical results show that morphology-aware segmenters perform the best in segmentation tasks but under-perform in MT. Nevertheless, we find that the choice of the segmentation setup to use for MT is highly dependent on the data size. For extreme low-resource scenarios, a combination of frequency and morphology-based segmentations is shown to perform the best. For more resourced settings, such a combination does not bring significant improvements over the use of frequency-based segmentation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.06990](https://arxiv.org/abs/2210.06990) [cs.CL]** |
|           | (or **[arXiv:2210.06990v1](https://arxiv.org/abs/2210.06990v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06990Focus to learn more |





<h2 id="2022-10-14-8">8. DICTDIS: Dictionary Constrained Disambiguation for Improved NMT
</h2>

Title: [DICTDIS: Dictionary Constrained Disambiguation for Improved NMT](https://arxiv.org/abs/2210.06996)

Authors: [Ayush Maheshwari](https://arxiv.org/search/cs?searchtype=author&query=Maheshwari%2C+A), [Piyush Sharma](https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+P), [Preethi Jyothi](https://arxiv.org/search/cs?searchtype=author&query=Jyothi%2C+P), [Ganesh Ramakrishnan](https://arxiv.org/search/cs?searchtype=author&query=Ramakrishnan%2C+G)

> Domain-specific neural machine translation (NMT) systems (e.g., in educational applications) are socially significant with the potential to help make information accessible to a diverse set of users in multilingual societies. It is desirable that such NMT systems be lexically constrained and draw from domain-specific dictionaries. Dictionaries could present multiple candidate translations for a source words/phrases on account of the polysemous nature of words. The onus is then on the NMT model to choose the contextually most appropriate candidate. Prior work has largely ignored this problem and focused on the single candidate setting where the target word or phrase is replaced by a single constraint. In this work we present DICTDIS, a lexically constrained NMT system that disambiguates between multiple candidate translations derived from dictionaries. We achieve this by augmenting training data with multiple dictionary candidates to actively encourage disambiguation during training. We demonstrate the utility of DICTDIS via extensive experiments on English-Hindi sentences in a variety of domains including news, finance, medicine and engineering. We obtain superior disambiguation performance on all domains with improved fluency in some domains of up to 4 BLEU points, when compared with existing approaches for lexically constrained and unconstrained NMT.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.06996](https://arxiv.org/abs/2210.06996) [cs.CL]** |
|           | (or **[arXiv:2210.06996v1](https://arxiv.org/abs/2210.06996v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06996Focus to learn more |







<h2 id="2022-10-14-9">9. Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation
</h2>

Title: [Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation](https://arxiv.org/abs/2210.07054)

Authors: [Jinhui Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+J), [Wenxiang Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+W), [Xing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Zhaopeng Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z)

> Sign language gloss translation aims to translate the sign glosses into spoken language texts, which is challenging due to the scarcity of labeled gloss-text parallel data. Back translation (BT), which generates pseudo-parallel data by translating in-domain spoken language texts into sign glosses, has been applied to alleviate the data scarcity problem. However, the lack of large-scale high-quality domain spoken language text data limits the effect of BT. In this paper, to overcome the limitation, we propose a Prompt based domain text Generation (PGEN) approach to produce the large-scale in-domain spoken language text data. Specifically, PGEN randomly concatenates sentences from the original in-domain spoken language text data as prompts to induce a pre-trained language model (i.e., GPT-2) to generate spoken language texts in a similar style. Experimental results on three benchmarks of sign language gloss translation in varied languages demonstrate that BT with spoken language texts generated by PGEN significantly outperforms the compared methods. In addition, as the scale of spoken language texts generated by PGEN increases, the BT technique can achieve further improvements, demonstrating the effectiveness of our approach. We release the code and data for facilitating future research in this field.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.07054](https://arxiv.org/abs/2210.07054) [cs.CL]** |
|           | (or **[arXiv:2210.07054v1](https://arxiv.org/abs/2210.07054v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.07054Focus to learn more |









<h2 id="2022-10-14-10">10. Incorporating Context into Subword Vocabularies
</h2>

Title: [Incorporating Context into Subword Vocabularies](https://arxiv.org/abs/2210.07095)

Authors: [Shaked Yehezkel](https://arxiv.org/search/cs?searchtype=author&query=Yehezkel%2C+S), [Yuval Pinter](https://arxiv.org/search/cs?searchtype=author&query=Pinter%2C+Y)

> Most current popular subword tokenizers are trained based on word frequency statistics over a corpus, without considering information about co-occurrence or context. Nevertheless, the resulting vocabularies are used in language models' highly contextualized settings. We present SaGe, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase. We show that SaGe does a better job than current widespread tokenizers in keeping token contexts cohesive, while not incurring a large price in terms of encoding efficiency or domain robustness. SaGe improves performance on English GLUE classification tasks as well as on NER, and on Inference and NER in Turkish, demonstrating its robustness to language properties such as morphological exponence and agglutination.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.07095](https://arxiv.org/abs/2210.07095) [cs.CL]** |
|           | (or **[arXiv:2210.07095v1](https://arxiv.org/abs/2210.07095v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.07095Focus to learn more |







<h2 id="2022-10-14-11">11. A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models
</h2>

Title: [A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models](https://arxiv.org/abs/2210.07111)

Authors: [Jimin Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J), [Patrick Fernandes](https://arxiv.org/search/cs?searchtype=author&query=Fernandes%2C+P), [Xinyi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G)

> Recent work on tokenizer-free multilingual pretrained models show promising results in improving cross-lingual transfer and reducing engineering overhead (Clark et al., 2022; Xue et al., 2022). However, these works mainly focus on reporting accuracy on a limited set of tasks and data settings, placing less emphasis on other important factors when tuning and deploying the models in practice, such as memory usage, inference speed, and fine-tuning data robustness. We attempt to fill this gap by performing a comprehensive empirical comparison of multilingual tokenizer-free and subword-based models considering these various dimensions. Surprisingly, we find that subword-based models might still be the most practical choice in many settings, achieving better performance for lower inference latency and memory usage. Based on these results, we encourage future work in tokenizer-free methods to consider these factors when designing and evaluating new models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.07111](https://arxiv.org/abs/2210.07111) [cs.CL]** |
|           | (or **[arXiv:2210.07111v1](https://arxiv.org/abs/2210.07111v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.07111Focus to learn more |







<h2 id="2022-10-14-12">12. How (Not) To Evaluate Explanation Quality
</h2>

Title: [How (Not) To Evaluate Explanation Quality](https://arxiv.org/abs/2210.07126)

Authors: [Hendrik Schuff](https://arxiv.org/search/cs?searchtype=author&query=Schuff%2C+H), [Heike Adel](https://arxiv.org/search/cs?searchtype=author&query=Adel%2C+H), [Peng Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+P), [Ngoc Thang Vu](https://arxiv.org/search/cs?searchtype=author&query=Vu%2C+N+T)

> The importance of explainability is increasingly acknowledged in natural language processing. However, it is still unclear how the quality of explanations can be assessed effectively. The predominant approach is to compare proxy scores (such as BLEU or explanation F1) evaluated against gold explanations in the dataset. The assumption is that an increase of the proxy score implies a higher utility of explanations to users. In this paper, we question this assumption. In particular, we (i) formulate desired characteristics of explanation quality that apply across tasks and domains, (ii) point out how current evaluation practices violate those characteristics, and (iii) propose actionable guidelines to overcome obstacles that limit today's evaluation of explanation quality and to enable the development of explainable systems that provide tangible benefits for human users. We substantiate our theoretical claims (i.e., the lack of validity and temporal decline of currently-used proxy scores) with empirical evidence from a crowdsourcing case study in which we investigate the explanation quality of state-of-the-art explainable question answering systems.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.07126](https://arxiv.org/abs/2210.07126) [cs.CL]** |
|           | (or **[arXiv:2210.07126v1](https://arxiv.org/abs/2210.07126v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.07126Focus to learn more |





<h2 id="2022-10-14-13">13. You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models
</h2>

Title: [You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models](https://arxiv.org/abs/2210.07135)

Authors: [Tomasz Limisiewicz](https://arxiv.org/search/cs?searchtype=author&query=Limisiewicz%2C+T), [Dan Malkin](https://arxiv.org/search/cs?searchtype=author&query=Malkin%2C+D), [Gabriel Stanovsky](https://arxiv.org/search/cs?searchtype=author&query=Stanovsky%2C+G)

> Multilingual models have been widely used for cross-lingual transfer to low-resource languages. However, the performance on these languages is hindered by their underrepresentation in the pretraining data. To alleviate this problem, we propose a novel multilingual training technique based on teacher-student knowledge distillation. In this setting, we utilize monolingual teacher models optimized for their language. We use those teachers along with balanced (sub-sampled) data to distill the teachers' knowledge into a single multilingual student. Our method outperforms standard training methods in low-resource languages and retrains performance on high-resource languages while using the same amount of data. If applied widely, our approach can increase the representation of low-resource languages in NLP systems.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.07135](https://arxiv.org/abs/2210.07135) [cs.CL]** |
|           | (or **[arXiv:2210.07135v1](https://arxiv.org/abs/2210.07135v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.07135Focus to learn more |







<h2 id="2022-10-14-14">14. Language Model Decoding as Likelihood-Utility Alignment
</h2>

Title: [Language Model Decoding as Likelihood-Utility Alignment](https://arxiv.org/abs/2210.07228)

Authors: [Martin Josifoski](https://arxiv.org/search/cs?searchtype=author&query=Josifoski%2C+M), [Maxime Peyrard](https://arxiv.org/search/cs?searchtype=author&query=Peyrard%2C+M), [Frano Rajic](https://arxiv.org/search/cs?searchtype=author&query=Rajic%2C+F), [Jiheng Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+J), [Debjit Paul](https://arxiv.org/search/cs?searchtype=author&query=Paul%2C+D), [Valentin Hartmann](https://arxiv.org/search/cs?searchtype=author&query=Hartmann%2C+V), [Barun Patra](https://arxiv.org/search/cs?searchtype=author&query=Patra%2C+B), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Emre Kıcıman](https://arxiv.org/search/cs?searchtype=author&query=Kıcıman%2C+E), [Boi Faltings](https://arxiv.org/search/cs?searchtype=author&query=Faltings%2C+B), [Robert West](https://arxiv.org/search/cs?searchtype=author&query=West%2C+R)

> A critical component of a successful language generation pipeline is the decoding algorithm. However, the general principles that should guide the choice of decoding algorithm remain unclear. Previous works only compare decoding algorithms in narrow scenarios and their findings do not generalize across tasks. To better structure the discussion, we introduce a taxonomy that groups decoding strategies based on their implicit assumptions about how well the model's likelihood is aligned with the task-specific notion of utility. We argue that this taxonomy allows a broader view of the decoding problem and can lead to generalizable statements because it is grounded on the interplay between the decoding algorithms and the likelihood-utility misalignment. Specifically, by analyzing the correlation between the likelihood and the utility of predictions across a diverse set of tasks, we provide the first empirical evidence supporting the proposed taxonomy, and a set of principles to structure reasoning when choosing a decoding algorithm. Crucially, our analysis is the first one to relate likelihood-based decoding strategies with strategies that rely on external information such as value-guided methods and prompting, and covers the most diverse set of tasks up-to-date.

| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.07228](https://arxiv.org/abs/2210.07228) [cs.CL]** |
|           | (or **[arXiv:2210.07228v1](https://arxiv.org/abs/2210.07228v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.07228Focus to learn more |










# 2022-10-13

[Return to Index](#Index)



<h2 id="2022-10-13-1">1. One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks
</h2>
Title: [One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks](https://arxiv.org/abs/2210.06379)
Authors: [Gregor Geigle](https://arxiv.org/search/cs?searchtype=author&query=Geigle%2C+G), [Chen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Jonas Pfeiffer](https://arxiv.org/search/cs?searchtype=author&query=Pfeiffer%2C+J), [Iryna Gurevych](https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I)

> Current multimodal models, aimed at solving Vision and Language (V+L) tasks, predominantly repurpose Vision Encoders (VE) as feature extractors. While many VEs -- of different architectures, trained on different data and objectives -- are publicly available, they are not designed for the downstream V+L tasks. Nonetheless, most current work assumes that a \textit{single} pre-trained VE can serve as a general-purpose encoder. In this work, we evaluate whether the information stored within different VEs is complementary, i.e. if providing the model with features from multiple VEs can improve the performance on a target task. We exhaustively experiment with three popular VEs on six downstream V+L tasks and analyze the attention and VE-dropout patterns. Our results and analyses suggest that diverse VEs complement each other, resulting in improved downstream V+L task performance, where the improvements are not due to simple ensemble effects (i.e. the performance does not always improve when increasing the number of encoders). We demonstrate that future VEs, which are not \textit{repurposed}, but explicitly \textit{designed} for V+L tasks, have the potential of improving performance on the target V+L tasks.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.06379](https://arxiv.org/abs/2210.06379) [cs.CV]** |
|           | (or **[arXiv:2210.06379v1](https://arxiv.org/abs/2210.06379v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06379Focus to learn more |





<h2 id="2022-10-13-2">2. Foundation Transformers
</h2>

Title: [Foundation Transformers](https://arxiv.org/abs/2210.06423)
Authors: [Hongyu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L), [Wenhui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Zhiliang Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+Z), [Yu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y), [Payal Bajaj](https://arxiv.org/search/cs?searchtype=author&query=Bajaj%2C+P), [Saksham Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal%2C+S), [Alon Benhaim](https://arxiv.org/search/cs?searchtype=author&query=Benhaim%2C+A), [Barun Patra](https://arxiv.org/search/cs?searchtype=author&query=Patra%2C+B), [Zhun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Vishrav Chaudhary](https://arxiv.org/search/cs?searchtype=author&query=Chaudhary%2C+V), [Xia Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> A big convergence of model architectures across language, vision, speech, and multimodal is emerging. However, under the same name "Transformers", the above areas use different implementations for better performance, e.g., Post-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We call for the development of Foundation Transformer for true general-purpose modeling, which serves as a go-to architecture for various tasks and modalities with guaranteed training stability. In this work, we introduce a Transformer variant, named Magneto, to fulfill the goal. Specifically, we propose Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up. Extensive experiments demonstrate its superior performance and better stability than the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| Cite as:  | **[arXiv:2210.06423](https://arxiv.org/abs/2210.06423) [cs.LG]** |
|           | (or **[arXiv:2210.06423v1](https://arxiv.org/abs/2210.06423v1) [cs.LG]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06423Focus to learn more |





<h2 id="2022-10-13-3">3. Shapley Head Pruning: Identifying and Removing Interference in Multilingual Transformers
</h2>

Title: [Shapley Head Pruning: Identifying and Removing Interference in Multilingual Transformers](https://arxiv.org/abs/2210.05709)
Authors: [William Held](https://arxiv.org/search/cs?searchtype=author&query=Held%2C+W), [Diyi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+D)

> Multilingual transformer-based models demonstrate remarkable zero and few-shot transfer across languages by learning and reusing language-agnostic features. However, as a fixed-size model acquires more languages, its performance across all languages degrades, a phenomenon termed interference. Often attributed to limited model capacity, interference is commonly addressed by adding additional parameters despite evidence that transformer-based models are overparameterized. In this work, we show that it is possible to reduce interference by instead identifying and pruning language-specific parameters. First, we use Shapley Values, a credit allocation metric from coalitional game theory, to identify attention heads that introduce interference. Then, we show that removing identified attention heads from a fixed model improves performance for a target language on both sentence classification and structural prediction, seeing gains as large as 24.7\%. Finally, we provide insights on language-agnostic and language-specific attention heads using attention visualization.

| Comments: | 8 Pages, 9 Figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.05709](https://arxiv.org/abs/2210.05709) [cs.CL]** |
|           | (or **[arXiv:2210.05709v1](https://arxiv.org/abs/2210.05709v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05709Focus to learn more |





<h2 id="2022-10-13-4">4. Scaling Up Deliberation for Multilingual ASR
</h2>

Title: [Scaling Up Deliberation for Multilingual ASR](https://arxiv.org/abs/2210.05785)
Authors: [Ke Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+K), [Bo Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B), [Tara N. Sainath](https://arxiv.org/search/cs?searchtype=author&query=Sainath%2C+T+N)

> Multilingual end-to-end automatic speech recognition models are attractive due to its simplicity in training and deployment. Recent work on large-scale training of such models has shown promising results compared to monolingual models. However, the work often focuses on multilingual models themselves in a single-pass setup. In this work, we investigate second-pass deliberation for multilingual speech recognition. Our proposed deliberation is multilingual, i.e., the text encoder encodes hypothesis text from multiple languages, and the decoder attends to multilingual text and audio. We investigate scaling the deliberation text encoder and decoder, and compare scaling the deliberation decoder and the first-pass cascaded encoder. We show that deliberation improves the average WER on 9 languages by 4% relative compared to the single-pass model. By increasing the size of the deliberation up to 1B parameters, the average WER improvement increases to 9%, with up to 14% for certain languages. Our deliberation rescorer is based on transformer layers and can be parallelized during rescoring.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05785](https://arxiv.org/abs/2210.05785) [cs.CL]** |
|           | (or **[arXiv:2210.05785v1](https://arxiv.org/abs/2210.05785v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05785Focus to learn more |





<h2 id="2022-10-13-5">5. CLIP also Understands Text: Prompting CLIP for Phrase Understanding
</h2>

Title: [CLIP also Understands Text: Prompting CLIP for Phrase Understanding](https://arxiv.org/abs/2210.05836)
Authors: [An Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+A), [Jiacheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Wanrong Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [Yujie Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y), [Julian McAuley](https://arxiv.org/search/cs?searchtype=author&query=McAuley%2C+J)

> Contrastive Language-Image Pretraining (CLIP) efficiently learns visual concepts by pre-training with natural language supervision. CLIP and its visual encoder have been explored on various vision and language tasks and achieve strong zero-shot or transfer learning performance. However, the application of its text encoder solely for text understanding has been less explored. In this paper, we find that the text encoder of CLIP actually demonstrates strong ability for phrase understanding, and can even significantly outperform popular language models such as BERT with a properly designed prompt. Extensive experiments validate the effectiveness of our method across different datasets and domains on entity clustering and entity set expansion tasks.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.05836](https://arxiv.org/abs/2210.05836) [cs.CL]** |
|           | (or **[arXiv:2210.05836v1](https://arxiv.org/abs/2210.05836v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05836Focus to learn more |







<h2 id="2022-10-13-6">6. Non-Autoregressive Machine Translation with Translation Memories
</h2>

Title: [Non-Autoregressive Machine Translation with Translation Memories](https://arxiv.org/abs/2210.06020)
Authors: [Jitao Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Josep Crego](https://arxiv.org/search/cs?searchtype=author&query=Crego%2C+J), [François Yvon](https://arxiv.org/search/cs?searchtype=author&query=Yvon%2C+F)

> Non-autoregressive machine translation (NAT) has recently made great progress. However, most works to date have focused on standard translation tasks, even though some edit-based NAT models, such as the Levenshtein Transformer (LevT), seem well suited to translate with a Translation Memory (TM). This is the scenario considered here. We first analyze the vanilla LevT model and explain why it does not do well in this setting. We then propose a new variant, TM-LevT, and show how to effectively train this model. By modifying the data presentation and introducing an extra deletion operation, we obtain performance that are on par with an autoregressive approach, while reducing the decoding load. We also show that incorporating TMs during training dispenses to use knowledge distillation, a well-known trick used to mitigate the multimodality issue.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.06020](https://arxiv.org/abs/2210.06020) [cs.CL]** |
|           | (or **[arXiv:2210.06020v1](https://arxiv.org/abs/2210.06020v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06020Focus to learn more |





<h2 id="2022-10-13-7">7. Using Massive Multilingual Pre-Trained Language Models Towards Real Zero-Shot Neural Machine Translation in Clinical Domain
</h2>

Title: [Using Massive Multilingual Pre-Trained Language Models Towards Real Zero-Shot Neural Machine Translation in Clinical Domain](https://arxiv.org/abs/2210.06068)
Authors: [Lifeng Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+L), [Gleb Erofeev](https://arxiv.org/search/cs?searchtype=author&query=Erofeev%2C+G), [Irina Sorokina](https://arxiv.org/search/cs?searchtype=author&query=Sorokina%2C+I), [Serge Gladkoff](https://arxiv.org/search/cs?searchtype=author&query=Gladkoff%2C+S), [Goran Nenadic](https://arxiv.org/search/cs?searchtype=author&query=Nenadic%2C+G)

> Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks. In this work, we investigate whether MMPLMs can be applied to zero-shot machine translation (MT) toward entirely new language pairs and new domains. We carry out an experimental investigation using Meta-AI's MMPLMs "wmt21-dense-24-wide-en-X and X-en (WMT21fb)" which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and opposite direction. We fine-tune these MMPLMs towards English-Spanish language pair which did not exist at all in their original pre-trained corpora both implicitly and explicitly. We prepare carefully aligned clinical domain data for this fine-tuning, which is different from their original mixed domain knowledge as well. Our experimental result shows that the fine-tuning is very successful using just 250k well-aligned in-domain EN-ES pairs/sentences for three sub-task translation tests: clinical cases, clinical terms, and ontology concepts. It achieves very close evaluation scores to another MMPLM NLLB from Meta-AI, which included Spanish as a high-resource setting in the pre-training. To the best of our knowledge, this is the first work on using MMPLMs towards real zero-shot NMT successfully for totally unseen languages during pre-training, and also the first in clinical domain for such a study.

| Comments: | 8 pages, 2 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2210.06068](https://arxiv.org/abs/2210.06068) [cs.CL]** |
|           | (or **[arXiv:2210.06068v1](https://arxiv.org/abs/2210.06068v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06068Focus to learn more |



<h2 id="2022-10-13-8">8. Improved Data Augmentation for Translation Suggestion
</h2>

Title: [Improved Data Augmentation for Translation Suggestion](https://arxiv.org/abs/2210.06138)
Authors: [Hongxiao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H), [Siyu Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+S), [Songming Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Hui Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H), [Yufeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Jinan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Jian Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> Translation suggestion (TS) models are used to automatically provide alternative suggestions for incorrect spans in sentences generated by machine translation. This paper introduces the system used in our submission to the WMT'22 Translation Suggestion shared task. Our system is based on the ensemble of different translation architectures, including Transformer, SA-Transformer, and DynamicConv. We use three strategies to construct synthetic data from parallel corpora to compensate for the lack of supervised data. In addition, we introduce a multi-phase pre-training strategy, adding an additional pre-training phase with in-domain data. We rank second and third on the English-German and English-Chinese bidirectional tasks, respectively.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.06138](https://arxiv.org/abs/2210.06138) [cs.CL]** |
|           | (or **[arXiv:2210.06138v1](https://arxiv.org/abs/2210.06138v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06138Focus to learn more |







<h2 id="2022-10-13-9">9. ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding
</h2>

Title: [ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding](https://arxiv.org/abs/2210.06155)
Authors: [Qiming Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+Q), [Yinxu Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+Y), [Wenjin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Bin Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+B), [Zhenyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Zhengjie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z), [Teng Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+T), [Weichong Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+W), [Yongfeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Yin Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Shikun Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S), [Yu Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y), [Hao Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+H), [Hua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H), [Haifeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H)

> Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However, most existing methods lack the systematic mining and utilization of layout-centered knowledge, leading to sub-optimal performances. In this paper, we propose ERNIE-Layout, a novel document pre-training solution with layout knowledge enhancement in the whole workflow, to learn better representations that combine the features from text, layout, and image. Specifically, we first rearrange input sequences in the serialization stage, and then present a correlative pre-training task, reading order prediction, to learn the proper reading order of documents. To improve the layout awareness of the model, we integrate a spatial-aware disentangled attention into the multi-modal transformer and a replaced regions prediction task into the pre-training phase. Experimental results show that ERNIE-Layout achieves superior performance on various downstream tasks, setting new state-of-the-art on key information extraction, document image classification, and document question answering datasets. The code and models are publicly available at [this http URL](http://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-layout).

| Comments: | Accepted to EMNLP 2022 (Findings)                            |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2210.06155](https://arxiv.org/abs/2210.06155) [cs.CL]** |
|           | (or **[arXiv:2210.06155v1](https://arxiv.org/abs/2210.06155v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06155Focus to learn more |





<h2 id="2022-10-13-10">10. Pruning Pre-trained Language Models Without Fine-Tuning
</h2>

Title: [Pruning Pre-trained Language Models Without Fine-Tuning](https://arxiv.org/abs/2210.06210)
Authors: [Ting Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+T), [Deqing Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D), [Fuzhen Zhuang](https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+F)

> To overcome the overparameterized problem in Pre-trained Language Models (PLMs), pruning is widely used as a simple and straightforward compression method by directly removing unimportant weights. Previous first-order methods successfully compress PLMs to extremely high sparsity with little performance drop. These methods, such as movement pruning, use first-order information to prune PLMs while fine-tuning the remaining weights. In this work, we argue fine-tuning is redundant for first-order pruning, since first-order pruning is sufficient to converge PLMs to downstream tasks without fine-tuning. Under this motivation, we propose Static Model Pruning (SMP), which only uses first-order pruning to adapt PLMs to downstream tasks while achieving the target sparsity level. In addition, we also design a new masking function and training objective to further improve SMP. Extensive experiments at various sparsity levels show SMP has significant improvements over first-order and zero-order methods. Unlike previous first-order methods, SMP is also applicable to low sparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter efficient than other methods due to it does not require fine-tuning.

| Comments: | Work in progress                                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.06210](https://arxiv.org/abs/2210.06210) [cs.CL]** |
|           | (or **[arXiv:2210.06210v1](https://arxiv.org/abs/2210.06210v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06210Focus to learn more |





<h2 id="2022-10-13-11">11. A context-aware knowledge transferring strategy for CTC-based ASR
</h2>

Title: [A context-aware knowledge transferring strategy for CTC-based ASR](https://arxiv.org/abs/2210.06244)
Authors: [Ke-Han Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+K), [Kuan-Yu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K)

> Non-autoregressive automatic speech recognition (ASR) modeling has received increasing attention recently because of its fast decoding speed and superior performance. Among representatives, methods based on the connectionist temporal classification (CTC) are still a dominating stream. However, the theoretically inherent flaw, the assumption of independence between tokens, creates a performance barrier for the school of works. To mitigate the challenge, we propose a context-aware knowledge transferring strategy, consisting of a knowledge transferring module and a context-aware training strategy, for CTC-based ASR. The former is designed to distill linguistic information from a pre-trained language model, and the latter is framed to modulate the limitations caused by the conditional independence assumption. As a result, a knowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is presented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2 datasets demonstrate the effectiveness of the proposed method.

| Comments: | Accepted by SLT 2022                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2210.06244](https://arxiv.org/abs/2210.06244) [cs.CL]** |
|           | (or **[arXiv:2210.06244v1](https://arxiv.org/abs/2210.06244v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06244Focus to learn more |





<h2 id="2022-10-13-12">12. Back to the Future: On Potential Histories in NLP
</h2>

Title: [Back to the Future: On Potential Histories in NLP](https://arxiv.org/abs/2210.06245)
Authors: [Zeerak Talat](https://arxiv.org/search/cs?searchtype=author&query=Talat%2C+Z), [Anne Lauscher](https://arxiv.org/search/cs?searchtype=author&query=Lauscher%2C+A)

> Machine learning and NLP require the construction of datasets to train and fine-tune models. In this context, previous work has demonstrated the sensitivity of these data sets. For instance, potential societal biases in this data are likely to be encoded and to be amplified in the models we deploy. In this work, we draw from developments in the field of history and take a novel perspective on these problems: considering datasets and models through the lens of historical fiction surfaces their political nature, and affords re-configuring how we view the past, such that marginalized discourses are surfaced. Building on such insights, we argue that contemporary methods for machine learning are prejudiced towards dominant and hegemonic histories. Employing the example of neopronouns, we show that by surfacing marginalized histories within contemporary conditions, we can create models that better represent the lived realities of traditionally marginalized and excluded communities.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.06245](https://arxiv.org/abs/2210.06245) [cs.CL]** |
|           | (or **[arXiv:2210.06245v1](https://arxiv.org/abs/2210.06245v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06245Focus to learn more |





<h2 id="2022-10-13-13">13. Task Compass: Scaling Multi-task Pre-training with Task Prefix
</h2>

Title: [Task Compass: Scaling Multi-task Pre-training with Task Prefix](https://arxiv.org/abs/2210.06277)
Authors: [Zhuosheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Yichong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Yuwei Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y), [Wenhao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+W), [Yang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y), [Hai Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M)

> Leveraging task-aware annotated data as supervised signals to assist with self-supervised learning on large-scale unlabeled data has become a new trend in pre-training language models. Existing studies show that multi-task learning with large-scale supervised tasks suffers from negative effects across tasks. To tackle the challenge, we propose a task prefix guided multi-task pre-training framework to explore the relationships among tasks. We conduct extensive experiments on 40 datasets, which show that our model can not only serve as the strong foundation backbone for a wide range of tasks but also be feasible as a probing tool for analyzing task relationships. The task relationships reflected by the prefixes align transfer learning performance between tasks. They also suggest directions for data augmentation with complementary tasks, which help our model achieve human-parity results on commonsense reasoning leaderboards. Code is available at [this https URL](https://github.com/cooelf/CompassMTL)

| Comments: | Findings of EMNLP 2022                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.06277](https://arxiv.org/abs/2210.06277) [cs.CL]** |
|           | (or **[arXiv:2210.06277v1](https://arxiv.org/abs/2210.06277v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06277Focus to learn more |





<h2 id="2022-10-13-14">14. Changing the Representation: Examining Language Representation for Neural Sign Language Production
</h2>

Title: [Changing the Representation: Examining Language Representation for Neural Sign Language Production](https://arxiv.org/abs/2210.06312)
Authors: [Harry Walsh](https://arxiv.org/search/cs?searchtype=author&query=Walsh%2C+H), [Ben Saunders](https://arxiv.org/search/cs?searchtype=author&query=Saunders%2C+B), [Richard Bowden](https://arxiv.org/search/cs?searchtype=author&query=Bowden%2C+R)

> Neural Sign Language Production (SLP) aims to automatically translate from spoken language sentences to sign language videos. Historically the SLP task has been broken into two steps; Firstly, translating from a spoken language sentence to a gloss sequence and secondly, producing a sign language video given a sequence of glosses. In this paper we apply Natural Language Processing techniques to the first step of the SLP pipeline. We use language models such as BERT and Word2Vec to create better sentence level embeddings, and apply several tokenization techniques, demonstrating how these improve performance on the low resource translation task of Text to Gloss. We introduce Text to HamNoSys (T2H) translation, and show the advantages of using a phonetic representation for sign language translation rather than a sign level gloss representation. Furthermore, we use HamNoSys to extract the hand shape of a sign and use this as additional supervision during training, further increasing the performance on T2H. Assembling best practise, we achieve a BLEU-4 score of 26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art baselines.

| Comments:    | 8 pages, 4 figures, 5 tables, SLTAT 2022                     |
| ------------ | ------------------------------------------------------------ |
| Subjects:    | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| MSC classes: | 68T50 (Primary)                                              |
| Cite as:     | **[arXiv:2210.06312](https://arxiv.org/abs/2210.06312) [cs.CL]** |
|              | (or **[arXiv:2210.06312v1](https://arxiv.org/abs/2210.06312v1) [cs.CL]** for this version) |
|              | https://doi.org/10.48550/arXiv.2210.06312Focus to learn more |



<h2 id="2022-10-13-15">15. InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings
</h2>

Title: [InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2210.06432)
Authors: [Xing Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X), [Chaochen Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C), [Zijia Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z), [Jizhong Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J), [Zhongyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Songlin Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S)

> Contrastive learning has been extensively studied in sentence embedding learning, which assumes that the embeddings of different views of the same sentence are closer. The constraint brought by this assumption is weak, and a good sentence representation should also be able to reconstruct the original sentence fragments. Therefore, this paper proposes an information-aggregated contrastive learning framework for learning unsupervised sentence embeddings, termed InfoCSE. InfoCSE forces the representation of [CLS] positions to aggregate denser sentence information by introducing an additional Masked language model task and a well-designed network. We evaluate the proposed InfoCSE on several benchmark datasets w.r.t the semantic text similarity (STS) task. Experimental results show that InfoCSE outperforms SimCSE by an average Spearman correlation of 2.60% on BERT-base, and 1.77% on BERT-large, achieving state-of-the-art results among unsupervised sentence representation learning methods. Our code are available at [this https URL](https://github.com/caskcsg/sentemb/info)

| Comments: | EMNLP 2022                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.06432](https://arxiv.org/abs/2210.06432) [cs.CL]** |
|           | (or **[arXiv:2210.06432v1](https://arxiv.org/abs/2210.06432v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06432Focus to learn more |





<h2 id="2022-10-13-16">16. Are Sample-Efficient NLP Models More Robust?
</h2>

Title: [Are Sample-Efficient NLP Models More Robust?](https://arxiv.org/abs/2210.06456)
Authors: [Nelson F. Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+N+F), [Ananya Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+A), [Percy Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P), [Robin Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+R)

> Recent work has observed that pre-trained models have higher out-of-distribution (OOD) robustness when they are exposed to less in-distribution (ID) training data (Radford et al., 2021). In particular, zero-shot models (e.g., GPT-3 and CLIP) have higher robustness than conventionally fine-tuned models, but these robustness gains fade as zero-shot models are fine-tuned on more ID data. We study this relationship between sample efficiency and robustness -- if two models have the same ID performance, does the model trained on fewer examples (higher sample efficiency) perform better OOD (higher robustness)? 
> Surprisingly, experiments across three tasks, 23 total ID-OOD settings, and 14 models do not reveal a consistent relationship between sample efficiency and robustness -- while models with higher sample efficiency are sometimes more robust, most often there is no change in robustness, with some cases even showing decreased robustness. Since results vary on a case-by-case basis, we conduct detailed case studies of two particular ID-OOD pairs (SST-2 -> IMDb sentiment and SNLI -> HANS) to better understand why better sample efficiency may or may not yield higher robustness; attaining such an understanding requires case-by-case analysis of why models are not robust on a particular ID-OOD setting and how modeling techniques affect model capabilities.

| Comments: | 25 pages, 21 figures                                         |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.06456](https://arxiv.org/abs/2210.06456) [cs.CL]** |
|           | (or **[arXiv:2210.06456v1](https://arxiv.org/abs/2210.06456v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.06456Focus to learn more |



# 2022-10-12

[Return to Index](#Index)



<h2 id="2022-10-12-1">1. Markup-to-Image Diffusion Models with Scheduled Sampling
</h2>

Title: [Markup-to-Image Diffusion Models with Scheduled Sampling](https://arxiv.org/abs/2210.05147)

Authors: [Yuntian Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y), [Noriyuki Kojima](https://arxiv.org/search/cs?searchtype=author&query=Kojima%2C+N), [Alexander M. Rush](https://arxiv.org/search/cs?searchtype=author&query=Rush%2C+A+M)

> Building on recent advances in image generation, we present a fully data-driven approach to rendering markup into images. The approach is based on diffusion models, which parameterize the distribution of data using a sequence of denoising operations on top of a Gaussian noise distribution. We view the diffusion denoising process as a sequential decision making process, and show that it exhibits compounding errors similar to exposure bias issues in imitation learning problems. To mitigate these issues, we adapt the scheduled sampling algorithm to diffusion training. We conduct experiments on four markup datasets: mathematical formulas (LaTeX), table layouts (HTML), sheet music (LilyPond), and molecular images (SMILES). These experiments each verify the effectiveness of the diffusion process and the use of scheduled sampling to fix generation issues. These results also show that the markup-to-image task presents a useful controlled compositional setting for diagnosing and analyzing generative image models.

| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05147](https://arxiv.org/abs/2210.05147) [cs.LG]** |
|           | (or **[arXiv:2210.05147v1](https://arxiv.org/abs/2210.05147v1) [cs.LG]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05147Focus to learn more |





<h2 id="2022-10-12-2">2. MAP: Modality-Agnostic Uncertainty-Aware Vision-Language Pre-training Model
</h2>

Title: [MAP: Modality-Agnostic Uncertainty-Aware Vision-Language Pre-training Model](https://arxiv.org/abs/2210.05335)

Authors: [Yatai Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+Y), [Junjie Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Yuan Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y), [Lin Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Yanru Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Y), [Hongfa Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H), [Jiaxing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J), [Tetsuya Sakai](https://arxiv.org/search/cs?searchtype=author&query=Sakai%2C+T), [Yujiu Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y)

> Multimodal semantic understanding often has to deal with uncertainty, which means the obtained message tends to refer to multiple targets. Such uncertainty is problematic for our interpretation, including intra-modal and inter-modal uncertainty. Little effort studies the modeling of this uncertainty, particularly in pre-training on unlabeled datasets and fine-tuning in task-specific downstream tasks. To address this, we project the representations of all modalities as probabilistic distributions via a Probability Distribution Encoder (PDE) by utilizing rich multimodal semantic information. Furthermore, we integrate uncertainty modeling with popular pre-training frameworks and propose suitable pre-training tasks: Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribution-based Image-Text Matching (D-ITM). The fine-tuned models are applied to challenging downstream tasks, including image-text retrieval, visual question answering, visual reasoning, and visual entailment, and achieve state-of-the-art results. Code is released at [this https URL](https://github.com/IIGROUP/MAP).

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Multimedia (cs.MM) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05335](https://arxiv.org/abs/2210.05335) [cs.CV]** |
|           | (or **[arXiv:2210.05335v1](https://arxiv.org/abs/2210.05335v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05335Focus to learn more |





<h2 id="2022-10-12-3">3. Multilingual Representation Distillation with Contrastive Learning
</h2>

Title: [Multilingual Representation Distillation with Contrastive Learning](https://arxiv.org/abs/2210.05033)

Authors: [Weiting Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+W), [Kevin Heffernan](https://arxiv.org/search/cs?searchtype=author&query=Heffernan%2C+K), [Holger Schwenk](https://arxiv.org/search/cs?searchtype=author&query=Schwenk%2C+H), [Philipp Koehn](https://arxiv.org/search/cs?searchtype=author&query=Koehn%2C+P)

> Multilingual sentence representations from large models can encode semantic information from two or more languages and can be used for different cross-lingual information retrieval tasks. In this paper, we integrate contrastive learning into multilingual representation distillation and use it for quality estimation of parallel sentences (find semantically similar sentences that can be used as translations of each other). We validate our approach with multilingual similarity search and corpus filtering tasks. Experiments across different low-resource languages show that our method significantly outperforms previous sentence encoders such as LASER, LASER3, and LaBSE.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05033](https://arxiv.org/abs/2210.05033) [cs.CL]** |
|           | (or **[arXiv:2210.05033v1](https://arxiv.org/abs/2210.05033v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05033Focus to learn more |





<h2 id="2022-10-12-4">4. Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions
</h2>

Title: [Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions](Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions)

Authors: [Cuong Hoang](https://arxiv.org/search/cs?searchtype=author&query=Hoang%2C+C), [Devendra Sachan](https://arxiv.org/search/cs?searchtype=author&query=Sachan%2C+D), [Prashant Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+P), [Brian Thompson](https://arxiv.org/search/cs?searchtype=author&query=Thompson%2C+B), [Marcello Federico](https://arxiv.org/search/cs?searchtype=author&query=Federico%2C+M)

> We explore zero-shot adaptation, where a general-domain model has access to customer or domain specific parallel data at inference time, but not during training. We build on the idea of Retrieval Augmented Translation (RAT) where top-k in-domain fuzzy matches are found for the source sentence, and target-language translations of those fuzzy-matched sentences are provided to the translation model at inference time. We propose a novel architecture to control interactions between a source sentence and the top-k fuzzy target-language matches, and compare it to architectures from prior work. We conduct experiments in two language pairs (En-De and En-Fr) by training models on WMT data and testing them with five and seven multi-domain datasets, respectively. Our approach consistently outperforms the alternative architectures, improving BLEU across language pair, domain, and number k of fuzzy matches.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05047](https://arxiv.org/abs/2210.05047) [cs.CL]** |
|           | (or **[arXiv:2210.05047v1](https://arxiv.org/abs/2210.05047v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05047Focus to learn more |





<h2 id="2022-10-12-5">5. Checks and Strategies for Enabling Code-Switched Machine Translation
</h2>

Title: [Checks and Strategies for Enabling Code-Switched Machine Translation](https://arxiv.org/abs/2210.05096)

Authors: [Thamme Gowda](https://arxiv.org/search/cs?searchtype=author&query=Gowda%2C+T), [Mozhdeh Gheini](https://arxiv.org/search/cs?searchtype=author&query=Gheini%2C+M), [Jonathan May](https://arxiv.org/search/cs?searchtype=author&query=May%2C+J)

> Code-switching is a common phenomenon among multilingual speakers, where alternation between two or more languages occurs within the context of a single conversation. While multilingual humans can seamlessly switch back and forth between languages, multilingual neural machine translation (NMT) models are not robust to such sudden changes in input. This work explores multilingual NMT models' ability to handle code-switched text. First, we propose checks to measure switching capability. Second, we investigate simple and effective data augmentation methods that can enhance an NMT model's ability to support code-switching. Finally, by using a glass-box analysis of attention modules, we demonstrate the effectiveness of these methods in improving robustness.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Computers and Society (cs.CY) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05096](https://arxiv.org/abs/2210.05096) [cs.CL]** |
|           | (or **[arXiv:2210.05096v1](https://arxiv.org/abs/2210.05096v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05096Focus to learn more |





<h2 id="2022-10-12-6">6. Mixture of Attention Heads: Selecting Attention Heads Per Token
</h2>

Title: [Mixture of Attention Heads: Selecting Attention Heads Per Token](https://arxiv.org/abs/2210.05144)

Authors: [Xiaofeng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X), [Yikang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y), [Zeyu Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J), [Wenge Rong](https://arxiv.org/search/cs?searchtype=author&query=Rong%2C+W), [Zhang Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+Z)

> Mixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However, the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads (MoA), a new architecture that combines multi-head attention with the MoE mechanism. MoA includes a set of attention heads that each has its own set of parameters. Given an input, a router dynamically selects a subset of k attention heads per token. This conditional computation schema allows MoA to achieve stronger performance than the standard multi-head attention layer. Furthermore, the sparsely gated MoA can easily scale up the number of attention heads and the number of parameters while preserving computational efficiency. In addition to the performance improvements, MoA also automatically differentiates heads' utilities, providing a new perspective to discuss the model's interpretability. We conducted experiments on several important tasks, including Machine Translation and Masked Language Modeling. Experiments have shown promising results on several tasks against strong baselines that involve large and very deep models.

| Comments: | accepted in EMNLP 2022                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.05144](https://arxiv.org/abs/2210.05144) [cs.CL]** |
|           | (or **[arXiv:2210.05144v1](https://arxiv.org/abs/2210.05144v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05144Focus to learn more |





<h2 id="2022-10-12-7">7. Understanding the Failure of Batch Normalization for Transformers in NLP
</h2>

Title: [Understanding the Failure of Batch Normalization for Transformers in NLP](https://arxiv.org/abs/2210.05153)

Authors: [Jiaxi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Ji Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+J), [Lei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L)

> Batch Normalization (BN) is a core and prevalent technique in accelerating the training of deep neural networks and improving the generalization on Computer Vision (CV) tasks. However, it fails to defend its position in Natural Language Processing (NLP), which is dominated by Layer Normalization (LN). In this paper, we are trying to answer why BN usually performs worse than LN in NLP tasks with Transformer models. We find that the inconsistency between training and inference of BN is the leading cause that results in the failure of BN in NLP. We define Training Inference Discrepancy (TID) to quantitatively measure this inconsistency and reveal that TID can indicate BN's performance, supported by extensive experiments, including image classification, neural machine translation, language modeling, sequence labeling, and text classification tasks. We find that BN can obtain much better test performance than LN when TID keeps small through training. To suppress the explosion of TID, we propose Regularized BN (RBN) that adds a simple regularization term to narrow the gap between batch statistics and population statistics of BN. RBN improves the performance of BN consistently and outperforms or is on par with LN on 17 out of 20 settings, involving ten datasets and two common variants of Transformer\footnote{Our code is available at \url{[this https URL](https://github.com/wjxts/RegularizedBN)}}.

| Comments: | 17 pages, 11 figures, NeurIPS 2022                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.05153](https://arxiv.org/abs/2210.05153) [cs.CL]** |
|           | (or **[arXiv:2210.05153v1](https://arxiv.org/abs/2210.05153v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05153Focus to learn more |





<h2 id="2022-10-12-8">8. Can Language Models Be Specific? How?
</h2>

Title: [Can Language Models Be Specific? How?](https://arxiv.org/abs/2210.05159)

Authors: [Jie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J), [Kevin Chen-Chuan Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K+C), [Jinjun Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+J), [Wen-mei Hwu](https://arxiv.org/search/cs?searchtype=author&query=Hwu%2C+W)

> A good speaker not only needs to be correct, but also has the ability to be specific when desired, and so are language models. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance, given ``J. K. Rowling was born in [MASK].'', we want to test whether a more specific answer will be better filled in by PLMs, e.g., Yate instead of England. From our evaluations, we show that existing PLMs have only a slight preference for more specific answers. We identify underlying factors affecting the specificity and design two prompt-based methods to improve the specificity. Results show that the specificity of the models can be improved by the proposed methods without additional training. We believe this work can provide new insights for language modeling and encourage the research community to further explore this important but understudied problem.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05159](https://arxiv.org/abs/2210.05159) [cs.CL]** |
|           | (or **[arXiv:2210.05159v1](https://arxiv.org/abs/2210.05159v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05159Focus to learn more |





<h2 id="2022-10-12-9">9. Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation
</h2>

Title: [Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation](https://arxiv.org/abs/2210.05193)

Authors: [Chenze Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+C), [Zhengrui Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Z), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y)

> Non-autoregressive models achieve significant decoding speedup in neural machine translation but lack the ability to capture sequential dependency. Directed Acyclic Transformer (DA-Transformer) was recently proposed to model sequential dependency with a directed acyclic graph. Consequently, it has to apply a sequential decision process at inference time, which harms the global translation accuracy. In this paper, we present a Viterbi decoding framework for DA-Transformer, which guarantees to find the joint optimal solution for the translation and decoding path under any length constraint. Experimental results demonstrate that our approach consistently improves the performance of DA-Transformer while maintaining a similar decoding speedup.

| Comments: | Findings of EMNLP 2022                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.05193](https://arxiv.org/abs/2210.05193) [cs.CL]** |
|           | (or **[arXiv:2210.05193v1](https://arxiv.org/abs/2210.05193v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05193Focus to learn more |





<h2 id="2022-10-12-10">10. CTC Alignments Improve Autoregressive Translation
</h2>

Title: [CTC Alignments Improve Autoregressive Translation](https://arxiv.org/abs/2210.05200)

Authors: [Brian Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+B), [Siddharth Dalmia](https://arxiv.org/search/cs?searchtype=author&query=Dalmia%2C+S), [Yosuke Higuchi](https://arxiv.org/search/cs?searchtype=author&query=Higuchi%2C+Y), [Graham Neubig](https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G), [Florian Metze](https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F), [Alan W Black](https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W), [Shinji Watanabe](https://arxiv.org/search/cs?searchtype=author&query=Watanabe%2C+S)

> Connectionist Temporal Classification (CTC) is a widely used approach for automatic speech recognition (ASR) that performs conditionally independent monotonic alignment. However for translation, CTC exhibits clear limitations due to the contextual and non-monotonic nature of the task and thus lags behind attentional decoder approaches in terms of translation quality. In this work, we argue that CTC does in fact make sense for translation if applied in a joint CTC/attention framework wherein CTC's core properties can counteract several key weaknesses of pure-attention models during training and decoding. To validate this conjecture, we modify the Hybrid CTC/Attention model originally proposed for ASR to support text-to-text translation (MT) and speech-to-text translation (ST). Our proposed joint CTC/attention models outperform pure-attention baselines across six benchmark translation tasks.

| Subjects: | **Computation and Language (cs.CL)**; Sound (cs.SD); Audio and Speech Processing (eess.AS) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05200](https://arxiv.org/abs/2210.05200) [cs.CL]** |
|           | (or **[arXiv:2210.05200v1](https://arxiv.org/abs/2210.05200v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05200Focus to learn more |





<h2 id="2022-10-12-11">11. Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting
</h2>

Title: [Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting](https://arxiv.org/abs/2210.05404)

Authors: [Zifan Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z), [Amit Moryossef](https://arxiv.org/search/cs?searchtype=author&query=Moryossef%2C+A), [Mathias Müller](https://arxiv.org/search/cs?searchtype=author&query=Müller%2C+M), [Sarah Ebling](https://arxiv.org/search/cs?searchtype=author&query=Ebling%2C+S)

> This paper presents work on novel machine translation (MT) systems between spoken and signed languages, where signed languages are represented in SignWriting, a sign language writing system. Our work seeks to address the lack of out-of-the-box support for signed languages in current MT systems and is based on the SignBank dataset, which contains pairs of spoken language text and SignWriting content. We introduce novel methods to parse, factorize, decode, and evaluate SignWriting, leveraging ideas from neural factored MT. In a bilingual setup--translating from American Sign Language to (American) English--our method achieves over 30 BLEU, while in two multilingual setups--translating in both directions between spoken languages and signed languages--we achieve over 20 BLEU. We find that common MT techniques used to improve spoken language translation similarly affect the performance of sign language translation. These findings validate our use of an intermediate text representation for signed languages to include them in natural language processing research.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05404](https://arxiv.org/abs/2210.05404) [cs.CL]** |
|           | (or **[arXiv:2210.05404v1](https://arxiv.org/abs/2210.05404v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05404Focus to learn more |





<h2 id="2022-10-12-12">12. Like a bilingual baby: The advantage of visually grounding a bilingual language model
</h2>

Title: [Like a bilingual baby: The advantage of visually grounding a bilingual language model](https://arxiv.org/abs/2210.05487)

Authors: [Khai-Nguyen Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+K), [Zixin Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Z), [Ankur Mali](https://arxiv.org/search/cs?searchtype=author&query=Mali%2C+A), [Alex Kelly](https://arxiv.org/search/cs?searchtype=author&query=Kelly%2C+A)

> Unlike most neural language models, humans learn language in a rich, multi-sensory and, often, multi-lingual environment. Current language models typically fail to fully capture the complexities of multilingual language use. We train an LSTM language model on images and captions in English and Spanish from MS-COCO-ES. We find that the visual grounding improves the model's understanding of semantic similarity both within and across languages and improves perplexity. However, we find no significant advantage of visual grounding for abstract words. Our results provide additional evidence of the advantages of visually grounded language models and point to the need for more naturalistic language data from multilingual speakers and multilingual datasets with perceptual grounding.

| Comments: | Preprint, 7 pages, 2 tables, 1 figure                        |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.05487](https://arxiv.org/abs/2210.05487) [cs.CL]** |
|           | (or **[arXiv:2210.05487v1](https://arxiv.org/abs/2210.05487v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05487Focus to learn more |





<h2 id="2022-10-12-13">13. Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems
</h2>

Title: [Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems](https://arxiv.org/abs/2210.05528)

Authors: [Neeraj Varshney](https://arxiv.org/search/cs?searchtype=author&query=Varshney%2C+N), [Chitta Baral](https://arxiv.org/search/cs?searchtype=author&query=Baral%2C+C)

> Do all instances need inference through the big models for a correct prediction? Perhaps not; some instances are easy and can be answered correctly by even small capacity models. This provides opportunities for improving the computational efficiency of systems. In this work, we present an explorative study on 'model cascading', a simple technique that utilizes a collection of models of varying capacities to accurately yet efficiently output predictions. Through comprehensive experiments in multiple task settings that differ in the number of models available for cascading (K value), we show that cascading improves both the computational efficiency and the prediction accuracy. For instance, in K=3 setting, cascading saves up to 88.93% computation cost and consistently achieves superior prediction accuracy with an improvement of up to 2.18%. We also study the impact of introducing additional models in the cascade and show that it further increases the efficiency improvements. Finally, we hope that our work will facilitate development of efficient NLP systems making their widespread adoption in real-world applications possible.

| Comments: | EMNLP 2022                                                   |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| Cite as:  | **[arXiv:2210.05528](https://arxiv.org/abs/2210.05528) [cs.CL]** |
|           | (or **[arXiv:2210.05528v1](https://arxiv.org/abs/2210.05528v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05528Focus to learn more |





<h2 id="2022-10-12-14">14. Enriching Biomedical Knowledge for Low-resource Language Through Translation
</h2>

Title: [Enriching Biomedical Knowledge for Low-resource Language Through Translation](https://arxiv.org/abs/2210.05598)

Authors: [Long Phan](https://arxiv.org/search/cs?searchtype=author&query=Phan%2C+L), [Tai Dang](https://arxiv.org/search/cs?searchtype=author&query=Dang%2C+T), [Hieu Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+H), [Vy Phan](https://arxiv.org/search/cs?searchtype=author&query=Phan%2C+V), [Lam D. Chau](https://arxiv.org/search/cs?searchtype=author&query=Chau%2C+L+D), [Trieu H. Trinh](https://arxiv.org/search/cs?searchtype=author&query=Trinh%2C+T+H)

> Biomedical data and benchmarks are highly valuable yet very limited in low-resource languages other than English such as Vietnamese. In this paper, we make use of a state-of-the-art translation model in English-Vietnamese to translate and produce both pretrained as well as supervised data in the biomedical domains. Thanks to such large-scale translation, we introduce ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20 million translated abstracts from the high-quality public PubMed corpus. ViPubMedT5 demonstrates state-of-the-art results on two different biomedical benchmarks in summarization and acronym disambiguation. Further, we release ViMedNLI - a new NLP task in Vietnamese translated from MedNLI using the recently public En-vi translation model and carefully refined by human experts, with evaluations of existing methods against ViPubmedT5.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05598](https://arxiv.org/abs/2210.05598) [cs.CL]** |
|           | (or **[arXiv:2210.05598v1](https://arxiv.org/abs/2210.05598v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05598Focus to learn more |





<h2 id="2022-10-12-15">15. MTet: Multi-domain Translation for English and Vietnamese
</h2>

Title: [MTet: Multi-domain Translation for English and Vietnamese](https://arxiv.org/abs/2210.05610)

Authors: [Chinh Ngo](https://arxiv.org/search/cs?searchtype=author&query=Ngo%2C+C), [Trieu H. Trinh](https://arxiv.org/search/cs?searchtype=author&query=Trinh%2C+T+H), [Long Phan](https://arxiv.org/search/cs?searchtype=author&query=Phan%2C+L), [Hieu Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+H), [Tai Dang](https://arxiv.org/search/cs?searchtype=author&query=Dang%2C+T), [Hieu Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+H), [Minh Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+M), [Minh-Thang Luong](https://arxiv.org/search/cs?searchtype=author&query=Luong%2C+M)

> We introduce MTet, the largest publicly available parallel corpus for English-Vietnamese translation. MTet consists of 4.2M high-quality training sentence pairs and a multi-domain test set refined by the Vietnamese research community. Combining with previous works on English-Vietnamese translation, we grow the existing parallel dataset to 6.2M sentence pairs. We also release the first pretrained model EnViT5 for English and Vietnamese languages. Combining both resources, our model significantly outperforms previous state-of-the-art results by up to 2 points in translation BLEU score, while being 1.6 times smaller.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05610](https://arxiv.org/abs/2210.05610) [cs.CL]** |
|           | (or **[arXiv:2210.05610v1](https://arxiv.org/abs/2210.05610v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05610Focus to learn more |





<h2 id="2022-10-12-16">16. Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models
</h2>

Title: [Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models](https://arxiv.org/abs/2210.05619)

Authors: [Isabel Papadimitriou](https://arxiv.org/search/cs?searchtype=author&query=Papadimitriou%2C+I), [Kezia Lopez](https://arxiv.org/search/cs?searchtype=author&query=Lopez%2C+K), [Dan Jurafsky](https://arxiv.org/search/cs?searchtype=author&query=Jurafsky%2C+D)

> While multilingual language models can improve NLP performance on low-resource languages by leveraging higher-resource languages, they also reduce average performance on all languages (the 'curse of multilinguality'). Here we show another problem with multilingual models: grammatical structures in higher-resource languages bleed into lower-resource languages, a phenomenon we call grammatical structure bias. We show this bias via a novel method for comparing the fluency of multilingual models to the fluency of monolingual Spanish and Greek models: testing their preference for two carefully-chosen variable grammatical structures (optional pronoun-drop in Spanish and optional Subject-Verb ordering in Greek). We find that multilingual BERT is biased toward the English-like setting (explicit pronouns and Subject-Verb-Object ordering) as compared to our monolingual control. With our case studies, we hope to bring to light the fine-grained ways in which dominant languages can affect and bias multilingual performance, and encourage more linguistically-aware fluency evaluation.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.05619](https://arxiv.org/abs/2210.05619) [cs.CL]** |
|           | (or **[arXiv:2210.05619v1](https://arxiv.org/abs/2210.05619v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.05619Focus to learn more |






# 2022-10-11

[Return to Index](#Index)



<h2 id="2022-10-11-1">1. AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models
</h2>

Title: [AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models](https://arxiv.org/abs/2210.03858)

Authors: [Se Jung Kwon](https://arxiv.org/search/cs?searchtype=author&query=Kwon%2C+S+J), [Jeonghoon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J), [Jeongin Bae](https://arxiv.org/search/cs?searchtype=author&query=Bae%2C+J), [Kang Min Yoo](https://arxiv.org/search/cs?searchtype=author&query=Yoo%2C+K+M), [Jin-Hwa Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J), [Baeseong Park](https://arxiv.org/search/cs?searchtype=author&query=Park%2C+B), [Byeongwook Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+B), [Jung-Woo Ha](https://arxiv.org/search/cs?searchtype=author&query=Ha%2C+J), [Nako Sung](https://arxiv.org/search/cs?searchtype=author&query=Sung%2C+N), [Dongsoo Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+D)

> There are growing interests in adapting large-scale language models using parameter-efficient fine-tuning methods. However, accelerating the model itself and achieving better inference efficiency through model compression has not been thoroughly explored yet. Model compression could provide the benefits of reducing memory footprints, enabling low-precision computations, and ultimately achieving cost-effective inference. To combine parameter-efficient adaptation and model compression, we propose AlphaTuning consisting of post-training quantization of the pre-trained language model and fine-tuning only some parts of quantized parameters for a target task. Specifically, AlphaTuning works by employing binary-coding quantization, which factorizes the full-precision parameters into binary parameters and a separate set of scaling factors. During the adaptation phase, the binary values are frozen for all tasks, while the scaling factors are fine-tuned for the downstream task. We demonstrate that AlphaTuning, when applied to GPT-2 and OPT, performs competitively with full fine-tuning on a variety of downstream tasks while achieving >10x compression ratio under 4-bit quantization and >1,000x reduction in the number of trainable parameters.

| Comments: | Findings of EMNLP 2022                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Machine Learning (cs.LG)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2210.03858](https://arxiv.org/abs/2210.03858) [cs.LG]** |
|           | (or **[arXiv:2210.03858v1](https://arxiv.org/abs/2210.03858v1) [cs.LG]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03858Focus to learn more |





<h2 id="2022-10-11-2">2. MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning
</h2>

Title: [MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning](https://arxiv.org/abs/2210.04183)

Authors: [Zijia Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z), [Longteng Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+L), [Xingjian He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X), [Shuai Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+S), [Zehuan Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+Z), [Jing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)

> Multimodal representation learning has shown promising improvements on various vision-language tasks. Most existing methods excel at building global-level alignment between vision and language while lacking effective fine-grained image-text interaction. In this paper, we propose a jointly masked multimodal modeling method to learn fine-grained multimodal representations. Our method performs joint masking on image-text input and integrates both implicit and explicit targets for the masked signals to recover. The implicit target provides a unified and debiased objective for vision and language, where the model predicts latent multimodal representations of the unmasked input. The explicit target further enriches the multimodal representations by recovering high-level and semantically meaningful information: momentum visual features of image patches and concepts of word tokens. Through such a masked modeling process, our model not only learns fine-grained multimodal interaction, but also avoids the semantic gap between high-level representations and low- or mid-level prediction targets (e.g. image pixels), thus producing semantically rich multimodal representations that perform well on both zero-shot and fine-tuned settings. Our pre-trained model (named MAMO) achieves state-of-the-art performance on various downstream vision-language tasks, including image-text retrieval, visual question answering, visual reasoning, and weakly-supervised visual grounding.

| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.04183](https://arxiv.org/abs/2210.04183) [cs.CV]** |
|           | (or **[arXiv:2210.04183v1](https://arxiv.org/abs/2210.04183v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.04183Focus to learn more |





<h2 id="2022-10-11-3">3. What the DAAM: Interpreting Stable Diffusion Using Cross Attention
</h2>

Title: [What the DAAM: Interpreting Stable Diffusion Using Cross Attention](https://arxiv.org/abs/2210.04885)

Authors: [Raphael Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+R), [Akshat Pandey](https://arxiv.org/search/cs?searchtype=author&query=Pandey%2C+A), [Zhiying Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z), [Gefei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+G), [Karun Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+K), [Jimmy Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J), [Ferhan Ture](https://arxiv.org/search/cs?searchtype=author&query=Ture%2C+F)

> Large-scale diffusion neural networks represent a substantial milestone in text-to-image generation, with some performing similar to real photographs in human evaluation. However, they remain poorly understood, lacking explainability and interpretability analyses, largely due to their proprietary, closed-source nature. In this paper, to shine some much-needed light on text-to-image diffusion models, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced large diffusion model. To produce pixel-level attribution maps, we propose DAAM, a novel method based on upscaling and aggregating cross-attention activations in the latent denoising subnetwork. We support its correctness by evaluating its unsupervised instance segmentation quality on its own generated imagery, compared to supervised segmentation models. We show that DAAM performs strongly on COCO caption-generated images, achieving an average precision (AP) of 61.0, and it outperforms supervised models on full-vocabulary segmentation, for an AP of 51.5. We further find that certain parts of speech, like punctuation and conjunctions, influence the generated imagery most, which agrees with the prior literature, while determiners and numerals the least, suggesting poor numeracy. To our knowledge, we are the first to propose and study word--pixel attribution for large-scale text-to-image diffusion models. Our code and data are at [this https URL](https://github.com/castorini/daam)

| Comments: | 5 pages, 5 figures                                           |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computer Vision and Pattern Recognition (cs.CV)**; Computation and Language (cs.CL) |
| Cite as:  | **[arXiv:2210.04885](https://arxiv.org/abs/2210.04885) [cs.CV]** |
|           | (or **[arXiv:2210.04885v1](https://arxiv.org/abs/2210.04885v1) [cs.CV]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.04885Focus to learn more |





<h2 id="2022-10-11-4">4. Visualize Before You Write: Imagination-Guided Open-Ended Text Generation
</h2>

Title: [Visualize Before You Write: Imagination-Guided Open-Ended Text Generation](https://arxiv.org/abs/2210.03765)

Authors: [Wanrong Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+W), [An Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+A), [Yujie Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y), [Wenda Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W), [Xin Eric Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X+E), [Miguel Eckstein](https://arxiv.org/search/cs?searchtype=author&query=Eckstein%2C+M), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y)

> Recent advances in text-to-image synthesis make it possible to visualize machine imaginations for a given context. On the other hand, when generating text, human writers are gifted at creative visualization, which enhances their writings by forming imaginations as blueprints before putting down the stories in words. Inspired by such a cognitive process, we ask the natural question of whether we can endow machines with the same ability to utilize visual information and construct a general picture of the context to guide text generation. In this work, we propose iNLG that uses machine-generated images to guide language models (LM) in open-ended text generation. The experiments and analyses demonstrate the effectiveness of iNLG on open-ended text generation tasks, including text completion, story generation, and concept-to-text generation in few-shot scenarios. Both automatic metrics and human evaluations verify that the text snippets generated by our iNLG are coherent and informative while displaying minor degeneration.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.03765](https://arxiv.org/abs/2210.03765) [cs.CL]** |
|           | (or **[arXiv:2210.03765v1](https://arxiv.org/abs/2210.03765v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03765Focus to learn more |





<h2 id="2022-10-11-5">5. Breaking BERT: Evaluating and Optimizing Sparsified Attention
</h2>

Title: [Breaking BERT: Evaluating and Optimizing Sparsified Attention](https://arxiv.org/abs/2210.03841)

Authors: [Siddhartha Brahma](https://arxiv.org/search/cs?searchtype=author&query=Brahma%2C+S), [Polina Zablotskaia](https://arxiv.org/search/cs?searchtype=author&query=Zablotskaia%2C+P), [David Mimno](https://arxiv.org/search/cs?searchtype=author&query=Mimno%2C+D)

> Transformers allow attention between all pairs of tokens, but there is reason to believe that most of these connections - and their quadratic time and memory - may not be necessary. But which ones? We evaluate the impact of sparsification patterns with a series of ablation experiments. First, we compare masks based on syntax, lexical similarity, and token position to random connections, and measure which patterns reduce performance the least. We find that on three common finetuning tasks even using attention that is at least 78% sparse can have little effect on performance if applied at later transformer layers, but that applying sparsity throughout the network reduces performance significantly. Second, we vary the degree of sparsity for three patterns supported by previous work, and find that connections to neighbouring tokens are the most significant. Finally, we treat sparsity as an optimizable parameter, and present an algorithm to learn degrees of neighboring connections that gives a fine-grained control over the accuracy-sparsity trade-off while approaching the performance of existing methods.

| Comments: | Shorter version accepted to SNN2021 workshop                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.03841](https://arxiv.org/abs/2210.03841) [cs.CL]** |
|           | (or **[arXiv:2210.03841v1](https://arxiv.org/abs/2210.03841v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03841Focus to learn more |





<h2 id="2022-10-11-6">6. Improving End-to-End Text Image Translation From the Auxiliary Text Translation Task
</h2>

Title: [Improving End-to-End Text Image Translation From the Auxiliary Text Translation Task](https://arxiv.org/abs/2210.03887)

Authors: [Cong Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+C), [Yaping Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y), [Mei Tu](https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+M), [Xu Han](https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X), [Linghui Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L), [Yang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y), [Yu Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y)

> End-to-end text image translation (TIT), which aims at translating the source language embedded in images to the target language, has attracted intensive attention in recent research. However, data sparsity limits the performance of end-to-end text image translation. Multi-task learning is a non-trivial way to alleviate this problem via exploring knowledge from complementary related tasks. In this paper, we propose a novel text translation enhanced text image translation, which trains the end-to-end model with text translation as an auxiliary task. By sharing model parameters and multi-task training, our model is able to take full advantage of easily-available large-scale text parallel corpus. Extensive experimental results show our proposed method outperforms existing end-to-end methods, and the joint multi-task learning with both text translation and recognition tasks achieves better results, proving translation and recognition auxiliary tasks are complementary.

| Comments: | Accepted at the 26TH International Conference on Pattern Recognition (ICPR 2022) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.03887](https://arxiv.org/abs/2210.03887) [cs.CL]** |
|           | (or **[arXiv:2210.03887v1](https://arxiv.org/abs/2210.03887v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03887Focus to learn more |





<h2 id="2022-10-11-7">7. Detecting Label Errors in Token Classification Data
</h2>

Title: [Detecting Label Errors in Token Classification Data](https://arxiv.org/abs/2210.03920)

Authors: [Wei-Chen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W), [Jonas Mueller](https://arxiv.org/search/cs?searchtype=author&query=Mueller%2C+J)

> Mislabeled examples are a common issue in real-world data, particularly for tasks like token classification where many labels must be chosen on a fine-grained basis. Here we consider the task of finding sentences that contain label errors in token classification datasets. We study 11 different straightforward methods that score tokens/sentences based on the predicted class probabilities output by a (any) token classification model (trained via any procedure). In precision-recall evaluations based on real-world label errors in entity recognition data from CoNLL-2003, we identify a simple and effective method that consistently detects those sentences containing label errors when applied with different token classification models.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.03920](https://arxiv.org/abs/2210.03920) [cs.CL]** |
|           | (or **[arXiv:2210.03920v1](https://arxiv.org/abs/2210.03920v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03920Focus to learn more |





<h2 id="2022-10-11-8">8. Sparse Teachers Can Be Dense with Knowledge
</h2>

Title: [Sparse Teachers Can Be Dense with Knowledge](https://arxiv.org/abs/2210.03923)

Authors: [Yi Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Chen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Dawei Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D)

> Recent advances in distilling pretrained language models have discovered that, besides the expressiveness of knowledge, the student-friendliness should be taken into consideration to realize a truly knowledgable teacher. Based on a pilot study, we find that over-parameterized teachers can produce expressive yet student-unfriendly knowledge, and are thus limited in overall knowledgableness. To remove the parameters that result in student-unfriendliness, we propose a sparse teacher trick under the guidance of an overall knowledgable score for each teacher parameter. The knowledgable score is essentially an interpolation of the expressiveness and student-friendliness scores. The aim is to ensure that the expressive parameters are retained while the student-unfriendly ones are removed. Extensive experiments on the GLUE benchmark show that the proposed sparse teachers can be dense with knowledge and lead to students with compelling performance in comparison with a series of competitive baselines.

| Comments: | 12 pages, 8 figures, 6 tables, accepted to EMNLP 2022. Code is available at [this https URL](https://github.com/GeneZC/StarK) |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Machine Learning (cs.LG) |
| Cite as:  | **[arXiv:2210.03923](https://arxiv.org/abs/2210.03923) [cs.CL]** |
|           | (or **[arXiv:2210.03923v1](https://arxiv.org/abs/2210.03923v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03923Focus to learn more |





<h2 id="2022-10-11-9">9. Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation
</h2>

Title: [Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation](https://arxiv.org/abs/2210.03953)

Authors: [Chenze Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+C), [Yang Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y)

> Non-autoregressive translation (NAT) models are typically trained with the cross-entropy loss, which forces the model outputs to be aligned verbatim with the target sentence and will highly penalize small shifts in word positions. Latent alignment models relax the explicit alignment by marginalizing out all monotonic latent alignments with the CTC loss. However, they cannot handle non-monotonic alignments, which is non-negligible as there is typically global word reordering in machine translation. In this work, we explore non-monotonic latent alignments for NAT. We extend the alignment space to non-monotonic alignments to allow for the global word reordering and further consider all alignments that overlap with the target sentence. We non-monotonically match the alignments to the target sentence and train the latent alignment model to maximize the F1 score of non-monotonic matching. Extensive experiments on major WMT benchmarks show that our method substantially improves the translation performance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14 En-De with only one-iteration decoding, closing the gap between non-autoregressive and autoregressive models.

| Comments: | NeurIPS 2022                                                 |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.03953](https://arxiv.org/abs/2210.03953) [cs.CL]** |
|           | (or **[arXiv:2210.03953v1](https://arxiv.org/abs/2210.03953v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03953Focus to learn more |





<h2 id="2022-10-11-10">10. SDA: Simple Discrete Augmentation for Contrastive Sentence Representation Learning
</h2>

Title: [SDA: Simple Discrete Augmentation for Contrastive Sentence Representation Learning](https://arxiv.org/abs/2210.03963)

Authors: [Zhenyu Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+Z), [Dongsheng Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+D), [Jinghui Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+J), [Rui Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+R), [Fei Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+F)

> Contrastive learning methods achieve state-of-the-art results in unsupervised sentence representation learning. Although playing essential roles in contrastive learning, data augmentation methods applied on sentences have not been fully explored. Current SOTA method SimCSE utilizes a simple dropout mechanism as continuous augmentation which outperforms discrete augmentations such as cropping, word deletion and synonym replacement. To understand the underlying rationales, we revisit existing approaches and attempt to hypothesize the desiderata of reasonable data augmentation methods: balance of semantic consistency and expression diversity. Based on the hypothesis, we propose three simple yet effective discrete sentence augmentation methods, i.e., punctuation insertion, affirmative auxiliary and double negation. The punctuation marks, auxiliaries and negative words act as minimal noises in lexical level to produce diverse sentence expressions. Unlike traditional augmentation methods which randomly modify the sentence, our augmentation rules are well designed for generating semantically consistent and grammatically correct sentences. We conduct extensive experiments on both English and Chinese semantic textual similarity datasets. The results show the robustness and effectiveness of the proposed methods.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.03963](https://arxiv.org/abs/2210.03963) [cs.CL]** |
|           | (or **[arXiv:2210.03963v1](https://arxiv.org/abs/2210.03963v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03963Focus to learn more |





<h2 id="2022-10-11-11">11. KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text Classification
</h2>

Title: [KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text Classification](https://arxiv.org/abs/2210.03970)

Authors: [Yong He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+Y), [Cheng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C), [Shun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S), [Nan Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+N), [Zhaorong Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z), [Zhenyu Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+Z)

> Medical text learning has recently emerged as a promising area to improve healthcare due to the wide adoption of electronic health record (EHR) systems. The complexity of the medical text such as diverse length, mixed text types, and full of medical jargon, poses a great challenge for developing effective deep learning models. BERT has presented state-of-the-art results in many NLP tasks, such as text classification and question answering. However, the standalone BERT model cannot deal with the complexity of the medical text, especially the lengthy clinical notes. Herein, we develop a new model called KG-MTT-BERT (Knowledge Graph Enhanced Multi-Type Text BERT) by extending the BERT model for long and multi-type text with the integration of the medical knowledge graph. Our model can outperform all baselines and other state-of-the-art models in diagnosis-related group (DRG) classification, which requires comprehensive medical text for accurate classification. We also demonstrated that our model can effectively handle multi-type text and the integration of medical knowledge graph can significantly improve the performance.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.03970](https://arxiv.org/abs/2210.03970) [cs.CL]** |
|           | (or **[arXiv:2210.03970v1](https://arxiv.org/abs/2210.03970v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03970Focus to learn more |





<h2 id="2022-10-11-12">12. Cross-Align: Modeling Deep Cross-lingual Interactions for Word Alignment
</h2>

Title: [Cross-Align: Modeling Deep Cross-lingual Interactions for Word Alignment](https://arxiv.org/abs/2210.04141)

Authors: [Siyu Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+S), [Zhen Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Fandong Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F), [Yufeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Jinan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J)

> Word alignment which aims to extract lexicon translation equivalents between source and target sentences, serves as a fundamental tool for natural language processing. Recent studies in this area have yielded substantial improvements by generating alignments from contextualized embeddings of the pre-trained multilingual language models. However, we find that the existing approaches capture few interactions between the input sentence pairs, which degrades the word alignment quality severely, especially for the ambiguous words in the monolingual context. To remedy this problem, we propose Cross-Align to model deep interactions between the input sentence pairs, in which the source and target sentences are encoded separately with the shared self-attention modules in the shallow layers, while cross-lingual interactions are explicitly constructed by the cross-attention modules in the upper layers. Besides, to train our model effectively, we propose a two-stage training framework, where the model is trained with a simple Translation Language Modeling (TLM) objective in the first stage and then finetuned with a self-supervised alignment objective in the second stage. Experiments show that the proposed Cross-Align achieves the state-of-the-art (SOTA) performance on four out of five language pairs.

| Comments: | Accepted by EMNLP 2022                                       |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.04141](https://arxiv.org/abs/2210.04141) [cs.CL]** |
|           | (or **[arXiv:2210.04141v1](https://arxiv.org/abs/2210.04141v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.04141Focus to learn more |





<h2 id="2022-10-11-13">13. SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters
</h2>

Title: [SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters](https://arxiv.org/abs/2210.04284)

Authors: [Shwai He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+S), [Liang Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L), [Daize Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+D), [Miao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Dacheng Tao](https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+D)

> Adapter Tuning, which freezes the pretrained language models (PLMs) and only fine-tunes a few extra modules, becomes an appealing efficient alternative to the full model fine-tuning. Although computationally efficient, the recent Adapters often increase parameters (e.g. bottleneck dimension) for matching the performance of full model fine-tuning, which we argue goes against their original intention. In this work, we re-examine the parameter-efficiency of Adapters through the lens of network pruning (we name such plug-in concept as \texttt{SparseAdapter}) and find that SparseAdapter can achieve comparable or better performance than standard Adapters when the sparse ratio reaches up to 80\%. Based on our findings, we introduce an easy but effective setting ``\textit{Large-Sparse}'' to improve the model capacity of Adapters under the same parameter budget. Experiments on five competitive Adapters upon three advanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g. 40\%) SparseAdapter can consistently outperform their corresponding counterpart. Encouragingly, with the \textit{Large-Sparse} setting, we can obtain further appealing gains, even outperforming the full fine-tuning by a large margin. Our code will be released at: \url{[this https URL](https://github.com/Shwai-He/SparseAdapter)}.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.04284](https://arxiv.org/abs/2210.04284) [cs.CL]** |
|           | (or **[arXiv:2210.04284v1](https://arxiv.org/abs/2210.04284v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.04284Focus to learn more |





<h2 id="2022-10-11-14">14. Parameter-Efficient Tuning with Special Token Adaptation
</h2>

Title: [Parameter-Efficient Tuning with Special Token Adaptation](https://arxiv.org/abs/2210.04382)

Authors: [Xiaoocong Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+X), [James Y. Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J+Y), [Wenxuan Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W), [Muhao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M)

> Parameter-efficient tuning aims at updating only a small subset of parameters when adapting a pretrained model to downstream tasks. In this work, we introduce PASTA, in which we only modify the special token representations (e.g., [SEP] and [CLS] in BERT) before the self-attention module at each layer in Transformer-based models. PASTA achieves comparable performance to fine-tuning in natural language understanding tasks including text classification and NER with up to only 0.029% of total parameters trained. Our work not only provides a simple yet effective way of parameter-efficient tuning, which has a wide range of practical applications when deploying finetuned models for multiple tasks, but also demonstrates the pivotal role of special tokens in pretrained language models.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.04382](https://arxiv.org/abs/2210.04382) [cs.CL]** |
|           | (or **[arXiv:2210.04382v1](https://arxiv.org/abs/2210.04382v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.04382Focus to learn more |





<h2 id="2022-10-11-15">15. Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation
</h2>

Title: [Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation](https://arxiv.org/abs/2210.04468)

Authors: [Ru Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+R), [Yawen Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+Y), [Junbo Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J)

> Past works on multimodal machine translation (MMT) elevate bilingual setup by incorporating additional aligned vision information. However, an image-must requirement of the multimodal dataset largely hinders MMT's development -- namely that it demands an aligned form of [image, source text, target text]. This limitation is generally troublesome during the inference phase especially when the aligned image is not provided as in the normal NMT setup. Thus, in this work, we introduce IKD-MMT, a novel MMT framework to support the image-free inference phase via an inversion knowledge distillation scheme. In particular, a multimodal feature generator is executed with a knowledge distillation module, which directly generates the multimodal feature from (only) source texts as the input. While there have been a few prior works entertaining the possibility to support image-free inference for machine translation, their performances have yet to rival the image-must translation. In our experiments, we identify our method as the first image-free approach to comprehensively rival or even surpass (almost) all image-must frameworks, and achieved the state-of-the-art result on the often-used Multi30k benchmark. Our code and data are available at: [this https URL](https://github.com/pengr/IKD-mmt/tree/master)..

| Comments: | Long paper accepted by EMNLP2022 main conference             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**                         |
| Cite as:  | **[arXiv:2210.04468](https://arxiv.org/abs/2210.04468) [cs.CL]** |
|           | (or **[arXiv:2210.04468v1](https://arxiv.org/abs/2210.04468v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.04468Focus to learn more |





<h2 id="2022-10-11-16">16. Automatic Evaluation and Analysis of Idioms in Neural Machine Translation
</h2>

Title: [Automatic Evaluation and Analysis of Idioms in Neural Machine Translation](https://arxiv.org/abs/2210.04545)

Authors: [Christos Baziotis](https://arxiv.org/search/cs?searchtype=author&query=Baziotis%2C+C), [Prashant Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+P), [Eva Hasler](https://arxiv.org/search/cs?searchtype=author&query=Hasler%2C+E)

> A major open problem in neural machine translation (NMT) is the translation of idiomatic expressions, such as "under the weather". The meaning of these expressions is not composed by the meaning of their constituent words, and NMT models tend to translate them literally (i.e., word-by-word), which leads to confusing and nonsensical translations. Research on idioms in NMT is limited and obstructed by the absence of automatic methods for quantifying these errors. In this work, first, we propose a novel metric for automatically measuring the frequency of literal translation errors without human involvement. Equipped with this metric, we present controlled translation experiments with models trained in different conditions (with/without the test-set idioms) and across a wide range of (global and targeted) metrics and test sets. We explore the role of monolingual pretraining and find that it yields substantial targeted improvements, even without observing any translation examples of the test-set idioms. In our analysis, we probe the role of idiom context. We find that the randomly initialized models are more local or "myopic" as they are relatively unaffected by variations of the idiom context, unlike the pretrained ones.

| Subjects: | **Computation and Language (cs.CL)**                         |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.04545](https://arxiv.org/abs/2210.04545) [cs.CL]** |
|           | (or **[arXiv:2210.04545v1](https://arxiv.org/abs/2210.04545v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.04545Focus to learn more |





<h2 id="2022-10-11-17">17. A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language Processing
</h2>

Title: [A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language Processing](https://arxiv.org/abs/2210.04675)

Authors: [Sophie Henning](https://arxiv.org/search/cs?searchtype=author&query=Henning%2C+S), [William H. Beluch](https://arxiv.org/search/cs?searchtype=author&query=Beluch%2C+W+H), [Alexander Fraser](https://arxiv.org/search/cs?searchtype=author&query=Fraser%2C+A), [Annemarie Friedrich](https://arxiv.org/search/cs?searchtype=author&query=Friedrich%2C+A)

> Many natural language processing (NLP) tasks are naturally imbalanced, as some target categories occur much more frequently than others in the real world. In such scenarios, current NLP models still tend to perform poorly on less frequent classes. Addressing class imbalance in NLP is an active research topic, yet, finding a good approach for a particular task and imbalance scenario is difficult. 
> With this survey, the first overview on class imbalance in deep-learning based NLP, we provide guidance for NLP researchers and practitioners dealing with imbalanced data. We first discuss various types of controlled and real-world class imbalance. Our survey then covers approaches that have been explicitly proposed for class-imbalanced NLP tasks or, originating in the computer vision community, have been evaluated on them. We organize the methods by whether they are based on sampling, data augmentation, choice of loss function, staged learning, or model design. Finally, we discuss open problems such as dealing with multi-label scenarios, and propose systematic benchmarking and reporting in order to move forward on this problem as a community.

| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.04675](https://arxiv.org/abs/2210.04675) [cs.CL]** |
|           | (or **[arXiv:2210.04675v1](https://arxiv.org/abs/2210.04675v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.04675Focus to learn more |



# 2022-10-10

[Return to Index](#Index)



<h2 id="2022-10-10-1">1. Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding
</h2>


Title: [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347)

Authors: [Kenton Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K), [Mandar Joshi](https://arxiv.org/search/cs?searchtype=author&query=Joshi%2C+M), [Iulia Turc](https://arxiv.org/search/cs?searchtype=author&query=Turc%2C+I), [Hexiang Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+H), [Fangyu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F), [Julian Eisenschlos](https://arxiv.org/search/cs?searchtype=author&query=Eisenschlos%2C+J), [Urvashi Khandelwal](https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+U), [Peter Shaw](https://arxiv.org/search/cs?searchtype=author&query=Shaw%2C+P), [Ming-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+M), [Kristina Toutanova](https://arxiv.org/search/cs?searchtype=author&query=Toutanova%2C+K)

> Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.

| Subjects: | **Computation and Language (cs.CL)**; Computer Vision and Pattern Recognition (cs.CV) |
| --------- | ------------------------------------------------------------ |
| Cite as:  | **[arXiv:2210.03347](https://arxiv.org/abs/2210.03347) [cs.CL]** |
|           | (or **[arXiv:2210.03347v1](https://arxiv.org/abs/2210.03347v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03347Focus to learn more |





<h2 id="2022-10-10-2">2. NMTSloth: Understanding and Testing Efficiency Degradation of Neural Machine Translation Systems
</h2>


Title: [NMTSloth: Understanding and Testing Efficiency Degradation of Neural Machine Translation Systems](https://arxiv.org/abs/2210.03696)

Authors: [Simin Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S), [Cong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C), [Mirazul Haque](https://arxiv.org/search/cs?searchtype=author&query=Haque%2C+M), [Zihe Song](https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Z), [Wei Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W)

> Neural Machine Translation (NMT) systems have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of NMT systems, which is of paramount importance due to often vast translation demands and real-time requirements, has surprisingly received little attention. In this paper, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art NMT systems. By analyzing the working mechanism and implementation of 1455 public-accessible NMT systems, we observe a fundamental property in NMT systems that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that NMT systems would have to go through enough iterations to satisfy the pre-configured threshold. We present NMTSloth, which develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level, which sufficiently delays the appearance of EOS and forces these inputs to reach the naturally-unreachable threshold. To demonstrate the effectiveness of NMTSloth, we conduct a systematic evaluation on three public-available NMT systems: Google T5, AllenAI WMT14, and Helsinki-NLP translators. Experimental results show that NMTSloth can increase NMT systems' response latency and energy consumption by 85% to 3153% and 86% to 3052%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by NMTSloth significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).

| Comments: | This paper has been accepted to ESEC/FSE 2022                |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE) |
| Cite as:  | **[arXiv:2210.03696](https://arxiv.org/abs/2210.03696) [cs.CL]** |
|           | (or **[arXiv:2210.03696v1](https://arxiv.org/abs/2210.03696v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03696Focus to learn more |





<h2 id="2022-10-10-3">3. SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training
</h2>


Title: [SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training](https://arxiv.org/abs/2210.03730)

Authors: [Ziqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z), [Long Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L), [Junyi Ao](https://arxiv.org/search/cs?searchtype=author&query=Ao%2C+J), [Shujie Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Lirong Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+L), [Jinyu Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F)

> The rapid development of single-modal pre-training has prompted researchers to pay more attention to cross-modal pre-training methods. In this paper, we propose a unified-modal speech-unit-text pre-training model, SpeechUT, to connect the representations of a speech encoder and a text decoder with a shared unit encoder. Leveraging hidden-unit as an interface to align speech and text, we can decompose the speech-to-text model into a speech-to-unit model and a unit-to-text model, which can be jointly pre-trained with unpaired speech and text data respectively. Our proposed SpeechUT is fine-tuned and evaluated on automatic speech recognition (ASR) and speech translation (ST) tasks. Experimental results show that SpeechUT gets substantial improvements over strong baselines, and achieves state-of-the-art performance on both the LibriSpeech ASR and MuST-C ST tasks. To better understand the proposed SpeechUT, detailed analyses are conducted. The code and pre-trained models are available at [this https URL](https://aka.ms/SpeechUT).

| Comments: | 14 pages, accepted by EMNLP 2022                             |
| --------- | ------------------------------------------------------------ |
| Subjects: | **Computation and Language (cs.CL)**; Audio and Speech Processing (eess.AS) |
| Cite as:  | **[arXiv:2210.03730](https://arxiv.org/abs/2210.03730) [cs.CL]** |
|           | (or **[arXiv:2210.03730v1](https://arxiv.org/abs/2210.03730v1) [cs.CL]** for this version) |
|           | https://doi.org/10.48550/arXiv.2210.03730Focus to learn more |

